{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# LitSearch AI"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "A RAG (Retrieval-Augmented Generation) pipeline for searching and querying scientific papers from arXiv.\n\n## Pipeline Overview\n\n1. **Data Collection**: Fetch papers from arXiv API with metadata\n2. **PDF Parsing**: Extract text content from PDFs\n3. **Data Validation**: Quality checks on extracted text\n4. **Chunking**: Split documents into searchable chunks with metadata\n5. **Embeddings**: Create vector embeddings using OpenAI\n6. **Vector Store**: Index chunks in ChromaDB for similarity search\n7. **RAG Chain**: Combine retrieval with LLM for question answering\n8. **Evaluation**: Measure faithfulness of generated answers"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Setup"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jupyter arxiv pypdf openai langchain langchain-core langchain-openai langchain-community langchain-text-splitters chromadb python-dotenv pandas requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import arxiv\n",
    "\n",
    "from pypdf import PdfReader\n",
    "import io\n",
    "import requests\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 1. Data Collection"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Fetch Papers from arXiv"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "search = arxiv.Search(\n      query='brain tumor detection AND deep learning AND MRI images',\n      max_results=150,\n      sort_by=arxiv.SortCriterion.Relevance)\n\npapers = []\nfor result in search.results():\n    paper = {\n        'article_id': result.entry_id.split('/')[-1],\n        'title': result.title,\n        'authors': [author.name for author in result.authors],\n        'published': result.published,\n        'summary': result.summary,\n        'pdf_url': result.pdf_url\n    }\n    papers.append(paper)\n    print(f\"✓ {paper['title'][:60]}...\")\n\nprint(f\"\\n{len(papers)} papers fetched\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Parse PDFs"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing 150 PDFs...\n",
      "0/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Impossible to decode XFormObject /Im3: 'bbox'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 45 0 (offset 0)\n",
      "Ignoring wrong pointing object 58 0 (offset 0)\n",
      "Ignoring wrong pointing object 60 0 (offset 0)\n",
      "Ignoring wrong pointing object 63 0 (offset 0)\n",
      "Ignoring wrong pointing object 65 0 (offset 0)\n",
      "Ignoring wrong pointing object 68 0 (offset 0)\n",
      "Ignoring wrong pointing object 70 0 (offset 0)\n",
      "Ignoring wrong pointing object 81 0 (offset 0)\n",
      "Ignoring wrong pointing object 83 0 (offset 0)\n",
      "Ignoring wrong pointing object 107 0 (offset 0)\n",
      "Ignoring wrong pointing object 118 0 (offset 0)\n",
      "Ignoring wrong pointing object 120 0 (offset 0)\n",
      "Ignoring wrong pointing object 128 0 (offset 0)\n",
      "Ignoring wrong pointing object 130 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/150\n",
      "30/150\n",
      "40/150\n",
      "50/150\n",
      "60/150\n",
      "70/150\n",
      "80/150\n",
      "90/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 35 0 (offset 0)\n",
      "Ignoring wrong pointing object 43 0 (offset 0)\n",
      "Ignoring wrong pointing object 57 0 (offset 0)\n",
      "Ignoring wrong pointing object 75 0 (offset 0)\n",
      "Ignoring wrong pointing object 81 0 (offset 0)\n",
      "Ignoring wrong pointing object 228 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing for Object Streams\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 36 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 43 0 (offset 0)\n",
      "Ignoring wrong pointing object 45 0 (offset 0)\n",
      "Ignoring wrong pointing object 67 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/150\n",
      "130/150\n",
      "140/150\n",
      "\n",
      "Done: 150/150 parsed\n"
     ]
    }
   ],
   "source": [
    "def parse_pdf(pdf_url):\n",
    "    try:\n",
    "        response = requests.get(pdf_url, timeout=30)\n",
    "        pdf = PdfReader(io.BytesIO(response.content))\n",
    "        text = \"\".join(page.extract_text() or \"\" for page in pdf.pages)\n",
    "        # Remove invalid unicode surrogate characters\n",
    "        text = text.encode('utf-8', errors='surrogatepass').decode('utf-8', errors='replace')\n",
    "        return text if len(text) > 500 else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "print(f\"\\nParsing {len(papers)} PDFs...\")\n",
    "\n",
    "for i, paper in enumerate(papers):\n",
    "    text = parse_pdf(paper['pdf_url'])\n",
    "    \n",
    "    if text:\n",
    "        paper['full_text'] = text\n",
    "    else:\n",
    "        paper['full_text'] = f\"{paper['title']}\\n\\n{paper['summary']}\"\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f\"{i}/{len(papers)}\")\n",
    "\n",
    "success = sum(1 for p in papers if len(p['full_text']) > 1000)\n",
    "print(f\"\\nDone: {success}/{len(papers)} parsed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Data Quality Analysis"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Length Statistics:\n",
      "count       150.000000\n",
      "mean      48011.713333\n",
      "std       30693.738474\n",
      "min       15618.000000\n",
      "25%       27611.500000\n",
      "50%       37797.000000\n",
      "75%       62723.250000\n",
      "max      235577.000000\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "Short papers (<2k chars): 0\n",
      "\n",
      "✓ Data validated\n"
     ]
    }
   ],
   "source": [
    "df_analysis = pd.DataFrame(papers)\n",
    "df_analysis['text_length'] = df_analysis['full_text'].str.len()\n",
    "\n",
    "print(\"Text Length Statistics:\")\n",
    "print(df_analysis['text_length'].describe())\n",
    "\n",
    "print(f\"\\nShort papers (<2k chars): {(df_analysis['text_length'] < 2000).sum()}\")\n",
    "\n",
    "print(\"\\n✓ Data validated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 2. RAG Pipeline"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Chunking"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking papers...\n",
      "0/150 - 78 chunks so far\n",
      "20/150 - 1397 chunks so far\n",
      "40/150 - 2908 chunks so far\n",
      "60/150 - 4071 chunks so far\n",
      "80/150 - 5291 chunks so far\n",
      "100/150 - 6382 chunks so far\n",
      "120/150 - 7525 chunks so far\n",
      "140/150 - 8748 chunks so far\n",
      "\n",
      "Done: 9283 total chunks\n",
      "Avg chunks per paper: 61.9\n"
     ]
    }
   ],
   "source": [
    "def chunk_paper(paper):\n",
    "    \"\"\"Create chunks with metadata\"\"\"\n",
    "    \n",
    "    title_abstract = f\"Title: {paper['title']}\\n\\nAbstract: {paper['summary']}\"\n",
    "    \n",
    "    chunks = [Document(\n",
    "        page_content=title_abstract,\n",
    "        metadata={\n",
    "            'arxiv_id': paper['article_id'],\n",
    "            'title': paper['title'],\n",
    "            'section': 'title_abstract',\n",
    "            'authors': ', '.join(paper['authors'][:3])\n",
    "        }\n",
    "    )]\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200\n",
    "    )\n",
    "    \n",
    "    body_chunks = splitter.create_documents(\n",
    "        texts=[paper['full_text']],\n",
    "        metadatas=[{\n",
    "            'arxiv_id': paper['article_id'],\n",
    "            'title': paper['title'],\n",
    "            'section': 'body',\n",
    "            'authors': ', '.join(paper['authors'][:3])\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    chunks.extend(body_chunks)\n",
    "    return chunks\n",
    "\n",
    "print(\"Chunking papers...\")\n",
    "\n",
    "all_chunks = []\n",
    "for i, paper in enumerate(papers):\n",
    "    paper_chunks = chunk_paper(paper)\n",
    "    all_chunks.extend(paper_chunks)\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        print(f\"{i}/{len(papers)} - {len(all_chunks)} chunks so far\")\n",
    "\n",
    "print(f\"\\nDone: {len(all_chunks)} total chunks\")\n",
    "print(f\"Avg chunks per paper: {len(all_chunks)/len(papers):.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Embeddings & Vector Store"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings...\n",
      "Chunks to process: 9283\n",
      "This will take ~2-3 minutes\n",
      "\n",
      "\n",
      "Vector store created\n",
      "  Total chunks indexed: 9283\n",
      "  Ready for retrieval\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating embeddings...\")\n",
    "print(f\"Chunks to process: {len(all_chunks)}\")\n",
    "print(\"This will take ~2-3 minutes\\n\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_chunks,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"litsearch_papers\"\n",
    ")\n",
    "\n",
    "print(f\"\\nVector store created\")\n",
    "print(f\"  Total chunks indexed: {len(all_chunks)}\")\n",
    "print(f\"  Ready for retrieval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### Test Retrieval"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 'What are the best performing segmentation techniques on BraTS dataset?'\n",
      "============================================================\n",
      "\n",
      "[1] Score: 0.765\n",
      "    ArXiv: 2307.15872v1\n",
      "    Title: Cross-dimensional transfer learning in medical ima...\n",
      "    Section: body\n",
      "    Text: These methods have all performed more or less e fficiently, some better than others on\n",
      "the different challenge tasks. This shows the e ffectiveness of...\n",
      "\n",
      "[2] Score: 0.777\n",
      "    ArXiv: 2306.12510v2\n",
      "    Title: Comparative Analysis of Segment Anything Model and...\n",
      "    Section: body\n",
      "    Text: • DL for Medical Image Segmentation is used. \n",
      "• Hardware Acceleration on SBCs (Google's Edge \n",
      "TPU)  \n",
      "Scenario 1: \n",
      "BUSI: 0.995 \n",
      "UDIAT: 0.949 \n",
      "Scenario ...\n",
      "\n",
      "[3] Score: 0.785\n",
      "    ArXiv: 2102.04525v4\n",
      "    Title: Unified Focal loss: Generalising Dice and cross en...\n",
      "    Section: body\n",
      "    Text: 25described in (Table 1). For 3D binary segmentation, we used the BraTS20\n",
      "dataset. Here, images were pre-processed, with the skull stripped and images...\n",
      "\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "\n",
      "Query: 'What are the main challenges that remain unsolved in brain tumor segmentation?'\n",
      "============================================================\n",
      "\n",
      "[1] Score: 0.758\n",
      "    ArXiv: 2307.15872v1\n",
      "    Title: Cross-dimensional transfer learning in medical ima...\n",
      "    Section: body\n",
      "    Text: These methods have all performed more or less e fficiently, some better than others on\n",
      "the different challenge tasks. This shows the e ffectiveness of...\n",
      "\n",
      "[2] Score: 0.765\n",
      "    ArXiv: 1904.05191v3\n",
      "    Title: Weakly-Supervised White and Grey Matter Segmentati...\n",
      "    Section: body\n",
      "    Text: processes, drug perfusion and decision support systems. In this setting, segmen-\n",
      "tation of brain structures can initialize image based registration [1...\n",
      "\n",
      "[3] Score: 0.786\n",
      "    ArXiv: 1910.08978v2\n",
      "    Title: Attention Enriched Deep Learning Model for Breast ...\n",
      "    Section: body\n",
      "    Text: al. 2018; Ribli et al. 2017), a body of literature used MRI (Jaeger et al. 2018),  and histology images (Lin et al. 2018). \n",
      "U-Net (Ronneberger et al. ...\n",
      "\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "\n",
      "\n",
      "Query: 'What are the most commonly used public datasets for brain tumor detection?'\n",
      "============================================================\n",
      "\n",
      "[1] Score: 0.662\n",
      "    ArXiv: 2306.01827v2\n",
      "    Title: Active Learning on Medical Image...\n",
      "    Section: body\n",
      "    Text: Burren, Nicole Porz, Johannes Slotboom, Roland Wiest, et al. The multimodal brain tumor image segmentation\n",
      "benchmark (brats). IEEE transactions on med...\n",
      "\n",
      "[2] Score: 0.664\n",
      "    ArXiv: 2409.19483v4\n",
      "    Title: MedCLIP-SAMv2: Towards Universal Text-Driven Medic...\n",
      "    Section: body\n",
      "    Text: respectively.\n",
      "6• Brain Tumor MRI : The Brain Tumor dataset (Cheng,\n",
      "2017), comprising 1,462 T1-weighted MRI scans for train-\n",
      "ing, 1,002 for validation,...\n",
      "\n",
      "[3] Score: 0.676\n",
      "    ArXiv: 2102.04525v4\n",
      "    Title: Unified Focal loss: Generalising Dice and cross en...\n",
      "    Section: body\n",
      "    Text: Among medical imaging datasets, those involving tumour segmentation\n",
      "are associated with high degrees of class imbalance. Manual tumour delin-\n",
      "eation i...\n",
      "\n",
      "\n",
      "────────────────────────────────────────────────────────────\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_retrieval(query, k=5):\n",
    "    \"\"\"Test semantic search\"\"\"\n",
    "    results = vectorstore.similarity_search_with_score(query, k=k)\n",
    "    \n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    for i, (doc, score) in enumerate(results):\n",
    "        print(f\"[{i+1}] Score: {score:.3f}\")\n",
    "        print(f\"    ArXiv: {doc.metadata['arxiv_id']}\")\n",
    "        print(f\"    Title: {doc.metadata['title'][:50]}...\")\n",
    "        print(f\"    Section: {doc.metadata['section']}\")\n",
    "        print(f\"    Text: {doc.page_content[:150]}...\")\n",
    "        print()\n",
    "\n",
    "test_queries = [\n",
    "      \"What are the best performing segmentation techniques on BraTS dataset?\",\n",
    "      \"What are the main challenges that remain unsolved in brain tumor segmentation?\",\n",
    "      \"What are the most commonly used public datasets for brain tumor detection?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    test_retrieval(query, k=3)\n",
    "    print(\"\\n\" + \"─\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### RAG Chain"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ RAG chain ready\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI research assistant. Use the context to answer. Cite sources as [arXiv:ID].\\n\\nContext: {context}\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([f\"[arXiv:{doc.metadata['arxiv_id']}]: {doc.page_content}\" for doc in docs])\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"✓ RAG chain ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Q: Can conformal predictions be used for unsupervised anomaly detection in images?\n",
      "============================================================\n",
      "\n",
      "The context provided does not mention the use of conformal predictions for unsupervised anomaly detection in images. The discussed methods for unsupervised anomaly detection in images include the use of generative models to synthesize healthy samples from diseased images, the use of an encoder network to replace the time-consuming iterative restoration process, and the creation of synthetic anomalies to train a discriminative model. However, it does not mention the use of conformal predictions in this process.\n",
      "\n",
      "\n",
      "============================================================\n",
      "Q: Are there machine learning methods to detect brain lesions using ultrasound?\n",
      "============================================================\n",
      "\n",
      "Yes, there are machine learning methods to detect brain lesions using ultrasound. For instance, the paper by H. Chen et al. discusses the use of iterative multi-domain regularized deep learning for anatomical structure detection and segmentation from ultrasound images [4]. However, the application of deep learning in ultrasound segmentation, particularly for brain lesions, is still an emerging field and more research is needed to make these methods more generalizable and applicable in various surgical settings [arXiv:1904.08655v1].\n",
      "\n",
      "\n",
      "============================================================\n",
      "Q: Transformers for multimodal image segmentation??Cross-attention for image classification?\n",
      "============================================================\n",
      "\n",
      "Transformers have been used for multimodal image segmentation. For instance, the TransBTS model uses transformers for multimodal brain tumor segmentation [30]. The model leverages the transformer's ability to capture contextual information and long-range dependencies in the input data, which is beneficial for medical image segmentation tasks.\n",
      "\n",
      "Cross-attention is also used in image classification tasks. In the context of ultrasound image segmentation, a Transformer decoder with cross-attention is proposed. This decoder consists of six decoder blocks and is trained with a set prediction objective. The cross-attention mechanism helps to capture long-range dependencies for better structure awareness [arXiv:2510.26568v1].\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_queries = [\n",
    "      \"Can conformal predictions be used for unsupervised anomaly detection in images?\",\n",
    "      \"Are there machine learning methods to detect brain lesions using ultrasound?\",\n",
    "      \"Transformers for multimodal image segmentation??\",\n",
    "      \"Cross-attention for image classification?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Q: {query}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    response = rag_chain.invoke(query)\n",
    "    print(response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Evaluation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import json\n",
    "\n",
    "def evaluate_faithfulness(question, answer, sources):\n",
    "    \"\"\"Evaluate if answer is faithful to sources using LLM judge\"\"\"\n",
    "    \n",
    "    context = \"\\n\\n\".join([f\"Source {i+1}: {s.page_content}\" for i, s in enumerate(sources)])\n",
    "    \n",
    "    prompt = f\"\"\"Rate faithfulness (0.0-1.0):\n",
    "\n",
    "Sources:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer: {answer}\n",
    "\n",
    "Is every claim supported by sources? Return only a number 0.0-1.0.\"\"\"\n",
    "    \n",
    "    judge = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "    score = judge.invoke(prompt).content.strip()\n",
    "    \n",
    "    return float(score)\n",
    "\n",
    "\n",
    "test_queries = [\n",
    "      \"What are the best performing segmentation techniques on BraTS dataset?\",\n",
    "      \"What are the main challenges that remain unsolved in brain tumor segmentation?\",\n",
    "      \"What are the most commonly used public datasets for brain tumor detection?\"\n",
    "]\n",
    "\n",
    "print(\"FAITHFULNESS EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "scores = []\n",
    "for query in test_queries:\n",
    "    answer = rag_chain.invoke(query)\n",
    "    sources = vectorstore.similarity_search(query, k=5)\n",
    "    score = evaluate_faithfulness(query, answer, sources)\n",
    "    scores.append(score)\n",
    "    \n",
    "    print(f\"{query[:45]}...\")\n",
    "    print(f\"  Faithfulness: {score:.2f}\\n\")\n",
    "\n",
    "print(f\"Average Faithfulness: {np.mean(scores):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}