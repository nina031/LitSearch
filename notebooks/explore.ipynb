{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LitSearch Setup and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ““ LitSearch AI - Plan de DÃ©veloppement MVP\n",
    "\n",
    "## Phase 1: DATA (Aujourd'hui - 3h)\n",
    "\n",
    "### Step 1: Fetch 100 papers cs.AI\n",
    "- TÃ©lÃ©charger 100 papers rÃ©cents depuis arXiv\n",
    "- CatÃ©gorie: cs.AI\n",
    "- PÃ©riode: 6 derniers mois\n",
    "- **Deliverable:** Liste de 100 papers avec mÃ©tadonnÃ©es\n",
    "\n",
    "### Step 2: Parse PDFs (texte brut)\n",
    "- TÃ©lÃ©charger les PDFs depuis arXiv\n",
    "- Extraire le texte brut avec pypdf\n",
    "- GÃ©rer les erreurs (PDFs corrompus, etc.)\n",
    "- **Deliverable:** Texte complet pour chaque paper\n",
    "\n",
    "### Step 3: Valider qualitÃ© des donnÃ©es\n",
    "- VÃ©rifier que les PDFs sont lisibles\n",
    "- Analyser longueur moyenne des textes\n",
    "- Identifier papers problÃ©matiques\n",
    "- **Deliverable:** Dataset propre et validÃ©\n",
    "\n",
    "---\n",
    "\n",
    "## Phase 2: RAG PIPELINE (Aujourd'hui soir + Demain - 5h)\n",
    "\n",
    "### Step 4: Chunking intelligent\n",
    "- StratÃ©gie: Title+Abstract + Body chunks\n",
    "- Taille chunks: 1000 chars avec overlap 200\n",
    "- PrÃ©server mÃ©tadonnÃ©es (arXiv ID, section)\n",
    "- **Deliverable:** chunks avec metadata\n",
    "\n",
    "### Step 5: Embeddings + Vector store\n",
    "- CrÃ©er embeddings avec OpenAI\n",
    "- Stocker dans ChromaDB\n",
    "- Indexer avec mÃ©tadonnÃ©es\n",
    "- **Deliverable:** Vector store opÃ©rationnel\n",
    "\n",
    "### Step 6: Test retrieval (sans LLM)\n",
    "- Tester similarity search\n",
    "- VÃ©rifier pertinence des rÃ©sultats\n",
    "- Ajuster paramÃ¨tres (k, threshold)\n",
    "- **Deliverable:** Retrieval qui fonctionne\n",
    "\n",
    "### Step 7: RAG chain complet (avec LLM)\n",
    "- CrÃ©er prompt template scientifique\n",
    "- IntÃ©grer LLM (GPT-4)\n",
    "- Chain retrieval + generation\n",
    "- **Deliverable:** RAG end-to-end fonctionnel\n",
    "\n",
    "### Step 8: Tester avec questions\n",
    "- PrÃ©parer 10 questions test\n",
    "- Ã‰valuer qualitÃ© des rÃ©ponses\n",
    "- Identifier problÃ¨mes\n",
    "- **Deliverable:** 5+ questions qui marchent bien\n",
    "\n",
    "---\n",
    "\n",
    "## Phase 3: POLISH (Demain aprÃ¨s-midi - 3h)\n",
    "\n",
    "### Step 9: Optimiser prompts\n",
    "- AmÃ©liorer qualitÃ© des rÃ©ponses\n",
    "- RÃ©duire hallucinations\n",
    "- Forcer citations systÃ©matiques\n",
    "- **Deliverable:** RÃ©ponses de meilleure qualitÃ©\n",
    "\n",
    "### Step 10: AmÃ©liorer citations\n",
    "- Format: arXiv:ID, Section, Page\n",
    "- Affichage clair des sources\n",
    "- Relevance scores\n",
    "- **Deliverable:** Citations professionnelles\n",
    "\n",
    "### Step 11: Nettoyer le notebook\n",
    "- Markdown explicatif entre cells\n",
    "- Supprimer code mort\n",
    "- Organiser logiquement\n",
    "- **Deliverable:** Notebook prÃ©sentable\n",
    "\n",
    "---\n",
    "\n",
    "## Phase 4: PRÃ‰SENTATION (Samedi matin - 2h)\n",
    "\n",
    "### Step 12: README + documentation\n",
    "- Architecture diagram\n",
    "- Origin story (amie chercheuse)\n",
    "- Lien avec INSPIRE AI\n",
    "- **Deliverable:** README professionnel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment & Install necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jupyter arxiv pypdf openai langchain langchain-core langchain-openai langchain-community langchain-text-splitters chromadb python-dotenv pandas requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import arxiv\n",
    "\n",
    "from pypdf import PdfReader\n",
    "import io\n",
    "import requests\n",
    "\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "openai_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "print(openai_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1 : Data extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Fetch Papers from arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zm/zdbpwn4958j3hydxnlzjt9d00000gn/T/ipykernel_71278/675248984.py:7: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ AAU-net: An Adaptive Attention U-net for Breast Lesions Segm...\n",
      "âœ“ VIS-MAE: An Efficient Self-supervised Learning Approach on M...\n",
      "âœ“ Deep Learning in Medical Ultrasound Image Segmentation: a Re...\n",
      "âœ“ Enhanced Uncertainty Estimation in Ultrasound Image Segmenta...\n",
      "âœ“ Active Learning on Medical Image...\n",
      "âœ“ DiffBoost: Enhancing Medical Image Segmentation via Text-Gui...\n",
      "âœ“ Cross-dimensional transfer learning in medical image segment...\n",
      "âœ“ Benchmarking Image Transformers for Prostate Cancer Detectio...\n",
      "âœ“ Attention Gated Networks: Learning to Leverage Salient Regio...\n",
      "âœ“ Detecting Heart Disease from Multi-View Ultrasound Images vi...\n",
      "âœ“ Semi-supervised Cervical Segmentation on Ultrasound by A Dua...\n",
      "âœ“ One-Stop Automated Diagnostic System for Carpal Tunnel Syndr...\n",
      "âœ“ Attention Enriched Deep Learning Model for Breast Tumor Segm...\n",
      "âœ“ Global Guidance Network for Breast Lesion Segmentation in Ul...\n",
      "âœ“ Domain and Geometry Agnostic CNNs for Left Atrium Segmentati...\n",
      "âœ“ Revisiting Data Augmentation for Ultrasound Images...\n",
      "âœ“ The Efficacy of Semantics-Preserving Transformations in Self...\n",
      "âœ“ Deep Attentive Features for Prostate Segmentation in 3D Tran...\n",
      "âœ“ Understanding the Mechanisms of Deep Transfer Learning for M...\n",
      "âœ“ VidFuncta: Towards Generalizable Neural Representations for ...\n",
      "âœ“ Interactive Segmentation Model for Placenta Segmentation fro...\n",
      "âœ“ Ultrasound Image Representation Learning by Modeling Sonogra...\n",
      "âœ“ Self-Supervised Ultrasound to MRI Fetal Brain Image Synthesi...\n",
      "âœ“ Multi-Atlas Segmentation and Spatial Alignment of the Human ...\n",
      "âœ“ BAAF: A benchmark attention adaptive framework for medical u...\n",
      "âœ“ Medical Instrument Detection in Ultrasound-Guided Interventi...\n",
      "âœ“ Experimental Validation of Ultrasound Beamforming with End-t...\n",
      "âœ“ Iterative Multi-domain Regularized Deep Learning for Anatomi...\n",
      "âœ“ Image quality assessment for closed-loop computer-assisted l...\n",
      "âœ“ Multi-source adversarial transfer learning for ultrasound im...\n",
      "âœ“ Automatic 3D Multi-modal Ultrasound Segmentation of Human Pl...\n",
      "âœ“ Synomaly Noise and Multi-Stage Diffusion: A Novel Approach f...\n",
      "âœ“ Automatic 3D Ultrasound Segmentation of Uterus Using Deep Le...\n",
      "âœ“ SA$^{2}$Net: Scale-Adaptive Structure-Affinity Transformatio...\n",
      "âœ“ Boundary-RL: Reinforcement Learning for Weakly-Supervised Pr...\n",
      "âœ“ Weakly-supervised Learning For Catheter Segmentation in 3D F...\n",
      "âœ“ A Comprehensive Review of Techniques, Algorithms, Advancemen...\n",
      "âœ“ Principled Ultrasound Data Augmentation for Classification o...\n",
      "âœ“ Multi-Task Learning Approach for Unified Biometric Estimatio...\n",
      "âœ“ MicroSegNet: A Deep Learning Approach for Prostate Segmentat...\n",
      "âœ“ CP-UNet: Contour-based Probabilistic Model for Medical Ultra...\n",
      "âœ“ ProsDectNet: Bridging the Gap in Prostate Cancer Detection v...\n",
      "âœ“ High Volume Rate 3D Ultrasound Reconstruction with Diffusion...\n",
      "âœ“ Automatic Ultrasound Image Segmentation of Supraclavicular N...\n",
      "âœ“ Combining Bayesian and Deep Learning Methods for the Delinea...\n",
      "âœ“ SamDSK: Combining Segment Anything Model with Domain-Specifi...\n",
      "âœ“ Dense Pixel-Labeling for Reverse-Transfer and Diagnostic Lea...\n",
      "âœ“ Bayesian approaches for Quantifying Clinicians' Variability ...\n",
      "âœ“ Automated Fetal Biometry Assessment with Deep Ensembles usin...\n",
      "âœ“ Vascular Segmentation of Functional Ultrasound Images using ...\n",
      "âœ“ Self-Supervised Ultrasound Representation Learning for Renal...\n",
      "âœ“ Deep BV: A Fully Automated System for Brain Ventricle Locali...\n",
      "âœ“ Multiview and Multiclass Image Segmentation using Deep Learn...\n",
      "âœ“ Online Reflective Learning for Robust Medical Image Segmenta...\n",
      "âœ“ A Recycling Training Strategy for Medical Image Segmentation...\n",
      "âœ“ Shape Detection In 2D Ultrasound Images...\n",
      "âœ“ Learning image quality assessment by reinforcing task amenab...\n",
      "âœ“ Weakly-Supervised White and Grey Matter Segmentation in 3D B...\n",
      "âœ“ DeepSPV: A Deep Learning Pipeline for 3D Spleen Volume Estim...\n",
      "âœ“ Semi-Supervised Dual-Threshold Contrastive Learning for Ultr...\n",
      "âœ“ A novel open-source ultrasound dataset with deep learning be...\n",
      "âœ“ Triple-View Feature Learning for Medical Image Segmentation...\n",
      "âœ“ Unified Focal loss: Generalising Dice and cross entropy-base...\n",
      "âœ“ Unsupervised multi-latent space reinforcement learning frame...\n",
      "âœ“ Tracked 3D Ultrasound and Deep Neural Network-based Thyroid ...\n",
      "âœ“ A Study of Domain Generalization on Ultrasound-based Multi-C...\n",
      "âœ“ Flip Learning: Weakly Supervised Erase to Segment Nodules in...\n",
      "âœ“ Medical Instrument Segmentation in 3D US by Hybrid Constrain...\n",
      "âœ“ Modifying the U-Net's Encoder-Decoder Architecture for Segme...\n",
      "âœ“ AiAReSeg: Catheter Detection and Segmentation in Interventio...\n",
      "âœ“ CSDN: Combing Shallow and Deep Networks for Accurate Real-ti...\n",
      "âœ“ Medical-Knowledge Driven Multiple Instance Learning for Clas...\n",
      "âœ“ SAS: Segment Anything Small for Ultrasound -- A Non-Generati...\n",
      "âœ“ Quantifying and Leveraging Predictive Uncertainty for Medica...\n",
      "âœ“ Multi-Center Study on Deep Learning-Assisted Detection and C...\n",
      "âœ“ Evaluating Reliability in Medical DNNs: A Critical Analysis ...\n",
      "âœ“ NiftyNet: a deep-learning platform for medical imaging...\n",
      "âœ“ Semantic Segmentation Refiner for Ultrasound Applications wi...\n",
      "âœ“ Deep Learning for Classification of Thyroid Nodules on Ultra...\n",
      "âœ“ RCA-IUnet: A residual cross-spatial attention guided incepti...\n",
      "âœ“ Machine Learning-Assisted Vocal Cord Ultrasound Examination:...\n",
      "âœ“ Super-Resolved Microbubble Localization in Single-Channel Ul...\n",
      "âœ“ Geo-UNet: A Geometrically Constrained Neural Framework for C...\n",
      "âœ“ Removing confounding information from fetal ultrasound image...\n",
      "âœ“ Fine tuning U-Net for ultrasound image segmentation: which l...\n",
      "âœ“ WATUNet: A Deep Neural Network for Segmentation of Volumetri...\n",
      "âœ“ Comparative Analysis of Segment Anything Model and U-Net for...\n",
      "âœ“ Learning Topological Interactions for Multi-Class Medical Im...\n",
      "âœ“ Echo from noise: synthetic ultrasound image generation using...\n",
      "âœ“ VertMatch: A Semi-supervised Framework for Vertebral Structu...\n",
      "âœ“ Ultrasound segmentation using U-Net: learning from simulated...\n",
      "âœ“ Self-Supervised Ultrasound-Video Segmentation with Feature P...\n",
      "âœ“ 3D Freehand Ultrasound using Visual Inertial and Deep Inerti...\n",
      "âœ“ A Simple Framework Uniting Visual In-context Learning with M...\n",
      "âœ“ MedCLIP-SAM: Bridging Text and Image Towards Universal Medic...\n",
      "âœ“ From Claims to Evidence: A Unified Framework and Critical An...\n",
      "âœ“ Transfer Learning U-Net Deep Learning for Lung Ultrasound Se...\n",
      "âœ“ Reslicing Ultrasound Images for Data Augmentation and Vessel...\n",
      "âœ“ DeNAS-ViT: Data Efficient NAS-Optimized Vision Transformer f...\n",
      "âœ“ Adaptable image quality assessment using meta-reinforcement ...\n",
      "âœ“ U-Net in Medical Image Segmentation: A Review of Its Applica...\n",
      "âœ“ SAM 3D: 3Dfy Anything in Images...\n",
      "âœ“ S-CycleGAN: Semantic Segmentation Enhanced CT-Ultrasound Ima...\n",
      "âœ“ A CNN Segmentation-Based Approach to Object Detection and Tr...\n",
      "âœ“ Domain Generalization for Prostate Segmentation in Transrect...\n",
      "âœ“ Automatic ultrasound vessel segmentation with deep spatiotem...\n",
      "âœ“ Evaluate Fine-tuning Strategies for Fetal Head Ultrasound Im...\n",
      "âœ“ MedCLIP-SAMv2: Towards Universal Text-Driven Medical Image S...\n",
      "âœ“ Machine Learning with Abstention for Automated Liver Disease...\n",
      "âœ“ Label-free segmentation from cardiac ultrasound using self-s...\n",
      "âœ“ Recurrent U-net for automatic pelvic floor muscle segmentati...\n",
      "âœ“ Invisible-to-Visible: Privacy-Aware Human Segmentation using...\n",
      "âœ“ FunKAN: Functional Kolmogorov-Arnold Network for Medical Ima...\n",
      "âœ“ FUSQA: Fetal Ultrasound Segmentation Quality Assessment...\n",
      "âœ“ Zero-shot performance of the Segment Anything Model (SAM) in...\n",
      "âœ“ Evaluation of Complexity Measures for Deep Learning Generali...\n",
      "âœ“ Development and evaluation of intraoperative ultrasound segm...\n",
      "âœ“ Pediatric Appendicitis Detection from Ultrasound Images...\n",
      "âœ“ Motion Informed Needle Segmentation in Ultrasound Images...\n",
      "âœ“ Fetal Ultrasound Image Segmentation for Measuring Biometric ...\n",
      "âœ“ Unsupervised Medical Image Segmentation with Adversarial Net...\n",
      "âœ“ Unsupervised Deformable Ultrasound Image Registration and It...\n",
      "âœ“ Detection of Medial Epicondyle Avulsion in Elbow Ultrasound ...\n",
      "âœ“ DBF-Net: A Dual-Branch Network with Feature Fusion for Ultra...\n",
      "âœ“ VeniBot: Towards Autonomous Venipuncture with Semi-supervise...\n",
      "âœ“ ESTAN: Enhanced Small Tumor-Aware Network for Breast Ultraso...\n",
      "âœ“ Rethinking the Unpretentious U-net for Medical Ultrasound Im...\n",
      "âœ“ Current Advances in Computational Lung Ultrasound Imaging: A...\n",
      "âœ“ Ambiguous Medical Image Segmentation using Diffusion Models...\n",
      "âœ“ Transfer Learning for Ultrasound Tongue Contour Extraction w...\n",
      "âœ“ POCOVID-Net: Automatic Detection of COVID-19 From a New Lung...\n",
      "âœ“ Fully Automatic Segmentation of 3D Brain Ultrasound: Learnin...\n",
      "âœ“ Diffusion Model-based Data Augmentation Method for Fetal Hea...\n",
      "âœ“ A Review on Deep-Learning Algorithms for Fetal Ultrasound-Im...\n",
      "âœ“ A Deep Learning Framework for Thyroid Nodule Segmentation an...\n",
      "âœ“ Annotation-Efficient Learning for Medical Image Segmentation...\n",
      "âœ“ W-Net: Dense Semantic Segmentation of Subcutaneous Tissue in...\n",
      "âœ“ Multi-Level Global Context Cross Consistency Model for Semi-...\n",
      "âœ“ PRISM Lite: A lightweight model for interactive 3D placenta ...\n",
      "âœ“ USEANet: Ultrasound-Specific Edge-Aware Multi-Branch Network...\n",
      "âœ“ ReViVD: Exploration and Filtering of Trajectories in an Imme...\n",
      "âœ“ Adnexal Mass Segmentation with Ultrasound Data Synthesis...\n",
      "âœ“ Region Proposal Network with Graph Prior and IoU-Balance Los...\n",
      "âœ“ UltraUNet: Real-Time Ultrasound Tongue Segmentation for Dive...\n",
      "âœ“ Ultrasound Nodule Segmentation Using Asymmetric Learning wit...\n",
      "âœ“ DEPT: Deep Extreme Point Tracing for Ultrasound Image Segmen...\n",
      "âœ“ Anatomy-Guided Representation Learning Using a Transformer-B...\n",
      "âœ“ PINN-EMFNet: PINN-based and Enhanced Multi-Scale Feature Fus...\n",
      "âœ“ BI-RADS-Net: An Explainable Multitask Learning Approach for ...\n",
      "âœ“ Hybrid Attention Network for Accurate Breast Tumor Segmentat...\n",
      "\n",
      "150 papers fetched (RAG-focused)\n"
     ]
    }
   ],
   "source": [
    "search = arxiv.Search(\n",
    "      query='3D ultrasound AND machine learning AND (anomaly detection OR segmentation) AND medical imaging',\n",
    "      max_results=150,\n",
    "      sort_by=arxiv.SortCriterion.Relevance)\n",
    "\n",
    "papers = []\n",
    "for result in search.results():\n",
    "    paper = {\n",
    "        'article_id': result.entry_id.split('/')[-1],\n",
    "        'title': result.title,\n",
    "        'authors': [author.name for author in result.authors],\n",
    "        'published': result.published,\n",
    "        'summary': result.summary,\n",
    "        'pdf_url': result.pdf_url\n",
    "    }\n",
    "    papers.append(paper)\n",
    "    print(f\"âœ“ {paper['title'][:60]}...\")\n",
    "\n",
    "print(f\"\\n{len(papers)} papers fetched (RAG-focused)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Parse PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parsing 150 PDFs...\n",
      "0/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Impossible to decode XFormObject /Im3: 'bbox'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 45 0 (offset 0)\n",
      "Ignoring wrong pointing object 58 0 (offset 0)\n",
      "Ignoring wrong pointing object 60 0 (offset 0)\n",
      "Ignoring wrong pointing object 63 0 (offset 0)\n",
      "Ignoring wrong pointing object 65 0 (offset 0)\n",
      "Ignoring wrong pointing object 68 0 (offset 0)\n",
      "Ignoring wrong pointing object 70 0 (offset 0)\n",
      "Ignoring wrong pointing object 81 0 (offset 0)\n",
      "Ignoring wrong pointing object 83 0 (offset 0)\n",
      "Ignoring wrong pointing object 107 0 (offset 0)\n",
      "Ignoring wrong pointing object 118 0 (offset 0)\n",
      "Ignoring wrong pointing object 120 0 (offset 0)\n",
      "Ignoring wrong pointing object 128 0 (offset 0)\n",
      "Ignoring wrong pointing object 130 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/150\n",
      "30/150\n",
      "40/150\n",
      "50/150\n",
      "60/150\n",
      "70/150\n",
      "80/150\n",
      "90/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 17 0 (offset 0)\n",
      "Ignoring wrong pointing object 19 0 (offset 0)\n",
      "Ignoring wrong pointing object 27 0 (offset 0)\n",
      "Ignoring wrong pointing object 35 0 (offset 0)\n",
      "Ignoring wrong pointing object 43 0 (offset 0)\n",
      "Ignoring wrong pointing object 57 0 (offset 0)\n",
      "Ignoring wrong pointing object 75 0 (offset 0)\n",
      "Ignoring wrong pointing object 81 0 (offset 0)\n",
      "Ignoring wrong pointing object 228 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "parsing for Object Streams\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ignoring wrong pointing object 6 0 (offset 0)\n",
      "Ignoring wrong pointing object 8 0 (offset 0)\n",
      "Ignoring wrong pointing object 10 0 (offset 0)\n",
      "Ignoring wrong pointing object 12 0 (offset 0)\n",
      "Ignoring wrong pointing object 14 0 (offset 0)\n",
      "Ignoring wrong pointing object 16 0 (offset 0)\n",
      "Ignoring wrong pointing object 18 0 (offset 0)\n",
      "Ignoring wrong pointing object 20 0 (offset 0)\n",
      "Ignoring wrong pointing object 22 0 (offset 0)\n",
      "Ignoring wrong pointing object 28 0 (offset 0)\n",
      "Ignoring wrong pointing object 31 0 (offset 0)\n",
      "Ignoring wrong pointing object 36 0 (offset 0)\n",
      "Ignoring wrong pointing object 38 0 (offset 0)\n",
      "Ignoring wrong pointing object 43 0 (offset 0)\n",
      "Ignoring wrong pointing object 45 0 (offset 0)\n",
      "Ignoring wrong pointing object 67 0 (offset 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120/150\n",
      "130/150\n",
      "140/150\n",
      "\n",
      "Done: 150/150 parsed\n"
     ]
    }
   ],
   "source": [
    "def parse_pdf(pdf_url):\n",
    "    try:\n",
    "        response = requests.get(pdf_url, timeout=30)\n",
    "        pdf = PdfReader(io.BytesIO(response.content))\n",
    "        text = \"\".join(page.extract_text() or \"\" for page in pdf.pages)\n",
    "        # Remove invalid unicode surrogate characters\n",
    "        text = text.encode('utf-8', errors='surrogatepass').decode('utf-8', errors='replace')\n",
    "        return text if len(text) > 500 else None\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "print(f\"\\nParsing {len(papers)} PDFs...\")\n",
    "\n",
    "for i, paper in enumerate(papers):\n",
    "    text = parse_pdf(paper['pdf_url'])\n",
    "    \n",
    "    if text:\n",
    "        paper['full_text'] = text\n",
    "    else:\n",
    "        paper['full_text'] = f\"{paper['title']}\\n\\n{paper['summary']}\"\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(f\"{i}/{len(papers)}\")\n",
    "\n",
    "success = sum(1 for p in papers if len(p['full_text']) > 1000)\n",
    "print(f\"\\nDone: {success}/{len(papers)} parsed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Data Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Length Statistics:\n",
      "count       150.000000\n",
      "mean      48011.713333\n",
      "std       30693.738474\n",
      "min       15618.000000\n",
      "25%       27611.500000\n",
      "50%       37797.000000\n",
      "75%       62723.250000\n",
      "max      235577.000000\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "Short papers (<2k chars): 0\n",
      "\n",
      "âœ“ Data validated\n"
     ]
    }
   ],
   "source": [
    "df_analysis = pd.DataFrame(papers)\n",
    "df_analysis['text_length'] = df_analysis['full_text'].str.len()\n",
    "\n",
    "print(\"Text Length Statistics:\")\n",
    "print(df_analysis['text_length'].describe())\n",
    "\n",
    "print(f\"\\nShort papers (<2k chars): {(df_analysis['text_length'] < 2000).sum()}\")\n",
    "\n",
    "print(\"\\nâœ“ Data validated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>published</th>\n",
       "      <th>summary</th>\n",
       "      <th>pdf_url</th>\n",
       "      <th>full_text</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2204.12077v3</td>\n",
       "      <td>AAU-net: An Adaptive Attention U-net for Breas...</td>\n",
       "      <td>[Gongping Chen, Yu Dai, Jianxun Zhang, Moi Hoo...</td>\n",
       "      <td>2022-04-26 05:12:00+00:00</td>\n",
       "      <td>Various deep learning methods have been propos...</td>\n",
       "      <td>https://arxiv.org/pdf/2204.12077v3</td>\n",
       "      <td>IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. xx...</td>\n",
       "      <td>61342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2402.01034v3</td>\n",
       "      <td>VIS-MAE: An Efficient Self-supervised Learning...</td>\n",
       "      <td>[Zelong Liu, Andrew Tieu, Nikhil Patel, Georgi...</td>\n",
       "      <td>2024-02-01 21:45:12+00:00</td>\n",
       "      <td>Artificial Intelligence (AI) has the potential...</td>\n",
       "      <td>https://arxiv.org/pdf/2402.01034v3</td>\n",
       "      <td>\\n \\nVIS-MAE: An Efficient Self-supervised \\n...</td>\n",
       "      <td>28003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2002.07703v3</td>\n",
       "      <td>Deep Learning in Medical Ultrasound Image Segm...</td>\n",
       "      <td>[Ziyang Wang]</td>\n",
       "      <td>2020-02-18 16:33:22+00:00</td>\n",
       "      <td>Applying machine learning technologies, especi...</td>\n",
       "      <td>https://arxiv.org/pdf/2002.07703v3</td>\n",
       "      <td>Deep Learning in Medical Ultrasound Image Segm...</td>\n",
       "      <td>37243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2407.21273v1</td>\n",
       "      <td>Enhanced Uncertainty Estimation in Ultrasound ...</td>\n",
       "      <td>[Rohini Banerjee, Cecilia G. Morales, Artur Du...</td>\n",
       "      <td>2024-07-31 01:36:47+00:00</td>\n",
       "      <td>Efficient intravascular access in trauma and c...</td>\n",
       "      <td>https://arxiv.org/pdf/2407.21273v1</td>\n",
       "      <td>Enhanced Uncertainty Estimation in Ultrasound\\...</td>\n",
       "      <td>27112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2306.01827v2</td>\n",
       "      <td>Active Learning on Medical Image</td>\n",
       "      <td>[Angona Biswas, MD Abdullah Al Nasim, Md Shahi...</td>\n",
       "      <td>2023-06-02 16:24:39+00:00</td>\n",
       "      <td>The development of medical science greatly dep...</td>\n",
       "      <td>https://arxiv.org/pdf/2306.01827v2</td>\n",
       "      <td>ACTIVE LEARNING ON MEDICAL IMAGE\\nA PREPRINT\\n...</td>\n",
       "      <td>44701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2310.12868v2</td>\n",
       "      <td>DiffBoost: Enhancing Medical Image Segmentatio...</td>\n",
       "      <td>[Zheyuan Zhang, Lanhong Yao, Bin Wang, Debesh ...</td>\n",
       "      <td>2023-10-19 16:18:02+00:00</td>\n",
       "      <td>Large-scale, big-variant, high-quality data ar...</td>\n",
       "      <td>https://arxiv.org/pdf/2310.12868v2</td>\n",
       "      <td>IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX,...</td>\n",
       "      <td>65195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2307.15872v1</td>\n",
       "      <td>Cross-dimensional transfer learning in medical...</td>\n",
       "      <td>[Hicham Messaoudi, Ahror Belaid, Douraied Ben ...</td>\n",
       "      <td>2023-07-29 02:50:38+00:00</td>\n",
       "      <td>Over the last decade, convolutional neural net...</td>\n",
       "      <td>https://arxiv.org/pdf/2307.15872v1</td>\n",
       "      <td>Cross-dimensional transfer learning in medical...</td>\n",
       "      <td>80079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2403.18233v1</td>\n",
       "      <td>Benchmarking Image Transformers for Prostate C...</td>\n",
       "      <td>[Mohamed Harmanani, Paul F. R. Wilson, Fahimeh...</td>\n",
       "      <td>2024-03-27 03:39:57+00:00</td>\n",
       "      <td>PURPOSE: Deep learning methods for classifying...</td>\n",
       "      <td>https://arxiv.org/pdf/2403.18233v1</td>\n",
       "      <td>Benchmarking Image Transformers for Prostate C...</td>\n",
       "      <td>17214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1808.08114v2</td>\n",
       "      <td>Attention Gated Networks: Learning to Leverage...</td>\n",
       "      <td>[Jo Schlemper, Ozan Oktay, Michiel Schaap, Mat...</td>\n",
       "      <td>2018-08-22 19:17:23+00:00</td>\n",
       "      <td>We propose a novel attention gate (AG) model f...</td>\n",
       "      <td>https://arxiv.org/pdf/1808.08114v2</td>\n",
       "      <td>Attention Gated Networks:\\nLearning to Leverag...</td>\n",
       "      <td>65625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2306.00003v3</td>\n",
       "      <td>Detecting Heart Disease from Multi-View Ultras...</td>\n",
       "      <td>[Zhe Huang, Benjamin S. Wessler, Michael C. Hu...</td>\n",
       "      <td>2023-05-25 18:22:12+00:00</td>\n",
       "      <td>Aortic stenosis (AS) is a degenerative valve c...</td>\n",
       "      <td>https://arxiv.org/pdf/2306.00003v3</td>\n",
       "      <td>Proceedings of Machine Learning Research 219:1...</td>\n",
       "      <td>89210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     article_id                                              title  \\\n",
       "0  2204.12077v3  AAU-net: An Adaptive Attention U-net for Breas...   \n",
       "1  2402.01034v3  VIS-MAE: An Efficient Self-supervised Learning...   \n",
       "2  2002.07703v3  Deep Learning in Medical Ultrasound Image Segm...   \n",
       "3  2407.21273v1  Enhanced Uncertainty Estimation in Ultrasound ...   \n",
       "4  2306.01827v2                   Active Learning on Medical Image   \n",
       "5  2310.12868v2  DiffBoost: Enhancing Medical Image Segmentatio...   \n",
       "6  2307.15872v1  Cross-dimensional transfer learning in medical...   \n",
       "7  2403.18233v1  Benchmarking Image Transformers for Prostate C...   \n",
       "8  1808.08114v2  Attention Gated Networks: Learning to Leverage...   \n",
       "9  2306.00003v3  Detecting Heart Disease from Multi-View Ultras...   \n",
       "\n",
       "                                             authors  \\\n",
       "0  [Gongping Chen, Yu Dai, Jianxun Zhang, Moi Hoo...   \n",
       "1  [Zelong Liu, Andrew Tieu, Nikhil Patel, Georgi...   \n",
       "2                                      [Ziyang Wang]   \n",
       "3  [Rohini Banerjee, Cecilia G. Morales, Artur Du...   \n",
       "4  [Angona Biswas, MD Abdullah Al Nasim, Md Shahi...   \n",
       "5  [Zheyuan Zhang, Lanhong Yao, Bin Wang, Debesh ...   \n",
       "6  [Hicham Messaoudi, Ahror Belaid, Douraied Ben ...   \n",
       "7  [Mohamed Harmanani, Paul F. R. Wilson, Fahimeh...   \n",
       "8  [Jo Schlemper, Ozan Oktay, Michiel Schaap, Mat...   \n",
       "9  [Zhe Huang, Benjamin S. Wessler, Michael C. Hu...   \n",
       "\n",
       "                  published  \\\n",
       "0 2022-04-26 05:12:00+00:00   \n",
       "1 2024-02-01 21:45:12+00:00   \n",
       "2 2020-02-18 16:33:22+00:00   \n",
       "3 2024-07-31 01:36:47+00:00   \n",
       "4 2023-06-02 16:24:39+00:00   \n",
       "5 2023-10-19 16:18:02+00:00   \n",
       "6 2023-07-29 02:50:38+00:00   \n",
       "7 2024-03-27 03:39:57+00:00   \n",
       "8 2018-08-22 19:17:23+00:00   \n",
       "9 2023-05-25 18:22:12+00:00   \n",
       "\n",
       "                                             summary  \\\n",
       "0  Various deep learning methods have been propos...   \n",
       "1  Artificial Intelligence (AI) has the potential...   \n",
       "2  Applying machine learning technologies, especi...   \n",
       "3  Efficient intravascular access in trauma and c...   \n",
       "4  The development of medical science greatly dep...   \n",
       "5  Large-scale, big-variant, high-quality data ar...   \n",
       "6  Over the last decade, convolutional neural net...   \n",
       "7  PURPOSE: Deep learning methods for classifying...   \n",
       "8  We propose a novel attention gate (AG) model f...   \n",
       "9  Aortic stenosis (AS) is a degenerative valve c...   \n",
       "\n",
       "                              pdf_url  \\\n",
       "0  https://arxiv.org/pdf/2204.12077v3   \n",
       "1  https://arxiv.org/pdf/2402.01034v3   \n",
       "2  https://arxiv.org/pdf/2002.07703v3   \n",
       "3  https://arxiv.org/pdf/2407.21273v1   \n",
       "4  https://arxiv.org/pdf/2306.01827v2   \n",
       "5  https://arxiv.org/pdf/2310.12868v2   \n",
       "6  https://arxiv.org/pdf/2307.15872v1   \n",
       "7  https://arxiv.org/pdf/2403.18233v1   \n",
       "8  https://arxiv.org/pdf/1808.08114v2   \n",
       "9  https://arxiv.org/pdf/2306.00003v3   \n",
       "\n",
       "                                           full_text  text_length  \n",
       "0   IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. xx...        61342  \n",
       "1   \\n \\nVIS-MAE: An Efficient Self-supervised \\n...        28003  \n",
       "2  Deep Learning in Medical Ultrasound Image Segm...        37243  \n",
       "3  Enhanced Uncertainty Estimation in Ultrasound\\...        27112  \n",
       "4  ACTIVE LEARNING ON MEDICAL IMAGE\\nA PREPRINT\\n...        44701  \n",
       "5  IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX,...        65195  \n",
       "6  Cross-dimensional transfer learning in medical...        80079  \n",
       "7  Benchmarking Image Transformers for Prostate C...        17214  \n",
       "8  Attention Gated Networks:\\nLearning to Leverag...        65625  \n",
       "9  Proceedings of Machine Learning Research 219:1...        89210  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2 : Rag pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking papers...\n",
      "0/150 - 78 chunks so far\n",
      "20/150 - 1397 chunks so far\n",
      "40/150 - 2908 chunks so far\n",
      "60/150 - 4071 chunks so far\n",
      "80/150 - 5291 chunks so far\n",
      "100/150 - 6382 chunks so far\n",
      "120/150 - 7525 chunks so far\n",
      "140/150 - 8748 chunks so far\n",
      "\n",
      "Done: 9283 total chunks\n",
      "Avg chunks per paper: 61.9\n"
     ]
    }
   ],
   "source": [
    "def chunk_paper(paper):\n",
    "    \"\"\"Create chunks with metadata\"\"\"\n",
    "    \n",
    "    title_abstract = f\"Title: {paper['title']}\\n\\nAbstract: {paper['summary']}\"\n",
    "    \n",
    "    chunks = [Document(\n",
    "        page_content=title_abstract,\n",
    "        metadata={\n",
    "            'arxiv_id': paper['article_id'],\n",
    "            'title': paper['title'],\n",
    "            'section': 'title_abstract',\n",
    "            'authors': ', '.join(paper['authors'][:3])\n",
    "        }\n",
    "    )]\n",
    "    \n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200\n",
    "    )\n",
    "    \n",
    "    body_chunks = splitter.create_documents(\n",
    "        texts=[paper['full_text']],\n",
    "        metadatas=[{\n",
    "            'arxiv_id': paper['article_id'],\n",
    "            'title': paper['title'],\n",
    "            'section': 'body',\n",
    "            'authors': ', '.join(paper['authors'][:3])\n",
    "        }]\n",
    "    )\n",
    "    \n",
    "    chunks.extend(body_chunks)\n",
    "    return chunks\n",
    "\n",
    "print(\"Chunking papers...\")\n",
    "\n",
    "all_chunks = []\n",
    "for i, paper in enumerate(papers):\n",
    "    paper_chunks = chunk_paper(paper)\n",
    "    all_chunks.extend(paper_chunks)\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        print(f\"{i}/{len(papers)} - {len(all_chunks)} chunks so far\")\n",
    "\n",
    "print(f\"\\nDone: {len(all_chunks)} total chunks\")\n",
    "print(f\"Avg chunks per paper: {len(all_chunks)/len(papers):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'title_abstract', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='Title: AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images\\n\\nAbstract: Various deep learning methods have been proposed to segment breast lesion from ultrasound images. However, similar intensity distributions, variable tumor morphology and blurred boundaries present challenges for breast lesions segmentation, especially for malignant tumors with irregular shapes. Considering the complexity of ultrasound images, we develop an adaptive attention U-net (AAU-net) to segment breast lesions automatically and stably from ultrasound images. Specifically, we introduce a hybrid adaptive attention module, which mainly consists of a channel self-attention block and a spatial self-attention block, to replace the traditional convolution operation. Compared with the conventional convolution operation, the design of the hybrid adaptive attention module can help us capture more features under different receptive fields. Different from existing attention mechanisms, the hybrid adaptive attention module can guide the network to adaptively select more robust representation in channel and space dimensions to cope with more complex breast lesions segmentation. Extensive experiments with several state-of-the-art deep learning segmentation methods on three public breast ultrasound datasets show that our method has better performance on breast lesion segmentation. Furthermore, robustness analysis and external experiments demonstrate that our proposed AAU-net has better generalization performance on the segmentation of breast lesions. Moreover, the hybrid adaptive attention module can be flexibly applied to existing network frameworks.'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. xx, NO.  X, NOVEMBER 2022                      1 \\n  \\n  \\nAAU-net: An Adaptive Attention U-net for Breast \\nLesions Segmentation in Ultrasound Images \\n \\nGongping Chen, Lei Li, Yu Dai, Jianxun Zhang, and Moi Hoon Yap \\n \\nAbstractâ€”Various deep learning methods have been \\nproposed to segment breast lesions from ultrasound images. \\nHowever, similar intensity distributions, variable tumor \\nmorphologies and blurred boundaries present challenges for \\nbreast lesions segmentation, especially f or malignant tumors \\nwith irregular shapes. Considering the complexity of ultrasound \\nimages, we develop an adaptive attention U -net (AAU-net) to \\nsegment breast lesions automatically and stably from ultrasound \\nimages. Specifically, we introduce a hybrid adap tive attention \\nmodule (HAAM), which mainly consists of a channel self -\\nattention block and a spatial self -attention block, to replace the \\ntraditional convolution operation. Compared with the'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='module (HAAM), which mainly consists of a channel self -\\nattention block and a spatial self -attention block, to replace the \\ntraditional convolution operation. Compared with the \\nconventional convolution operation, the design of the hybrid \\nadaptive attention module can help us capture more features \\nunder different receptive fields. Different from existing attention \\nmechanisms, the HAAM module can guide the network to \\nadaptively select more robust representation in channel and \\nspace dimensions to cope with more complex breast lesions \\nsegmentation. Extensive experiments with several state -of-the-\\nart deep learning segmentation methods on three public breast \\nultrasound datasets show that our method has better \\nperformance on breast lesions segmentation . Furthermore, \\nrobustness analysis and external experiments demonstrate that \\nour proposed AAU-net has better generalization performance in \\nthe breast lesion segmentation. Moreover, the HAAM module'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content=\"robustness analysis and external experiments demonstrate that \\nour proposed AAU-net has better generalization performance in \\nthe breast lesion segmentation. Moreover, the HAAM module \\ncan be flexibly applied to existing network frameworks.  The \\nsource code is available on https://github.com/CGPxy/AAU-net \\n \\nIndex Terms â€”Ultrasound images, Breast tumors \\nsegmentation, Hybrid attention, Adaptive learning, Deep \\nlearning. \\nI. INTRODUCTION \\nreast cancer is a common female disease, which seriously \\nthreatens women's health and life [1], [2] . Therefore, \\nregular breast screening and diagnosis are very important to \\nformulate treatment plans and improve  survival rates. Due to \\nthe flexibility and convenience of ultrasound imaging , it has \\nbecome a convention modality for breast tumors screening  [3]. \\nThe segmentation of breast ultrasound (BUS) images can help \\nus characterize and localize breast tumors, which is one of the \\nkey steps in computer -aided diagnosis (CAD) [4]. In recent\"),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='The segmentation of breast ultrasound (BUS) images can help \\nus characterize and localize breast tumors, which is one of the \\nkey steps in computer -aided diagnosis (CAD) [4]. In recent \\nyears, m any deep learning methods have been proposed to \\nsegment breast lesions from ultrasound images [5]. However, \\ncomplex ultrasound patterns, similar intensity distributions,  \\n \\nThis work is supported by the National Natural Science Foundation of China Grant U1913207 and Grant 51875394. (Corresponding author: Yu \\nDai, e-mail: daiyu@nankai.edu.cn) \\nGongping Chen, Yu Dai and Jianxun Zhang are with the College of Artificial Intelligence, Nankai University , Tianjin, 300350 China (e-mail: \\ncgp110@mail.nankai.edu.cn, daiyu@nankai.edu.cn, zhangjx@nankai.edu.cn).  \\nLei Li is with Institute of Biomedical Engineering, University of Oxford, London, U.K. (e-mail: lei.li@eng.ox.ac.uk)'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='cgp110@mail.nankai.edu.cn, daiyu@nankai.edu.cn, zhangjx@nankai.edu.cn).  \\nLei Li is with Institute of Biomedical Engineering, University of Oxford, London, U.K. (e-mail: lei.li@eng.ox.ac.uk) \\nMoi Hoon Yap is with the School of Computing, Mathematics and Digital Technology, Manchester University, Manchester M1 5GD, U.K. (e-mail: \\nm.yap@mmu.ac.uk). \\nvariable tumor morpholog ies and blurred boundaries seriously \\ninterfere with the segmentation accuracy of breast lesions, and \\neven breast tumors cannot be detected, as shown in Fig. 1. \\n \\n     \\nFig. 1.  Various BUS images and the segmentation results by U-net \\nand our method. The red curve is the ground-truth boundary of the lesion. \\nThe yellow and green curves are the segmentation results of our method \\nand U-net, respectively. It can be seen from these images that tumor \\nmorphology, blurred borders and similar surrounding tissue (background) \\nseverely affect the segmentation accuracy of breast lesions, especially'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='morphology, blurred borders and similar surrounding tissue (background) \\nseverely affect the segmentation accuracy of breast lesions, especially \\nfor small and malignant tumors. \\n \\nThe powerful nonlinear learning ability makes full \\nconvolution network (FCN) and U -net have achieve d great \\nsuccess in medical images segmentation [2], [6] â€“[13]. \\nEnlightened by this, many deep learning methods are proposed \\nto segment breast lesions from ultrasound images [5], [14]â€“[17]. \\nIn 2018, Yap et al. are the first to systematically eval uate the \\nimpact of different FCN variants on breast lesions segmentation \\nand achieve segmentation results that outperform traditional \\nmethods [18]. In 2018, Almajalid et al.  improved the \\nsegmentation accuracy of U -net for breast lesions by contrast \\nenhancement and speckle noise removal strategy  [19]. These \\npreprocessing operations can improve network performance, \\nbut they destroy the original spatial feature distribution of'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='enhancement and speckle noise removal strategy  [19]. These \\npreprocessing operations can improve network performance, \\nbut they destroy the original spatial feature distribution of \\nobjects. Moreover, Ning et al.  pointed out that  the complex \\nultrasound pattern and the i nterference of surrounding tissue \\nmake it difficult for these simple frameworks to achieve ideal \\nsegmentation results on BUS images [20], as shown in Fig. 1. \\nTo further refine the segmentation results of BUS lesions, \\ntwo optimization strategies: enlarging the receptive  field [10], \\n[21]â€“[23] and the attention mechanism [24]â€“[27] have been \\nwidely used. The dilated convolution operation is a commonly \\nused strategy to expand the receptive field  [28]â€“[30]. For \\nexample, Hu et al. obtained the large receptive field of breast \\ntumors by using dilated convolutions in deeper network  layers \\nB \\nNo Detected 2 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. xx, NO. x, 2022'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='tumors by using dilated convolutions in deeper network  layers \\nB \\nNo Detected 2 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. xx, NO. x, 2022 \\n \\n[30]. However, Xue et al. point out that deeper convolutional \\nlayers tend to focus more on the extraction of local features, and \\nit is difficult to obtain the true global view using dilated \\nconvolutional operations on deeper convolutional layers  [27]. \\nIrfan et al.  developed an end -to-end semantic segmentation \\nnetwork using dilated convolution operations to segment breast \\ntumors from the BUS image [28]. Cao et al. integrated a set of \\nhybrid dilated convolutions into D 2U-Net to alleviate the \\nchallenges posed by different lesion sizes and shapes  [29]. \\nHowever, merely using dilated convolution operations to obtain \\na larger receptive field cannot  fully cope with the perturbation \\ncaused by surrounding tissues and blurred boundaries [31].  \\nIn terms of attention mechanism,  Lee et al.  proposed a'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='a larger receptive field cannot  fully cope with the perturbation \\ncaused by surrounding tissues and blurred boundaries [31].  \\nIn terms of attention mechanism,  Lee et al.  proposed a \\nchannel attention module to further improve the performance of \\nU-net for breast lesion s segmentation [26]. Abraham et al.  \\nutilized an optimized attention U -net, which includes three \\nmodules of multi -scale input s, attention U-net and deep \\nsupervision, to segment breast tumors from ultrasound images  \\n[24]. Since U-net uses \\n33\\uf0b4  convolution operations , the \\nattention mechanism  can only  be performed on the fixed \\nreceptive field, which limits the  segmentation performance of \\nthe U-net variant network [32]. Inspired by the work of Li et al. \\n[32], Byra et al . const ructed U -net with selected kernel \\nconvolution (SKNet) to segment breast lesions  [31]. However, \\nthe original selection kernel convolution ignores feature \\nselection under different receptive fields in spatial dimensions.'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='convolution (SKNet) to segment breast lesions  [31]. However, \\nthe original selection kernel convolution ignores feature \\nselection under different receptive fields in spatial dimensions. \\nRecently, Xue et al. developed a global guidance network for \\nbreast lesions segmentation by integrating a channel attention \\nmodule, a spatial attention module, and a boundary detection \\nmodule [27]. Huang et al. proposed a novel boundary-rendering \\nframework to further refine the contours of breast lesions [2]. \\nThese two methods further alleviate the interference of various \\nfactors in the segmentation accuracy of breast lesions. \\nTo obtain the segmentation results of breast lesions closer to \\nthe ground-truth mask, some segmentation networks \\nintegrating dilated convolution and attention mechanisms are \\nproposed [33], [34]. Yan et al. proposed an attention enhanced \\nU-net (AE U -net) with hybrid dilated convolution to segment \\nbreast tumors from ultrasound images  [33]. Zhuang et al.'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='proposed [33], [34]. Yan et al. proposed an attention enhanced \\nU-net (AE U -net) with hybrid dilated convolution to segment \\nbreast tumors from ultrasound images  [33]. Zhuang et al. \\ndeveloped a residual dilated attention -gate U-net (RDAU-Net) \\nto segment breast lesions by introducing dilated convolution \\nand residual learning strategies on attention U -net [34]. \\nAlthough the performance of segmentation networks can be \\noptimized by integrating dilated convolution and attention \\nmechanisms, there are still some shortcomings (fixed receptive \\nfield size s and single -attention operations ) that need to be \\nfurther overcome.  \\nTo alleviate the above challenge, we design a hybrid adaptive \\nmodule to aggregate information from multiple kernels to \\nachieve adaptive receptive field size of neurons. The designed \\nhybrid adaptive module mainly consists of three parts: \\nconvolutional la yers with different kernel sizes, channel self -'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='achieve adaptive receptive field size of neurons. The designed \\nhybrid adaptive module mainly consists of three parts: \\nconvolutional la yers with different kernel sizes, channel self -\\nattention block, and spatial self-attention block. Different from \\nexisting methods, the hybrid adaptive module can adaptively \\nselect receptive fields with different scales from the channel and \\nspatial dimensions. Subsequently, we  use the hybrid adaptive \\nmodule to propose a novel adaptive attention U-net to improve \\nbreast lesions segmentation by learning generic representations \\nfrom BUS images. Compared with conventional convolution \\noperations with fixed receptive fields, the hybrid adaptive \\nattention module can help the network select more robust \\nrepresentations from multiple perspectives . Extensive \\nexperiments demonstrate that our adaptive attention U -net \\nbrings significant and consistent improvements in breast lesions \\nsegmentation. Our main contributions are as follows:'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='experiments demonstrate that our adaptive attention U -net \\nbrings significant and consistent improvements in breast lesions \\nsegmentation. Our main contributions are as follows: \\nâš« We design a novel hybrid adaptive attention module, \\nwhich can adaptively select receptive fields with different \\nscales from the channel and spatial dimensions. \\nâš« A novel adaptive attention U -net is developed to segment \\nbreast lesions  from ultrasound images . The network can \\nimprove breast lesions segmentation by learning generic \\nrepresentations from BUS images. \\nâš« Extensive experiments on three public BUS datase ts \\ndemonstrate that our approach consistently improves the \\nsegmentation accuracy of breast lesions, outperforming \\nthe strong baseline and the state -of-the-art medical image \\nsegmentation methods. \\nIn the remainders of this paper, we introduce our \\nsegmentation network  and loss function  in Section II. The \\npublic ultrasound datasets, experimental settings and evaluation'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='In the remainders of this paper, we introduce our \\nsegmentation network  and loss function  in Section II. The \\npublic ultrasound datasets, experimental settings and evaluation \\nindicators are elaborated in Section III. The experimental \\nresults are presented in Section IV. Finally, our discussion and \\nconclusions are given in Sections V and VI, respectively. \\nII. METHODOLOGY \\nThe Fig. 2 is an illustration  of our developed adaptive \\nattention U-net (AAU-net) for breast lesions segmentation. Our \\nAAU-net has the same core architecture as U -net [10] mainly \\nincluding four down-sampling, four up-sampling and four skip-\\nconnections. The difference is we propose a hybrid adaptive \\nattention module (HAAM) to replace the original convolution \\nlayer to better adapt to the segmentation of breast lesions. As \\nshown in Fig. 2, each encoding or decoding stage consists of \\ntwo HAAMs. Convolutional  layers with different kernel sizes \\nin HAAM can provide receptive fields with different scales ,'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='shown in Fig. 2, each encoding or decoding stage consists of \\ntwo HAAMs. Convolutional  layers with different kernel sizes \\nin HAAM can provide receptive fields with different scales , \\nwhich can improve the adaptability of the network to different \\ninput images . Moreover, AAU-net can learn more robust \\nrepresentations from BUS images through cha nnel dimension \\nand spatial dimension constraints. \\nA. Hybrid Adaptive Attention Module (HAAM) \\nThe designed HAAM mainly consists of three parts: \\nconvolutional layers with different kernel sizes, channel self -\\nattention block, and spatial self -attention block.  Specifically, \\nthe input HAAM feature map s are processed by three parallel \\nconvolutional layers to obtain three feature maps with different \\nreceptive fields.  The three convolutional layers are \\n33\\uf0b4  \\nconvolution, \\n55\\uf0b4  convolution, and  \\n33\\uf0b4  dilated convolution \\nwith a dilation rate of \\n3 . The feature map captured by three \\nconvolution layers can be represented as: \\n \\n3'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='33\\uf0b4  \\nconvolution, \\n55\\uf0b4  convolution, and  \\n33\\uf0b4  dilated convolution \\nwith a dilation rate of \\n3 . The feature map captured by three \\nconvolution layers can be represented as: \\n \\n3\\n33 inputF W F \\uf0b4=\\uf0b4\\n                                   (1) 3 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. xx, NO. x, 2022 \\n \\n \\nFig. 2.  The description of the adaptive attention U-net (AAU-net). The network is still a U-shaped network including four down-sampling and four \\nup-sampling operations. Each stage consists of two hybrid adaptive attention modules (HAAM). \\n \\n5\\n55 inputF W F \\uf0b4=\\uf0d7\\n                              (2) \\n \\n33\\nDD\\ninputF W F \\uf0b4=\\uf0d7\\n                               (3) \\n \\nwhere \\nc h w\\ninputF\\n\\uf0a2\\uf0b4\\uf0b4\\uf0ce\\n  represents the input feature map, \\n33W \\uf0b4  \\nand \\n55W \\uf0b4  denote the matrix of \\n33\\uf0b4  convolution and \\n55\\uf0b4  \\nconvolution, respectively. \\n33\\nDW \\uf0b4  represents the matrix of dilated \\nconvolution. \\n3 c h wF \\uf0b4\\uf0b4\\uf0ce\\n  and \\n5 c h wF \\uf0b4\\uf0b4\\uf0ce\\n  denote the feature \\nmap captured by \\n33\\uf0b4  convolution layer and \\n55\\uf0b4  convolution'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='33\\nDW \\uf0b4  represents the matrix of dilated \\nconvolution. \\n3 c h wF \\uf0b4\\uf0b4\\uf0ce\\n  and \\n5 c h wF \\uf0b4\\uf0b4\\uf0ce\\n  denote the feature \\nmap captured by \\n33\\uf0b4  convolution layer and \\n55\\uf0b4  convolution \\nlayer, respectively. \\nD c h wF \\uf0b4\\uf0b4\\uf0ce\\n  indicates the feature map \\ncaptured by dilated convolution layer. The receptive field sizes \\ncaptured by the three convolutional layers  are shown in Fig. 3. \\nSubsequently, these feature maps are int egrated into channel \\nself-attention block and spatial self -attention block.  It can be \\nseen from Fig . 3 that capturing receptive fields with different \\nscales from input images can not only improve the adaptability \\nof the network to different inputs, but also better characterize \\nBUS images. \\n \\n \\nFig. 3. Receptive fields captured by three convolutional layers . We \\ncan see that the receptive field obtained by \\n55\\uf0b4  convolution layer is \\nequivalent to 2 convolution layers with kernel size is \\n33\\uf0b4 , and the'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='can see that the receptive field obtained by \\n55\\uf0b4  convolution layer is \\nequivalent to 2 convolution layers with kernel size is \\n33\\uf0b4 , and the \\nreceptive field of dilated convolutions is the same as that of 5 convolution \\nlayers with kernel size is \\n33\\uf0b4 . \\nB. Channel Self-Attention Block \\nTo capture useful objective features from different receptive \\nfields, we first develop a channel self -attention module to \\nadaptively guide the network to learn more robust feature \\nrepresentations. Fig. 2 (a) illustrates the channel self-attention \\nblock, which purpose is to guide the  segmentation network to \\nselect more representative features from the channel dimension. \\nSpecifically, we first compress the co mbined feature maps of \\n5 c h wF \\uf0b4\\uf0b4\\uf0ce\\n and \\nD c h wF \\uf0b4\\uf0b4\\uf0ce\\n  into a new feature map  \\n2 1 1GcF \\uf0b4\\uf0b4\\uf0ce\\n of size  \\n11\\uf0b4  through a global average pooling \\n(GAP) operation. The obtained feature can be expressed as: \\n \\n5()GDF GAP F F=\\uf0c5\\n                           (4) \\n \\nwhere \\n5F  and'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='2 1 1GcF \\uf0b4\\uf0b4\\uf0ce\\n of size  \\n11\\uf0b4  through a global average pooling \\n(GAP) operation. The obtained feature can be expressed as: \\n \\n5()GDF GAP F F=\\uf0c5\\n                           (4) \\n \\nwhere \\n5F  and \\nDF  denote the feature map captured by \\n55\\uf0b4  \\nconvolution layer and dilated  convolution layer, respectively. \\n\\uf0c5\\n represents the element-wise addition. Then, the feature map \\nGF\\n is input to a fully connected layer followed  by a batch-\\nnormalization layer and a ReLU layer to produce a new feature \\nmap. The obtained feature can be expressed as: \\n \\n( ( ))GG\\nf r fcF B W F\\uf073=\\uf0d7\\n                           (5) \\n \\nwhere \\nfcW  represents the matrix of fully connected layers . \\n()B \\uf0d7\\n and \\n()r\\uf073 \\uf0d7  denote batch -normalization and ReLU \\nactivation operations, respectively. The feature map \\nG\\nfF  again \\nundergoes a fully connected operation to obtain a new feature \\nmap, \\n \\nGG\\nf fc fF W F\\n\\uf0a2\\n=\\uf0d7\\n                               (6) \\n4 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. xx, NO. x, 2022'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='undergoes a fully connected operation to obtain a new feature \\nmap, \\n \\nGG\\nf fc fF W F\\n\\uf0a2\\n=\\uf0d7\\n                               (6) \\n4 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. xx, NO. x, 2022 \\n \\nFinally, the feature map \\nG\\nfF\\n\\uf0a2  is performed sigmoid activation to \\nobtain the channel attention map: \\n \\n() G\\nsf F\\uf061\\uf073\\n\\uf0a2\\n=\\n                                   (7) \\n \\nWe let \\n11[0,1]c\\uf061 \\uf0b4\\uf0b4\\uf0ce  and \\n11[0,1]c\\uf061 \\uf0b4\\uf0b4\\uf0a2\\uf0ce  represent the channel \\nattention maps of \\nDF  and \\n5F , respectively. Each value of \\n\\uf061\\uf061 \\uf0a2\\uf02f\\n indicates the importance of channel information at the \\ncorresponding voxel in  \\n5DFF \\uf02f . It is worth noting that  \\n\\uf061\\uf0a2  is \\nderived from \\n\\uf061 , and its value is  \\n1 \\uf061âˆ’ . These two channel \\nattention maps can help us adaptively extract more \\nrepresentative feature maps from receptive fields with different \\nscales. In order to achieve automatic feature selection , we use \\nthe channel attention map \\n\\uf061  to calibrate the feature map \\nDF , \\nand the channel attention map \\n\\uf061\\uf0a2  to calibrate the feature map \\n5F'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='scales. In order to achieve automatic feature selection , we use \\nthe channel attention map \\n\\uf061  to calibrate the feature map \\nDF , \\nand the channel attention map \\n\\uf061\\uf0a2  to calibrate the feature map \\n5F\\n. The feature map after channel attention map calibration can \\nbe expressed as: \\n \\nDD\\nCFF \\uf061=\\uf0c4\\n                                  (8) \\n \\n55\\nCFF \\uf061\\uf0a2=\\uf0c4\\n                                   (9) \\n \\nSubsequently, the feature maps \\nD c h w\\nCF \\uf0b4\\uf0b4\\uf0ce\\n  and \\n5 c h w\\nCF \\uf0b4\\uf0b4\\uf0ce\\n  \\nare integrated and used as the input of the next stage. \\nC. Spatial Self-Attention Block \\nAs we all know, the channel attention mechanism focuses on \\nthe category of the feature, and the spatial attention mechanism \\nfocuses on the location of the feature  [35]. To further improve \\nthe robustness of network representation features, we develop a \\nnovel spatial self -attention block  as shown in Fig. 2 (b). The \\nfeature maps obtained by the \\n33\\uf0b4  convolution layer and the'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='the robustness of network representation features, we develop a \\nnovel spatial self -attention block  as shown in Fig. 2 (b). The \\nfeature maps obtained by the \\n33\\uf0b4  convolution layer and the \\nchannel self-attention block are used as the input of the spatial  \\nself-attention block.  To refine the location information of the \\nobjective, we first perform a \\n11\\uf0b4  convolution operation on the \\ninput feature map. The feature map after \\n11\\uf0b4  convolution can \\nbe defined as: \\n \\n13\\n11\\nSF W F \\uf0b4=\\uf0d7\\n                              (10) \\n \\n15\\n11 ()SD\\nC C CF W F F\\uf0b4= \\uf0d7 \\uf0c5\\n                        (11) \\n \\nwhere \\nD\\nCF  and \\n5\\nCF  denote the output of the channel self-\\nattention block, \\n3F  represents the feature map obtained by \\nperforming \\n33\\uf0b4  convolution operations on the input of \\nHAAM. Subsequently, the  feature map s fused  with \\n1SF  and \\n1S\\nCF\\n undergo a ReLU activation \\n()r\\uf073 \\uf0d7 , a \\n11\\uf0b4  convolution \\noperation and a sigmoid activation \\n()\\uf073 \\uf0d7 , to obtain the spatial \\nattention map: \\n \\n11\\n11( ( )) SS'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='1SF  and \\n1S\\nCF\\n undergo a ReLU activation \\n()r\\uf073 \\uf0d7 , a \\n11\\uf0b4  convolution \\noperation and a sigmoid activation \\n()\\uf073 \\uf0d7 , to obtain the spatial \\nattention map: \\n \\n11\\n11( ( )) SS\\nrCW F F\\uf062 \\uf073 \\uf073 \\uf0b4= \\uf0d7 \\uf0c5\\n                    (12) \\nWe let \\n[0,1]\\uf062 \\uf0ce  and \\n[0,1]\\uf062 \\uf0a2\\uf0ce  represent the spatial attention \\nmaps of \\n1S\\nCF  and \\n1SF , respectively. It is worth noting that the \\nvalue of \\n\\uf062 \\uf0a2  is \\n1 \\uf062âˆ’ . Each value of \\n/\\uf062\\uf062 \\uf0a2  indicates the \\nimportance of spatial information at the corresponding voxel in \\n11/SS\\nCFF\\n. To perform calibration on  \\n1S\\nCF , \\n\\uf062  is resampled to \\nobtain a spatial attention map with the same number of channels \\nas \\n1S\\nCF . Similarly, the resample operation is performed for \\n\\uf062 \\uf0a2 . \\nThe feature maps calibrated by \\n\\uf062  and \\n\\uf062 \\uf0a2  can be denoted as \\n1S\\nCF\\n\\uf0a2\\n and \\n1SF\\n\\uf0a2 , respectively . Finally, the output of the spatial \\nself-attention block is obtained after the connected  \\n1S\\nCF\\n\\uf0a2  and \\n1SF\\n\\uf0a2\\n are subjected to the convolution operation. \\n \\n11\\n11 () SS\\nout CF W F F\\n\\uf0a2\\uf0a2\\n\\uf0b4= \\uf0d7 \\uf0c5'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='self-attention block is obtained after the connected  \\n1S\\nCF\\n\\uf0a2  and \\n1SF\\n\\uf0a2\\n are subjected to the convolution operation. \\n \\n11\\n11 () SS\\nout CF W F F\\n\\uf0a2\\uf0a2\\n\\uf0b4= \\uf0d7 \\uf0c5\\n                     (13) \\n \\noutF\\n is also the output of the entire hybrid adaptive attention \\nmodule (HAAM). \\nD. Loss Function \\nBinary cross entropy (BCE) [36] is one of the widely used \\nloss functions in two -class image segmentation tasks, which \\nreflects the direct difference between predicted masks and \\nground-truth labels. Its definition can be expressed as: \\n \\n( , )\\nË† Ë†( , ) log ( , ) (1 ( , )) log(1 ( , )BCE\\nij\\nY i j Y i j Y i j Y i j= âˆ’ \\uf0d7 + âˆ’ \\uf0d7 âˆ’\\uf0e5\\n(14) \\n \\nwhere \\n( , ) [0,1]Y i j \\uf0ce  denotes the ground-truth label of the pixel \\n( , )ij\\n, \\nË†( , ) [0,1]Y i j \\uf0ce  represents the predict masks. In this study, \\nwe use BCE loss for the training of the network. \\n \\nTABLE I  \\nSAMPLE DISTRIBUTION OF THE THREE PUBLIC BUS DATASET. \\n Benign Malignant Normal Total Cross-\\nvalidation \\nExternal-\\nvalidation \\nBUSI 437 210 133 780 True False'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='TABLE I  \\nSAMPLE DISTRIBUTION OF THE THREE PUBLIC BUS DATASET. \\n Benign Malignant Normal Total Cross-\\nvalidation \\nExternal-\\nvalidation \\nBUSI 437 210 133 780 True False \\nDataset B 110 53 No 163 True False \\nSTU Unknow Unknow No 42 False True \\nIII. DATASETS AND EXPERIMENTAL SETTINGS \\nA. BUS Datasets \\nIn this paper, three  widely used  public BUS datasets with \\ndifferent scales are used to evaluate the segmentation network \\nperformance. Table I describes the sample distribution of these \\nthree public BUS datasets. The first BUS dataset (denotes as \\nBUSI) is constructed by Al-Dhabyani et al. [37]. The dataset \\ncontains 780 images acquired by two types of ultrasound \\nequipment (LOGIQ E9 ultrasound and LOGIQ E9 Agile \\nultrasound system) in the Baheya Hospital. The average image \\nsize of t hese images is \\n500 500\\uf0b4  pixels. The second BUS \\ndataset used in this paper named Dataset B is collected by Yap \\net al. [38]. Dataset B contains 163 images with average image \\nsize of'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='size of t hese images is \\n500 500\\uf0b4  pixels. The second BUS \\ndataset used in this paper named Dataset B is collected by Yap \\net al. [38]. Dataset B contains 163 images with average image \\nsize of \\n760 570\\uf0b4  pixels collected by Siemens ACUSON  \\nSequoia C512 system. The third public BUS dataset is the STU  5 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. xx, NO. x, 2022 \\n \\nTABLE II  \\nTHE SEGMENTATION RESULTS (MEAN Â± STD) OF DIFFERENT NETWORK COMPONENTS ON BUSI AND DATASET B. RED ARROWS REPRESENT INCREASES. THE BEST \\nRESULTS ARE MARKED WITH BOLD TEXTS. \\n Jaccard Precision Recall Specificity Dice \\nBUSI \\nBaseline U-net 60.70\\uf0b12.36 71.88\\uf0b12.41 76.30\\uf0b12.48 96.18\\uf0b10.55 70.10\\uf0b12.20 \\nU-net with channel self-attention block 62.43\\uf0b11.95â†‘ 74.63\\uf0b11.21â†‘ 75.64\\uf0b11.83â†‘ 97.13\\uf0b10.85â†‘ 72.06\\uf0b11.04â†‘ \\nU-net with spatial self-attention block 65.12\\uf0b11.10â†‘ 76.11\\uf0b11.43â†‘ 78.02\\uf0b11.21â†‘ 97.45\\uf0b10.96â†‘ 75.86\\uf0b11.07â†‘ \\nU-net with HAAM (Ours) 68.82\\uf0b10.44â†‘ 79.61\\uf0b11.07â†‘ 80.10\\uf0b10.52â†‘ 97.57\\uf0b10.24â†‘ 77.51\\uf0b10.68â†‘ \\nDataset B'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='U-net with spatial self-attention block 65.12\\uf0b11.10â†‘ 76.11\\uf0b11.43â†‘ 78.02\\uf0b11.21â†‘ 97.45\\uf0b10.96â†‘ 75.86\\uf0b11.07â†‘ \\nU-net with HAAM (Ours) 68.82\\uf0b10.44â†‘ 79.61\\uf0b11.07â†‘ 80.10\\uf0b10.52â†‘ 97.57\\uf0b10.24â†‘ 77.51\\uf0b10.68â†‘ \\nDataset B \\nBaseline U-net 58.44\\uf0b14.26 70.27\\uf0b16.11 75.32\\uf0b12.85 98.44\\uf0b10.40 68.20\\uf0b14.23 \\nU-net with channel self-attention block 62.76\\uf0b13.60â†‘ 74.28\\uf0b14.56â†‘ 78.45\\uf0b14.11â†‘ 98.74\\uf0b10.28â†‘ 72.37\\uf0b13.29â†‘ \\nU-net with spatial self-attention block 66.94\\uf0b12.26â†‘ 76.33\\uf0b12.61â†‘ 80.97\\uf0b14.03â†‘ 98.75\\uf0b10.39â†‘ 75.77\\uf0b11.84â†‘ \\nU-net with HAAM (Ours) 69.10\\uf0b12.98â†‘ 78.83\\uf0b12.40â†‘ 82.22\\uf0b13.84â†‘ 98.82\\uf0b10.35â†‘ 78.14\\uf0b12.41â†‘ \\n \\nprovided by Zhuang et al.  [34]. The STU contains 42 BUS \\nimages with average image size of \\n128 128\\uf0b4  pixels acquired by \\nthe Imaging Department of the First Affiliated Hospital of \\nShantou University using the GE Voluson E10 ultrasonic \\ndiagnostic system.  Since the STU dataset contains too few \\nimages, it is only used as external validation data to evaluate the \\ngeneralization performance of the segmentation network.'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='diagnostic system.  Since the STU dataset contains too few \\nimages, it is only used as external validation data to evaluate the \\ngeneralization performance of the segmentation network. \\nB. Experimental Settings \\nTo fully verify the effectiveness and robustness of our method, \\nwe use three datasets to conduct extensive experiments, such as \\nablation study, comparison with state -of-the-art segmentation \\nmethods, and robustness analysis. Our ablation studies mainly \\nconsist of component ablation and paramet er ablation. In the \\nablation study, we perform four -fold cross-validation on BUSI \\nand Dataset B, respectively. In comparative experiments with \\nstate-of-the-art segmentation methods , we perform four -fold \\ncross-validation on BUSI  and Dataset B, respectively.  The \\nrobustness analysis mainly includes four parts: the robustness \\non benign and malignant lesions  segmentation, the externa l \\nvalidation, the comparison on BUSI with normal images  and'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='robustness analysis mainly includes four parts: the robustness \\non benign and malignant lesions  segmentation, the externa l \\nvalidation, the comparison on BUSI with normal images  and \\nthe comparison with different attention-based methods. Dataset \\nB and STU contain too few malignant lesions, so we choose \\nBUSI for robustness analysis of malignant lesions segmentation. \\nSimilarly, benign lesions in BUSI are selected to evaluate the \\nrobustness of different networks for segmenting benign lesions. \\nDepending on the number of samples,  we perform four -fold \\ncross-validation on benign images and three -fold cross -\\nvalidation on mal ignant images.  In the external validation \\nexperiments, the STU dataset is used as test data to evaluate the \\nsegmentation performance of each method after training on \\nDataset B.  We train each  segmentation method by four -fold \\ncross-validation on BUSI with no rmal images to evaluate the \\nimpact of normal ultrasound images in the breast lesion'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='Dataset B.  We train each  segmentation method by four -fold \\ncross-validation on BUSI with no rmal images to evaluate the \\nimpact of normal ultrasound images in the breast lesion  \\nsegmentation. In the comparative experiment with attention -\\nbased methods, we perform four-fold cross-validation on BUSI \\nand Dataset B, respectively.  During the training pro cess, the \\ntraining data and test data of each fold do not have any overlap.  \\nWe choose Adam optimizer to train our network. The initial \\nlearning rate of our network is 0.001. Multiple cross-validation \\nshow that the best segmentation performance is obtained when \\nepoch size and batch size are set to 50 and 12, respectively. Our \\nexperimental device is a PC with two NVIDIA RTX 3090 \\nGPUs. The development environment is Ubuntu 20.04, python \\n3.6 and TensorFlow 2.6.0.  \\nC. Evaluation Metrics \\nTo quantitatively evaluate the segmentation performance of \\ndifferent methods on breast lesions, we use nine widely used'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='3.6 and TensorFlow 2.6.0.  \\nC. Evaluation Metrics \\nTo quantitatively evaluate the segmentation performance of \\ndifferent methods on breast lesions, we use nine widely used \\nsegmentation metrics. For a detailed description of the nine \\nevaluation indicators of Jaccard, Precision, Recall, Specificity , \\nDice, AUC, Hausdorff distance (HD) , average boundary \\ndistance (ABD)  and average symmetric surface distance \\n(ASSD), please refer to  [27], [39] . Due to the complexity of \\nultrasound patterns, existing deep learning s egmentation \\nmethods are prone to fail to detect objective regions on \\nindividual images , as shown in Fig. 1 and Fig. 5 . It is well \\nknown that boundary-based metrics cannot fairly evaluate these \\nsegmentation-failed images. To ensure the absolute fairness of \\nthe comparison , the three metrics (HD, ABD and ASSD) are \\nonly used in external validation experiments. \\nIV. EXPERIMENTAL RESULTS \\nIn this section , we first  conduct the ablation study on the'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='the comparison , the three metrics (HD, ABD and ASSD) are \\nonly used in external validation experiments. \\nIV. EXPERIMENTAL RESULTS \\nIn this section , we first  conduct the ablation study on the \\ncomponents and parameters of our network. Then, we compare \\nour method  with state -of-the-art deep learning segmentation \\nmethods. Finally, the robustness of our network is analyzed. \\nA. Ablation Study \\n1) Architecture Ablation \\nTo evaluate the performance of different network \\ncomponents, we perform ablation experimen ts on BUSI and \\nDataset B. In the ablation experiments, U -net is used as the \\nbenchmark network and four-fold cross-validation is performed \\non BUSI and Dataset B, respectively. Table II shows the \\nexperimental results of different components on BUSI and \\nDataset B. The results of ablation experiments indicate that \\nthese network components designed in this paper all play a role \\nin improving network performance.  From Table II, we can see'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='Dataset B. The results of ablation experiments indicate that \\nthese network components designed in this paper all play a role \\nin improving network performance.  From Table II, we can see \\nthat the hybrid adaptive attention module (HAAM) enables the \\nnetwork to achieve the best segmentation results on BUSI and \\nDataset B. This suggests that integrating the constraints of \\nchannel self-attention block and spatial self-attention block can \\nhelp the network learn more robust representations from BUS \\nimages. \\n2) Parameter Ablation \\nTo further evaluate the perturbation of receptive field size on \\nsegmentation performance, we analyze the impact of smaller \\nand larger receptive fields on segmentation results. To generate 6 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. xx, NO. x, 2022 \\n \\nTABLE III  \\nTHE SEGMENTATION RESULTS (MEAN Â± STD) OF DIFFERENT PARAMETERS ON BUSI AND DATASET B. THE BEST RESULTS ARE MARKED WITH BOLD TEXTS.'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='TABLE III  \\nTHE SEGMENTATION RESULTS (MEAN Â± STD) OF DIFFERENT PARAMETERS ON BUSI AND DATASET B. THE BEST RESULTS ARE MARKED WITH BOLD TEXTS. \\n Kernel size and Dilation rate Jaccard Precision Recall Specificity Dice \\nBUSI \\n33\\uf0b4\\n;\\n33\\uf0b4 ;\\n3 3, r\\uf0b4 = \\uf032  (the smaller receptive fields) 68.15\\uf0b11.02 79.02\\uf0b11.69 80.59\\uf0b10.83 97.17\\uf0b10.93 77.05\\uf0b10.95 \\n33\\uf0b4\\n;\\n55\\uf0b4 ;\\n3 3 r=3\\uf0b4 ï¼Œ  (Ours) 68.82\\uf0b10.44 79.61\\uf0b11.07 81.10\\uf0b10.52 97.57\\uf0b10.24 77.51\\uf0b10.68 \\n55\\uf0b4\\n;\\n55\\uf0b4 ;\\n3 3 r=3\\uf0b4 ï¼Œ  (the larger receptive fields) 68.26\\uf0b10.88 79.18\\uf0b11.62 80.66\\uf0b10.97 97.28\\uf0b10.66 77.13\\uf0b10.98 \\nDataset \\nB \\n33\\uf0b4\\n;\\n33\\uf0b4 ;\\n3 3 r=2\\uf0b4 ï¼Œ  (the smaller receptive fields) 68.32\\uf0b13.93 78.17\\uf0b13.26 81.64\\uf0b14.13 98.33\\uf0b10.35 77.52\\uf0b12.49 \\n33\\uf0b4\\n;\\n55\\uf0b4 ;\\n3 3 r=3\\uf0b4 ï¼Œ  (Ours) 69.10\\uf0b12.98 78.83\\uf0b12.40 82.22\\uf0b13.84 98.82\\uf0b10.35 78.14\\uf0b12.41 \\n55\\uf0b4\\n;\\n55\\uf0b4 ;\\n3 3 r=3\\uf0b4 ï¼Œ  (the larger receptive fields) 68.53\\uf0b12.43 78.45\\uf0b12.57 81.75\\uf0b13.96 98.49\\uf0b10.37 77.68\\uf0b12.01 \\n \\nTABLE IV \\nTHE SEGMENTATION RESULTS (MEAN Â± STD) OF DIFFERENT COMPETING METHODS ON BUSI AND DATASET B. THE BEST RESULTS ARE MARKED WITH BOLD'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='TABLE IV \\nTHE SEGMENTATION RESULTS (MEAN Â± STD) OF DIFFERENT COMPETING METHODS ON BUSI AND DATASET B. THE BEST RESULTS ARE MARKED WITH BOLD \\nTEXTS. ASTERISKS INDICATE THAT THE DIFFERENCE BETWEEN OUR METHOD AND THE COMPETING METHOD IS SIGNIFICANT USING A PAIRED STUDENTâ€™S T-TEST. \\n(*:\\n0.05p \\uf03c  ). \\n Method U-net Att U-net RDAU-Net U-net++ Abraham et al. U-net3+ SegNet AE U-net SKNet Ours \\nBUSI  \\n Jaccard 60.70\\uf0b12.36 57.09\\uf0b11.22 63.75\\uf0b13.36 61.38\\uf0b11.73 61.62\\uf0b12.69 63.03\\uf0b12.79 67.31\\uf0b11.87 64.57\\uf0b12.91 68.10\\uf0b11.63* 68.82\\uf0b10.44 \\nPrecision 71.88\\uf0b12.41 78.78\\uf0b14.67 71.25\\uf0b14.11 79.68\\uf0b13.07 73.77\\uf0b12.90 71.89\\uf0b13.28 76.09\\uf0b12.00 74.44\\uf0b13.74 78.62\\uf0b11.66* 79.61\\uf0b11.07 \\nRecall 76.30\\uf0b12.48 66.97\\uf0b14.08 78.90\\uf0b11.35 71.44\\uf0b12.77 76.87\\uf0b12.58 80.58\\uf0b12.48 80.85\\uf0b11.03* 79.00\\uf0b12.11 79.53\\uf0b11.93 81.10\\uf0b10.52 \\nSpecificity 96.18\\uf0b10.55 96.87\\uf0b10.83 96.63\\uf0b10.76 97.04\\uf0b10.54 96.40\\uf0b10.62 96.19\\uf0b10.68 96.99\\uf0b10.53 96.80\\uf0b10.54 97.33\\uf0b10.45* 97.57\\uf0b10.24'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='Specificity 96.18\\uf0b10.55 96.87\\uf0b10.83 96.63\\uf0b10.76 97.04\\uf0b10.54 96.40\\uf0b10.62 96.19\\uf0b10.68 96.99\\uf0b10.53 96.80\\uf0b10.54 97.33\\uf0b10.45* 97.57\\uf0b10.24 \\nDice 70.10\\uf0b12.20 67.99\\uf0b11.18 71.94\\uf0b13.46 71.58\\uf0b12.09 71.35\\uf0b12.67 71.85\\uf0b12.73 75.64\\uf0b11.80 73.47\\uf0b13.03 76.92\\uf0b11.57* 77.51\\uf0b10.68 \\nDataset B \\nJaccard 58.44\\uf0b14.26 59.93\\uf0b14.53 58.17\\uf0b14.91 61.19\\uf0b15.86 63.09\\uf0b13.04 65.63\\uf0b15.26* 62.83\\uf0b12.20 62.37\\uf0b12.16 64.25\\uf0b14.01 69.10\\uf0b12.98 \\nPrecision 70.27\\uf0b16.11 70.40\\uf0b16.05 70.49\\uf0b14.26 68.32\\uf0b15.73 73.70\\uf0b15.08 73.50\\uf0b16.21 71.72\\uf0b11.70 72.27\\uf0b11.91 75.27\\uf0b16.70* 78.83\\uf0b12.40 \\nRecall 75.32\\uf0b12.85 76.15\\uf0b14.21 73.55\\uf0b15.28 79.64\\uf0b13.84 79.24\\uf0b11.72 80.29\\uf0b13.93* 80.15\\uf0b13.90 78.97\\uf0b12.29 79.36\\uf0b12.50 82.22\\uf0b13.84 \\nSpecificity 98.44\\uf0b10.40 98.43\\uf0b10.33 98.37\\uf0b10.39 98.44\\uf0b10.41 98.61\\uf0b10.36 98.60\\uf0b10.36 98.59\\uf0b10.30 98.67\\uf0b10.28 98.68\\uf0b10.39* 98.82\\uf0b10.35 \\nDice 68.20\\uf0b14.23 69.30\\uf0b14.07 68.22\\uf0b14.94 69.77\\uf0b15.30 72.32\\uf0b13.14 73.98\\uf0b14.72* 72.16\\uf0b11.52 72.23\\uf0b12.14 73.53\\uf0b14.05 78.14\\uf0b12.41 \\n \\nthe smaller receptive field,  we first replace the \\n55\\uf0b4  \\nconvolution with the \\n33\\uf0b4  convolution, and then reduce th e \\ndilation rate of the'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='the smaller receptive field,  we first replace the \\n55\\uf0b4  \\nconvolution with the \\n33\\uf0b4  convolution, and then reduce th e \\ndilation rate of the \\n33\\uf0b4  dilated convolution from 3 to 2. To \\nobtain the larger receptive field, a \\n55\\uf0b4  convolution is used to \\nsubstitute the \\n33\\uf0b4  convolution, and the remaining two \\nconvolutions do not make any changes. The comparison results \\nof different receptive field sizes on BUSI and Dataset B are \\nshown in Table III.  According to the segmentation results in \\nTable III, we can conclude that the smaller and larger receptive \\nfields are not beneficial for breast lesions segmentation. The \\nabove comparison also proves the rationality of our network \\nconvolution kernel size and dilation rate settings. \\nB. Comparison with State-of-the-Art Methods \\nTo evaluate the robustness and effectiveness of the method \\nproposed in this paper, our method is first compared with state-\\nof-the-art deep learning methods  for BUS images and medical'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='To evaluate the robustness and effectiveness of the method \\nproposed in this paper, our method is first compared with state-\\nof-the-art deep learning methods  for BUS images and medical \\nimages segmentation. Our comparative methods include  U-net \\n[10], SegNet [9], Att U -net [25], U -net++ [7], U -net3+ [8], \\nAbraham et al. [24], SKNet [31], AE U-Net [33] and RDAU-\\nNet [34]. To ensure the fairness of the comparison, we perform \\nfour-fold cross-validation on BUSI and Dataset B, respectively.  \\nThe quantitative evaluation results of different segmentation \\nmethods are presented in Table IV. From Table IV, we can see \\nthat our method achieves the best results on five evaluation \\nmetrics. The five evaluation index values of our method on \\nBUSI are 67.97, 78.66, 80.63, 9 7.75 and 77.21, respectively. \\nCompared to the second results, these metrics are improved by \\n5.4\\n\\uf025 , 4.4%, 1.5%, 0.6% and 4.3%, respectively. Compared to \\nthe second results on Dataset B, our method improves these five'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content=\"Compared to the second results, these metrics are improved by \\n5.4\\n\\uf025 , 4.4%, 1.5%, 0.6% and 4.3%, respectively. Compared to \\nthe second results on Dataset B, our method improves these five \\nmetrics by 5.3%, 4.7%, 2.4%, 0.1% and 5.6%, respectively. To \\nfurther demonstrate the advantages of our method , we perform \\npaired student's t-test with the second results, and the  p-value \\n(\\n0.05p \\uf03c ) indicates a significant difference between our \\nmethod and the comparison methods. From the above analysis, \\nit can be concluded that our method has  a very superior \\nperformance in breast lesions segmentation. \\nWe also illustrate the P -R curves and  the ROC curves of \\ndifferent segmentation methods on BUSI and Dataset B in Fig. \\n4. The P-R curve represents the confidence level that the true \\npositive and false positive classes are predicted  correctly. The \\nROC curve represents the confidence level that a met hod \\npredicts correctly. The AUC scores are shown in the ROC\"),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='positive and false positive classes are predicted  correctly. The \\nROC curve represents the confidence level that a met hod \\npredicts correctly. The AUC scores are shown in the ROC \\ncurves. Compared to other methods, our method achieves the \\nhighest AUC values on both BUSI and Dataset B. According to \\nthe comparison of P -R and ROC curves, it can be concluded \\nthat our method achieves the highest confidence level on BUSI \\nand Dataset B segmentation. \\nFig. 5 shows the visual segmentation results on BUSI and \\nDataset B by different  segmentation methods. Compared with \\nthe segmentation results of other methods, our method not only \\neffectively alleviate s the perturbation of tumor size, \\nsurrounding tissue and cascade, but also achieves segmentation \\nresults that are closer to the ground-truth masks. Moreover, the \\nmethod proposed in this paper can alleviate the influence of \\nheterostructure on  segmentation results.  Comprehensive'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='results that are closer to the ground-truth masks. Moreover, the \\nmethod proposed in this paper can alleviate the influence of \\nheterostructure on  segmentation results.  Comprehensive \\nevaluation results and visual effects show that our method \\nachieves the best segmentation results  with fewer missed and \\nfalse detections in the breast lesion segmentation. 7 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. xx, NO. x, 2022 \\n \\n \\nFig. 4.  P-R and ROC curves of different segmentation methods on BUSI and Dataset B. \\n \\n           \\n           \\n           \\n           \\n           \\n           \\n           \\n           \\nFig. 5.  Segmentation results of different methods on BUSI and Dataset B. From left to right are images of Input, U-net, Att U-net, RDAU-Net, U-\\nnet++, Abraham et al., U-net3+, SegNet, AE U-net, SKNet and Ours. The red curve is the boundary of the breast lesion. \\n \\nC. Robustness Analysis \\nTo further evaluate the robustness of our method, we  first'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='C. Robustness Analysis \\nTo further evaluate the robustness of our method, we  first \\nanalyze the segmentation performance of different methods on \\nbenign and malignant breast tumors. Then, we use STU as \\nexternal validation data to evaluate the performance of different \\nmethods. Furthermore, we evaluate the impact of normal \\nultrasound images in the network segmentation performance.  \\nFinally, we compare with several state -of-the-art attention -\\nbased segmentation methods. \\n1) Robustness on Benign and Malignant Lesions \\nCompared wi th benign lesions, malignant lesions have \\nirregular shapes and blurred borders. In addition, the intensity \\ndistribution is more heterogeneous in malignant lesions \\ncompared to benign lesions.  We conduct comparative \\nexperiments on benign and malignant BUS im ages of BUSI to \\nevaluate the robustness of the network to segment malignant \\nand benign lesions. We perform four -fold cross-validation on'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='experiments on benign and malignant BUS im ages of BUSI to \\nevaluate the robustness of the network to segment malignant \\nand benign lesions. We perform four -fold cross-validation on \\nbenign images and three -fold cross -validation on malignant \\nimages. Table V presents the segmentation results of different \\nmethods on malignant and benign breast lesions.  Obviously, \\nour method achieves higher scores in the segmentation of \\nbenign and malignant lesions. Moreover, the p-value compared \\nto the second results indicates that our method has a significant \\nimprovement in segmentation accuracy.  In Fig. 6 we draw the \\nROC curves of different segmentation methods on benign and \\nNo Detected \\nNo Detected No Detected \\nNo Detected No Detected No Detected \\nNo Detected No Detected No Detected No Detected \\nNo Detected \\nNo Detected \\nNo Detected \\nNo Detected \\nNo Detected \\nNo Detected No Detected No Detected No Detected No Detected'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='No Detected No Detected No Detected No Detected \\nNo Detected \\nNo Detected \\nNo Detected \\nNo Detected \\nNo Detected \\nNo Detected No Detected No Detected No Detected No Detected \\nNo Detected 8 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. xx, NO. x, 2022 \\n \\nTABLE V \\nTHE SEGMENTATION RESULTS (MEAN Â± STD) OF BENIGN AND MALIGNANT LESIONS IN BUSI BY DIFFERENT METHODS. THE BEST RESULTS ARE MARKED WITH \\nBOLD TEXTS. ASTERISKS INDICATE THAT THE DIFFERENCE BETWEEN OUR METHOD AND THE COMPETING METHOD IS SIGNIFICANT USING A PAIRED STUDENTâ€™S \\nT-TEST. (*: P < 0.05).  \\n  Method U-net Att U-net RDAU-Net U-net3+ Abraham et al. U-net++ SegNet AE U-net SKNet Ours \\n Benign \\nJaccard 61.53\\uf0b13.98 65.03\\uf0b12.05 64.70\\uf0b12.17 67.63\\uf0b11.86 66.74\\uf0b12.10 68.25\\uf0b12.75 67.89\\uf0b13.31 67.89\\uf0b11.96 69.91\\uf0b12.11* 73.33\\uf0b12.09 \\n Precision 74.97\\uf0b12.80 75.24\\uf0b11.68 72.54\\uf0b11.57 75.58\\uf0b12.88 76.74\\uf0b12.94 75.93\\uf0b13.66 76.96\\uf0b13.11 77.17\\uf0b13.63 79.15\\uf0b12.05* 82.70\\uf0b12.90'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='Precision 74.97\\uf0b12.80 75.24\\uf0b11.68 72.54\\uf0b11.57 75.58\\uf0b12.88 76.74\\uf0b12.94 75.93\\uf0b13.66 76.96\\uf0b13.11 77.17\\uf0b13.63 79.15\\uf0b12.05* 82.70\\uf0b12.90 \\n Recall 73.97\\uf0b15.81 79.44\\uf0b12.84 79.36\\uf0b10.98 81.07\\uf0b11.27 79.97\\uf0b11.64 81.58\\uf0b11.09* 79.57\\uf0b12.21 80.54\\uf0b11.25 81.54\\uf0b12.17 83.14\\uf0b10.87 \\n Specificity 97.72\\uf0b10.59 97.68\\uf0b10.62 97.79\\uf0b10.28 97.72\\uf0b10.58 97.75\\uf0b10.60 97.74\\uf0b10.62 97.98\\uf0b10.46 97.95\\uf0b10.60 98.06\\uf0b10.52* 98.39\\uf0b10.47 \\n Dice 70.49\\uf0b13.23 73.30\\uf0b12.00 72.70\\uf0b11.62 75.07\\uf0b12.10 74.82\\uf0b12.26 75.56\\uf0b12.79 75.47\\uf0b12.91 75.77\\uf0b11.82 77.88\\uf0b12.98* 80.88\\uf0b12.06 \\n Malignant \\nJaccard 51.11\\uf0b12.62 51.12\\uf0b12.35 51.63\\uf0b11.62 54.77\\uf0b13.55 54.12\\uf0b12.96 54.03\\uf0b13.03 54.89\\uf0b11.78 55.38\\uf0b11.77 57.06\\uf0b12.42* 60.60\\uf0b11.70 \\n Precision 64.96\\uf0b12.55 61.62\\uf0b10.97 60.85\\uf0b15.01 65.78\\uf0b12.66 67.46\\uf0b13.40 65.50\\uf0b12.94 63.79\\uf0b12.65 67.87\\uf0b13.81 69.59\\uf0b14.20* 72.62\\uf0b13.13 \\n Recall 68.86\\uf0b14.27 72.57\\uf0b12.17 71.89\\uf0b12.55 74.38\\uf0b13.21 72.36\\uf0b15.05 73.43\\uf0b12.10 76.25\\uf0b14.02* 72.88\\uf0b13.47 73.58\\uf0b16.75 76.63\\uf0b15.66 \\n Specificity 93.63\\uf0b11.28 93.12\\uf0b11.00 93.47\\uf0b11.45 93.82\\uf0b11.06 93.94\\uf0b11.25 93.73\\uf0b11.31 94.00\\uf0b11.14 94.43\\uf0b11.33 94.65\\uf0b11.49* 95.11\\uf0b11.27'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='Specificity 93.63\\uf0b11.28 93.12\\uf0b11.00 93.47\\uf0b11.45 93.82\\uf0b11.06 93.94\\uf0b11.25 93.73\\uf0b11.31 94.00\\uf0b11.14 94.43\\uf0b11.33 94.65\\uf0b11.49* 95.11\\uf0b11.27 \\n Dice 63.47\\uf0b12.38 62.95\\uf0b12.14 62.44\\uf0b12.21 66.19\\uf0b13.37 65.77\\uf0b12.58 65.52\\uf0b12.75 65.90\\uf0b11.97 66.50\\uf0b11.52 68.19\\uf0b12.28* 71.54\\uf0b11.74 \\n \\n \\nFig. 6.  ROC curves of different segmentation methods on benign and malignant breast lesions. \\n \\nTABLE VI \\nSEGMENTATION RESULTS (MEAN Â± STD) OF DIFFERENT COMPETING METHODS ON THE EXTERNAL VALIDATION DATASET STU. THE BEST RESULTS ARE MARKED \\nWITH BOLD TEXTS. ASTERISKS INDICATE THAT THE DIFFERENCE BETWEEN OUR METHOD AND THE COMPETING METHOD IS SIGNIFICANT USING A PAIRED \\nSTUDENTâ€™S T-TEST. (*: P < 0.05). \\n Method Att U-net U-net U-net++ RDAU-Net Abraham et al. U-net3+ SegNet AE U-net SKNet Ours \\nSTU on Dataset B \\nJaccard 52.65\\uf0b12.29 58.90\\uf0b13.75 59.18\\uf0b14.21 60.11\\uf0b13.02 58.11\\uf0b13.24 61.51\\uf0b11.78 62.70\\uf0b13.09 62.46\\uf0b12.44 66.94\\uf0b13.15* 68.99\\uf0b13.29'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='STU on Dataset B \\nJaccard 52.65\\uf0b12.29 58.90\\uf0b13.75 59.18\\uf0b14.21 60.11\\uf0b13.02 58.11\\uf0b13.24 61.51\\uf0b11.78 62.70\\uf0b13.09 62.46\\uf0b12.44 66.94\\uf0b13.15* 68.99\\uf0b13.29 \\nPrecision 59.26\\uf0b12.86 66.27\\uf0b15.46 64.86\\uf0b15.36 63.49\\uf0b12.86 66.05\\uf0b13.69 67.12\\uf0b12.22 66.57\\uf0b13.05 66.79\\uf0b12.95 71.99\\uf0b14.08* 74.91\\uf0b13.18 \\nRecall 86.35\\uf0b11.29 86.88\\uf0b11.60 89.67\\uf0b11.59 91.37\\uf0b11.07 86.17\\uf0b10.99 89.96\\uf0b11.01 91.36\\uf0b10.38 91.24\\uf0b10.42 91.44\\uf0b11.08* 92.12\\uf0b10.75 \\nSpecificity 93.41\\uf0b10.41 94.54\\uf0b10.74 94.33\\uf0b10.83 94.70\\uf0b10.45 94.35\\uf0b10.47 94.49\\uf0b10.31 95.04\\uf0b10.49 95.00\\uf0b10.39 95.40\\uf0b10.51* 95.94\\uf0b10.71 \\nDice 65.19\\uf0b12.73 71.41\\uf0b13.67 70.70\\uf0b14.19 72.40\\uf0b12.74 70.32\\uf0b13.01 72.96\\uf0b11.69 73.50\\uf0b13.62 74.41\\uf0b12.30 78.29\\uf0b13.05* 80.23\\uf0b12.60 \\nHD 85.01\\uf0b12.04 74.05\\uf0b111.63 64.61\\uf0b16.56 52.55\\uf0b13.04* 83.38\\uf0b110.10 72.10\\uf0b110.03 65.58\\uf0b12.11 62.86\\uf0b15.00 57.37\\uf0b19.69 45.50\\uf0b13.20 \\nABD 17.16\\uf0b12.89 18.03\\uf0b12.52 14.71\\uf0b11.27 12.02\\uf0b10.43 18.60\\uf0b10.87 15.09\\uf0b11.25 11.33\\uf0b10.74 13.62\\uf0b10.94 10.73\\uf0b11.84* 9.62\\uf0b11.10 \\nASSD 3.98\\uf0b11.28 5.51\\uf0b11.45 3.28\\uf0b10.87 1.85\\uf0b10.78 5.22\\uf0b10.76 2.73\\uf0b10.85 1.00\\uf0b10.36* 2.12\\uf0b10.52 1.73\\uf0b10.42 0.81\\uf0b10.29'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='ASSD 3.98\\uf0b11.28 5.51\\uf0b11.45 3.28\\uf0b10.87 1.85\\uf0b10.78 5.22\\uf0b10.76 2.73\\uf0b10.85 1.00\\uf0b10.36* 2.12\\uf0b10.52 1.73\\uf0b10.42 0.81\\uf0b10.29 \\n \\nmalignant breast lesions to further demonstrate the confidence \\nlevel of our method . According to the ROC curves, we can \\nclearly find that our method not only achieves convincing \\nresults on benign lesions, but also achieves the most \\ncompetitive performance in the malignant lesion segmentation.  \\n2) External Validation \\nDue to the differences between different sites, there are large \\ndifferences between the collected data  [20]. These differences \\ncan cause the model to perform well in the training dataset, but \\nnot perform well in the external data. To further evaluate the \\nrobustness of the proposed method in this paper, we use STU \\nas external data to test the models trained on Dataset B by \\ndifferent methods. Compared with BUSI, Dataset B has a \\nsmaller number of samples. Therefore, comparative analysis on \\nthe smaller dataset  (Dataset B ) can better reflect the'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='different methods. Compared with BUSI, Dataset B has a \\nsmaller number of samples. Therefore, comparative analysis on \\nthe smaller dataset  (Dataset B ) can better reflect the  \\nsuperiorities of different methods. The segmentation results of  \\nvarious methods on the external validation dataset STU are \\npresented in Table VI. Our method still achieves the best results  \\n9 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. xx, NO. x, 2022 \\n \\n           \\n           \\n           \\n           \\nFig. 7. Illustration of the segmentation results of different methods on STU. From left to right are images of Input , Att U-net, U-net, U-net++, \\nRDAU-Net, Abraham et al., U-net3+, SegNet, AE U-net, SKNet and Ours. The red curve is the boundary of the breast tumor. \\n \\nTABLE VII \\nTHE SEGMENTATION RESULTS (MEAN Â± STD) OF DIFFERENT COMPETING METHODS ON BUSI WITH NORMAL IMAGES. THE BEST RESULTS ARE MARKED WITH BOLD'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='TABLE VII \\nTHE SEGMENTATION RESULTS (MEAN Â± STD) OF DIFFERENT COMPETING METHODS ON BUSI WITH NORMAL IMAGES. THE BEST RESULTS ARE MARKED WITH BOLD \\nTEXTS. ASTERISKS INDICATE THAT THE DIFFERENCE BETWEEN OUR METHOD AND THE COMPETING METHOD IS SIGNIFICANT USING A PAIRED STUDENTâ€™S T-TEST. \\n(*: P < 0.05). \\n Method U-net Att U-net RDAU-Net U-net++ Abraham et al. U-net3+ SegNet AE U-net SKNet Ours \\nBUSI (with \\nnormal images \\n) \\nJaccard 47.61\\uf0b12.85 48.73\\uf0b12.65 49.86\\uf0b12.89 51.06\\uf0b12.49 50.06\\uf0b12.14 51.13\\uf0b12.37 51.07\\uf0b12.38 51.28\\uf0b12.17 52.69\\uf0b12.15* 58.53\\uf0b12.12 \\nPrecision 61.23\\uf0b12.35 60.85\\uf0b12.54 59.68\\uf0b12.57 63.05\\uf0b12.81 63.55\\uf0b12.73 62.64\\uf0b13.05 62.85\\uf0b13.10 63.84\\uf0b13.97 66.84\\uf0b12.83* 68.16\\uf0b13.52 \\nRecall 63.05\\uf0b13.12 65.96\\uf0b12.77 65.23\\uf0b12.96 68.03\\uf0b12.54 66.57\\uf0b12.62 68.96\\uf0b12.75* 68.31\\uf0b12.87 66.97\\uf0b12.71 68.12\\uf0b12.63 69.85\\uf0b12.84 \\nSpecificity 96.09\\uf0b11.22 95.98\\uf0b10.65 96.23\\uf0b10.91 96.09\\uf0b11.16 96.21\\uf0b11.42 96.10\\uf0b11.05 96.37\\uf0b10.72 96.71\\uf0b11.03 96.73\\uf0b11.22* 97.15\\uf0b10.86'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content=\"Specificity 96.09\\uf0b11.22 95.98\\uf0b10.65 96.23\\uf0b10.91 96.09\\uf0b11.16 96.21\\uf0b11.42 96.10\\uf0b11.05 96.37\\uf0b10.72 96.71\\uf0b11.03 96.73\\uf0b11.22* 97.15\\uf0b10.86 \\nDice 56.02\\uf0b11.98 58.39\\uf0b12.47 56.87\\uf0b12.43 59.41\\uf0b12.27 59.06\\uf0b12.45 59.32\\uf0b12.31 59.85\\uf0b12.21 60.49\\uf0b12.97 61.52\\uf0b12.58* 65.83\\uf0b12.55 \\n \\non eight evaluation metrics.  This shows that the method \\nproposed in this paper has better robustness compared to other \\nmethods, and is more suitable for breast lesions segmentation.  \\nIn addition, the p-value based on Student's T-test also indicates \\nthe superiority of our method.  SegNet, AE U -net and SKNet \\nstill obtain competitive results among the compared methods, \\nwhich indicates that these t hree methods have certain potential \\nin breast lesion s segmentation. It is worth noting that the \\nsegmentation performance of Att U -net and U -net++ is \\ndegraded on STU. The occurrence of this phenomenon may be \\ncaused by their poor generalization ability. Fig. 7 shows the  \\nvisual segmentation result of different methods on the external\"),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='degraded on STU. The occurrence of this phenomenon may be \\ncaused by their poor generalization ability. Fig. 7 shows the  \\nvisual segmentation result of different methods on the external \\nvalidation dataset STU. Visually, our method achieves the best \\nsegmentation results with fewer missed and false detections. \\nFrom the first and fourth rows, it can be seen that the \\nheterostructure affects the segmentation accuracy of various \\nmethods. Fortunately, our method is able to mitigate their \\nperturbations. Overall, our method achieves the best \\nsegmentation results on external validation data STU. From the \\nabove analysis, it can be concluded that our method is \\ninsensitive to input data and has good generalization ability. \\n3) Comparison on BUSI with Normal Images \\nThe general purpose of breast lesion s segmentation in the \\nclinical usage is mainly for the lesion assessme nt, tracking the \\nlesion change, and identifying distribution and seriousness of'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='The general purpose of breast lesion s segmentation in the \\nclinical usage is mainly for the lesion assessme nt, tracking the \\nlesion change, and identifying distribution and seriousness of \\nlesion. Therefore, people usually assume that the input \\nultrasound samples possess one or more lesions, and then \\nconduct the breast lesion segmentation for clinical analysis. I n \\nthis paper we conduct some new comparative experiments by \\nintroducing normal ultrasound images of BUSI. We perform \\nfour-fold cross -validation on BUSI with normal ultrasound \\nimages. Table VII presents the segmentation results of various \\nmethods on BUSI wi th normal ultrasound images. Compared \\nwith Table IV, the introduction of normal ultrasound images of \\nBUSI severely affected the performance of the segmentation \\nnetwork. Similarly, existing work (GG-Net) also shows that the \\nintroduction of normal ultrasound  images in BUSI is not \\nbeneficial for the segmentation of breast lesions [27]. From'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content=\"network. Similarly, existing work (GG-Net) also shows that the \\nintroduction of normal ultrasound  images in BUSI is not \\nbeneficial for the segmentation of breast lesions [27]. From \\nTable IV and Table VII, we can see that our method achieves \\nthe best segmentation performance on BUSI with and without \\nnormal ultrasound images. This indicates that our method can \\nalleviate the perturbation of the segmentation results by \\nsurrounding tissues with similar intensity distributions to a \\ncertain extent. In addition, the p-value based on Student's T-test \\nalso indicates the superiority of our method. \\n4) Comparison with different Attention-Based Methods \\nIn recent years, many attention -based methods have been \\nproposed to improve the performance of networks, such as \\nAGNet [40], SANet [41], ECA-Net [42], scSENet [43], SENet \\n[44]. To further highlight the advantages of the hybrid adaptive \\nattention module (HAAM), we compare with these five\"),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='AGNet [40], SANet [41], ECA-Net [42], scSENet [43], SENet \\n[44]. To further highlight the advantages of the hybrid adaptive \\nattention module (HAAM), we compare with these five \\nattention-based methods. In the experiment , we perform four -\\nfold cross-validation on BUSI and Dataset B, respectively. The \\nquantitative evaluation of the segmentation results of BUSI and \\nDataset B by different segmentation methods is presented in \\nTable VIII. Our method  still achieves the best segmentation \\nperformance on BUSI and Dataset B, as shown in Table VIII.  \\n10 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. xx, NO. x, 2022 \\n \\nTABLE VIII \\nTHE SEGMENTATION RESULTS (MEAN Â± STD) OF DIFFERENT COMPETING METHODS ON BUSI AND DATASET B. THE BEST RESULTS ARE MARKED WITH BOLD \\nTEXTS. ASTERISKS INDICATE THAT THE DIFFERENCE BETWEEN OUR METHOD AND THE COMPETING METHOD IS SIGNIFICANT USING A PAIRED STUDENTâ€™S T-TEST. \\n(*: P < 0.05). \\n BUSI Dataset B'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='TEXTS. ASTERISKS INDICATE THAT THE DIFFERENCE BETWEEN OUR METHOD AND THE COMPETING METHOD IS SIGNIFICANT USING A PAIRED STUDENTâ€™S T-TEST. \\n(*: P < 0.05). \\n BUSI Dataset B \\nMethods Jaccard Precision Recall Specificity Dice Jaccard Precision Recall Specificity Dice \\nAGNet 62.23\\uf0b12.02 72.11\\uf0b12.77 78.80\\uf0b12.00 96.30\\uf0b10.68 71.35\\uf0b12.10 64.15\\uf0b11.35* 74.35\\uf0b14.72* 80.19\\uf0b14.30* 98.70\\uf0b10.31* 73.30\\uf0b11.03* \\nSANet 65.96\\uf0b12.78 74.84\\uf0b14.18 80.76\\uf0b12.30* 96.75\\uf0b10.70 74.46\\uf0b12.76 63.26\\uf0b17.77 72.40\\uf0b110.19 77.81\\uf0b15.07 98.61\\uf0b10.57 71.84\\uf0b17.96 \\nSENet 67.75\\uf0b13.09 78.70\\uf0b13.51 80.04\\uf0b13.07 97.15\\uf0b10.71 76.71\\uf0b12.88 60.77\\uf0b16.41 71.95\\uf0b18.50 78.18\\uf0b16.77 98.49\\uf0b10.27 70.45\\uf0b15.30 \\nscSENet 67.68\\uf0b12.28 78.95\\uf0b12.73* 79.58\\uf0b11.14 97.26\\uf0b10.48* 76.67\\uf0b12.20 62.17\\uf0b15.03 71.31\\uf0b15.44 79.16\\uf0b15.79 98.52\\uf0b10.22 71.30\\uf0b14.24 \\nECA-Net 68.17\\uf0b12.21* 78.59\\uf0b12.77 80.73\\uf0b11.91 97.20\\uf0b10.53 77.10\\uf0b12.17* 63.23\\uf0b15.26 72.65\\uf0b14.19 78.53\\uf0b14.99 98.62\\uf0b10.26 72.09\\uf0b14.74 \\nOurs 68.82\\uf0b10.44 79.61\\uf0b11.07 81.10\\uf0b10.52 97.57\\uf0b10.24 77.51\\uf0b10.68 69.10\\uf0b12.98 78.83\\uf0b12.40 82.22\\uf0b13.84 98.82\\uf0b10.35 78.14\\uf0b12.41'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content=\"Ours 68.82\\uf0b10.44 79.61\\uf0b11.07 81.10\\uf0b10.52 97.57\\uf0b10.24 77.51\\uf0b10.68 69.10\\uf0b12.98 78.83\\uf0b12.40 82.22\\uf0b13.84 98.82\\uf0b10.35 78.14\\uf0b12.41 \\nFrom Table VIII, we can see that the compared methods have \\ninconsistent segmentation performance on BUSI and Dataset B. \\nThis shows that these  methods are more sensitive and less \\nrobust to different input data.  Furthermore, the p-value based \\non Student's T -test indicates that our method is significantly \\ndifferent from these comparative methods . The comparison \\nwith attention-based methods further shows that our method has \\nbetter robustness and generalization ability. \\nV. DISCUSSION \\nIn this study, we propose a novel adaptive attention U-net \\n(AAU-net) to alleviate the challenge of breast lesion s \\nsegmentation. To evaluate the effectiveness of netw ork \\ncomponents and parameters, we first perform the ablation study. \\nAccording to the experimental results in Table II and Table III, \\nwe can clearly see that the settings of our network components\"),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='components and parameters, we first perform the ablation study. \\nAccording to the experimental results in Table II and Table III, \\nwe can clearly see that the settings of our network components \\nand parameters enable the network to achieve the best \\nperformance in the breast lesion segmentation. \\nAccording to the comparative experimental results of state -\\nof-the-art segmentation methods, we can draw several \\nconclusions. In general, the U-net-based variant network (such \\nas U-net++ and U -net3+) achieves better segmentation results \\nthan the original U -net, which indicates that the use of skip -\\nconnection operations to fuse low-level features in the encoding \\nstage with high-level features in the decoding stage is beneficial \\nfor the segmentation of breast lesions. B y analyzing the \\nsegmentation results of the U-shaped network with the attention \\nmechanism (such as RDAU -Net), it can be concluded that the \\nintroduction of the attention mechanism can also improve the'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='segmentation results of the U-shaped network with the attention \\nmechanism (such as RDAU -Net), it can be concluded that the \\nintroduction of the attention mechanism can also improve the \\nsegmentation performance of the network. Compared with t he \\nresults of AE U -net and SKNet, the optimized attention \\nmechanism can further improve the performance of the network \\nto segment breast lesions. Compared with U-net, Att U-net has \\na poor segmentation result on BUSI, but it has a better \\nsegmentation result  on Dataset B. This shows that the use of \\nthis attention mechanism will increase the sensitivity of the \\nnetwork to breast ultrasound images.  From the segmentation \\nresults of the method proposed by Abraham et al., it can be seen \\nthat introducing the strateg y of deep supervision and multi -\\nscale inputs into Att U -net can help improve the segmentation \\naccuracy of breast lesions. \\nBased on the segmentation results of SegNet, it can be seen'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='scale inputs into Att U -net can help improve the segmentation \\naccuracy of breast lesions. \\nBased on the segmentation results of SegNet, it can be seen \\nthat using the location information of the features in the U -\\nshaped network can achieve better results than most \\nsegmentation methods. Generally speaking, SKNet achieves \\ngood performance on breast lesions segmentation among the \\ncompared methods. Compared with SKNet, the larger receptive \\nfield and spatial self-attention block introduced by our method \\ncan effectively improve the performance of the network in \\nsegmenting breast lesions. Based on the visual segmentation \\nresults shown in Fig. 5, we can summarize four key points. \\nAccording to the segmentation results of the  first and second \\nrows in Fig. 5, we can see that various methods have some \\nmissed detection for small breast tumors, and even fail to detect \\nbreast tumors. From the segmentation results from the first row'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='rows in Fig. 5, we can see that various methods have some \\nmissed detection for small breast tumors, and even fail to detect \\nbreast tumors. From the segmentation results from the first row \\nto fourth row in Fig. 5, we can conclude that the surrounding \\ntissue (background) with similar intensity distribution can cause \\nserious missed detections and false detections of breast lesions. \\nSevere heterogeneity makes breast lesions undetectable by \\nvarious methods as shown in the fifth and sixth rows i n Fig. 5. \\nFurthermore, accurate tumor contours cannot be captured from \\nblurred or cascaded BUS images as shown in the last two rows \\nof images in Fig. 5. Although our method still suffers from false \\ndetections and missed detections, it achieves significant \\nimprovements compared to other methods. \\nThe experimental results of the robustness analysis not only \\ndemonstrate the good generalization ability of our network, but \\nalso further highlight the advantages of the hybrid adaptive'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='The experimental results of the robustness analysis not only \\ndemonstrate the good generalization ability of our network, but \\nalso further highlight the advantages of the hybrid adaptive \\nattention module (HAAM). From the robustness experiments \\non benign and malignant lesion segmentation, we can see that \\nthe segmentation performance of various methods is relatively \\nstable. Although the advantage of our method is reduced, it still \\nachieves better segmentation performance than other compared \\nmethods. This shows that our method can well adapt to different \\ndata inputs. The experimental results of external validation also \\nfurther prove that our network has good generalization ability. \\nAccording to the experimental results wit h attention -based \\nsegmentation methods, it can be concluded that their \\nsegmentation performances on BUSI and Dataset B are \\nsignificantly different, which indicates that they are sensitive to \\ndifferent input data. The method proposed in this paper'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='segmentation performances on BUSI and Dataset B are \\nsignificantly different, which indicates that they are sensitive to \\ndifferent input data. The method proposed in this paper \\nachieves consistently good segmentation results on BUSI and \\nDataset B. Compared with existing attention modules, our \\nhybrid adaptive attention module (HAAM) can help the \\nnetwork learn the more generic representation of breast lesions \\nfrom ultrasound images. From th e above analysis, it can be \\nconcluded that our hybrid adaptive attention module (HAAM) 11 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. xx, NO. x, 2022 \\n \\noutperforms existing attention models in the breast lesion \\nsegmentation. \\nAlthough the method proposed in this paper has achieved \\ngood performance on breast lesions segmentation, it can be seen \\nfrom Fig. 5 , Fig. 7  and Fig. 8 that our method still has some \\nshortcomings. (1) For more complex BUS images segmentation, \\nour method still needs to be further optimized to reduce the false'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='from Fig. 5 , Fig. 7  and Fig. 8 that our method still has some \\nshortcomings. (1) For more complex BUS images segmentation, \\nour method still needs to be further optimized to reduce the false \\ndetection and missed detection rate. (2) How to obtain accurate \\nobject contours is still a challenging task. (3) The similar \\nintensity distribution of surrounding tissues seriously affects the \\nsegmentation accuracy of breast lesions. To alleviate the above \\nchallenges, we will introduce boundary con straints to further \\nimprove the segmentation performance of the network. In \\naddition, designing a reasonable data augmentation algorithm \\nto expand the sample space is also our research direction. \\nVI. CONCLUSION \\nTo better address the challenge of breast tumor segmentation, \\nwe design a novel hybrid adaptive attention module (HAAM) \\nand use it to construct an adaptive attention U -net (AAU-net) \\nfor breast lesions segmentation. The hybrid adaptive attention'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='we design a novel hybrid adaptive attention module (HAAM) \\nand use it to construct an adaptive attention U -net (AAU-net) \\nfor breast lesions segmentation. The hybrid adaptive attention \\nmodule can guide the network to adaptively select more robus t \\nrepresentation in both channel and space dimensions to cope \\nwith more complex breast lesions segmentation. Extensive \\nexperiments (comparative experiments, robustness analysis and \\nexternal validation) with several state -of-the-art deep learning \\nsegmentation methods demonstrate that our method has better \\nperformance on breast lesions segmentation. The source code is \\npublicly available on https://github.com/CGPxy/AAU-net \\nREFERENCES \\n[1] S. Y. Shin, S. Lee, I. D. Yu n, S. M. Kim, and K. M. Lee, â€œJoint weakly \\nand semi-supervised deep learning for localization and classification of \\nmasses in breast ultrasound images,â€ IEEE Trans. Med. Imaging, vol. 38, \\nno. 3, pp. 762â€“774, 2018.'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='and semi-supervised deep learning for localization and classification of \\nmasses in breast ultrasound images,â€ IEEE Trans. Med. Imaging, vol. 38, \\nno. 3, pp. 762â€“774, 2018. \\n[2] R. Huang, M. Lin, H. Dou, Z. Lin, Q. Y ing, X. Jia, W. Xu, Z. Mei, X. \\nYang, and Y. Dong, â€œBoundary -rendering Network for Breast Lesion \\nSegmentation in Ultrasound Images,â€ Med. Image Anal., p. 102478, 2022. \\n[3] J. A. Noble and D. Boukerroui, â€œUltrasound image segmentation: a \\nsurvey,â€ IEEE Trans Med Imaging, vol. 25, no. 8, pp. 987â€“1010, 2006. \\n[4] M. Xian, Y. Zhang, H. -D. Cheng, F. Xu, B. Zhang, and J. Ding, \\nâ€œAutomatic breast ultrasound image segmentation: A survey,â€ Pattern \\nRecognit., vol. 79, pp. 340â€“355, 2018. \\n[5] J. Bai, R. Posner, T. Wang, C. Yang, and S. Nabavi, â€œApplying deep \\nlearning in digi tal breast tomosynthesis for automatic breast cancer \\ndetection: A review,â€ Med. Image Anal., vol. 71, p. 102049, 2021. \\n[6] X. Zhang, Z. Xiao, H. Fu, Y. Hu, J. Yuan, Y. Xu, R. Higashita, and J. Liu,'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='detection: A review,â€ Med. Image Anal., vol. 71, p. 102049, 2021. \\n[6] X. Zhang, Z. Xiao, H. Fu, Y. Hu, J. Yuan, Y. Xu, R. Higashita, and J. Liu, \\nâ€œAttention to region: Region -based integration -and-recalibration \\nnetworks for nuclear cataract classification using AS-OCT images,â€ Med. \\nImage Anal., p. 102499, 2022. \\n[7] Z. Zhou, M. Siddiquee, N. Tajbakhsh, and J. Liang, â€œUNet++: \\nRedesigning Skip Connections to Exploit Multiscale Features in Image \\nSegmentation,â€ IEEE Trans. Med. Imaging, vol. 39, no. 6, pp. 1856â€“1867, \\n2020. \\n[8] H. Huang, L. Lin, R. Tong, H. Hu, and J. B. T.-I. 2020-2020 I. I. C. on A. \\nWu  Speech and Signal Processing (ICASSP), â€œUNet 3+: A Full -Scale \\nConnected UNet for Medical Image Segmentation,â€ 2020. \\n[9] V. Badrinarayanan, A. Kendall, and R. Cipolla, â€œSegNet: A Deep \\nConvolutional Encoder-Decoder Architecture for Image Segmentation,â€ \\nIEEE Trans Pattern Anal Mach Intell , vol. 39, no. 12, pp. 2481 â€“2495, \\n2017.'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='Convolutional Encoder-Decoder Architecture for Image Segmentation,â€ \\nIEEE Trans Pattern Anal Mach Intell , vol. 39, no. 12, pp. 2481 â€“2495, \\n2017. \\n[10] O. Ronneberger, P. Fischer, and T. Brox, â€œU-net: Convolutional networks \\nfor biomedical image segmentation,â€ in International Conference on \\nMedical image computing and computer-assisted intervention, 2015, pp. \\n234â€“241. \\n[11] J. Long, E. Shelhamer, and T. Darrell, â€œFully convolutional networks for \\nsemantic segmentation,â€ in Proceedings of the IEEE conference on \\ncomputer vision and pattern recognition, 2015, pp. 3431â€“3440. \\n[12] M. Xian, Y. Zhang, and H. -D. Cheng, â€œFully automatic segmentation of \\nbreast ultrasound images based on breast characte ristics in space and \\nfrequency domains,â€ Pattern Recognit., vol. 48, no. 2, pp. 485â€“497, 2015. \\n[13] Y. Wang, H. Dou, X. Hu, L. Zhu, X. Yang, M. Xu, J. Qin, P.-A. Heng, T. \\nWang, and D. Ni, â€œDeep attentive features for prostate segmentation in'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='[13] Y. Wang, H. Dou, X. Hu, L. Zhu, X. Yang, M. Xu, J. Qin, P.-A. Heng, T. \\nWang, and D. Ni, â€œDeep attentive features for prostate segmentation in \\n3D transrectal ultrasound,â€ IEEE Trans. Med. Imaging , vol. 38, no. 12, \\npp. 2768â€“2778, 2019. \\n[14] E. H. Houssein, M. M. Emam, A. A. Ali, and P. N. Suganthan, â€œDeep and \\nmachine learning techniques for medical imaging-based breast cancer: A \\ncomprehensive review,â€ Expert Syst. Appl., vol. 167, p. 114161, 2021. \\n[15] L. Abdelrahman, M. Al Ghamdi, F. Collado -Mesa, and M. Abdel -\\nMottaleb, â€œConvolutional neural networks for breast cancer detection in \\nmammography: A survey,â€ Comput. Biol. Med., vol. 131, p. 104248, 2021. \\n[16] C. Ch en, Y. Wang, J. Niu, X. Liu, Q. Li, and X. Gong, â€œDomain \\nKnowledge Powered Deep Learning for Breast Cancer Diagnosis Based \\non Contrast-Enhanced Ultrasound Videos,â€ IEEE Trans. Med. Imaging , \\nvol. 40, no. 9, pp. 2439â€“2451, 2021. \\n[17] Y. Wang, N. Wang, M. Xu, J. Yu, C. Qin, X. Luo, X. Yang, T. Wang, A.'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='on Contrast-Enhanced Ultrasound Videos,â€ IEEE Trans. Med. Imaging , \\nvol. 40, no. 9, pp. 2439â€“2451, 2021. \\n[17] Y. Wang, N. Wang, M. Xu, J. Yu, C. Qin, X. Luo, X. Yang, T. Wang, A. \\nLi, and D. Ni, â€œDeeply-supervised networks with threshold loss for cancer \\ndetection in automated breast ultrasound,â€ IEEE Trans. Med. Imaging, vol. \\n39, no. 4, pp. 866â€“876, 2019. \\n[18] M. H. Yap, G. Pons, J. MartÃ­ , S. Ganau, M. SentÃ­ s, R. Zwiggelaar, A. K. \\nDavison, and R. MartÃ­, â€œAutomated Breast Ultrasound Lesions Detection \\nUsing Convolutional Neural Networks,â€ IEEE J. Biomed. Heal. \\nInformatics, vol. 22, no. 4, pp. 1218â€“1226, 2018. \\n[19] R. Almajalid, J. Shan, Y. Du, and M. Zhang, â€œDevelopment of a deep -\\nlearning-based method for breast ultrasound image segmentation,â€ in \\n2018 17th IEEE International Conference on Machine Learning and \\nApplications (ICMLA), 2018, pp. 1103â€“1108. \\n[20] Z. Ning, S. Zhong, Q. Feng, W. Chen, and Y. Zhang, â€œSMU -Net:'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='2018 17th IEEE International Conference on Machine Learning and \\nApplications (ICMLA), 2018, pp. 1103â€“1108. \\n[20] Z. Ning, S. Zhong, Q. Feng, W. Chen, and Y. Zhang, â€œSMU -Net: \\nSaliency-guided Morphology -aware U -Net for Breast Lesion \\nSegmentation in Ultrasound Image,â€ IEEE Trans. Med. Imaging, 2021. \\n[21] C. Li, X. Wang, W. Liu, L. J. Latecki, B. Wang, and J. Huang, â€œWeakly \\nsupervised mitosis detection in breast histopathology images using \\nconcentric loss,â€ Med. Image Anal., vol. 53, pp. 165â€“178, 2019. \\n[22] G. Chen, Y. Dai, J. Zhang, X. Yin, and L. Cui, â€œA novel convolutional \\nneural network for kidney ultrasound image segmentation,â€ Comput. \\nMethods Programs Biomed., vol. 218, p. 106712, 2022. \\n[23] G. Chen, Y. Dai, and J. Zhang, â€œC -Net: Cascaded Convolutional Neural \\nNetwork with Global Guidance and Refinement Residuals for Breast \\nUltrasound Images Segmentation,â€ Comput. Methods Programs Biomed., \\np. 107086, 2022.'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='Network with Global Guidance and Refinement Residuals for Breast \\nUltrasound Images Segmentation,â€ Comput. Methods Programs Biomed., \\np. 107086, 2022. \\n[24] N. Abraham and N. M. B. T. Khan, â€œA Novel Focal Tversky Loss \\nFunction With Improved Attention U -Net for Lesion Segmentation,â€ in \\n2019 IEEE 16th International Symposium on Biomedical Imaging (ISBI), \\n2019, pp. 683â€“687. \\n[25] O. Oktay, J. Schlemper, L. Le Folgoc, M. Lee, M. Heinrich, K. Misawa, \\nK. Mori, S. McDonagh, N. Y. Hammerla, B. Kainz, B. Glocker, and D. \\nRueckert, â€œAttention U -Net: learning where to look for the pancreas,â€ \\narXiv Prepr. arXiv1804.03999, no. Midl, 2018. \\n[26] H. Lee, J. Park, and J. Y. Hwang, â€œChannel attention module with \\nmultiscale grid average pooling for breast cancer segmentation in an \\nultrasound image,â€ IEEE Trans. Ultrason. Ferroelectr. Freq. Control, vol. \\n67, no. 7, pp. 1344â€“1353, 2020. \\n[27] C. Xue, L. Zhu, H. Fu, X. Hu, X. Li, H. Zhang, and P. Heng, â€œGlobal'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='ultrasound image,â€ IEEE Trans. Ultrason. Ferroelectr. Freq. Control, vol. \\n67, no. 7, pp. 1344â€“1353, 2020. \\n[27] C. Xue, L. Zhu, H. Fu, X. Hu, X. Li, H. Zhang, and P. Heng, â€œGlobal \\nguidance network for breast lesion segmentation in ultrasound images,â€ \\nMed. Image Anal., vol. 70, p. 101989, 2021. \\n[28] R. Irfan, A. A. Almazroi, H. T. Rauf, R. DamaÅ¡eviÄius, E. A. Na sr, and \\nA. E. Abdelgawad, â€œDilated semantic segmentation for breast ultrasonic \\nlesion detection using parallel feature fusion,â€ Diagnostics, vol. 11, no. 7, \\np. 1212, 2021. \\n[29] X. Cao, H. Chen, Y. Li, Y. Peng, S. Wang, and L. Cheng, â€œDilated densely \\nconnected U -Net with uncertainty focus loss for 3D ABUS mass \\nsegmentation,â€ Comput. Methods Programs Biomed., vol. 209, p. 106313, \\n2021. \\n[30] Y. Hu, Y. Guo, Y. Wang, J. Yu, J. Li, S. Zhou, and C. Chang, â€œAutomatic \\ntumor segmentation in breast ultrasound images u sing a dilated fully \\nconvolutional network combined with an active contour model,â€ Med.'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='tumor segmentation in breast ultrasound images u sing a dilated fully \\nconvolutional network combined with an active contour model,â€ Med. \\nPhys., vol. 46, no. 1, pp. 215â€“228, 2019. 12 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. xx, NO. x, 2022 \\n \\n[31] M. Byra, P. Jarosik, A. Szubert, M. Galperin, H. Ojeda-Fournier, L. Olson, \\nM. Oâ€™Boyle, C. Comstock, and M. Andre, â€œBreast mass segmentation in \\nultrasound with selective kernel U -Net convolutional neural network,â€ \\nBiomed. Signal Process. Control, vol. 61, 2020. \\n[32] X. Li, W. Wang, X. Hu, and J. Yang, â€œSelective kernel networks,â€ in \\nProceedings of the IEEE/CVF Conference on C omputer Vision and \\nPattern Recognition, 2019, pp. 510â€“519. \\n[33] Y. Yan, Y. Liu, Y. Wu, H. Zhang, Y. Zhang, and L. Meng, â€œAccurate \\nsegmentation of breast tumors using AE U -net with HDC model in \\nultrasound images,â€ Biomed. Signal Process. Control, vol. 72, no. PA, p. \\n103299, 2022. \\n[34] Z. Zhuang, N. Li, A. N. Joseph Raj, V. G. V Mahesh, and S. Qiu, â€œAn'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='ultrasound images,â€ Biomed. Signal Process. Control, vol. 72, no. PA, p. \\n103299, 2022. \\n[34] Z. Zhuang, N. Li, A. N. Joseph Raj, V. G. V Mahesh, and S. Qiu, â€œAn \\nRDAU-NET model for lesion segmentation in breast ultrasound images,â€ \\nPLoS One, vol. 14, no. 8, p. e0221535, 2019. \\n[35] S. Woo, J. Park, J.-Y. Lee, and I. S. Kweon, â€œCbam: Convolutional block \\nattention module,â€ in Proceedings of the European conference on \\ncomputer vision (ECCV), 2018, pp. 3â€“19. \\n[36] P.-T. De Boer, D. P. Kroese, S. Mannor, and R. Y. Rubinstein, â€œA tutorial \\non the cross-entropy method,â€ Ann. Oper. Res., vol. 134, no. 1, pp. 19â€“67, \\n2005. \\n[37] W. Al-Dhabyani, M. Gomaa, H. Khaled, and A. Fahmy, â€œDataset of breast \\nultrasound images,â€ Data Br., vol. 28, p. 104863, 2020. \\n[38] M. H. Yap, M. Goyal, F. Osman, R. MartÃ­ , E. Denton, A. Juette, and R. \\nZwiggelaar, â€œBreast ultrasound region of interest detection and lesion \\nlocalisation,â€ Artif. Intell. Med., vol. 107, p. 101880, 2020.'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='Zwiggelaar, â€œBreast ultrasound region of interest detection and lesion \\nlocalisation,â€ Artif. Intell. Med., vol. 107, p. 101880, 2020. \\n[39] G. Chen, Y. Dai, J. Zhang, X. Yin, and L. Cui, â€œSDFNet: Automatic \\nsegmentation of kidney ultrasound images using multi -scale lo w-level \\nstructural feature,â€ Expert Syst. Appl., vol. 185, p. 115619, 2021. \\n[40] S. Zhang, H. Fu, Y. Yan, Y. Zhang, Q. Wu, M. Yang, M. Tan, and Y. Xu, \\nâ€œAttention guided network for retinal image segmentation,â€ in \\nInternational conference on medical image c omputing and computer -\\nassisted intervention, 2019, pp. 797â€“805. \\n[41] Z. Zhong, Z. Q. Lin, R. Bidart, X. Hu, I. Ben Daya, Z. Li, W. -S. Zheng, \\nJ. Li, and A. Wong, â€œSqueeze -and-attention networks for semantic \\nsegmentation,â€ in Proceedings of the IEEE/CVF conference on computer \\nvision and pattern recognition, 2020, pp. 13065â€“13074. \\n[42] Q. Wang, B. Wu, P. Zhu, P. Li, and Q. B. T. Hu, â€œECA -Net: Efficient'),\n",
       " Document(metadata={'arxiv_id': '2204.12077v3', 'title': 'AAU-net: An Adaptive Attention U-net for Breast Lesions Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Gongping Chen, Yu Dai, Jianxun Zhang'}, page_content='vision and pattern recognition, 2020, pp. 13065â€“13074. \\n[42] Q. Wang, B. Wu, P. Zhu, P. Li, and Q. B. T. Hu, â€œECA -Net: Efficient \\nChannel Attention for Deep Convolutional Neural Networks,â€ in \\nProceedings of the IEEE conference on computer vision and pattern \\nrecognition, 2020, pp. 11531â€“11539. \\n[43] A. G. Roy, N. Navab, and C. Wachinger, â€œConcurrent spatial and channel \\nâ€˜squeeze & excitationâ€™in fully convolutional networks,â€ in International \\nconference on medical image computing and computer -assisted \\nintervention, 2018, pp. 421â€“429. \\n[44] J. Hu, L. Shen, and G. Sun, â€œSqueeze -and-excitation networks,â€ in \\nProceedings of the IEEE conference on computer vision and pattern \\nrecognition, 2018, pp. 7132â€“7141.'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'title_abstract', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='Title: VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification\\n\\nAbstract: Artificial Intelligence (AI) has the potential to revolutionize diagnosis and segmentation in medical imaging. However, development and clinical implementation face multiple challenges including limited data availability, lack of generalizability, and the necessity to incorporate multi-modal data effectively. A foundation model, which is a large-scale pre-trained AI model, offers a versatile base that can be adapted to a variety of specific tasks and contexts. Here, we present VIsualization and Segmentation Masked AutoEncoder (VIS-MAE), novel model weights specifically designed for medical imaging. Specifically, VIS-MAE is trained on a dataset of 2.5 million unlabeled images from various modalities (CT, MR, PET,X-rays, and ultrasound), using self-supervised learning techniques. It is then adapted to classification and segmentation tasks using explicit labels. VIS-MAE has high label efficiency, outperforming several benchmark models in both in-domain and out-of-domain applications. In addition, VIS-MAE has improved label efficiency as it can achieve similar performance to other models with a reduced amount of labeled training data (50% or 80%) compared to other pre-trained weights. VIS-MAE represents a significant advancement in medical imaging AI, offering a generalizable and robust solution for improving segmentation and classification tasks while reducing the data annotation workload. The source code of this work is available at https://github.com/lzl199704/VIS-MAE.'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='VIS-MAE: An Efficient Self-supervised \\nLearning Approach on Medical Image \\nSegmentation and Classification \\n \\nZelong Liu1, Andrew Tieu1, Nikhil Patel1, George Soultanidis1, Louisa Deyer1, \\nYing Wang2, Sean Huver3, Alexander Zhou1, Yunhao Mei4, Zahi A. Fayad1, \\nTimothy Deyer5,6, and Xueyan Mei1,7 \\n1 BioMedical Engineering and Imaging Institute, Icahn School of Medicine at Mount Sinai, \\nNew York, NY, USA \\n2 Department of Mathematics, University of Oklahoma, Norman, OK, USA \\n3 NVIDIA, Santa Clara, CA, USA \\n4 Erasmus University Rotterdam, Rotterdam, The Netherlands \\n5 East River Medical Imaging, New York, NY, USA \\n6 Department of Radiology, Cornell Medicine, New York, NY, USA \\n7 Windreich Department of Artificial Intelligence and Human Health, Icahn School of Medicine \\nat Mount Sinai, New York, NY, USA \\n \\n \\nAbstract. Artificial Intelligence (AI) has the potential to revolutionize diagno - \\nsis and segmentation in medical imaging. However, development and clinical'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='at Mount Sinai, New York, NY, USA \\n \\n \\nAbstract. Artificial Intelligence (AI) has the potential to revolutionize diagno - \\nsis and segmentation in medical imaging. However, development and clinical \\nimplementation face multiple challenges including limited data availability, lack \\nof generalizability, and the necessity to incorporate multi-modal data effectively. \\nA foundation model, which is a large-scale pre-trained AI model, offers a versa- \\ntile base that can be adapted to a variety of specific tasks and contexts. Here, we \\npresent VIsualization and Segmentation Masked AutoEncoder (VIS-MAE), novel \\nmodel weights specifically designed for medical imaging. Specifically, VIS-MAE \\nis trained on a dataset of 2.5 million unlabeled images from various modalities \\n(CT, MR, PET, X-rays, and ultrasound), using self-supervised learning techniques. \\nIt is then adapted to classification and segmentation tasks using explicit labels.'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='(CT, MR, PET, X-rays, and ultrasound), using self-supervised learning techniques. \\nIt is then adapted to classification and segmentation tasks using explicit labels. \\nVIS-MAE has high label efficiency, outperforming several benchmark models \\nin both in -domain and out -of-domain applications. In addition, VIS -MAE has \\nimproved label efficiency as it can achieve similar  performance to other models \\nwith a reduced amount of labeled training data (50% or 80%) compared to other \\npre-trained weights. VIS-MAE represents a significant advancement in medical \\nimaging AI, offering a generalizable and robust solution for improving segmen- \\ntation and classification tasks while reducing the data annotation workload. The \\nsource code of this work is available at https://github.com/lzl199704/VIS-MAE. \\nKeywords: Self-supervised Learning Â· Masked Autoencoder Â· Medical Image \\nSegmentation and Classification Â· Label efficiency \\n \\n \\n  \\n1 Introduction'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='Keywords: Self-supervised Learning Â· Masked Autoencoder Â· Medical Image \\nSegmentation and Classification Â· Label efficiency \\n \\n \\n  \\n1 Introduction \\nRecent advances in the field of artificial intelligence (AI) have given rise to the develop- \\nment of foundation models, machine learning models that are trained on large, diverse \\ndatasets that can be adapted to a wide variety of downstream tasks [ 1]. While tradi - \\ntional deep learning models are specifically trained for designated applications, such \\nas the classification of interstitial lung disease [ 2] or COVID-19 [3], and consistently \\nunderperform when repurposed for other tasks [4], foundation models offer more gen- \\neralized and adaptable capabilities. Following initial training, foundation models can be \\nfine-tuned to numerous different tasks. In the field of medical imaging, foundation mod- \\nels have a wide range of potential applications such as improved diagnostic accuracy,'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='fine-tuned to numerous different tasks. In the field of medical imaging, foundation mod- \\nels have a wide range of potential applications such as improved diagnostic accuracy, \\nincreased efficiency of reads, and prediction of disease outcomes. The incorporation of \\nfoundation models into clinical workflows has the potential to revolutionize the field \\nof healthcare delivery. Despite these advances, there remain several challenges in the \\ndevelopment of foundation models. Training of foundation models through traditional \\nsupervised learning approaches is limited by the need for large quantities of labeled data, \\nwhich is often a prohibitively time-consuming and cost-intensive process [5]. This is fur- \\nther compounded by an overall lack of high -quality and open-source medical imaging \\ndata [6â€“8], hindering the generalizability and accuracy of developed models [9, 10]. \\nThe Segment Anything Model (SAM) [11], a foundation model trained on a dataset'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='data [6â€“8], hindering the generalizability and accuracy of developed models [9, 10]. \\nThe Segment Anything Model (SAM) [11], a foundation model trained on a dataset \\nof 11 million natural images with over 1 billion image masks, showcased the ability to \\nautomatically segment any image using prompts. Soon after, this capability was applied \\nto medical images with the release of MedSAM [ 12], trained on a large -scale medical \\nimaging dataset with over 1 million medical image -mask pairs across a wide array of \\nmodalities and protocols, thus opening the door to many possibilities in the field of med- \\nical imaging analysis. Following MedSAMâ€™s release, increasing numbers of medical \\nimaging foundation models have been reported for a variety of applications. RETFound \\n[13], a foundation model trained on 1.3 million retinal images, offers generalizable capa- \\nbilities to detect multiple retinal diseases. Other medical foundation models can perform'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='[13], a foundation model trained on 1.3 million retinal images, offers generalizable capa- \\nbilities to detect multiple retinal diseases. Other medical foundation models can perform \\nsegmentation of moving structures [ 14], volumetric segmentation [ 15], diagnosis and \\nprognosis of ocular disease [16], and assessment of clinical pathology images [17]. \\nIn response to the lack of high quality open -source medical imaging data, current \\napproaches to medical imaging analysis have adopted the use of self-supervised learning \\n(SSL), a training method in which models are able to infer labels through latent features \\nof unlabeled data. The success of SSL relies on pretext tasks designed on unlabeled data, \\nwhich can learn image representations through reconstruction, jigsaw puzzle solving, \\nor contrastive learning [ 18]. SSL methods such as a simple framework for contrastive \\nlearning of visual representations (SimCLR) [ 19] and Masked Autoencoder [ 20] have'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='or contrastive learning [ 18]. SSL methods such as a simple framework for contrastive \\nlearning of visual representations (SimCLR) [ 19] and Masked Autoencoder [ 20] have \\nbeen shown to achieve comparable results or even outperform models using supervised \\nlearning methods. The architecture of SimCLR can employ a simple and efficient back- \\nbone and a classifier head, and SimCLR has been shown to be an effective pre-training \\nstrategy for medical image classification [ 21]. However, since SimCLR only provides \\npre-trained encoder weights, while an MAE could offer pre-trained weights for both the \\nencoder and decoder, an MAE could be more beneficial for both segmentation and clas- \\nsification. Recently, Swin MAE [22], a masked autoencoder using Swin Transformer as  \\n \\n \\nits backbone, demonstrated the feasibility of achieving such results on smaller datasets \\nwithout the use of pre-trained models. \\nTo improve the training efficiency and model generalizability, we introduce a'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='without the use of pre-trained models. \\nTo improve the training efficiency and model generalizability, we introduce a  \\nself-supervised learning model with weights trained on a large -scale database, the \\nVIsualization and Segmentation Masked AutoEncoder (VIS-MAE) model. This model \\nemploys a Swin Transformer -based masked autoencoder, designed for a wide range  \\nof downstream tasks. The pre -trained weights of VIS -MAE were developed using a \\ndataset of 2.5 million images from various imaging modalities, including CT, MRI, \\nPET/CT, radiography (X-ray), and ultrasound (US). The generic VIS-MAE (VIS-MAE- \\nGeneric) weights were trained on the entire dataset to ensure broad applicability across \\ndifferent medical imaging types, while the modality -specific VIS -MAE (VIS -MAE- \\nModality) weights were trained from images from individual modalities, enhancing \\ntheir ability to identify modality-specific anatomical and pathological features. Our study'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='Modality) weights were trained from images from individual modalities, enhancing \\ntheir ability to identify modality-specific anatomical and pathological features. Our study \\nshows that VIS-MAE can outperform traditional supervised models in terms of perfor- \\nmance and generalizability across diverse medical imaging tasks, while also reducing \\nthe dependency on extensively labeled datasets. \\n \\n2 Methodology \\n2.1 VIS-MAE Model Development \\nAs shown in Fig. 1, our VIS-MAE was created by incorporating Swin MAE [ 22] with \\nan additional segmentation decoder or classification layer. VIS -MAE incorporates a \\nmodified version of Swin Transformer [ 23] as its backbone, while a window masking \\nmethod was applied during the MAE process. The modification includes the same block \\nnumber in each stage to reduce the trainable parameters and improve training efficiency. \\nEach Swin Transformer block contains a LayerNorm (LN) layer, a window-based multi-'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='number in each stage to reduce the trainable parameters and improve training efficiency. \\nEach Swin Transformer block contains a LayerNorm (LN) layer, a window-based multi- \\nhead self-attention (W-MSA) module, a residual connection and a multilayer perceptron \\n(MLP) layer with GELU non -linearity activation. The output of lth Swin Transformer \\nblock layer, zl, can be annotated as: \\nzË†l = W âˆ’ MSA (LN (zlâˆ’1)) + zlâˆ’1 (1) \\n \\nzl = MLP (LN (zË†l)) + zË†l (2) \\nEquations 1 and 2 illustrate the feature extraction process in VIS-MAE. Each atten- \\ntion stage is wrapped with a residual connection, following the Swin Transformer layers \\nto facilitate deeper network architectures without degradation in performance. The out- \\nput from the W -MSA is summed with its input to preserve the flow of gradients and \\nprevent the vanishing gradient problem. \\nWe also modified the masking method in VIS -MAE because Swin Transformer'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='put from the W -MSA is summed with its input to preserve the flow of gradients and \\nprevent the vanishing gradient problem. \\nWe also modified the masking method in VIS -MAE because Swin Transformer \\nblocks use a 4 Ã— 4 patch size which leads to easy reconstruction for deep learning models. \\nIn our study, the minimum size (16 Ã— 16 pixels) of a random mask will contain multiple \\nimage patches. Masking could encompass up to 75% of the original image. The objective \\nfunction of the VIS-MAE encourages the model to minimize the reconstruction error \\n-- \\nbetween the original image patches X and the reconstructed image patches X, which are  \\nâˆ’  1 \\n \\ngenerated from the masked version of the original image through the encoder-decoder \\narchitecture of the VIS-MAE: \\nLoss = 1X \\n-- \\n2 (3) \\nX 2 \\nIn Eq. 3, the squared L2 norm function will quantify the pixel -wise reconstruction \\nerror across the masked regions of ground truth and the predicted images. The inte -'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='Loss = 1X \\n-- \\n2 (3) \\nX 2 \\nIn Eq. 3, the squared L2 norm function will quantify the pixel -wise reconstruction \\nerror across the masked regions of ground truth and the predicted images. The inte - \\ngration of Swin Transformer with the modified masking strategy allows VIS -MAE to \\neffectively handle diverse and complex large-scale training datasets, providing a pow- \\nerful tool for developing pre-trained model weights for precise image segmentation and \\nclassification. In addition, the encoder-decoder architecture of VIS-MAE can be easily \\ntailored to the needs of visual understanding tasks by adding a skip connection layer  \\nor classification head, enabling our model to achieve state -of-the-art performance on \\nstandard benchmarks. \\n \\nFig. 1. The architecture of VIS-MAE and its implication on downstream tasks, including segmen- \\ntation. VIS-MAE consists of an encoder, bottleneck blocks, and a decoder. Images are first masked'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='Fig. 1. The architecture of VIS-MAE and its implication on downstream tasks, including segmen- \\ntation. VIS-MAE consists of an encoder, bottleneck blocks, and a decoder. Images are first masked \\nrandomly with a patch, and then the network reconstructs the original image. The VIS -MAE \\nmodel weights can then be efficiently adapted to downstream applications and have additional \\nskip connection layers added for segmentation tasks. \\n \\n \\n \\n3 Experiments and Results \\n3.1 Datasets \\nThe dataset used to develop VIS-MAE was collected from an outpatient radiology prac- \\ntice (RadImageNet LLC) in New York between 2005 and 2022. It consists of 2,486,425 \\nimages from five imaging modalities: MR (1,199,904 images), CT (570,943 images), \\nPET/CT (65,731 images), X-rays (438,521 images), and ultrasound (211,325 images).  \\n \\n \\nThis dataset expands the RadImageNet [24] project, adding new modalities and anatom- \\nical regions. MR, CT, and PET/CT images were specifically selected for the presence'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='This dataset expands the RadImageNet [24] project, adding new modalities and anatom- \\nical regions. MR, CT, and PET/CT images were specifically selected for the presence \\nof significant pathology by a radiologist, while all images from X -ray and ultrasound \\nstudies were included. The patient cohort had a mean age of 54 (SD = 18), with 191,193 \\nfemale and 169,915 male participants; 175 did not report gender. Sampled ethnicity data \\nindicate 18.73% Hispanic or Latino and 81.27% not, while race was 70.93% White, \\n14.78% Black or African American, 7.26% Asian, and small percentages for other cate- \\ngories. All demographic data were self-reported. All images were resized to 224 Ã— 224 \\npixels and normalized to 0â€“1 for model development. For CT, before image transforma- \\ntion all images had a window/level applied as described in the corresponding DICOM \\nheader. \\nFor downstream tasks, we assessed VIS-MAE pre-trained weights on eight segmen-'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='tion all images had a window/level applied as described in the corresponding DICOM \\nheader. \\nFor downstream tasks, we assessed VIS-MAE pre-trained weights on eight segmen- \\ntation datasets, including the BTCV-Abdomen dataset [25] with 2,178 CT slices of 13 \\nabdominal organs, the ACDC dataset [26] with 2,978 cardiac MR slices of heart regions, \\nthe AMOS dataset [27] with 2,476 abdomen MR slices of 12 organs, a prostate dataset \\n[28] with 3,554 MR slices, a brain segmentation dataset with 1,373 MR slices for glioma \\n[29], the BUSI segmentation dataset [30] of 647 breast ultrasound images, the Thyroid \\nUltrasound Cine-clip (TUCC) [31] dataset with 17,641 thyroid ultrasound images, and \\nthe ISIC [32] dataset with 1,279 dermoscopy images. \\nAdditionally, VIS -MAE was evaluated on six classification tasks: a COVID -19 \\ndataset [3] of 9,050 CT chest images, an internal sarcoidosis dataset of 1,231 PET/MR \\nimages collected from the Mount Sinai Hospital, the BUSI dataset [ 29] of 647 images'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='dataset [3] of 9,050 CT chest images, an internal sarcoidosis dataset of 1,231 PET/MR \\nimages collected from the Mount Sinai Hospital, the BUSI dataset [ 29] of 647 images \\nfor breast lesion malignancy detection, an ACL dataset [33] of 1,021 MR knee images, \\na knee osteoarthritis dataset [34] of 1,650 X-ray images with five grades, and the NIH \\nChest X-ray [35] dataset with 112,120 images of 14 pulmonary classes. \\n \\n3.2 VIS-MAE Implementation Details \\nUpstream Model Development. The VIS-MAE model training was extensive, devel- \\noping distinct pre-trained weights for different imaging modalities over 800 epochs with \\na batch size of 640, using the AdamW optimizer and a mean square error loss function. \\nThe initial learning rate was 0.0001, decreasing with training. There was a 10 -epoch \\nwarmup. Training took between 11 and 516 h on 8 NVIDIA DGX A100 GPUs, depend- \\ning on the modality. To explore modality-specific features and their comparison to a uni-'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='warmup. Training took between 11 and 516 h on 8 NVIDIA DGX A100 GPUs, depend- \\ning on the modality. To explore modality-specific features and their comparison to a uni- \\nfied model, we developed two versions of VIS-MAE: VIS-MAE-Modality for specific \\nimaging modalities and VIS-MAE-Generic as a comprehensive model. The VIS-MAE- \\nGeneric was created using a dataset comprised of 2.5 million images from five imaging \\nmodalities. In contrast, VIS -MAE-Modality consists of five models: VIS -MAE-MR, \\nVIS-MAE-PET, VIS-MAE-CT, VIS-MAE-XRAY, and VIS-MAE-ultrasound, designed \\nto address the nuances of specific imaging modalities. \\nDownstream Model Development.  To evaluate the utility of VIS -MAE for various \\nmedical applications, eight segmentation and six classification downstream tasks were \\nchosen based on their anatomical, modal, and pathological diversity. The VIS -MAE  \\n \\npre-trained weights were finetuned on these downstream applications with the follow-'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='chosen based on their anatomical, modal, and pathological diversity. The VIS -MAE  \\n \\npre-trained weights were finetuned on these downstream applications with the follow- \\ning strategies. VIS-MAE models had standardized warmup periods (40 epochs for seg- \\nmentation, 10 for classification) and training durations varied (150 epochs for segmen- \\ntation, 50 for classification), with adjustable learning rates and batch sizes. We first \\nevaluated the performance of the modality-specific VIS-MAE models against the VIS- \\nMAE-Generic model. In addition, we compared VIS-MAE against several benchmark \\nmodels in both segmentation and classification tasks. For segmentation, we utilized \\nstrategies including nnU-Net [36], TransUNet [37], and SimCLR. nnU-Net was adapted \\nto match the VIS-MAE data distribution and underwent a similar training regime, uti - \\nlizing a unique pre -processing method tailored to enhance performance. TransUNet'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='to match the VIS-MAE data distribution and underwent a similar training regime, uti - \\nlizing a unique pre -processing method tailored to enhance performance. TransUNet \\nemployed the SGD optimizer, leveraging pre-trained weights from ImageNet-1k [38], a \\nvast dataset of approximately 1.3 million natural images across 1,000 categories, widely \\nused for training models via traditional supervised learning. Both nnU -Net and Tran - \\nsUNet were configured using the default parameters recommended in their original pub- \\nlications. We also used models pre-trained on RadImageNet, a specialized radiological \\ndataset featuring 1.35 million images annotated across 165 pathological labels and 14 \\nanatomical regions, including CT, MR, and ultrasound modalities. Both RadImageNet \\nand ImageNet-1k pre-trained weights were configured to follow similar downstream \\nparameters as VIS -MAE for consistency, allowing for a direct comparison of perfor -'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='and ImageNet-1k pre-trained weights were configured to follow similar downstream \\nparameters as VIS -MAE for consistency, allowing for a direct comparison of perfor - \\nmance. Additionally, the SimCLR weights were pre-trained using a Swin Transformer \\narchitecture with the same data as VIS-MAE, ensuring aligned training parameters for \\nsubsequent tasks. Except for nnU-Net and TransUNet, all models employed distinct pre- \\ntrained weights for segmentation and classification but utilized the same architectural \\nframework and fine-tuning processes for downstream applications. Models were evalu- \\nated with the Dice Similarity Coefficient (DSC), precision, and recall for segmentation \\ntasks, and with the area under the ROC curve (ROC AUC) and Precision-Recall curve \\nAUC (PR AUC) score for classification tasks. For each dataset containing fewer than \\n4000 images, we conducted five-fold cross-validation and reported the average metrics'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='AUC (PR AUC) score for classification tasks. For each dataset containing fewer than \\n4000 images, we conducted five-fold cross-validation and reported the average metrics \\nfrom these five folds. In developing the segmentation models, we employed a combi - \\nnation of the dice loss and cross -entropy loss in a 4:6 ratio. For training classification \\nmodels, we utilized cross-entropy loss. \\n \\n \\n3.3 VIS-MAE on Segmentation and Classification Datasets \\nThe VIS-MAEs models were assessed across eight medical image segmentation datasets \\nwith various imaging modalities and anatomies. They were compared to several bench- \\nmarks, including nnU-Net, TransUNet, two supervised learning pre-trained Swin Trans- \\nformer weights, RadImageNet and ImageNet, and another SSL strategy, SimCLR. VIS- \\nMAE showed superior or comparable performance to these models, as illustrated in \\nTables 1a and 1b. Additionally, VIS -MAEâ€™s performance on classification datasets'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='MAE showed superior or comparable performance to these models, as illustrated in \\nTables 1a and 1b. Additionally, VIS -MAEâ€™s performance on classification datasets \\nwere compared to RadImageNet, ImageNet, and SimCLR weights and outperformed \\nor achieved equivalent performance to these existing models, with detailed outcomes \\npresented in Table 2.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nTable 1a. Model performance on segmentation datasets in DSC, precision, and recall. \\n \\nMethods BTCV \\n(CT) \\nACDC \\n(MR) \\nAMOS \\n(MR) \\nGlioma \\n(MR) \\nDSC Precision Recall DSC Precision Recall DSC Precision Recall DSC Precision Recall \\nVIS-MAE-Modality 0.844 0.809 0.765 0.879 0.893 0.878 0.891 0.899 0.875 0.866 0.881 0.875 \\nVIS-MAE-Generic 0.854 0.815 0.781 0.882 0.895 0.877 0.862 0.903 0.869 0.871 0.870 0.887 \\nnnU-Net 0.842 0.803 0.761 0.883 0.899 0.873 0.877 0.847 0.823 0.881 0.889 0.890 \\nTransUNet 0.858 0.794 0.748 0.874 0.895 0.855 0.863 0.897 0.836 0.833 0.856 0.840'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='nnU-Net 0.842 0.803 0.761 0.883 0.899 0.873 0.877 0.847 0.823 0.881 0.889 0.890 \\nTransUNet 0.858 0.794 0.748 0.874 0.895 0.855 0.863 0.897 0.836 0.833 0.856 0.840 \\nRadImageNet 0.852 0.831 0.790 0.880 0.891 0.875 0.873 0.899 0.875 0.874 0.878 0.881 \\nImageNet 0.855 0.820 0.781 0.882 0.895 0.879 0.869 0.893 0.892 0.874 0.881 0.886 \\nSimCLR 0.813 0.768 0.726 0.861 0.881 0.853 0.847 0.860 0.851 0.852 0.864 0.862  \\n \\n \\n \\n \\n \\n \\n \\n \\nTable 1b. Model performance on segmentation datasets in DSC, precision, and recall. \\n \\nMethods Prostate \\n(MR) \\nTUCC \\n(US) \\nBUSI \\n(US) \\nISIC \\n(Dermoscopy) \\nDSC Precision Recall DSC Precision Recall DSC Precision Recall DSC Precision Recall \\nVIS-MAE-Modality 0.902 0.840 0.824 0.720 0.740 0.717 0.780 0.797 0.791 0.923 0.926 0.906 \\nVIS-MAE-Generic 0.825 0.834 0.816 0.706 0.687 0.786 0.775 0.789 0.794 0.908 0.920 0.915 \\nnnU-Net 0.833 0.837 0.833 0.716 0.765 0.756 0.783 0.803 0.811 0.904 0.900 0.930'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='VIS-MAE-Generic 0.825 0.834 0.816 0.706 0.687 0.786 0.775 0.789 0.794 0.908 0.920 0.915 \\nnnU-Net 0.833 0.837 0.833 0.716 0.765 0.756 0.783 0.803 0.811 0.904 0.900 0.930 \\nTransUNet 0.815 0.839 0.797 0.696 0.788 0.704 0.776 0.769 0.747 0.919 0.936 0.918 \\nRadImageNet 0.829 0.830 0.833 0.705 0.759 0.758 0.780 0.805 0.814 0.916 0.928 0.912 \\nImageNet 0.832 0.844 0.832 0.718 0.736 0.799 0.788 0.810 0.822 0.919 0.930 0.921 \\nSimCLR 0.802 0.816 0.803 0.619 0.708 0.723 0.758 0.781 0.781 0.910 0.923 0.914  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nTable 2. VIS-MAE performance on classification datasets in ROC AUC score and PR AUC score. \\n \\nDatasets COVID-19 (CT) Sarcoidosis (PET/MR) ACL tear (MR) Knee osteoarthritis \\n(X-ray) \\nBUSI \\n(US) \\nNIH Chest \\n(X-Ray) \\nROC \\nAUC \\nPR \\nAUC \\nROC \\nAUC \\nPR AUC ROC \\nAUC \\nPR AUC ROC \\nAUC \\nPR AUC ROC \\nAUC \\nPR AUC ROC \\nAUC \\nPR \\nAUC \\nVIS-MAE-Modality 0.857 0.856 0.624 0.237 0.946 0.958 0.938 0.797 0.867 0.794 0.802 0.262'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='ROC \\nAUC \\nPR \\nAUC \\nROC \\nAUC \\nPR AUC ROC \\nAUC \\nPR AUC ROC \\nAUC \\nPR AUC ROC \\nAUC \\nPR AUC ROC \\nAUC \\nPR \\nAUC \\nVIS-MAE-Modality 0.857 0.856 0.624 0.237 0.946 0.958 0.938 0.797 0.867 0.794 0.802 0.262 \\nVIS-MAE-Generic 0.862 0.867 0.680 0.338 0.936 0.953 0.800 0.531 0.736 0.591 0.799 0.256 \\nRaImageNet 0.867 0.876 0.637 0.284 0.938 0.950 0.911 0.735 0.868 0.806 0.804 0.269 \\nImageNet 0.863 0.861 0.616 0.234 0.937 0.956 0.926 0.766 0.787 0.658 0.812 0.275 \\nSimCLR 0.819 0.827 0.644 0.231 0.912 0.931 0.898 0.707 0.899 0.841 0.749 0.189  \\n3.4 Label Efficiency in Segmentation and Classification Applications \\nLabel efficiency measures the training data and annotations required to achieve desired \\nperformance, reflecting the burden on medical professionals. To assess the label effi - \\nciency of VIS-MAE, we compared its performance in segmentation and classification \\ndatasets using only 5%, 10%, 25%, 50%, and 80% of the original training data. VIS -'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='ciency of VIS-MAE, we compared its performance in segmentation and classification \\ndatasets using only 5%, 10%, 25%, 50%, and 80% of the original training data. VIS - \\nMAEâ€™s performance across various datasets compared to other models was demonstrated \\nin Figs. 2 and 3. \\n \\nFig. 2. Label efficiency of pre -trained weights VIS -MAE-Modality, VIS-MAE-Generic, Sim- \\nCLR, RadImageNet and ImageNet on eight segmentation datasets, using different percentages of \\ntraining data. All models were trained using 5%, 10%, 25%, 50%, 80%, and 100% of the training \\ndata in each segmentation dataset, and the DSCs were reported.  \\n \\n \\n \\nFig. 3. Label efficiency of pre -trained weights VIS -MAE-Modality, VIS-MAE-Generic, Sim- \\nCLR, RadImageNet and ImageNet on six classification datasets, using different percentages of \\ntraining data. All models were trained using 5%, 10%, 25%, 50%, 80%, and 100% of the training \\ndata in each classification dataset, and the ROC AUC scores were reported. \\n \\n4 Conclusion'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='training data. All models were trained using 5%, 10%, 25%, 50%, 80%, and 100% of the training \\ndata in each classification dataset, and the ROC AUC scores were reported. \\n \\n4 Conclusion \\nThis paper presents VIS -MAE, a self -supervised learning model utilizing a masked \\nautoencoder trained on a large collection of radiological images. VIS -MAEâ€™s model \\nweights enhance various medical imaging tasks by either integrating skip connections for \\nsegmentation or substituting the decoder with a classification layer. Our extensive eval- \\nuation demonstrates VIS-MAEâ€™s superior performance across multiple imaging modal- \\nities, improving both segmentation and classification. Moreover, VIS -MAE enhances \\nlabel efficiency, reducing the need for extensive annotated data while maintaining high \\nperformance and generalizability in medical imaging. \\n \\nAcknowledgement. X.M. is supported by the Eric and Wendy Schmidt AI in Human Health \\nProgram.'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='performance and generalizability in medical imaging. \\n \\nAcknowledgement. X.M. is supported by the Eric and Wendy Schmidt AI in Human Health \\nProgram. \\n \\n \\nDisclosure of Interests. T.D. is the managing partner of RadImageNet LLC. X.M. has been a \\npaid consultant to RadImageNet LLC. \\n \\nReferences \\n1. Bommasani, R., et al.: On the opportunities and risks of foundation models (2021) \\n2. Mei, X., et al.: Interstitial lung disease diagnosis and prognosis using an AI system integrating \\nlongitudinal data. Nat. Commun. 14, 2272 (2023) \\n3. Mei, X., et al.: Artificial intelligence â€“enabled rapid diagnosis of patients with COVID -19. \\nNat. Med. 26, 1224â€“1228 (2020) \\n4. Zech, J.R., et al.: Variable generalization performance of a deep learning model to detect \\npneumonia in chest radiographs: a cross-sectional study. PLoS Med. 15, e1002683 (2018) \\n5. Chen, X., et al.: Recent advances and clinical applications of deep learning in medical image \\nanalysis. Med. Image Anal. 79, 102444 (2022)'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='5. Chen, X., et al.: Recent advances and clinical applications of deep learning in medical image \\nanalysis. Med. Image Anal. 79, 102444 (2022) \\n6. Soffer, S., et al.: Convolutional neural networks for radiologic images: a radiologistâ€™s guide. \\nRadiology 290, 590â€“606 (2019) \\n7. Langlotz, C.P., et al.: A Roadmap for foundational research on artificial intelligence in medical \\nimaging: from the 2018 NIH/RSNA/ACR/the academy workshop. Radiology 291, 781â€“791 \\n(2019) \\n8. Li, J., et al.: A systematic collection of medical image datasets for deep learning (2021) \\n9. Willemink, M.J., et al.: Preparing medical imaging data for machine learning. Radiology 295, \\n4â€“15 (2020) \\n10. Park, S.H., Han, K.: Methodologic guide for evaluating clinical performance and effect of \\nartificial intelligence technology for medical diagnosis and prediction. Radiology 286, 800â€“ \\n809 (2018) \\n11. Kirillov, A., et al.: Segment anything (2023) \\n12. Ma, J., et al.: Segment anything in medical images (2023)'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='809 (2018) \\n11. Kirillov, A., et al.: Segment anything (2023) \\n12. Ma, J., et al.: Segment anything in medical images (2023) \\n13. Zhou, Y., et al.: A foundation model for generalizable disease detection from retinal images. \\nNature 622, 156â€“163 (2023) \\n14. Yan, Z., et al.: A foundation model for general moving object segmentation in medical images \\n(2023) \\n15. Du, Y., et al.: SegVol: universal and interactive volumetric medical image segmentation (2023) \\n16. Qiu, J., et al.: VisionFM: a multi-modal multi-task vision foundation model for generalist \\nophthalmic artificial intelligence (2023) \\n17. Campanella, G., et al.: Computational pathology at health system scale -- self-supervised \\nfoundation models from three billion images (2023) \\n18. Liu, Z., et al.: A review of self-supervised, generative, and few-shot deep learning methods \\nfor data-limited magnetic resonance imaging segmentation. NMR Biomed. e5143 (2024)'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='18. Liu, Z., et al.: A review of self-supervised, generative, and few-shot deep learning methods \\nfor data-limited magnetic resonance imaging segmentation. NMR Biomed. e5143 (2024) \\n19. Chen, T., et al.: A simple framework for contrastive learning of visual representations (2020) \\n20. He, K., et al.: Masked autoencoders are scalable vision learners. In: 2022 IEEE/CVF Confer- \\nence on Computer Vision and Pattern Recognition (CVPR), pp. 15979 â€“15988. IEEE, New \\nOrleans (2022) \\n21. Azizi, S., et al.: Big self -supervised models advance medical image classification. In: 2021 \\nIEEE/CVF International Conference on Computer Vision (ICCV), pp. 3458 â€“3468. IEEE, \\nMontreal (2021) \\n22. Xu, Z., et al.: Swin MAE: masked autoencoders for small datasets. Comput. Biol. Med. 161, \\n107037 (2023) \\n23. Liu, Z., et al.: Swin transformer: hierarchical vision transformer using shifted windows. In: \\n2021 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 9992 â€“10002. \\nIEEE, Montreal (2021)'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='2021 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 9992 â€“10002. \\nIEEE, Montreal (2021) \\n24. Mei, X., et al.: RadImageNet: an open radiologic deep learning research dataset for effective \\ntransfer learning. Radiol. Artif. Intell. 4, e210315 (2022)  \\n \\n25. Landman, B., Xu, Z., Igelsias, J.E., Styner, M., Langerak, T., Klein, A.: Segmentation Outside \\nthe Cranial Vault Challenge (2015) \\n26. Bernard, O., et al.: Deep learning techniques for automatic MRI cardiac multi -structures \\nsegmentation and diagnosis: is the problem solved? IEEE Trans. Med. Imaging 37, 2514â€“2525 \\n(2018) \\n27. Ji, Y., et al.: AMOS: a large -scale abdominal multi-organ benchmark for versatile medical \\nimage segmentation (2022). http://arxiv.org/abs/2206.08023 \\n28. Adams, L.C., et al.: Prostate158 - an expert-annotated 3T MRI dataset and algorithm for \\nprostate cancer detection. Comput. Biol. Med. 148, 105817 (2022)'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='28. Adams, L.C., et al.: Prostate158 - an expert-annotated 3T MRI dataset and algorithm for \\nprostate cancer detection. Comput. Biol. Med. 148, 105817 (2022) \\n29. Buda, M., et al.: Association of genomic subtypes of lower-grade gliomas with shape features \\nautomatically extracted by a deep learning algorithm. Comput. Biol. Med. 109, 218â€“225 \\n(2019) \\n30. Al-Dhabyani, W., et al.: Dataset of breast ultrasound images. Data Brief 28, 104863 (2020) \\n31. Thyroid Ultrasound Cine -clip (2021). https://stanfordaimi.azurewebsites.net/datasets/a72 \\nf2b02-7b53-4c5d-963c-d7253220bfd5 \\n32. Codella, N.C.F., et al.: Skin lesion analysis toward melanoma detection: a challenge at the \\n2017 International symposium on biomedical imaging (ISBI), hosted by the international skin \\nimaging collaboration (ISIC). In: 2018 IEEE 15th International Symposium on Biomedical \\nImaging (ISBI 2018), pp. 168â€“172. IEEE, Washington, DC (2018)'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='imaging collaboration (ISIC). In: 2018 IEEE 15th International Symposium on Biomedical \\nImaging (ISBI 2018), pp. 168â€“172. IEEE, Washington, DC (2018) \\n33. Bien, N., et al.: Deep -learning-assisted diagnosis for knee magnetic resonance imaging: \\ndevelopment and retrospective validation of MRNet. PLoS Med. 15, e1002699 (2018) \\n34. Gornale, S.S., et al.: Automatic detection and classification of knee osteoarthritis using Huâ€™s \\ninvariant moments. Front. Robot. AI. 7, 591827 (2020) \\n35. Wang, X., et al.: ChestX -Ray8: hospital -scale chest X -ray database and benchmarks on \\nweakly-supervised classification and localization of common thorax diseases. In: 2017 IEEE \\nConference on Computer Vision and Pattern Recognition (CVPR), pp. 3462 â€“3471. IEEE, \\nHonolulu (2017) \\n36. Isensee, F., et al.: NnU -Net: a self-configuring method for deep learning -based biomedical \\nimage segmentation. Nat. Methods 18, 203â€“211 (2021)'),\n",
       " Document(metadata={'arxiv_id': '2402.01034v3', 'title': 'VIS-MAE: An Efficient Self-supervised Learning Approach on Medical Image Segmentation and Classification', 'section': 'body', 'authors': 'Zelong Liu, Andrew Tieu, Nikhil Patel'}, page_content='Honolulu (2017) \\n36. Isensee, F., et al.: NnU -Net: a self-configuring method for deep learning -based biomedical \\nimage segmentation. Nat. Methods 18, 203â€“211 (2021) \\n37. Chen, J., et al.: TransUNet: transformers make strong encoders for medical image segmenta- \\ntion (2021) \\n38. Deng, J., et al.: ImageNet: a large -scale hierarchical image database. In: 2009 IEEE \\nConference on Computer Vision and Pattern Recognition, pp. 248â€“255. IEEE, Miami (2009)'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'title_abstract', 'authors': 'Ziyang Wang'}, page_content='Title: Deep Learning in Medical Ultrasound Image Segmentation: a Review\\n\\nAbstract: Applying machine learning technologies, especially deep learning, into medical image segmentation is being widely studied because of its state-of-the-art performance and results. It can be a key step to provide a reliable basis for clinical diagnosis, such as 3D reconstruction of human tissues, image-guided interventions, image analyzing and visualization. In this review article, deep-learning-based methods for ultrasound image segmentation are categorized into six main groups according to their architectures and training at first. Secondly, for each group, several current representative algorithms are selected, introduced, analyzed and summarized in detail. In addition, common evaluation methods for image segmentation and ultrasound image segmentation datasets are summarized. Further, the performance of the current methods and their evaluations are reviewed. In the end, the challenges and potential research directions for medical ultrasound image segmentation are discussed.'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='Deep Learning in Medical Ultrasound Image Segmentation: a Review\\nZiyang Wang1âˆ—\\n1University of Oxford\\nziyang.wang@cs.ox.ac.uk\\nAbstract\\nApplying machine learning technologies, espe-\\ncially deep learning, into medical image segmen-\\ntation is being widely studied because of its state-\\nof-the-art performance and results. It can be a\\nkey step to provide a reliable basis for clinical di-\\nagnosis, such as the 3D reconstruction of human\\ntissues, image-guided interventions, image analyz-\\ning and visualization. In this review article, deep-\\nlearning-based methods for ultrasound image seg-\\nmentation are categorized into six main groups ac-\\ncording to their architectures and training methods\\nat ï¬rst. Secondly, for each group, several current\\nrepresentative algorithms are selected, introduced,\\nanalyzed and summarized in detail. In addition,\\ncommon evaluation methods and datasets for ultra-\\nsound image segmentation are summarized. Fur-\\nthermore, the performance of the current methods'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='analyzed and summarized in detail. In addition,\\ncommon evaluation methods and datasets for ultra-\\nsound image segmentation are summarized. Fur-\\nthermore, the performance of the current methods\\nand their evaluation results are reviewed. In the\\nend, the challenges and potential research direc-\\ntions for medical ultrasound image segmentation\\nare discussed.\\n1 Introduction\\nMedical ultrasound imaging provides the inside structure of\\nthe human body with high-frequency sound waves which is\\nsafe, painless, noninvasive, non-ionized and real-time. Ultra-\\nsound imaging as compared to other imaging tools, such as\\nComputed Tomography (CT) and Magnetic Resonance Imag-\\ning (MRI), is cheaper, portable and more prevalent. It helps\\nto diagnose the causes of pain, swelling, and infection in in-\\nternal organs, for evaluation and treatment of medical condi-\\ntions. Ultrasound imaging has turned into a general checkup\\nmethod for prenatal diagnosis. However, several drawbacks'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='ternal organs, for evaluation and treatment of medical condi-\\ntions. Ultrasound imaging has turned into a general checkup\\nmethod for prenatal diagnosis. However, several drawbacks\\nsuch as the necessity of a skilled operator, the difï¬culty of\\ndistinguishing imaging structures between tissue and gas, and\\nthe limitation of the ï¬eld of view bring more challenges on\\nimage processing algorithms study. Many approaches have\\nbeen proposed to solve these problems, and one of the most\\nimportant approaches is deep learning. The development of\\ndeep-learning-based medical ultrasound image segmentation\\nâˆ—Contact Author\\ntechnology plays an essential and fundamental role in the\\nanalysis of biomedical images and signiï¬cantly contributes\\nto classiï¬cation, recognition, visualization, 3D reconstruction\\nand image-guided intervention, which can provide reliable\\nguidance for doctors in clinical diagnosis. Compared with\\nprevious papers on image segmentation and deep learning,'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='and image-guided intervention, which can provide reliable\\nguidance for doctors in clinical diagnosis. Compared with\\nprevious papers on image segmentation and deep learning,\\nthis is the ï¬rst review on applying deep learning approaches\\nto medical ultrasound image segmentation. The following\\ncontributions are made for deep learning and ultrasound im-\\nage segmentation community:\\nâ€¢ To the best of our knowledge, this review paper is the only\\npaper provides comprehensive introduction and memoriza-\\ntion on medical ultrasound image segmentation with deep\\nlearning methods.\\nâ€¢ This review paper groups the related deep learning litera-\\nture into six sections based on different architectures or train-\\ning method of deep learning models including: Fully Con-\\nvolutional Neural Networks (FCN), Encoder-Decoder Neu-\\nral Networks (EN-DEcoder), Recurrent Neural Networks\\n(RNN), Generative Adversarial Networks (GAN), Weakly\\nSupervised Learning (WSL) and Deep Reinforcement Learn-'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='ral Networks (EN-DEcoder), Recurrent Neural Networks\\n(RNN), Generative Adversarial Networks (GAN), Weakly\\nSupervised Learning (WSL) and Deep Reinforcement Learn-\\ning (DFL) methods. The overview of the number of published\\npapers in recent years is shown in Figure 1.\\nâ€¢ The evaluation methods, common datasets and experimen-\\ntal performance results of current deep learning approaches\\nare systematically organized and reviewed.\\nThe following Section 2, Section 3, Section 4, Section 5,\\nSection 6, and Section 7 review each categorized group of\\ndeep learning models, respectively. Section 8 summarizes\\ncommon evaluation methods, datasets and the comparison of\\nresults. Section 9 summarizes and discusses the key issues\\nand potential research directions in the ï¬eld of deep learning\\nin ultrasound image segmentation.\\n2 Fully Convolutional Neural Networks for\\nSegmentation\\nFully Convolutional Networks (FCN) is ï¬rstly introduced by\\n[Long et al., 2015], which is one of the most commonly used'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='2 Fully Convolutional Neural Networks for\\nSegmentation\\nFully Convolutional Networks (FCN) is ï¬rstly introduced by\\n[Long et al., 2015], which is one of the most commonly used\\nCNN-based neural networks for image semantic segmenta-\\ntion. It is a pixels-to-pixels supervised learning approach\\nwhich is compatible to any size of images. The FCN architec-\\nture shown in Figure 2a is mainly developed by a classiï¬ca-\\ntion network where fully connected layers are replaced with\\narXiv:2002.07703v3  [eess.IV]  5 Mar 2021(a) The percentage of number\\nof papers for each group\\n(b) The number of published\\npapers for US segmentation\\nFigure 1: Overview of published papers\\nfully convolutional networks. A skip step (skip layer and bi-\\nlinear interpolation) is proposed to extend the application of\\nclassiï¬cation network to dense prediction, so that accurate\\npixel-level segmentation is achieved on the entire images.\\nMeanwhile, several state-of-the-art FCN based methods'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='classiï¬cation network to dense prediction, so that accurate\\npixel-level segmentation is achieved on the entire images.\\nMeanwhile, several state-of-the-art FCN based methods\\nhave been proposed in medical ultrasound image semantic\\nsegmentation. [Zhang et al. , 2016] proposed a lymph node\\nsegmentation framework in 2016. The framework consists\\nof two FCN-based modules which the ï¬rst one is to collect\\nraw images to produce potential objectsâ€™ segmentation, and\\nthe other one generates ï¬nal lymph nodes segmentation with\\nthe intermediate results and the raw images. Multi-stage in-\\ncremental learning and multi FCN modules concept are in-\\ntroduced and investigated. The CFS-FCN model is shown in\\nFigure 2b\\n[Wu et al., 2017] came up a cascaded fully convolutional\\nnetworks for ultrasound image segmentation in 2017. To\\navoid the boundary incompleteness and ambiguity, an Auto-\\nContext scheme modiï¬ed by join operator is implanted into\\nFCN model. [Mishra et al. , 2018 ] proposed a FCN model'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='avoid the boundary incompleteness and ambiguity, an Auto-\\nContext scheme modiï¬ed by join operator is implanted into\\nFCN model. [Mishra et al. , 2018 ] proposed a FCN model\\nwith attentional deep supervision named DSN-OB in 2018.\\nCoarse resolution layers are trained to discriminate the ob-\\nject region from background, and ï¬ne resolution layers are\\ntrained for object boundary deï¬nitions. A speciï¬c training\\nscheme and fusion layer are developed to avoid the broken\\nboundaries which is a common ultrasound image segmenta-\\ntion challenge. The framework is ï¬nally approved a promis-\\ning performance for blood region and lesion segmentation,\\nrespectively.\\nIn addition, 3D Ultrasound imaging can provide richer spa-\\ntial information of the tissues than the traditional 2D X-ray\\nimaging. To better exploit the 3D spatial information while\\nmaking full use of the 2D pretrained model, Yang et al. pro-\\nposed a catheter detection method named Direction-Fused'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='imaging. To better exploit the 3D spatial information while\\nmaking full use of the 2D pretrained model, Yang et al. pro-\\nposed a catheter detection method named Direction-Fused\\nFCN (DF-FCN), which exploits 3D information through re-\\norganized cross-sections to segment the catheter [Yang et al.,\\n2019]. In this work, the 3D ultrasound volume is divided into\\nseveral 2D images ï¬rstly, then each image is processed by\\nthe FCN to acquire the probability prediction. Moreover, as\\nshown in Figure 2c, the results predicted by FCN are stacked\\ntogether on the original position to construct the feature maps\\nin three different directions, and these three feature maps are\\ndirection-fused to enhance the inter-slice context information.\\nAt last, the ï¬nal segmentation results are obtained by apply-\\ning a 3D convolution and a softmax layer.\\n3 Encoder-Decoder Neural Networks for\\nSegmentation\\nTo recover the pixelsâ€™ location information lost in pooling'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='ing a 3D convolution and a softmax layer.\\n3 Encoder-Decoder Neural Networks for\\nSegmentation\\nTo recover the pixelsâ€™ location information lost in pooling\\noperation, encoder-decoder architecture based networks were\\nproposed introducing opposite operations including convolu-\\ntion and deconvolution (or transpose convolution), pooling\\nand unpooling. The encoder approach is to collect pixel lo-\\ncation features, and then decoder approach is to restore the\\nspatial dimension and pixel location features. Therefore, the\\ninformation of feature and pixel location can be fully retained\\nand analyzed.\\nIn 2015, a completely symmetric architecture, DeconvNet\\nwas proposed by [Noh et al., 2015] with deconvolution and\\nunpooling layers. It was applied to ultrasound segmentation\\nof cervical muscle in [Cunningham et al. , 2019 ], compar-\\ning performance of integrating with Exponential Linear Unit\\n(ELU) and Maxout to solve â€˜dying ReLUâ€™ problem.\\nIn the same year, [Ronneberger and et al, 2015 ] proposed'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='ing performance of integrating with Exponential Linear Unit\\n(ELU) and Maxout to solve â€˜dying ReLUâ€™ problem.\\nIn the same year, [Ronneberger and et al, 2015 ] proposed\\nU-net, as shown in Figure 2d, with increased channel num-\\nber in decoder for propagation of context feature in higher\\nresolution layers, and cropped feature maps concatenated\\nfrom encoder to decoder for localizing each pixel. Intro-\\nducing short connection into encoder part of U-net to avoid\\noverï¬tting and dynamically ï¬ne tune in speciï¬c test task,\\nZhou et al. proposed a dynamic convolution neural network\\nfor media-adventitia and lumen-intima segmentation in ultra-\\nsound [Zhou et al. , 2019 ]. [Zhuang et al. , 2019 ] proposed\\nGrouped-Resaunet (GRA U-net) for slice-by-slice nipple seg-\\nmentation and localization on breast ultrasound. GRA U-net\\nis an architecture based on U-net, employing residual block\\nfor solving vanishing gradient, group convolution for com-\\nputational efï¬ciency and attention gates for focusing on rel-'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='is an architecture based on U-net, employing residual block\\nfor solving vanishing gradient, group convolution for com-\\nputational efï¬ciency and attention gates for focusing on rel-\\nevant areas of input. Kim et al. proposed a network with\\nthe multi-scale input and hybrid multi-label loss function for\\nsegmentation of coronary arteries in intravascular ultrasound\\nimage [Kim et al., 2018]. Neural Architecture Search (NAS)\\nwas applied to semantic segmentation network in [Weng et\\nal., 2019] based on the structure of U-net, which was applied\\non a nerve ultrasound dataset.\\nAfter the success in 2D medical image segmentation\\nachieved by U-net, [Ã‡iÃ§ek et al., 2016] ï¬rst extended U-net\\nto 3D architecture for volumetric segmentation with 3D con-\\nvolution. In the same year, V-net shown in Figure 2e was\\nproposed by Milletari et al. for volumetric segmentation, in-\\ntegrated with 3D convolution and residual blocks [Milletari\\net al., 2016]. Lei et al. introduced a multi-directional deeply'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='proposed by Milletari et al. for volumetric segmentation, in-\\ntegrated with 3D convolution and residual blocks [Milletari\\net al., 2016]. Lei et al. introduced a multi-directional deeply\\nsupervised V-net, and applied it to prostate segmentation in\\nultrasound [Lei et al., 2019]. The network predicts different\\nresolution segmentation from each stage of decoder part, uses\\nmulti-directional contour reï¬nement processing for fusion of\\nsegmentation and applies a stage-wise hybrid loss function to\\nreduce convergence time.\\n4 Recurrent Neural Networks for\\nSegmentation\\nTo extract image historical context information and global\\nfeatures, Recurrent Neural Networks (RNN) is designed to\\nhandle sequential data, where the past learned knowledge(a) FCN\\n (b) CFS-FCN\\n (c) DF-FCN\\n (d) U-Net\\n(e) V-Net\\n (f) RNN\\n (g) 3D FCN and RNN\\n (h) GAN\\nFigure 2: The architecture for common deep learning neural networks\\ncan help make present decisions through a recurrent path.'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='(c) DF-FCN\\n (d) U-Net\\n(e) V-Net\\n (f) RNN\\n (g) 3D FCN and RNN\\n (h) GAN\\nFigure 2: The architecture for common deep learning neural networks\\ncan help make present decisions through a recurrent path.\\nLong Short-Term Memory (LSTM) and Gated Recurrent Unit\\n(GRU) are two most widely used architectures in the fam-\\nily of RNN, which can enable the network to model a long-\\nterm memory. As shown in Figure 2f, given a sequence of\\ndata, such as continuous 2D scan slices, a RNN model takes\\nthe ï¬rst image as input, extracts the features to make predic-\\ntion on each pixel and memorizes these information. Then,\\nthese information are propagated to help make decisions for\\nthe next image. In this way, the context information from ad-\\njacent slices can be fully utilized to improve the segmentation\\nperformance.\\n[Chen et al. , 2015 ] proposed a novel framework for au-\\ntomatic fetal ultrasound standard plane detection, which is\\na hybrid model integrating CNN and RNN. A ROI detector'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='performance.\\n[Chen et al. , 2015 ] proposed a novel framework for au-\\ntomatic fetal ultrasound standard plane detection, which is\\na hybrid model integrating CNN and RNN. A ROI detector\\nis ï¬rstly trained via the joint learning of convolutional neu-\\nral networks (J-CNN) to mitigate the challenge of limited\\nannotated data. Then, the LSTM model is used to explore\\nthe spatio-temporal features based on the ROIs in consecu-\\ntive frames. Finally, the standard planes are acquired if the\\nprediction score is larger than a threshold.\\n[Yang et al., 2017b] applied a general 3D FCN to achieve\\ndense voxel-wise semantic segmentation in ultrasound vol-\\numes, including fetus, gestational sac and placenta. He in-\\ntroduced a RNN trained with hierarchical deep supervision\\n(HiDS) through leveraging the 3D context information be-\\ntween sequential information ï¬‚ows from different directions\\nto reï¬ne the local segmentation results. The overall archi-\\ntecture of the network is shown in Figure 2g. In addition,'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='tween sequential information ï¬‚ows from different directions\\nto reï¬ne the local segmentation results. The overall archi-\\ntecture of the network is shown in Figure 2g. In addition,\\n[Yang et al. , 2017a ] proposed a novel framework for auto-\\nmatic prostate segmentation in ultrasound images. He intro-\\nduced a Boundary Completion Recurrent Neural Networks\\n(BCRNN) to learn the shape prior with the biologically plau-\\nsible and exploit these information to infer the incomplete-\\nness along prostate boundary which can help improve the\\nsegmentation performance. Then, multi-view predictions are\\nfused to obtain a comprehensive prediction. Finally, a multi-\\nscale Auto-Context scheme is applied to reï¬ne the details of\\nthe ï¬nal predictions.\\n5 Generative Adversarial Networks for\\nSegmentation\\nGenerative Adversarial Network (GAN) and their extensions\\nhave attracted a great deal of attention recently relying on the\\nability of tackling well known and challenging medical image'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='Segmentation\\nGenerative Adversarial Network (GAN) and their extensions\\nhave attracted a great deal of attention recently relying on the\\nability of tackling well known and challenging medical image\\nanalysis problems such segmentation, reconstruction, classi-\\nï¬cation or data simulation[Kazeminia et al., 2018]. The gen-\\neral idea of GAN is the combination of two neural networks\\nwith opposing goals and use adversarial training to realize\\njoint optimization. The ï¬rst network is a generator network\\nand jointly optimized with a discriminator as the second net-\\nwork. The goal of the generator is to generate outputs which\\ncan not be distinguished from a dataset of real examples by\\nthe discriminator network [Wolterink et al., 2018].\\n[Luc et al., 2016] presented the ï¬rst application of adver-\\nsarial training to semantic segmentation, which can detect and\\ncorrect higher-order inconsistencies between the segmenta-\\ntion results and ground truth segmentation maps. Their ap-'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='sarial training to semantic segmentation, which can detect and\\ncorrect higher-order inconsistencies between the segmenta-\\ntion results and ground truth segmentation maps. Their ap-\\nproach enforced long-range spatial label contiguity without\\nincreasing the model complexity. In the image segmenta-\\ntion, dense and pixel-level labeling are needed. It is challeng-\\ning to produce sufï¬cient and stable gradient feedback to the\\nnetworks from the single scalar classic GANâ€™s discriminator\\noutput. To overcome this limitation, a novel end-to-end ad-\\nversarial neural network with a multi-scale L1 loss function\\nwas proposed in [Xue et al. , 2018 ] for medical image seg-\\nmentation task. In this framework, the segmentor and critic\\nnetworks were trained to learn both global and local features,\\nwhich can capture long-range and short-range spatial rela-\\ntionships among pixels. For medical imaging, it is difï¬cult to\\ncapture 3D semantics in an effective way while keep compu-'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='which can capture long-range and short-range spatial rela-\\ntionships among pixels. For medical imaging, it is difï¬cult to\\ncapture 3D semantics in an effective way while keep compu-\\ntation efï¬cient at the same time. To address this, [Khosravan\\net al. , 2019] showed a novel projective adversarial network\\nâ€˜PANâ€™, which represented high-level 3D information through\\n2D projections. In this framework, He also introduced anattention module to realize a selective integration of global\\ninformation from segmentor to adversarial network directly.\\nFor automated ultrasound image segmentation, when come\\nacross appearance discrepancy, the deep models tend to per-\\nform poorly even on congeneric corpus where objects have\\nsimilar structure but slightly different appearance. [Yang et\\nal., 2018] tried to solve this general problem by using a novel\\nonline adversarial appearance conversion method shown in\\nFigure 2h. They put forward with a self-play training strat-'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='al., 2018] tried to solve this general problem by using a novel\\nonline adversarial appearance conversion method shown in\\nFigure 2h. They put forward with a self-play training strat-\\negy to pre-train all the adversarial modules for acquiring the\\nstructure and appearance distributions of source corpus. In\\naddition, the composite constraints for appearance and struc-\\nture in the framework also helped to remove appearance dis-\\ncrepancy iteratively in a weakly-supervised model.\\n6 Weakly Supervised Learning for\\nSegmentation\\nThe manually pixel-level labeled image dataset providing a\\nlot of detailed information can signiï¬cantly improve super-\\nvised training efï¬ciency and segmentation accuracy. Com-\\npared with the supervised training for CNN-based network,\\nsome researchers study on weakly supervised learning, be-\\ncause weakly labeled data requires low cost in labeling.\\n[Kim and Hwang, 2016 ] proposed a weakly supervised\\nframework with deconvolutional layers for semantic segmen-'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='cause weakly labeled data requires low cost in labeling.\\n[Kim and Hwang, 2016 ] proposed a weakly supervised\\nframework with deconvolutional layers for semantic segmen-\\ntation in 2016. Several datasets including lesion segmenta-\\ntion and natural visual object segmentation are tested. Exper-\\niments show that the deconvolutional layers reduce the false\\npositives, because these layers help to eliminate less discrim-\\ninative features.\\nFickleNet, a weakly or semi-supervised model for gen-\\neral image semantic segmentation, is proposed by [Lee et al.,\\n2019]. To achieve classiï¬cation task to get the precise bound-\\nary with coarse annotation, it adopts to select hidden units\\nrandomly for calculating activation scores. In this way, the lo-\\ncation information in the feature map are extracted and can be\\ngenerated from a single image. In the end, location maps with\\nregion and objects information are used for training. [Peng\\net al., 2019] came up a alternating direction method of mul-'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='generated from a single image. In the end, location maps with\\nregion and objects information are used for training. [Peng\\net al., 2019] came up a alternating direction method of mul-\\ntipliers (ADMM) algorithms to achieve training discretely-\\nconstrained neural networks in 2019. Providing weakly an-\\nnotations including size constraints and boundary length, the\\nsegmentation for medical MRI images is achieved. Perone\\nproposed a self-ensembling method for unsupervised domain\\nadaptation[Perone et al., 2019].\\n7 Reinforcement Learning for Segmentation\\nDeep Reinforcement Learning (DRL) is a class of methods\\nthat uses neural networks as value function estimators. Its\\nmain advantage is using deep neural networks to extract state\\nfeatures automatically, avoiding manually deï¬ning state fea-\\nture bands. This kind of inaccuracy makes the agent learn in\\na more primitive state. The general model for deep reinforce-\\nment learning agent is illustrated in Figure 3.'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='ture bands. This kind of inaccuracy makes the agent learn in\\na more primitive state. The general model for deep reinforce-\\nment learning agent is illustrated in Figure 3.\\nSahba et al. introduced a Q-learning based method for\\nultrasound image segmentation in 2006 and 2008 [Sahba et\\nal., 2006 ][Sahba et al. , 2008 ]. A novel approach for set-\\nting local thresholding and structuring element values is in-\\nFigure 3: General model for a reinforcement learning agent\\ntroduced, and then the quality of segmented image can be\\nused to ï¬ll a Q-metrix. Chitsaz et al. proposed a multi-\\nobject medical image segmentation framework in 2009[Chit-\\nsaz and Seng, 2009 ]. Each reinforcement agent is trained to\\nï¬nd a optimal value for each object. Each state is associated\\ndeï¬ned actions, and punish/reward functions are calculated.\\nWang et al. came up a context-speciï¬c medical image seg-\\nmentation framework with online reinforcement learning in\\n2013[Wang et al., 2013]. To make the model be adaptive to'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='Wang et al. came up a context-speciï¬c medical image seg-\\nmentation framework with online reinforcement learning in\\n2013[Wang et al., 2013]. To make the model be adaptive to\\nprior knowledge/userâ€™s interaction, a general reinforcement\\nlearning which is suitable for large state-action space is pro-\\nposed. Finally, this generic framework developed by user in-\\nteraction and image content is tested to four different segmen-\\ntation tasks.\\n8 Evaluation Methods, Datasets and Results\\nTo collect the solid and objective performance results of each\\ndeep-learning-based approaches, comprehensive evaluation\\nmethods and public datasets are essential in the experiments.\\nThe section 8 reviews main evaluation methods of image seg-\\nmentation, common datasets and summarize the results.\\n8.1 Evaluation Method\\nAfter the Convolutional Neural Network (CNN) was applied\\nfor visual data analysis, the performance results of proposed\\nmethods have to be evaluated and compared comprehen-'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='8.1 Evaluation Method\\nAfter the Convolutional Neural Network (CNN) was applied\\nfor visual data analysis, the performance results of proposed\\nmethods have to be evaluated and compared comprehen-\\nsively. Commonly used performance evaluation indicators\\ninclude average recall, pixel accuracy and intersection over\\nunion. To simply the expression of different evaluation meth-\\nods, several parameters and notations are deï¬ned in detail.\\nWithout loss of generality,k + 1 classes of target Organ O ar-\\neas (e.g. O0 refers to a lung, O1 refers to a kidney, Ok refers\\nto background and etc.), and the total of Pixels P (Pij refers\\nto the number of pixels organ Oi are predicted to belong to\\nthe class of organ Oj) are considered. In other words, Pii\\nrefers to the true positive number of pixels. Pij refers to the\\nfalse positive number of pixels. Pji refers to the false nega-\\ntive number of pixels. Pjj refers to the true positive number\\nof pixels.\\nPixel Accuracy'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='false positive number of pixels. Pji refers to the false nega-\\ntive number of pixels. Pjj refers to the true positive number\\nof pixels.\\nPixel Accuracy\\nPixel accuracy is a ratio between the number of correctly clas-\\nsiï¬ed pixels and the total number of pixels.\\nP A =\\nâˆ‘k\\ni=0 Pii\\nâˆ‘k\\ni=0\\nâˆ‘k\\nj=0 Pij\\n(1)Mean Pixel Accuracy\\nMean pixel accuracy is a average ratio for all classes of organs\\nbetween the number of correctly classiï¬ed pixels and the total\\nnumber of pixels.\\nM P A =\\n1\\nk + 1\\nkâˆ‘\\ni=0\\nPii\\nâˆ‘k\\nj=0 Pij\\n(2)\\nPixel Precision\\nPixel precision is a ratio between the number of correctly\\nclassiï¬ed positive pixels and the total number of positive pre-\\ndicted pixels.\\nP P =\\nâˆ‘k\\ni=0 Pii\\nâˆ‘k\\ni=0(Pii + Pij )\\n(3)\\nMean Pixel Precision\\nMean pixel precision is a average ratio for all classes of or-\\ngans between the number of correctly classiï¬ed positive pix-\\nels and the total number of positive predicted pixels.\\nM P P =\\n1\\nk + 1\\nkâˆ‘\\ni=0\\nPii\\nPii + Pij\\n(4)\\nPixel Recall'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='gans between the number of correctly classiï¬ed positive pix-\\nels and the total number of positive predicted pixels.\\nM P P =\\n1\\nk + 1\\nkâˆ‘\\ni=0\\nPii\\nPii + Pij\\n(4)\\nPixel Recall\\nPixel recall is the proportion of boundary pixels in the ground\\ntruth that were correctly classiï¬ed by the segmentation.\\nP R =\\nâˆ‘k\\ni=0 Pii\\nâˆ‘k\\ni=0 Pii + âˆ‘k\\nj=0 Pjj\\n(5)\\nDice Coefï¬cient\\nDice coefï¬cient, also known as SÃ¸rensenâ€“Dice index, is a\\nstatistic used to gauge the similarity of two boundary.\\nDSC =\\n2 âˆ— âˆ‘k\\ni=0 Pii\\n2 âˆ— âˆ‘k\\ni=0 Pii + âˆ‘k\\ni=0\\nâˆ‘k\\nj=0(Pij + Pji )\\n(6)\\nConformity Coefï¬cient\\nConformity coefï¬cient is developed based on Dice coefï¬cient\\nwhich can also be used to discriminate between surface, vol-\\nume and boundary.\\nCF =\\n3 âˆ— DSC âˆ’ 2\\nDSC\\n(7)\\nIntersection Over Union\\nIntersection over union, also known as Jaccard index, is the\\npercent overlap between the target mask and the prediction\\noutput.\\nIOU =\\nâˆ‘k\\ni=0 Pii\\nâˆ‘k\\ni=0\\nâˆ‘k\\nj=0 Pij âˆ’ âˆ‘k\\nj=0 Pjj\\n(8)\\nMean Intersection Over Union'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='percent overlap between the target mask and the prediction\\noutput.\\nIOU =\\nâˆ‘k\\ni=0 Pii\\nâˆ‘k\\ni=0\\nâˆ‘k\\nj=0 Pij âˆ’ âˆ‘k\\nj=0 Pjj\\n(8)\\nMean Intersection Over Union\\nMean Intersection over union is the average percent overlap\\nfor each class between the target mask and the prediction out-\\nput.\\nM IOU =\\n1\\nk + 1\\nkâˆ‘\\ni=0\\nPii\\nâˆ‘k\\nj=0 Pij + âˆ‘k\\nj=0 Pji âˆ’ Pii\\n(9)\\nFrequency Weighted Intersection Over Union\\nFrequency weighted Intersection over union is the average\\npercent overlap for each class importance depending on ap-\\npearance frequency between the target mask and the predic-\\ntion output.\\nF W IOU =\\n1\\nâˆ‘k\\ni=0\\nâˆ‘k\\nj=0 Pij\\nkâˆ‘\\ni=0\\nâˆ‘k\\nj=0 PiiPij\\nâˆ‘k\\nj=0 Pij + âˆ‘k\\nj=0 Pji âˆ’ Pii\\n(10)\\n8.2 Dataset\\nTo evaluate each segmentation method fairly and objectively,\\nan authoritative dataset plays an essential role in evaluation.\\nTable 1 summarizes the common public datasets for ultra-\\nsound image segmentation.\\n8.3 Experimental Comparison of Segmentation\\nAccuracy\\nTo illustrate the contributions and evaluation results for dif-'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='Table 1 summarizes the common public datasets for ultra-\\nsound image segmentation.\\n8.3 Experimental Comparison of Segmentation\\nAccuracy\\nTo illustrate the contributions and evaluation results for dif-\\nferent methods, Table 2 summarizes each method according\\nto the category, contributions, datasets and results.\\n9 Discussion and Potential Directions\\nDeep learning was widely applied to medical image segmen-\\ntation in the past decade. This review mainly focuses on deep\\nlearning approaches for medical ultrasound image segmenta-\\ntion. Typical methods are categorized into six groups, intro-\\nduced and summarized. Based on the existing researches and\\nstudies, this review summarizes key issues, challenges and\\nsome potential research directions.\\nâ€¢ In the last ï¬ve years, deep learning has demonstrated\\nstate-of-the-art performance in ultrasound image segmenta-\\ntion tasks. Nearly 80% deep learning models, however, are\\ndeveloped based on 2D fully convolutional neural networks'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='state-of-the-art performance in ultrasound image segmenta-\\ntion tasks. Nearly 80% deep learning models, however, are\\ndeveloped based on 2D fully convolutional neural networks\\nor Encoder-decoder networks. These approaches request high\\ncost in labeling data and can not study on the sequence of ul-\\ntrasound images or the position of ultrasound scanners, which\\nresults in the loss of context information. Furthermore, clini-\\ncians or other health-workers normally labels ultrasound im-\\nages with bounding boxes, lines or object types on the most\\ndistinctive image rather than pixel-level boundary annotation,\\nso the shortcomings of accurately labeling datasets remain a\\nchallenge. Therefore, there is still a lot of study work to do\\non applying 3D fully convolutional networks, recurrent neu-\\nral networks, generative adversarial networks, weakly super-\\nvised learning to make full use of current public datasets for\\nultrasound image segmentation.'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='ral networks, generative adversarial networks, weakly super-\\nvised learning to make full use of current public datasets for\\nultrasound image segmentation.\\nâ€¢ Segmentation performance is usually evaluated by accuracy\\nrather than speed or memory cost. With the development of\\nmedical hardware, the ultrasound equipment is developed to-\\nward miniaturization, high efï¬ciency and ease of daily use\\nwith a portable computer. Therefore, the machine learning\\nmodel should be as lightweight as possible to save more time\\nwhile ensuring accuracy and stability. It can be promising\\nonce the model is deployed on a entry-level CPU.\\nâ€¢ The heterogeneous appearance of the organ is one of the\\nbiggest challenges in ultrasound image segmentation. The\\npattern of ultrasound images can be different varies between\\nthe organ location, depth, neighboring tissues, hardness evenTable 1: Common Datasets for Medical Ultrasound Image\\nDataset Resolution Size of set Description Device'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='the organ location, depth, neighboring tissues, hardness evenTable 1: Common Datasets for Medical Ultrasound Image\\nDataset Resolution Size of set Description Device\\nUltrasound Nerve Segmentation Challenge 580 Ã—420 5636 A collection of nerves called the Brachial Plexus (BP) N/A\\nBreast Ultrasound Teaching File Average 377 Ã—396 6600 Age, Menopausal, Hormonal Replacement, B&K Medical SystemFamily History Physical Exam are provided\\nBreast Ultrasound Lesions Dataset Average 760 Ã—570 469 306 (60 malignant and 246 benign) Siemens ACUSON Sequoia C512163 (53 malignant and 110 benign)\\nMalignant and Benign Breast Lesions N/A 100 78 women aged from 24 to 75 Ultrasonix SonixTouch\\nResearch ultrasound scanner\\nMalignant Solid Mass 200 Ã—200-300Ã—400 241 cases Labeled by three experts by the majority A Philips iU22 ultrasound machinevoting rule (two out of three)\\nCervical Dystonia 491 Ã—525 3272 61 adults: 35 cervical dystonia and 26 normal Probe (7.5MHz, SonixTouch,\\nUltrasonix, USA)'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='Cervical Dystonia 491 Ã—525 3272 61 adults: 35 cervical dystonia and 26 normal Probe (7.5MHz, SonixTouch,\\nUltrasonix, USA)\\nBiometric Measurements from Fetal Ultrasound Images N/A 284 14 fetus, 90 femur, 90 head, 90 abdomen N/A\\nFetal US Image Segmentation 756 Ã—546 9 Training set, 1 Test set Fetal head, abdomen, Philips HD9and femur sub-challenges\\nVessel Segmentation N/A TBU Released per month with over 7000 cases GE V oluson US imaging system\\nIVUS Challenge, MICCAI up to 113 Âµm 512 Ã— 5 frames Outer wall of media and adventitia segmentation, 40 MHz IVUS scannerinner wall of lumen segmentation\\nSYSU-FLL-CEUS 768 Ã—576 10 videos Three types: 186 HCC, Aplio SSA-770A\\n109 HEM and 58 FNH instances (Toshiba Medical System)\\nTable 2: Comparison of Medical Ultrasound Image Segmentation Results\\nCategory Model Contributions Dataset Evaluation\\nFCN\\nCFS-FCN[Zhang et al., 2016] Multi-stage incremental learning 80 ultrasound images MIOU: 5%Several small-size FCN'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='Category Model Contributions Dataset Evaluation\\nFCN\\nCFS-FCN[Zhang et al., 2016] Multi-stage incremental learning 80 ultrasound images MIOU: 5%Several small-size FCN\\nCasFCN[Wu et al., 2017] Cascaded framework 900 fetal head and 688 abdomen images DSC: 0.9843Auto-Context scheme\\nDSN-OB[Mishra et al., 2018] Auxiliary losses for boundary detection MICCAI 2011 IVUS challenge DSC: 0.91\\nFCN-TN[Li et al., 2018] Dropout to prevent overï¬tting 300 Thyroid Nodules US images IOU: 91%\\nDF-FCN[Yang et al., 2019] A catheter detection method to reorganized cross-sections 25 3D US dataset DSC: 57.7%\\nEn-Decoder\\nDeconvNet+ [Cunningham et al., 2019] Completely symmetric structure with Maxout 1100 transversal neck US images of 28 adults JI: 0.532, HD: 5.7mm\\nDynamic CNN[Zhou et al., 2019] Dynamically ï¬ne tuned 144 carotid 3D ultrasound DSC: 0.928â€“0.965\\nGRA U-Net[Zhuang et al., 2019] Modiï¬ed U-Net to Grouped-Resaunet segment 25 patients with 131 slices MIOU: 0.847'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='GRA U-Net[Zhuang et al., 2019] Modiï¬ed U-Net to Grouped-Resaunet segment 25 patients with 131 slices MIOU: 0.847\\n[Kim et al., 2018] Multi-label loss function IVUS dataset DSC: 0.84Weighted pixel-wise cross-entropy\\nNAS-Unet[Weng et al., 2019] Network architecture search Kaggle Ultrasound nerve segmentation MIOU: 0.992\\nDS-CR-V-Net[Lei et al., 2019] 3D convolution,residual blocks 44 patientsâ€™ TRUS data DSC: 0.919Multi-directional deep supervision\\nRNN\\nT-RNN & J-CNN[Chen et al., 2015] Knowledge transferred recurrent neural networks 300 videos with 37376 US images AUC: 0.95\\n3D-FCN[Yang et al., 2017b] & RNN Deep supervision for volumetric segmenta tion 104 anonymized prenatal ultrasound volumes DSC: 0.882\\nMulti-view BCRNN[Yang et al., 2017a] Utilize shape prior to infer the boundary 17 trans-rectal ultrasound volumes DSC: 0.9239\\nGAN\\nPAN[Khosravan et al., 2019] Address computational burden TCIA dataset DSC:86.8'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='GAN\\nPAN[Khosravan et al., 2019] Address computational burden TCIA dataset DSC:86.8\\nSegAN[Xue et al., 2018] Multi-scale loss function MICCAI BRATS DSC: 0.85, Precision: 0.92\\n[Yang et al., 2018] Convert the target corpus on the-ï¬‚y 2699 US Images DSC: 93.379A self-play training strategy\\nWSL\\n[Kim and Hwang, 2016] Unpooling-deconvolution networks MC (80 normal, 58 abnormal) IOU: 21.61, 24.61Shenzhen (326 normal, 336 abnormal)\\nFickleNet[Lee et al., 2019] Stochastic selection of hidden layers PASCAL VOC 2012 MIOU: 61.2\\n[Peng et al., 2019] Training with discrete constraints Automated Cardiac Diagnosis Challenge Mean DSC: 0.901\\n[Perone et al., 2019] Unsupervised domain adaptation Spinal Cord Gray Matter Challenge DSC: 84.72\\nDRL\\n[Sahba et al., 2008] Utilize local to extract the prostate 60 images with speciï¬c prostate shape Area Overlap: 0.9096\\n[Chitsaz and Seng, 2009] Different ROIs segmentation A series of 512 Ã—512 medical images Accuracy: 0.93'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='[Chitsaz and Seng, 2009] Different ROIs segmentation A series of 512 Ã—512 medical images Accuracy: 0.93\\n[Wang et al., 2013] Context-speciï¬c segmentation LV , RV Segmentation Dataset Jaccard Index: 0.922\\nthe operators. Several non-machine learning approaches such\\nas deformable models, watershed, region grow and graph-\\nbased methods are essential for general ultrasound image seg-\\nmentation tasks. The study of deep learning can be potentially\\ninspired by traditional methods, and the evaluation should be\\nproved in general segmentation tasks.\\nâ€¢ In clinical medical imaging, there are a number of multiple\\nmodalities methods such as MRI, X-ray, PET and CT. The\\nmedical imaging methods for diagnostic are adopted depend-\\ning on different cases. Ultrasound image is regarded as the\\nï¬rst-line method because of its low cost and non-radiation.\\nMRI or CT is time consuming, costly but rich in texture. To\\nsolve the processing work for multi resources, cross-modal'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='ï¬rst-line method because of its low cost and non-radiation.\\nMRI or CT is time consuming, costly but rich in texture. To\\nsolve the processing work for multi resources, cross-modal\\ntransfer learning can be a potential research direction.\\nâ€¢ From the perspective of clinical diagnosis, on the one hand,\\nthe intrinsic speckle noises may affect the imaging effects to\\na certain extent. On the other hand, some small organ struc-\\ntures may be obscured by large human organs, so restoring\\nobstructed lesions to reduce missed diagnosis is a challenge.\\nFurthermore, early prediction of diseases is the potential re-\\nsearch direction in the future. The survival rate of patients\\nwill be greatly improved, if the diseases can be detected ear-\\nlier, but all of these deep-learning-based models are trained\\non the datasets labeled by doctors, the features failed to be\\nlabeled by doctors can not be learned by the training models.\\nTo this end, training models with the labeled data and then'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='on the datasets labeled by doctors, the features failed to be\\nlabeled by doctors can not be learned by the training models.\\nTo this end, training models with the labeled data and then\\nexpanding knowledge to make advance diagnosis prediction\\nis of great signiï¬cance in the future.\\nReferences\\n[Chen et al., 2015] Hao Chen, Qi Dou, Dong Ni, and et al.\\nAutomatic fetal ultrasound standard plane detection using\\nknowledge transferred recurrent neural networks. In MIC-\\nCAI, pages 507â€“514. Springer, 2015.\\n[Chitsaz and Seng, 2009] Mahsa Chitsaz and Woo Chaw\\nSeng. Medical image segmentation by using reinforce-\\nment learning agent. In 2009 ICDIP , pages 216â€“219.\\nIEEE, 2009.[Ã‡iÃ§ek et al., 2016] Ã–zgÃ¼n Ã‡iÃ§ek, Ahmed Abdulkadir, and\\net al. 3d u-net: learning dense volumetric segmenta-\\ntion from sparse annotation. In MICCAI, pages 424â€“432.\\nSpringer, 2016.\\n[Cunningham et al., 2019] Ryan Cunningham, MarÃ­a B\\nSÃ¡nchez, and Ian D Loram. Ultrasound segmentation'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='tion from sparse annotation. In MICCAI, pages 424â€“432.\\nSpringer, 2016.\\n[Cunningham et al., 2019] Ryan Cunningham, MarÃ­a B\\nSÃ¡nchez, and Ian D Loram. Ultrasound segmentation\\nof cervical muscle during head motion: a dataset and a\\nbenchmark using deconvolutional neural networks. 2019.\\n[Kazeminia et al., 2018] Salome Kazeminia, Christoph\\nBaur, Arjan Kuijper, Bram van Ginneken, Nassir Navab,\\nShadi Albarqouni, and Anirban Mukhopadhyay. Gans for\\nmedical image analysis. arXiv preprint arXiv:1809.06222,\\n2018.\\n[Khosravan et al., 2019] Naji Khosravan, Aliasghar Mortazi,\\nand et al. Pan: Projective adversarial network for medical\\nimage segmentation. In MICCAI, pages 68â€“76. Springer,\\n2019.\\n[Kim and Hwang, 2016] Hyo-Eun Kim and Sangheum\\nHwang. Deconvolutional feature stacking for weakly-\\nsupervised semantic segmentation. arXiv preprint\\narXiv:1602.04984, 2016.\\n[Kim et al., 2018] Sekeun Kim, Yeonggul Jang, Byunghwan\\nJeon, and et al. Fully automatic segmentation of coronary'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='supervised semantic segmentation. arXiv preprint\\narXiv:1602.04984, 2016.\\n[Kim et al., 2018] Sekeun Kim, Yeonggul Jang, Byunghwan\\nJeon, and et al. Fully automatic segmentation of coronary\\narteries based on deep neural network in intravascular ul-\\ntrasound images. In Intravascular Imaging and Computer\\nAssisted Stenting and Large-Scale Annotation of Biomed-\\nical Data and Expert Label Synthesis , pages 161â€“168.\\nSpringer, 2018.\\n[Lee et al., 2019] Jungbeom Lee, Eunji Kim, and Lee et al.\\nFicklenet: Weakly and semi-supervised semantic image\\nsegmentation using stochastic inference. In Proceedings\\nof the IEEE conference on computer vision and pattern\\nrecognition, pages 5267â€“5276, 2019.\\n[Lei et al., 2019] Yang Lei, Sibo Tian, Xiuxiu He, and\\net al. Ultrasound prostate segmentation based on mul-\\ntidirectional deeply supervised v-net. Medical physics ,\\n46(7):3194â€“3206, 2019.\\n[Li et al., 2018] Xuewei Li, Shuaijie Wang, and et al. Fully\\nconvolutional networks for ultrasound image segmentation'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='tidirectional deeply supervised v-net. Medical physics ,\\n46(7):3194â€“3206, 2019.\\n[Li et al., 2018] Xuewei Li, Shuaijie Wang, and et al. Fully\\nconvolutional networks for ultrasound image segmentation\\nof thyroid nodules. In 2018 IEEE HPCC/SmartCity/DSS ,\\npages 886â€“890. IEEE, 2018.\\n[Long et al., 2015] Jonathan Long, Evan Shelhamer, and\\nTrevor Darrell. Fully convolutional networks for seman-\\ntic segmentation. In CVPR, pages 3431â€“3440, 2015.\\n[Luc et al., 2016] Pauline Luc, Camille Couprie, and et al.\\nSemantic segmentation using adversarial networks. arXiv\\npreprint arXiv:1611.08408, 2016.\\n[Milletari et al., 2016] Fausto Milletari, Nassir Navab, and\\net al. V-net: Fully convolutional neural networks for vol-\\numetric medical image segmentation. In 2016 3DV, pages\\n565â€“571. IEEE, 2016.\\n[Mishra et al., 2018] Deepak Mishra, Santanu Chaudhury,\\nand et al. Ultrasound image segmentation: a deeply super-\\nvised network with attention to boundaries.IEEE Transac-'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='565â€“571. IEEE, 2016.\\n[Mishra et al., 2018] Deepak Mishra, Santanu Chaudhury,\\nand et al. Ultrasound image segmentation: a deeply super-\\nvised network with attention to boundaries.IEEE Transac-\\ntions on Biomedical Engineering, 66(6):1637â€“1648, 2018.\\n[Noh et al., 2015] Hyeonwoo Noh, Seunghoon Hong, and\\nBohyung Han. Learning deconvolution network for se-\\nmantic segmentation. In Proceedings of the IEEE interna-\\ntional conference on computer vision , pages 1520â€“1528,\\n2015.\\n[Peng et al., 2019] Jizong Peng, Hoel Kervadec, and et al.\\nDiscretely-constrained deep network for weakly super-\\nvised segmentation. arXiv preprint arXiv:1908.05770 ,\\n2019.\\n[Perone et al., 2019] Christian S Perone, Pedro Ballester,\\nRodrigo C Barros, and Julien Cohen-Adad. Unsupervised\\ndomain adaptation for medical imaging segmentation with\\nself-ensembling. NeuroImage, 194:1â€“11, 2019.\\n[Ronneberger and et al, 2015] Olaf Ronneberger and et al.\\nU-net: Convolutional networks for biomedical image seg-'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='self-ensembling. NeuroImage, 194:1â€“11, 2019.\\n[Ronneberger and et al, 2015] Olaf Ronneberger and et al.\\nU-net: Convolutional networks for biomedical image seg-\\nmentation. In MICCAI, pages 234â€“241. Springer, 2015.\\n[Sahba et al., 2006] Farhang Sahba, Hamid R Tizhoosh, and\\net al. A reinforcement learning framework for medical\\nimage segmentation. In 2006 IEEE IJCNN , pages 511â€“\\n517. IEEE, 2006.\\n[Sahba et al., 2008] Farhang Sahba, Hamid R Tizhoosh, and\\nMagdy MA Salama. Application of reinforcement learn-\\ning for segmentation of transrectal ultrasound images.\\nBMC medical imaging, 8(1):8, 2008.\\n[Wang et al., 2013] Lichao Wang, Karim Lekadir, and et al.\\nA general framework for context-speciï¬c image segmen-\\ntation using reinforcement learning. IEEE transactions on\\nmedical imaging, 32(5):943â€“956, 2013.\\n[Weng et al., 2019] Yu Weng, Tianbao Zhou, Yujie Li, and\\nXiaoyu Qiu. Nas-unet: Neural architecture search for\\nmedical image segmentation. IEEE Access , 7:44247â€“\\n44257, 2019.'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='[Weng et al., 2019] Yu Weng, Tianbao Zhou, Yujie Li, and\\nXiaoyu Qiu. Nas-unet: Neural architecture search for\\nmedical image segmentation. IEEE Access , 7:44247â€“\\n44257, 2019.\\n[Wolterink et al., 2018] Jelmer M Wolterink, Konstantinos\\nKamnitsas, and et al. Generative adversarial networks and\\nadversarial methods in biomedical image analysis. arXiv\\npreprint arXiv:1810.10352, 2018.\\n[Wu et al., 2017] Lingyun Wu, Yang Xin, and et al. Cas-\\ncaded fully convolutional networks for automatic prenatal\\nultrasound image segmentation. In ISBI 2017, pages 663â€“\\n666. IEEE, 2017.\\n[Xue et al., 2018] Yuan Xue, Tao Xu, and et al. Segan: Ad-\\nversarial network with multi-scale l 1 loss for medical\\nimage segmentation. Neuroinformatics, 16(3-4):383â€“392,\\n2018.\\n[Yang et al., 2017a] Xin Yang, Lequan Yu, and et al. Fine-\\ngrained recurrent neural networks for automatic prostate\\nsegmentation in ultrasound images. In AAAI, 2017.\\n[Yang et al., 2017b] Xin Yang, Lequan Yu, and et al. To-'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='grained recurrent neural networks for automatic prostate\\nsegmentation in ultrasound images. In AAAI, 2017.\\n[Yang et al., 2017b] Xin Yang, Lequan Yu, and et al. To-\\nwards automatic semantic segmentation in volumetric ul-\\ntrasound. In MICCAI, pages 711â€“719. Springer, 2017.\\n[Yang et al., 2018] Xin Yang, Haoran Dou, and et al. Gen-\\neralizing deep models for ultrasound image segmentation.\\nIn MICCAI, pages 497â€“505. Springer, 2018.[Yang et al., 2019] Hongxu Yang, Caifeng Shan, and et al.\\nImproving catheter segmentation & localization in 3d car-\\ndiac ultrasound using direction-fused fcn. In ISBI 2019 ,\\npages 1122â€“1126. IEEE, 2019.\\n[Zhang et al., 2016] Yizhe Zhang, Michael TC Ying, and\\net al. Coarse-to-ï¬ne stacked fully convolutional nets for\\nlymph node segmentation in ultrasound images. In 2016\\nIEEE BIBM, pages 443â€“448. IEEE, 2016.\\n[Zhou et al., 2019] Ran Zhou, Aaron Fenster, Yujiao Xia,\\nand et al. Deep learning-based carotid media-adventitia\\nand lumen-intima boundary segmentation from three-'),\n",
       " Document(metadata={'arxiv_id': '2002.07703v3', 'title': 'Deep Learning in Medical Ultrasound Image Segmentation: a Review', 'section': 'body', 'authors': 'Ziyang Wang'}, page_content='[Zhou et al., 2019] Ran Zhou, Aaron Fenster, Yujiao Xia,\\nand et al. Deep learning-based carotid media-adventitia\\nand lumen-intima boundary segmentation from three-\\ndimensional ultrasound images. Medical physics ,\\n46(7):3180â€“3193, 2019.\\n[Zhuang et al., 2019] Zhemin Zhuang, Alex Noel Joseph\\nRaj, and et al. Nipple segmentation and localization us-\\ning modiï¬ed u-net on breast ultrasound images. Journal\\nof Medical Imaging and Health Informatics , 9(9):1827â€“\\n1837, 2019.'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'title_abstract', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='Title: Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net\\n\\nAbstract: Efficient intravascular access in trauma and critical care significantly impacts patient outcomes. However, the availability of skilled medical personnel in austere environments is often limited. Autonomous robotic ultrasound systems can aid in needle insertion for medication delivery and support non-experts in such tasks. Despite advances in autonomous needle insertion, inaccuracies in vessel segmentation predictions pose risks. Understanding the uncertainty of predictive models in ultrasound imaging is crucial for assessing their reliability. We introduce MSU-Net, a novel multistage approach for training an ensemble of U-Nets to yield accurate ultrasound image segmentation maps. We demonstrate substantial improvements, 18.1% over a single Monte Carlo U-Net, enhancing uncertainty evaluations, model transparency, and trustworthiness. By highlighting areas of model certainty, MSU-Net can guide safe needle insertions, empowering non-experts to accomplish such tasks.'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'body', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='Enhanced Uncertainty Estimation in Ultrasound\\nImage Segmentation with MSU-Net\\nRohini Banerjee*, Cecilia G. Morales*, and Artur Dubrawski\\nCarnegie Mellon University, Pittsburgh PA 15213, USA\\n{rohinib,cgmorale,awd}@andrew.cmu.edu\\nAbstract. Efficient intravascular access in trauma and critical care sig-\\nnificantly impacts patient outcomes. However, the availability of skilled\\nmedical personnel in austere environments is often limited. Autonomous\\nrobotic ultrasound systems can aid in needle insertion for medication\\ndelivery and support non-experts in such tasks. Despite advances in au-\\ntonomous needle insertion, inaccuracies in vessel segmentation predic-\\ntions pose risks. Understanding the uncertainty of predictive models in\\nultrasound imaging is crucial for assessing their reliability. We introduce\\nMSU-Net, a novel multistage approach for training an ensemble of U-\\nNets to yield accurate ultrasound image segmentation maps. We demon-'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'body', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='MSU-Net, a novel multistage approach for training an ensemble of U-\\nNets to yield accurate ultrasound image segmentation maps. We demon-\\nstrate substantial improvements, 18.1% over a single Monte Carlo U-Net,\\nenhancing uncertainty evaluations, model transparency, and trustworthi-\\nness. By highlighting areas of model certainty, MSU-Net can guide safe\\nneedle insertions, empowering non-experts to accomplish such tasks.\\nKeywords: Uncertainty Quantification Â· Ultrasound Image Segmenta-\\ntion Â· Trustworthy and Interpretable Medical AI\\n1 Introduction\\nTrauma, the leading cause of death among young individuals in the U.S. [25],\\noften results in blood loss, which requires rapid fluid resuscitation for vital organ\\noxygenation. In austere settings, accessing timely medical care can be challeng-\\ning due to limited access, dangerous conditions, time constraints, or the absence\\nof medical infrastructure, making expertise for optimal needle insertion sites crit-'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'body', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='ing due to limited access, dangerous conditions, time constraints, or the absence\\nof medical infrastructure, making expertise for optimal needle insertion sites crit-\\nical. Autonomous robotic systems can assist in intravenous fluid administration\\nwhen medical experts are unavailable, providing support in emergencies. These\\nsystems can also guide non-experts in accurate performance of phlebotomy tasks,\\nempowering them to contribute effectively in dire medical situations.\\nUltrasound imaging is widely used for locating vessels to enable fluid resus-\\ncitation due to its affordability, speed, safety, and portability, unlike CT or MRI\\nimaging that are not portable and use ionizing radiation [9]. Despite advance-\\nments in autonomous needle insertion into blood vessels [9], a critical challenge\\npersists: inaccurate predictions can lead to severe consequences. Failing to antic-\\nipate vessel structure during intravenous cannulation may result in catastrophic'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'body', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='persists: inaccurate predictions can lead to severe consequences. Failing to antic-\\nipate vessel structure during intravenous cannulation may result in catastrophic\\n* These authors contributed equally to this work\\narXiv:2407.21273v1  [cs.CV]  31 Jul 20242 R. Banerjee et al.\\noutcomes for critically injured individuals; incorrectly predicting two adjacent\\nvessels as a single one could lead to laceration of the vessel wall and cause hem-\\norrhage upon needle insertion [18]. Both medical personnel and automated tools\\nneed to assess the certainty of their estimates of vessel location and structure to\\nmitigate this risk [17]. As healthcare increasingly adopts automation, reliance on\\nArtificialIntelligence(AI)modelsgrows.Buildingamodelthatcommunicatesits\\nuncertainty becomes essential to help users focus their attention and actions on\\nwhere the model is confident, such as guiding needle insertion accurately within\\nvessel segments. In this paper, we attack the problem of uncertainty estimation'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'body', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='where the model is confident, such as guiding needle insertion accurately within\\nvessel segments. In this paper, we attack the problem of uncertainty estimation\\nof models trained to guide needle insertion tasks. Our contributions include: (1)\\nWe introduce MSU-Net, a novel Multistaged Monte Carlo U-Net, demonstrating\\nsignificant advancements in uncertainty quantification; (2) Our model achieves\\nstate-of-the-art performance in ultrasound image segmentation, marking the first\\nknown improvement in uncertainty estimation for this application.\\n2 Related Works\\n2.1 Uncertainty Quantification in Deep Learning\\nUncertainty quantification is vital in understanding the reliability of AI model\\npredictions. Traditionally, the frequentist approach assumes a single point esti-\\nmate of network weights, using estimated class likelihoods as confidence mea-\\nsures for predictions. However, studies have shown that these likelihoods often'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'body', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='mate of network weights, using estimated class likelihoods as confidence mea-\\nsures for predictions. However, studies have shown that these likelihoods often\\noverestimate accuracy [11], and the popular metric used to quantify confidence,\\nexpected calibration error, has been criticized for bias and inconsistency [10].\\nThis limits utility, motivating the need for alternative approaches to accurately\\nquantify model uncertainty rather than just confidence.\\nPredictive uncertainty comprises of aleatoric and epistemic components [8].\\nAleatoric uncertainty accounts for inherent noise in observations, while epistemic\\nuncertainty arises from limited training data and model parameter uncertainty.\\nRecent advancements in Bayesian inference and Bayesian neural networks have\\nprovided robust frameworks to quantify both forms of uncertainty by estimating\\nposterior distributions over model weights. Gal and Ghahramani [7] introduced'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'body', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='provided robust frameworks to quantify both forms of uncertainty by estimating\\nposterior distributions over model weights. Gal and Ghahramani [7] introduced\\nMonte Carlo (MC) dropout for Bayesian inference in deep learning, leveraging\\ndropout in convolutional layers for stochastic forward passes to approximate\\nBayesian variational inference. Bayesian approximation using MC dropout has\\nbeen extensively applied: Kendall and Gal [13] developed Bayesian SegNet for\\nscene understanding, while Dechesne and Lassalle [4] used it in U-Net for high-\\naccuracy image segmentation. Seedat [22] proposed a human-in-the-loop system\\nusing model uncertainty. Yet, single-model architectures are now supplanted by\\nmodel ensembles due to difficulties in capturing inherent variability.\\n2.2 Multistage Neural Network Ensembles\\nTraining multiple individual models through model ensembling offers a range of'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'body', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='model ensembles due to difficulties in capturing inherent variability.\\n2.2 Multistage Neural Network Ensembles\\nTraining multiple individual models through model ensembling offers a range of\\ntools to encourage member model diversity and improve overall accuracy. Di-Uncertainty Estimation with MSU-Net 3\\nversity can be achieved through several techniques, the most popular being bag-\\nging [2], stacking [27], and boosting [6]. Non-Bayesian deep ensembles improve\\npredictive uncertainty estimation compared to single model architectures [16],\\nyet remain limited by naÃ¯ve aggregation strategies such as simple or weighted av-\\neraging or majority vote [28]. Yang et al. [29] addresses drawbacks of traditional\\nensembles by training a secondary neural network to adaptively assign weights,\\ninspired by stacked generalization. This approach offers flexibility in candidate\\nselection and leverages non-linear modeling capabilities of neural networks.'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'body', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='inspired by stacked generalization. This approach offers flexibility in candidate\\nselection and leverages non-linear modeling capabilities of neural networks.\\nLai et al. [15] designed a multistage reliability-based neural network ensemble\\nlearning approach for credit card evaluation based on a decorrelation maximiza-\\ntion procedure to select diverse models. Their approach combines outputs using\\ndifferent aggregation strategies, achieving superior accuracy over single and hy-\\nbrid models. Multistage ensembling with U-Nets also proves beneficial; Yin and\\nHu [30] designed Paw-Net, a multistage ensemble for semantic segmentation.\\nPaw-Net demonstrates higher final Intersection-over-Union (IoU) scores by in-\\ntegrating outputs of multiple U-Nets specialized in different classes.\\nOur proposed MSU-Net architecture combines the benefits of these appro-\\naches: candidate U-Nets are first overproduced and then selected based on decor-'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'body', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='Our proposed MSU-Net architecture combines the benefits of these appro-\\naches: candidate U-Nets are first overproduced and then selected based on decor-\\nrelationtocomposeanensemble.ThefinalstagecombinesoutputsusingaMonte\\nCarlo U-Net approach to produce accurate segmentation maps.\\n3 Methods\\n3.1 Monte Carlo U-Net (MCU-Net)\\nWe introduce stochasticity into our inference process by incorporating dropout\\nlayers into our chosen U-Net architecture, thereby enabling MC dropout, as in\\nMCU-Net [22]. MCU-Net is henceforth referred to as our baseline model. Indis-\\ncriminately placing dropout after each convolutional layer can lead to reduced fit\\nand poor test performance [13]. We opt to situate dropout layers in the decoder\\nsection of our U-Net instead. Each decoder block consists of two sets of con-\\nvolutional layers (3x3 convolution filter, batch normalization, ReLU activation)\\nand an attention block. This enables us to approximate Bayesian inference by'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'body', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='volutional layers (3x3 convolution filter, batch normalization, ReLU activation)\\nand an attention block. This enables us to approximate Bayesian inference by\\nconducting T forward passes, or MC samples, of the U-Net during testing. Our\\nempirical results show no significant improvement beyondT = 30. Our model\\noutputs both logits and logit variances [3] to use for segmentation and epistemic\\nuncertainty maps, with [bpt,bÏƒ2\\nt ] = f bÏ‰t (x) representing the t-th forward pass of\\nMCU-Net with learned weightsbÏ‰t. Given sigmoid activation,ÏƒSIG, the ensem-\\nble prediction is aggregated by averaging individual model outputs,ÏƒSIG(bpt),\\nacross all T samples. Similarly, the raw epistemic uncertainty map is obtained\\nby averaging logit variances across all MC samples.\\n3.2 Multistage U-Net (MSU-Net)\\nThe MSU-Net architecture, illustrated in Fig 1, is structured in three main\\nstages: (1) ensembling, (2) candidate selection, and (3) combining. In stage 1,4 R. Banerjee et al.'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'body', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='The MSU-Net architecture, illustrated in Fig 1, is structured in three main\\nstages: (1) ensembling, (2) candidate selection, and (3) combining. In stage 1,4 R. Banerjee et al.\\nwe train multiple models referred to ascandidates. Bagging is employed to in-\\nduce diversity into our training data; given the training dataset of sizenT R, we\\nrepeatedly sample with replacement to generateM training subsets, each also\\nof size nT R. We then proceed to train a total of 15 bootstrapped models [16].\\nHyperparameter optimization, specifically determining the number of epochs of\\ntraining for each candidate model, is performed using the VS1 validation set,\\nheld out for this purpose. Early stopping is applied to each candidate when\\nvalidation loss begins to increase, to mitigate model overfitting.\\nIn stage 2, candidates are selected to form a diverse and efficient ensemble,\\naiming to reduce computational costs by minimizing correlation. We apply the'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'body', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='In stage 2, candidates are selected to form a diverse and efficient ensemble,\\naiming to reduce computational costs by minimizing correlation. We apply the\\ndecorrelation maximization method from [15], using the Brier score loss matrix\\ncomputedfrommodels f bÏ‰1\\n1 , f bÏ‰2\\n2 , Â· Â· Â· , f bÏ‰M\\nM onaseparatevalidationsetVS2within\\nthe specified region of interest, replacing binary error with Brier score for finer\\nerror measurement. The mean, variance, and covariance of the Brier score loss\\nmatrix is then computed in order to construct the correlation matrixR, where\\nthe ith row andjth column represents the degree of correlation betweenf bÏ‰i\\ni and\\nf bÏ‰j\\nj . We writeR in a block form for candidate modelf bÏ‰i\\ni as\\nR\\nextract principle submatrix\\nâˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ âˆ’ â†’\\n\\x14Râˆ’i ri\\nrT\\ni 1\\n\\x15\\n(1)\\nwhere Râˆ’i is the principle submatrix ofR resulting from deleting theith row\\nand ith column. Subsequently, we compute the plural-correlation coefficient [20]\\nÏ2\\ni = rT\\ni Râˆ’1\\nâˆ’i ri (2)'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'body', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='rT\\ni 1\\n\\x15\\n(1)\\nwhere Râˆ’i is the principle submatrix ofR resulting from deleting theith row\\nand ith column. Subsequently, we compute the plural-correlation coefficient [20]\\nÏ2\\ni = rT\\ni Râˆ’1\\nâˆ’i ri (2)\\nby which candidatef bÏ‰i\\ni is kept ifÏ2\\ni â‰¤ Î¸ for some thresholdÎ¸, else discarded, in\\norder to build our final ensemble. Empirical trials testingÎ¸ thresholds indicate\\nthat K = 3 performs almost equally as well asK = 15, suggesting that minimal\\nadditional training will suffice for our new architecture.\\nIn stage 3, we train a final MCU-Net combiner taking ensemble member out-\\nputsasinputsandoutputtingsegmentationmaps.Wecomputethesegmentation\\nand epistemic uncertainty maps of MSU-Net analogous to MCU-Net.\\nTraining Set (TR)\\nTR1 TR2 TRMâ€¦\\nUnet1 Unet2 UnetMâ€¦\\nUnet*1 Unet*2 Unet*Kâ€¦\\nValidation Set (VS)\\nVS1/VS2\\nMSU-Net \\nArchitecture\\nMCU-Net \\nCombiner\\nTesting Set (TS)\\nStage-1\\nStage-3\\nStage-2\\nFig.1: Proposed MSU-Net architecture. U-Nets are trained on bootstrap samples'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'body', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='Validation Set (VS)\\nVS1/VS2\\nMSU-Net \\nArchitecture\\nMCU-Net \\nCombiner\\nTesting Set (TS)\\nStage-1\\nStage-3\\nStage-2\\nFig.1: Proposed MSU-Net architecture. U-Nets are trained on bootstrap samples\\nand validated on VS1. Decorrelated ensemble members are chosen using VS2.Uncertainty Estimation with MSU-Net 5\\n4 Experiments\\nDataset and Training We used a ultrasound scanning system described by\\nMorales et al. [19] to scan the CAE Blue Phantom anthropomorphic gel model\\nsimulating femoral vessels. Equipped with a 5MHz linear transducer, the system\\ncan scan up to 5cm in depth, producing 2D transverse ultrasound images. Expert\\nclinicians annotated these images using the Computer Vision Annotating Tool\\n(CVAT) [23], followed by cropping and resizing to 256Ã— 256 pixels. The dataset\\nwas split into training (1392 images), validation (907 images), and testing (856\\nimages). The validation set was further randomly split into two disjoint sets,\\nVS1 and VS2.'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'body', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='was split into training (1392 images), validation (907 images), and testing (856\\nimages). The validation set was further randomly split into two disjoint sets,\\nVS1 and VS2.\\nFor image segmentation, we employed U-Net architectures with a ResNet34\\nbackbone in Pytorch, utilizing the Segmentation Models library [12] pretrained\\non ImageNet. To integrate MC Dropout, dropout layers with optimal rates of 0.4\\nand 0.5 were added after each ReLU activation in the decoder [13,14]. Training\\nused a batch size of 8, Adam optimizer with a learning rate of 0.0001, and early\\nstopping based on validation loss stabilization. MSU-Net incorporated bagging\\nfor ensembling and the plural-correlation coefficient as a correlation metric.\\nFig.2: Epistemic uncertainty maps on test data for (a) MCU-Net and (b) MSU-\\nNet. Darker/lighter colors indicate lower/higher uncertainty values. To alleviate\\nclass imbalance, we limit evaluations to a region of interest delineated in white.'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'body', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='Net. Darker/lighter colors indicate lower/higher uncertainty values. To alleviate\\nclass imbalance, we limit evaluations to a region of interest delineated in white.\\nDistribution Divergence EstimationModel quality hinges on uncertainty\\ndistribution: low for correct, high for incorrect predictions, enhancing calibration\\nand user trust [24]. We utilize the RÃ©nyi divergence (RD) statistic as our metric\\nfor comparison. It is a generalization of Kullback-Leibler (KL) divergence that\\nmeasures the dissimilarity between two probability distributionsp and q. van\\nErven and HarremoÃ«s [5] relay an operational characterization of RD as the\\nnumber of bits by which a mixture of two codes,p and q, can be compressed.\\nFor some measurable setM0 that p and q lie in and orderÎ± âˆˆ R \\\\ {1}, RD of\\ndistribution p from distribution q is defined as:6 R. Banerjee et al.\\nRÎ±(p || q) = 1\\nÎ± âˆ’ 1 ln\\nZ\\nM0\\npÎ±(x)q1âˆ’Î±(x) dx (3)\\nA nonparametric estimator of RD that is conditionally L2-consistent us-'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'body', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='distribution p from distribution q is defined as:6 R. Banerjee et al.\\nRÎ±(p || q) = 1\\nÎ± âˆ’ 1 ln\\nZ\\nM0\\npÎ±(x)q1âˆ’Î±(x) dx (3)\\nA nonparametric estimator of RD that is conditionally L2-consistent us-\\ning only k-nearest-neighbor statistics has been proposed to considerably reduce\\ncomputational effort [21]. LetX1:n0 = ( X1, Â· Â· Â· , Xn0 ) be an i.i.d. sample from\\na distribution with densityp and Y1:n1 = ( Y1, Â· Â· Â· , Yn1 ) an i.i.d. sample from a\\ndistribution with densityq. We denoteÏk(i) to be thek-th nearest neighbor of\\nobservation Xi in X1:n0 and vk(i) the k-th nearest neighbor ofXi in Y1:n1. With\\nBk,Î± = Î“ (k)2\\nÎ“ (kâˆ’Î±+1)Î“ (k+Î±âˆ’1), we can estimate the RD by:\\nbRÎ±(p || q) = 1\\nÎ± âˆ’ 1 ln\\n \\nnâˆ’1\\n0\\nn0X\\ni=1\\n\\x12 (n0 âˆ’ 1)Ïk(i)\\nn1vk(i)\\n\\x131âˆ’Î±\\nBk,Î±\\n!\\n(4)\\nWe use RD to measure the ability of our method to distinguish correct pre-\\ndictions (p) from incorrect predictions (q). We conduct permutation testing to\\nassess deterministicity and bootstrapping to obtain confidence intervals for the'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'body', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='dictions (p) from incorrect predictions (q). We conduct permutation testing to\\nassess deterministicity and bootstrapping to obtain confidence intervals for the\\nresults. Nearest neighbor estimators are sensitive to perturbations in the un-\\nderlying distribution, and hence their limited variance cannot be consistently\\nestimated by a naÃ¯ve Efron-type bootstrap [1]. Since this behavior may result in\\na non-negligible positive bias in bootstrap estimates, we instead apply a direct\\nM-out-of-N (MooN) type bootstrap [26] for this metric, shown in Algorithm 1.\\nAlgorithm 1M-out-of-N bootstrapping\\n1: Input: Distributions p, q. Degree of undersampling,Î³ âˆˆ (0, 1].\\n2: Output: Bootstrap estimates of nonparametric RÃ©nyi divergence statistic\\n3: n0, n1 â† |p|, |q|\\n4: Î±n â† n0\\nn1\\n5: N âˆ— â†\\n\\x04\\n(n0 + n1)Î³ + 1\\n2\\n\\x05\\n6: nâˆ—\\n0 â†\\nj\\nÎ±n\\n1+Î±n\\nN âˆ— + 1\\n2\\nk\\n7: nâˆ—\\n1 â† N âˆ— âˆ’ nâˆ—\\n0\\n8: samples â† []\\n9: for i in range 1000 do\\n10: boot_p â† resample(p, n_samples= nâˆ—\\n0) with replacement\\n11: boot_q â† resample(q, n_samples= nâˆ—'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'body', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='2\\n\\x05\\n6: nâˆ—\\n0 â†\\nj\\nÎ±n\\n1+Î±n\\nN âˆ— + 1\\n2\\nk\\n7: nâˆ—\\n1 â† N âˆ— âˆ’ nâˆ—\\n0\\n8: samples â† []\\n9: for i in range 1000 do\\n10: boot_p â† resample(p, n_samples= nâˆ—\\n0) with replacement\\n11: boot_q â† resample(q, n_samples= nâˆ—\\n1) with replacement\\n12: samples\\n+\\n= [bRÎ±=0.85(boot_p, boot_q)]\\n13: end for\\n14: return samples\\n5 Results and Discussion\\nWe evaluate MSU-Net using quantitative and qualitative metrics. To alleviate\\nheavy class imbalance, we choose to average metrics over a predefined region of\\ninterest (ROI), shown in Fig. 2. Fig. 3 displays model uncertainty distributionsUncertainty Estimation with MSU-Net 7\\non the test set. We employ kernel density estimation with a Gaussian kernel\\nand optimal bandwidth via Silvermanâ€™s rule of thumb to visualize difference in\\nmeans between correct and incorrect prediction uncertainty distributions over\\n100,000 samples. To quantify separation between these distributions with RD,\\nwe select k = 4 for k-nearest neighbors and Î± = 0 .85 for numerical stability'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'body', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='100,000 samples. To quantify separation between these distributions with RD,\\nwe select k = 4 for k-nearest neighbors and Î± = 0 .85 for numerical stability\\nand interpretability [21]. We performB = 1000 permutations and bootstrapped\\nsamples. For MooN-type bootstrap,Î³ = 0 .8 is selected to maintain the largest\\nproportion of original data while achieving the closest coverage probability of\\n0.95 for 95% intervals. Our final results are displayed in Table 1.\\nTable 1: Model quality in ROI. Arrows show direction of better performance.\\nbÂµcorr bÂµincorr âˆ†bÂµ(â†‘) bRÎ±(corr || incorr)(â†‘) 95% CI oncRÎ± p-value (â†“)\\nMCU-Net 7.230 11.229 3.999 0.429 [0.426, 0.453] 0.003\\nMSU-Net 20.783 33.876 13.093 0.638 [0.603, 0.667] 0.003\\nFig. 4a shows training behavior for both models. MSU-Net shows a more\\nstable convergence and consistently outperforms MCU-Net during validation\\nperformed after each epoch. MSU-Net achieves18.1% better mean IoU and a'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'body', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='stable convergence and consistently outperforms MCU-Net during validation\\nperformed after each epoch. MSU-Net achieves18.1% better mean IoU and a\\nsignificant improvement in sensitivity and false negative rate scores at alpha level\\n0.05, while other metrics remain similar. We additionally utilize precision-recall\\ncurves, seen in Fig. 4b, which are resilient to unbalanced classes since they only\\nfocus on positive class predictions. Performance results are displayed in Table 2.\\nTable 2: Model performance in ROI.\\nMean test IoU(â†‘) Specificity(â†‘) Sensitivity(â†‘) FPR(â†“) FNR(â†“)\\nMCU-Net 0.679 0.998 0.673 0.002 0.327\\nMSU-Net 0.860 0.996 0.890 0.004 0.110\\nFig.3: Epistemic uncertainty distributions for correct (blue) and incorrect (or-\\nange) predictions for MCU-Net (top) and MSU-Net (bottom). Our approach\\nyields a markedly better differentiation of correct and incorrect predictions.\\nPermutation tests for MCU-Net and MSU-Net (p â‰¤ 0.003 for both) indicate'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'body', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='yields a markedly better differentiation of correct and incorrect predictions.\\nPermutation tests for MCU-Net and MSU-Net (p â‰¤ 0.003 for both) indicate\\nthat the observed separation between correct and incorrect predictions is not8 R. Banerjee et al.\\nrandom in both. Yet, at the95%confidence level, we see no overlap between their\\n95% confidence intervals, showing that the ability of MSU-Net to distinguish\\ncorrect from incorrect predictions is significantly better than that of MCU-Net.\\nQualitative uncertainty maps visually validate our findings and capture local\\nvariations in model performance [4]. MCU-Net exhibits indiscriminately low un-\\ncertainty in Fig. 2, whereas MSU-Net provides more interpretable uncertainty\\nvalues. MSU-Net shows clearer vessel tops with lower uncertainty and higher\\nuncertanties at the bottom of vessels which is consistent with our assessment.\\nMSU-Net yields a higher credibility in assessing its predictive reliability.'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'body', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='uncertanties at the bottom of vessels which is consistent with our assessment.\\nMSU-Net yields a higher credibility in assessing its predictive reliability.\\nFig.4: (a) Training (left) and validation (right) curves for MCU-Net and MSU-\\nNet. Early stopping delineated by gray lines. (b) Precision-recall curves.\\nMSU-Net generally outperforms MCU-Net. Although it performs marginally\\nworse at specificity and false positive rate (FPR), the precision-recall curves\\nfrom Fig. 4b show a higher average precision score for MSU-Net at 0.945 than\\nMCU-Net at 0.875 compared to the baseline score of 0.09 for a â€˜no-skillâ€™ classi-\\nfier. As such, at the same level of recall, MSU-Net correctly classifies a greater\\nproportion of pixels that are actually vessels than MCU-Net. Crucially, MSU-\\nNet achieves a considerably lower false negative rate (FNR) than MCU-Net. In\\nthis context, failing to anticipate a true vessel can have more catastrophic con-'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'body', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='Net achieves a considerably lower false negative rate (FNR) than MCU-Net. In\\nthis context, failing to anticipate a true vessel can have more catastrophic con-\\nsequences to a critically injured individual than incorrectly anticipating a vessel.\\nMSU-Net improves credibility not only through higher quality results, but also\\nthrough more accurate results while avoiding potentially disastrous deficiencies\\nof predictive modeling in the context of medical image segmentation.\\n6 Conclusion\\nThis paper introduces MSU-Net, a multistaged Monte Carlo U-Net for uncertain\\nultrasound image segmentations. It improves model transparency and trustwor-\\nthiness compared to standard U-Net by enhancing uncertainty evaluation, de-\\nspite minimal additional training. Our framework sets a benchmark for future\\nstudies, offering qualitative maps for intuitive model assessment. While our pre-\\nliminary results are limited to phantom data and binary image segmentation,'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'body', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='studies, offering qualitative maps for intuitive model assessment. While our pre-\\nliminary results are limited to phantom data and binary image segmentation,\\nwe aim to validate these findings on live animal and human data in future work.Uncertainty Estimation with MSU-Net 9\\nAcknowledgments. Authors thank Nico Zevallos, Dr. Michael R. Pinsky, and Dr.\\nHernando Gomez for gathering experiment data. This work was partially supported by\\nthe U.S. Dept. of Defense contracts W81XWH-19-C0083 and W81XWH-19-C0101.\\nDisclosure of Interests. The authors have no competing interests to declare that\\nare relevant to the content of this article.10 R. Banerjee et al.\\nReferences\\n1. Abadie, A., Imbens, G.W.: On the failure of the bootstrap for matching esti-\\nmators. Econometrica 76(6), 1537â€“1557 (2008).https://doi.org/https://doi.\\norg/10.3982/ECTA6474, https://onlinelibrary.wiley.com/doi/abs/10.3982/\\nECTA6474\\n2. Breiman, L.: Bagging predictors. Machine Learning24(2), 123â€“140 (1996).https:'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'body', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='org/10.3982/ECTA6474, https://onlinelibrary.wiley.com/doi/abs/10.3982/\\nECTA6474\\n2. Breiman, L.: Bagging predictors. Machine Learning24(2), 123â€“140 (1996).https:\\n//doi.org/10.1007/BF00058655, https://doi.org/10.1007/BF00058655\\n3. Chen, E., Choset, H., Galeotti, J.: Uncertainty-based adaptive data augmentation\\nfor ultrasound imaging anatomical variations. In: 2021 IEEE 18th International\\nSymposium on Biomedical Imaging (ISBI). pp. 438â€“442 (2021).https://doi.org/\\n10.1109/ISBI48211.2021.9433979\\n4. Dechesne, C., Lassalle, P., LefÃ¨vre, S.: Bayesian u-net: Estimating uncertainty in\\nsemantic segmentation of earth observation images. Remote Sensing13(19) (2021).\\nhttps://doi.org/10.3390/rs13193836, https://www.mdpi.com/2072-4292/13/\\n19/3836\\n5. van Erven, T., Harremoes, P.: RÃ©nyi divergence and kullback-leibler di-\\nvergence. IEEE Transactions on Information Theory 60(7), 3797â€“3820 (Jul\\n2014). https://doi.org/10.1109/tit.2014.2320500, http://dx.doi.org/10.\\n1109/TIT.2014.2320500'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'body', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='vergence. IEEE Transactions on Information Theory 60(7), 3797â€“3820 (Jul\\n2014). https://doi.org/10.1109/tit.2014.2320500, http://dx.doi.org/10.\\n1109/TIT.2014.2320500\\n6. Freund, Y., Schapire, R.E.: A short introduction to boosting (1999),https://api.\\nsemanticscholar.org/CorpusID:9621074\\n7. Gal, Y., Ghahramani, Z.: Dropout as a bayesian approximation: Representing\\nmodel uncertainty in deep learning (2016)\\n8. Ghoshal, B., Tucker, A., Sanghera, B., Lup Wong, W.: Estimating uncer-\\ntainty in deep learning for reporting confidence to clinicians in medical im-\\nage segmentation and diseases detection. Computational Intelligence37(2), 701â€“\\n734 (2021). https://doi.org/https://doi.org/10.1111/coin.12411, https://\\nonlinelibrary.wiley.com/doi/abs/10.1111/coin.12411\\n9. Goel, R., Morales, C., Singh, M., Dubrawski, A., Galeotti, J.M., Choset,\\nH.: Motion-aware needle segmentation in ultrasound images. https://api.\\nsemanticscholar.org/CorpusID:265660400'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'body', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='9. Goel, R., Morales, C., Singh, M., Dubrawski, A., Galeotti, J.M., Choset,\\nH.: Motion-aware needle segmentation in ultrasound images. https://api.\\nsemanticscholar.org/CorpusID:265660400\\n10. Gruber, S.G., Buettner, F.: Better uncertainty calibration via proper scores for\\nclassification and beyond (2024)\\n11. Guo, C., Pleiss, G., Sun, Y., Weinberger, K.Q.: On calibration of modern neu-\\nral networks. In: Proceedings of the 34th International Conference on Machine\\nLearning - Volume 70. p. 1321â€“1330. ICMLâ€™17, JMLR.org (2017)\\n12. Iakubovskii, P.: Segmentation models (2019)\\n13. Kendall, A., Badrinarayanan, V., Cipolla, R.: Bayesian segnet: Model uncertainty\\nin deep convolutional encoder-decoder architectures for scene understanding (2016)\\n14. Kim, B.J., Choi, H., Jang, H., Lee, D., Kim, S.W.: How to use dropout correctly\\non residual networks with batch normalization (2023)\\n15. Lai, K.K., Yu, L., Wang, S., Zhou, L.: Credit risk analysis using a reliability-based'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'body', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='on residual networks with batch normalization (2023)\\n15. Lai, K.K., Yu, L., Wang, S., Zhou, L.: Credit risk analysis using a reliability-based\\nneural network ensemble model. In: Kollias, S., Stafylopatis, A., Duch, W., Oja,\\nE. (eds.) Artificial Neural Networks â€“ ICANN 2006. pp. 682â€“690. Springer Berlin\\nHeidelberg, Berlin, Heidelberg (2006)\\n16. Lakshminarayanan, B., Pritzel, A., Blundell, C.: Simple and scalable predictive\\nuncertainty estimation using deep ensembles (2017)Uncertainty Estimation with MSU-Net 11\\n17. Morales, C.G., Chen, H., Yao, J., Dubrawski, A.: 3d ultrasound reconstruction\\nand visualization tool. In: New Evolutions in Surgical Robotics: Embracing Multi-\\nmodal Imaging Guidance, Intelligence, and Bio-inspired Mechanisms. International\\nConference on Robotics and Automation (ICRA), London, UK (June 2 2023)\\n18. Morales, C.G., Srikanth, D., Good, J.H., Dufendach, K.A., Dubrawski, A.: Bifur-\\ncation identification for ultrasound-driven robotic cannulation (2024), submitted'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'body', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='18. Morales, C.G., Srikanth, D., Good, J.H., Dufendach, K.A., Dubrawski, A.: Bifur-\\ncation identification for ultrasound-driven robotic cannulation (2024), submitted\\nto the International Conference on Intelligent Robots and Systems (IROS)\\n19. Morales, C.G., Yao, J., Rane, T., Edman, R., Choset, H., Dubrawski, A.:\\nReslicing Ultrasound Images for Data Augmentation and Vessel Recon-\\nstruction. 2023 IEEE International Conference on Robotics and Automa-\\ntion (ICRA) pp. 2710â€“2716 (2023), https://www.semanticscholar.org/\\npaper/Reslicing-Ultrasound-Images-for-Data-Augmentation-Morales-Yao/\\ncb3e8ae71dc3c189442427093b9124f35e373790\\n20. Oâ€™Neill, B.: Multiple linear regression and correlation: A geometric analysis (2021)\\n21. Poczos, B., Schneider, J.: On the estimation of Î±-divergences. In: Gordon, G.,\\nDunson, D., DudÃ­k, M. (eds.) Proceedings of the Fourteenth International Con-\\nference on Artificial Intelligence and Statistics. Proceedings of Machine Learning'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'body', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='Dunson, D., DudÃ­k, M. (eds.) Proceedings of the Fourteenth International Con-\\nference on Artificial Intelligence and Statistics. Proceedings of Machine Learning\\nResearch, vol. 15, pp. 609â€“617. PMLR, Fort Lauderdale, FL, USA (11â€“13 Apr\\n2011), https://proceedings.mlr.press/v15/poczos11a.html\\n22. Seedat, N.: Mcu-net: A framework towards uncertainty representations for decision\\nsupport system patient referrals in healthcare contexts (2020)\\n23. Sekachev, B., Manovich, N., Zhiltsov, M., Zhavoronkov, A., Kalinin, D., Hoff,\\nB., TOsmanov, Kruchinin, D., Zankevich, A., DmitriySidnev, Markelov, M., Jo-\\nhannes222, Chenuet, M., a andre, telenachos, Melnikov, A., Kim, J., Ilouz, L.,\\nGlazov, N., Priya4607, Tehrani, R., Jeong, S., Skubriev, V., Yonekura, S., vugia\\ntruong, zliang7, lizhming, Truong, T.: opencv/cvat: v1.1.0 (Aug 2020).https://\\ndoi.org/10.5281/zenodo.4009388, https://doi.org/10.5281/zenodo.4009388\\n24. Shamsi, A., Asgharnezhad, H., Tajally, A., Nahavandi, S., Leung, H.: An'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'body', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='doi.org/10.5281/zenodo.4009388, https://doi.org/10.5281/zenodo.4009388\\n24. Shamsi, A., Asgharnezhad, H., Tajally, A., Nahavandi, S., Leung, H.: An\\nuncertainty-aware loss function for training neural networks with calibrated pre-\\ndictions (2023)\\n25. Wallace, H., Regunath, H.: Fluid resuscitation. StatPearls [Internet] (jan 2024),\\nhttps://www.ncbi.nlm.nih.gov/books/NBK534791/\\n26. Walsh, C., Jentsch, C.: Nearest neighbor matching: M-out-of-n bootstrapping with-\\nout bias correction vs. the naive bootstrap. Econometrics and Statistics (2023).\\nhttps://doi.org/https://doi.org/10.1016/j.ecosta.2023.04.005, https://\\nwww.sciencedirect.com/science/article/pii/S245230622300031X\\n27. Wolpert, D.H.: Stacked generalization. Neural Networks 5(2), 241â€“259\\n(1992). https://doi.org/https://doi.org/10.1016/S0893-6080(05)80023-1,\\nhttps://www.sciencedirect.com/science/article/pii/S0893608005800231\\n28. Yang, S., Browne, A.: Neural network ensembles: combining multiple models for'),\n",
       " Document(metadata={'arxiv_id': '2407.21273v1', 'title': 'Enhanced Uncertainty Estimation in Ultrasound Image Segmentation with MSU-Net', 'section': 'body', 'authors': 'Rohini Banerjee, Cecilia G. Morales, Artur Dubrawski'}, page_content='https://www.sciencedirect.com/science/article/pii/S0893608005800231\\n28. Yang, S., Browne, A.: Neural network ensembles: combining multiple models for\\nenhanced performance using a multistage approach. Expert Systems21(5), 279â€“\\n288 (2004). https://doi.org/https://doi.org/10.1111/j.1468-0394.2004.\\n00285.x, https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-0394.\\n2004.00285.x\\n29. Yang, S., Browne, A., Picton, P.: Multistage neural network ensembles. vol. 2364,\\npp. 91â€“97 (06 2002).https://doi.org/10.1007/3-540-45428-4_9\\n30. Yin, B., Hu, Q., Zhu, Y., Zhao, C., Zhou, K.: Paw-net: Stacking ensem-\\nble deep learning for segmenting scanning electron microscopy images\\nof fine-grained shale samples. Computers and Geosciences 168, 105218\\n(2022). https://doi.org/https://doi.org/10.1016/j.cageo.2022.105218,\\nhttps://www.sciencedirect.com/science/article/pii/S0098300422001674'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'title_abstract', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content=\"Title: Active Learning on Medical Image\\n\\nAbstract: The development of medical science greatly depends on the increased utilization of machine learning algorithms. By incorporating machine learning, the medical imaging field can significantly improve in terms of the speed and accuracy of the diagnostic process. Computed tomography (CT), magnetic resonance imaging (MRI), X-ray imaging, ultrasound imaging, and positron emission tomography (PET) are the most commonly used types of imaging data in the diagnosis process, and machine learning can aid in detecting diseases at an early stage. However, training machine learning models with limited annotated medical image data poses a challenge. The majority of medical image datasets have limited data, which can impede the pattern-learning process of machine-learning algorithms. Additionally, the lack of labeled data is another critical issue for machine learning. In this context, active learning techniques can be employed to address the challenge of limited annotated medical image data. Active learning involves iteratively selecting the most informative samples from a large pool of unlabeled data for annotation by experts. By actively selecting the most relevant and informative samples, active learning reduces the reliance on large amounts of labeled data and maximizes the model's learning capacity with minimal human labeling effort. By incorporating active learning into the training process, medical imaging machine learning models can make more efficient use of the available labeled data, improving their accuracy and performance. This approach allows medical professionals to focus their efforts on annotating the most critical cases, while the machine learning model actively learns from these annotated samples to improve its diagnostic capabilities.\"),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='ACTIVE LEARNING ON MEDICAL IMAGE\\nA PREPRINT\\nAngona Biswas\\nResearch and Development Department, Pioneer Alpha,\\nDhaka, Bangladesh\\nangonabiswas28@gmail.com\\nMD Abdullah Al Nasim\\nResearch and Development Department, Pioneer Alpha,\\nDhaka, Bangladesh\\nnasim.abdullah@ieee.org\\nMd Shahin Ali\\nDepartment of Biomedical Engineering, Islamic University, Kushtia-7003, Bangladesh\\nshahinbme.iu@gmail.com\\nIsmail Hossain\\nUniversity of Alabama at Birmingham\\nAlabama, USA\\nihossain@uab.edu\\nDr . Md Azim Ullah\\nUniversity of Memphis\\nmullah@memphis.edu\\nSajedul Talukder\\nUniversity of Alabama at Birmingham\\nAlabama, USA\\nstalukder@uab.edu\\nAugust 31, 2024\\nABSTRACT\\nThe development of medical science greatly depends on the increased utilization of machine learning\\nalgorithms. By incorporating machine learning, the medical imaging field can significantly improve\\nin terms of the speed and accuracy of the diagnostic process. Computed tomography (CT), magnetic'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='algorithms. By incorporating machine learning, the medical imaging field can significantly improve\\nin terms of the speed and accuracy of the diagnostic process. Computed tomography (CT), magnetic\\nresonance imaging (MRI), X-ray imaging, ultrasound imaging, and positron emission tomography\\n(PET) are the most commonly used types of imaging data in the diagnosis process, and machine\\nlearning can aid in detecting diseases at an early stage. However, training machine learning models\\nwith limited annotated medical image data poses a challenge. The majority of medical image datasets\\nhave limited data, which can impede the pattern-learning process of machine-learning algorithms.\\nAdditionally, the lack of labeled data is another critical issue for machine learning. In this context,\\nactive learning techniques can be employed to address the challenge of limited annotated medical\\nimage data. Active learning involves iteratively selecting the most informative samples from a'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='active learning techniques can be employed to address the challenge of limited annotated medical\\nimage data. Active learning involves iteratively selecting the most informative samples from a\\nlarge pool of unlabeled data for annotation by experts. By actively selecting the most relevant\\nand informative samples, active learning reduces the reliance on large amounts of labeled data and\\nmaximizes the modelâ€™s learning capacity with minimal human labeling effort. By incorporating active\\nlearning into the training process, medical imaging machine learning models can make more efficient\\nuse of the available labeled data, improving their accuracy and performance. This approach allows\\nmedical professionals to focus their efforts on annotating the most critical cases, while the machine\\nlearning model actively learns from these annotated samples to improve its diagnostic capabilities.\\nKeywords Medical imaging, diagnosis, machine learning, MRI, labeled data'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='learning model actively learns from these annotated samples to improve its diagnostic capabilities.\\nKeywords Medical imaging, diagnosis, machine learning, MRI, labeled data\\narXiv:2306.01827v2  [eess.IV]  7 Jun 2023A PREPRINT - AUGUST 31, 2024\\nFigure 1: The loop of training, testing, identifying uncertainty, annotating, and retraining is repeated until the model\\nmeets a predetermined performance level [3].\\n1 Introduction\\nHumans are dying of various types of diseases around the world every year. Some deadly diseases are excessively\\nserious because if they are not detected at the early stage it becomes strenuous to save the patientâ€™s life. In that manner,\\nthe medical image screening process was invented and this scheme is vastly advanced now. Physicians were unable\\nto see images of what was happening within a patientâ€™s body before November 8, 1895 [1]. As radiographs became\\na crucial component of medical diagnosis, new uses, and knowledge about disease representation soon accumulated,'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='a crucial component of medical diagnosis, new uses, and knowledge about disease representation soon accumulated,\\nRoentgen was given the Nobel Prize in Physics in 1901 for X-ray. At the EMI research facilities in 1967, Sir Godfrey\\nHounsfield created the first CT scanner. On October 1, 1971, the first live patient was scanned. The development of\\ncomputed tomography (CT) involved creating a two-dimensional picture from radiography projections made from\\nvarious angles. The first NMR pictures were published by Paul Lauterbur in 1973, and in 2003, he was awarded the 2003\\nNobel Prize in the field of Physiology and Medicine. In contrast to CT, magnetic resonance imaging is based on distinct\\nphysical concepts. Then another ultrasound or US technology is mentionable. Ultrasound employs high-frequency\\nsound waves that are sent into the body and are above the human hearing range. A positron emission tomography (PET)'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='sound waves that are sent into the body and are above the human hearing range. A positron emission tomography (PET)\\nscan is an effective imaging procedure that can assist show how your tissues and organsâ€™ metabolisms or biochemical\\nprocesses work [2]. These are mostly used in the medical imaging process for the diagnosis of diseases.\\nConventionally, a radiologist helps to diagnose or assist the doctor in predicting the deadly disease which is time-\\nconsuming and can be an unstable prediction. The involvement of technology is created a wide range of opportunities\\nto diagnose medical images. Even more than a century ago, statistical methods of automated decision-making and\\nmodeling have been discovered (and updated) in a range of industries. The domain of Artificial Intelligence or Machine\\nLearning created an automated process for the classification of deadly disease types and detection schemes which'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='Learning created an automated process for the classification of deadly disease types and detection schemes which\\nis a rapid and advanced procedure. Technology advances and federated learning make it possible for healthcare\\norganizations to train machine learning models with private data without compromising patient confidentiality. We can\\nget a concept of leveraging federated learning for x-ray images in the papers [4, 5].\\nIn [6] a collaborative federated learning system that enables deep-learning image analysis and classifying diabetic\\nretinopathy without transferring patient data between healthcare organizations has been introduced. Along with image\\ndata, healthcare patientsâ€™ statistical data can be used to train machine learning models in order to predict disease severity.\\nIn [7, 8] potential lead exposure at the zip code level is predicted using machine learning on patientsâ€™ Blood Lead Levels\\n(BLL) dataset.'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='In [7, 8] potential lead exposure at the zip code level is predicted using machine learning on patientsâ€™ Blood Lead Levels\\n(BLL) dataset.\\nFigure 2 is depicting the process of how machine learning is utilized for medical image diagnosis. A machine learning\\nalgorithm learns from the training data and uses what it has learned to create a prediction if it is applied to a collection\\nof data (in our case, tumor pictures) and information about these data (in our case, benign or malignant tumors) [3].\\nIn past, various tremendous works are performed based on medical imaging. In [9], a more precise automated brain\\ntumor classification method using fuzzy C-means and ANN is suggested. MRI brain tumor is classified using ANN.\\nThis suggested method outperforms other current detection methods in terms of 99.8% accuracy, 100% sensitivity, and\\n99.59% specificity. The main objective of this research [ 10] is to evaluate several computer-aided methods, examine'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='99.59% specificity. The main objective of this research [ 10] is to evaluate several computer-aided methods, examine\\nthe most effective method already in use, pinpoint its drawbacks, and then propose a new model that improves on\\n2A PREPRINT - AUGUST 31, 2024\\nFigure 2: The creation of machine learning models and their use in medical picture categorization problems [3].\\nthe most effective method already in use. The strategy employed involved grouping and listing lung cancer detection\\nmethods according to how well they could find the disease. Therefore, the goal of this research is to raise accuracy\\nto 100%. The lack of data produces poor results and raises additional unsolved questions regarding AI. It makes us\\nrealize that access is necessary for data preparation for machine learning and that even the greatest software would\\nbe meaningless without sufficient data filling. So, the problem arises when there are less amount of labeled medical'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='be meaningless without sufficient data filling. So, the problem arises when there are less amount of labeled medical\\nimages and the perfect detection or diagnosis is indispensable. This limitation is overcome by the process of â€œActive\\nLearningâ€. The conceptualization of active learning on medical images will be discussed in the following section.\\nThe annotators evaluate and identify the samples with a high level of uncertainty before sending them back. The model\\nis then retrained using the freshly labeled data by ML developers before being tested once again on new unlabeled data.\\nThe early predictions that the model produces on the unlabelled data wonâ€™t be accurate because the active learning\\npipeline starts with a short labelled sample.\\nThis chapter will discuss the AI active learning (AL) method, which can assist human labelers by continuously arranging\\nthe unlabeled photos based on the level of information obtained, prompting the labeler to label the most informative\\nimage next.'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='the unlabeled photos based on the level of information obtained, prompting the labeler to label the most informative\\nimage next.\\n2 Literature Review\\nMedical imaging continues to be very interesting in artificial intelligence (AI). The primary challenges to creating and\\nutilizing AI algorithms in clinical settings involve the availability of suitably large, properly managed, and indicative\\ntraining data that includes expert annotations or labeling. Willemink et al. [ 11] outlines the basic procedures for\\nreadying medical imaging data for use in developing AI algorithms. They discuss the present shortcomings in data\\nmanagement and explore innovative methods for resolving the issue of limited data access. The task of classifying brain\\ntumors is difficult, and Biswas et al. [9] introduced two clustering algorithms and an Artificial Neural Network (ANN)\\nas techniques for classification. K-means and FCM are popular clustering methods used in medical image processing. A'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='as techniques for classification. K-means and FCM are popular clustering methods used in medical image processing. A\\nlot of fact-finding work is currently being done to improve radiology applications utilizing these algorithms to identify\\nillness diagnosis system mistakes that might lead to very unclear medical therapies. The primary objective of Latif\\net al. [12] is to emphasize the utilization of machine learning and deep learning methods in medical imaging. They\\naim to offer a summary of the current techniques employed in medical imaging for researchers and to underscore the\\nbenefits and drawbacks of these algorithms, while also considering potential future developments in the field. The\\nuse of artificial intelligence (AI) models is becoming more prominent in biomedical research and healthcare services.\\nCastiglioni et al. [13] examine the challenges and issues that need to be addressed to effectively develop AI applications'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='Castiglioni et al. [13] examine the challenges and issues that need to be addressed to effectively develop AI applications\\nthat function as clinical decision support systems in real-world situations. In past, various improved experiment was\\ndone and successful research works were taken place for medical image diagnosis or classification. But there is a few\\nresearch interlude. One of the major obstacles to the development and clinical use of AI systems is the absence of\\nsufficiently large, vetted, representative data sets with expert labeling (eg, annotations). Before imagery may be used,\\nseveral significant issues must be resolved. First, because picture annotation and tagging must be done, scalability is\\nconstrained. Second, each facility may need the replication and placement of significant computation resources. To\\n3A PREPRINT - AUGUST 31, 2024\\nresolve the less amount of labeled data, the concept of active learning is introduced which is acceptable for medical'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='3A PREPRINT - AUGUST 31, 2024\\nresolve the less amount of labeled data, the concept of active learning is introduced which is acceptable for medical\\ndata. In contrast to other healthcare concerns, identifying COVID-19 requires AI-powered tools that implement active\\nlearning-based cross-population train/test models and incorporate multitudinal and multimodal data. Santosh et al. [14]â€™s\\nmain objective is to address this need.\\n2.1 Machine Learning in the Context of Medical Images\\nWhen there is a lot of information that has to be analyzed, diagnosing a brain tumor can be frustrating. The extraction\\nof tumor areas from pictures becomes difficult due to the great degree of visual variety and similarities between brain\\ntumors and normal tissues that characterize brain cancers [ 15, 16, 17]. The suggested study [ 18] uses a Machine-\\nLearning-Technique (MLT) to analyze the brain MRI slices and classify the tumor locations as low or high grade'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='Learning-Technique (MLT) to analyze the brain MRI slices and classify the tumor locations as low or high grade\\ndepending on the analysis. After a series of processes, the Gray Level Co-occurrence Matrix (GLCM) is then used\\nto extract the essential information from the tumor area. The Support Vector Machine with Radial Basis Function\\n(SVM-RBF) kernel is then used to create a two-class classifier, and its performance is checked against that of other\\nclassifiers like the Random-Forest and k-Nearest Neighbor. The proposed workâ€™s findings support that using a tool with\\nthe SVM-RBF to implement helps reach an accuracy of greater than 94% on the benchmark BRATS2015 database. The\\nmain focus of this [9] research is on accurately classifying brain tumors. In this chapter, two techniques for classifying\\nMRI brain tumors are suggested by contrasting two key clustering algorithms with artificial neural networks (ANN).'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='MRI brain tumors are suggested by contrasting two key clustering algorithms with artificial neural networks (ANN).\\nMRI images are initially pre-processed by a reseized and sharpening filter for algorithm 1. Second, k-means clustering\\nis used to segment the photos. Thirdly, features are extracted using a 2D discrete wavelet transform and then compressed\\nusing principal component analysis. The final step is to use compressed features for ANN training, validation, and\\ntesting. For the second algorithm, C-means clustering is used along with a similar previous process. Finally, it is\\nobserved that the second algorithm is better than the first one. Segmentation of images has been addressed in [ 19]\\nwhich has numerous medical imaging applications.\\nAn integrated AI and IoT technology that can aid in earlier breast cancer diagnosis. Mammograms are the primary\\nmethod for finding breast cancer. In order to identify and treat breast cancer early and reduce the number of fatalities,'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='method for finding breast cancer. In order to identify and treat breast cancer early and reduce the number of fatalities,\\nseveral imaging techniques have been created. Additionally, numerous strategies for diagnosing breast cancer have\\nbeen utilized to improve diagnostic precision [ 20]. The core principle of active learning stipulates that a machine\\nlearning approach may perform much better with even less training if provided the flexibility to choose the information\\nit learns from [21]. The percentage of persons afflicted by malignant and benign tumors has been predicted, and the\\nfindings have been shown using pictures. For an unbiased estimate in this suggested method, the authors have employed\\nsupervised learning techniques including K-Nearest-Neighbors, Naive Bayes, Decision Trees, and the Support Vector\\nMachine Modelling Techniques. With an accuracy rate of 91.6%, the findings showed that K-Nearest-Neighbors is the\\nstrongest predictor.'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='Machine Modelling Techniques. With an accuracy rate of 91.6%, the findings showed that K-Nearest-Neighbors is the\\nstrongest predictor.\\nAmong the increasing number of research on mass identification and segmentation, attention has recently been drawn to\\nthe automatic segmentation of breast ultrasound pictures into functional tissues. In the study of [22], we propose to\\nemploy convolutional neural networks (CNNs) to partition 3-dimensional breast ultrasound pictures into four primary\\ntissues: skin, fibroglandular tissue, mass, and fatty tissue.\\nA blood clot called deep vein thrombosis (DVT), which is most frequently detected in the leg and can result in a\\ncatastrophic pulmonary embolism (PE). In a pre-clinical investigation of [ 23] the authors gather photos and look\\ninto a deep learning method for automatically interpreting compressed ultrasound images. Our technique assists'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='into a deep learning method for automatically interpreting compressed ultrasound images. Our technique assists\\nnon-specialists in identifying DVT and offers direction for free-hand ultrasonography. We use ultrasound films from\\n255 volunteers to train a deep learning system, and we use 53 prospective NHS DVT diagnostic clinic patients and\\nThirty prospective German DVT hospital patients as our sample size for evaluation.Through all compressions, the\\nsegmentation is strong which is shown in 3. To rule out DVT, the venous region is assessed for total compressibility.\\n2.2 Deep Learning in the Context of Medical Images\\nPre-processing is employed in the [ 24] study to lessen the impact of intensity fluctuations across CT slices. The\\nbackground of the CT lung image is then isolated using histogram thresholding. Each CT lung scan is subjected\\nto feature extraction using a Q-deformed entropy technique and deep learning. Using a long short-term memory'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='to feature extraction using a Q-deformed entropy technique and deep learning. Using a long short-term memory\\n(LSTM) neural network classifier, the collected characteristics are categorized. The performance of the LSTM network\\nis then considerably enhanced by merging all retrieved characteristics in order to accurately distinguish between\\nCOVID-19, pneumonia, and healthy patients. The obtained dataset, which includes 321 cases, may be classified with\\nan accuracy of up to 99.68%. The main goal of the publication [25] is to employ several deep learning approaches to\\ndistinguish between CT scan pictures of COVID-19 and non-COVID 19 patients. With an accuracy rate of 82.1%, a\\nself-developed model with the name CTnet-10 was created for the COVID-19 diagnostic. DenseNet-169, VGG-16,\\n4A PREPRINT - AUGUST 31, 2024\\nFigure 3: Examples of high quality for the segmentation capabilities of our model [23]\\n.\\nFigure 4: Utilizing Deep Learning for automatic CT scan screening [25]\\n.'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='4A PREPRINT - AUGUST 31, 2024\\nFigure 3: Examples of high quality for the segmentation capabilities of our model [23]\\n.\\nFigure 4: Utilizing Deep Learning for automatic CT scan screening [25]\\n.\\nResNet-50, InceptionV3, and VGG-19 are further models we examined. By having an accuracy of 94.52%, the VGG-19\\noutperformed all other deep learning models. Figure 4 shows that Deep Learning is effective for COVID-19 detection\\nusing lung CT Scan at the clinical stage.\\nArtificial intelligence algorithms may be utilized to determine the existence and severity of the illness since there\\nare obvious variations between the X-ray pictures of an infected individual and a healthy one. Pretrained models\\nbased on convolutional neural networks (CNN) have been widely utilized to classify Corona Virus Disease 2019\\n(COVID-19) [26] using chest X-rays (CXR). This kind of successful work had been done by [27, 28, 29] successfully\\nduring the pandemic situation.'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='(COVID-19) [26] using chest X-rays (CXR). This kind of successful work had been done by [27, 28, 29] successfully\\nduring the pandemic situation.\\nThis [30] study suggests a two-stage categorization phase for identifying anomalies (fractures) in the upper extremity\\nbones. For both classification phases, two convolutions neural network (CNN) models, ResNet-50 and Inception-v3, are\\n5A PREPRINT - AUGUST 31, 2024\\nFigure 5: Analysis of Medical Images Using Deep Learning (Brain tumor MRI diagnosis with the help of deep learning\\nnetwork) [31]\\n.\\nexamined. The outcomes show that the Inception model is better than all other classifiers for both phases. The average\\naccuracy obtained surpasses that of previous investigations. The Mura dataset was used for all studies.\\nFigure 5 is showing how deep learning is effectively utilized for brain tumor detection. MR images are inputted into the'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='Figure 5 is showing how deep learning is effectively utilized for brain tumor detection. MR images are inputted into the\\ndeep learning network and the layers of the network extract the features finally the last layer provides the decision.\\n2.3 Issue of Inadequately Labeled Medical Data\\nIt is clear that the Artificial Intelligence domain needs data to learn the machine. After that, it can perform like a human\\nor more perfectly. This is true for machine learning as well as deep learning. On the other hand, data annotation is\\nnecessary to ensure that systems produce reliable results and helps modules identify the components of speech and\\ncomputer vision models. For some jobs, deep learning has proven to be more effective than traditional machine learning\\ntechniques, including those involving nature photographs. Deep learning has a variety of uses in medical imaging, such'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='techniques, including those involving nature photographs. Deep learning has a variety of uses in medical imaging, such\\nas the diagnosis of various diseases through the study of retinal fundus pictures and the categorization of lung and brain\\nailments. The capacity of deep learning to automatically extract the most pertinent characteristics for data interpretation\\nand inference straight from the annotated data is what has led to its fast growth and deployment in such a wide range of\\napplications. To train the deep learning model, however, a sizable amount of data with expert annotations is necessary.\\nIn reality, gathering a sizable volume of expert-annotated datasets in the field of medical images is difficult. The reason\\nis that in order to appropriately identify the medical pictures, professionals such as radiologists or doctors who are\\noccupied with clinical activities are needed in order to extract annotation from medical images. Additionally, medical'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='occupied with clinical activities are needed in order to extract annotation from medical images. Additionally, medical\\npictures like CT, MRI, and PET are often difficult and expensive to collect, in contrast to natural image analysis, where\\nthe images might be simply taken with a regular camera. Deep learning is therefore increasingly important in medical\\npicture applications to achieve generalizable learning from limited datasets.\\n2.4 Active learning concept for medical images\\nIn a unique utilization of machine learning known asâ€œactive learning,â€ fresh data points are labeled with the intended\\noutputs by proactively querying a user (as well as another information source). It is occasionally referred to as the\\noptimum design of experiments in the statistics literature. There are instances where there is a large amount of unlabeled\\ndata yet human labeling is expensive. Learning algorithms can actively ask the user or teacher for labels in this situation.'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='data yet human labeling is expensive. Learning algorithms can actively ask the user or teacher for labels in this situation.\\nActive learning is the name given to this recurrent supervised learning method.\\nThe basic idea behind the active learner algorithm is that if an ML algorithm were given free rein to select the data it\\nwishes to learn from, it could be able to achieve a greater degree of accuracy while utilizing fewer training labels. As\\na result, throughout the training phase, active learners are welcome to ask questions in an interactive manner. These\\nrequests are often sent as unlabeled data instances, and a human annotator is asked to label the instance. As one of\\nthe most effective instances of accomplishment in the living person paradigm, active learning is now included in that\\nparadigm. The following are the three types of active learning: 1. Stream-based selective sampling 2. Pool-based'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='paradigm. The following are the three types of active learning: 1. Stream-based selective sampling 2. Pool-based\\nsampling 3. Membership query synthesis. The two types of learning are more similar when they are active. Models are\\ntrained using both labeled and unlabeled data since it is a sort of semi-supervised learning. Semi-supervised learning is\\nbased on the hypothesis that labeling a limited sample of data may provide results that are as accurate as or perhaps\\nmore accurate than completely labeled training data. The only difficult part is figuring out what that sample is. During\\nthe training phase, active learning machine learning labels the data progressively and dynamically so that the algorithm\\nmay choose which label would be the most helpful for it to learn from.\\n6A PREPRINT - AUGUST 31, 2024\\n2.4.1 General Algorithm.\\n1. use the original training dataset to train the classifier\\n2. determines the precision\\n3. while (accuracy wanted accuracy):'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='6A PREPRINT - AUGUST 31, 2024\\n2.4.1 General Algorithm.\\n1. use the original training dataset to train the classifier\\n2. determines the precision\\n3. while (accuracy wanted accuracy):\\n4. pick the most important data items (in general points close to the decision boundary)\\n5. ask the human oracle for a label for the relevant data point(s).\\n6. includes the aforementioned data point(s) in our first training dataset.\\n7. Train the model again\\n8. Update the accuracy calculation\\nThe technique of prioritizing the data that has to be labeled in order to have the most influence on training a supervised\\nmodel is known as active learning. When there is too much data to label and smart labeling has to be prioritized because\\nthere is too much data to label, active learning can indeed be employed. In the case of medical images, there is less\\namount of labeled data that is generated regularly.\\n3 Methodology\\n3.1 Case Description with Dataset information'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='amount of labeled data that is generated regularly.\\n3 Methodology\\n3.1 Case Description with Dataset information\\nThis section will discuss the details of the case which is based on MRI detection. When one brain tumor is diagnosed\\nclinically, the radiologist evaluates the tumorâ€™s size, location, and effect on the surrounding area. It is obvious that\\naccurate tumor-range diagnosis at the earliest stage will significantly increase the odds of survival for cancer patients.\\nBrain tumors, traumatic brain injury, developmental abnormalities, multiple sclerosis, dementia, infection, stroke, and\\nheadache reasons can all be found with MRI technology [32]. In order to use deep learning algorithms, a lot of photos\\nmust have excellent annotations. However, categorizing a lot of medical photos is difficult since it takes a lot of time\\nand knowledge to annotate them. The lack of imaging studies and a lack of expert human annotations for images are'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='and knowledge to annotate them. The lack of imaging studies and a lack of expert human annotations for images are\\nthe two key barriers to the usefulness of deep learning in diagnostic imaging. The research on active learning, on the\\nother hand, suggests that it may be a viable strategy for developing a competitive classifier at a low annotation cost.\\nWith a reduced percentage of the training cohort, our proposed uncertainty sampling technique in this retrospective\\nstudy utilizes transfer as well as active learning to provide consistent test results.\\nHowever, the size of the dataset with annotations has a significant impact on the performance of deep learning\\napproaches. Given the complexity and abundance of medical data, it is very difficult to label a significant number of\\nmedical pictures. In this study, the authors present an innovative transfer learning-based active learning framework to'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='medical pictures. In this study, the authors present an innovative transfer learning-based active learning framework to\\nlower the annotation cost while retaining the robustness and stability of the model performance for classifying brain\\ntumors. In this retrospective study, the authors utilized the magnetic resonance imaging (MRI) training dataset of 203\\npatients and the validation dataset of 66 patients as the baseline to train and fine-tune our model. The BRATS 2019\\ndataset [33, 34] which contains 335 individuals with brain tumor diagnoses, served as the basis for this study (259\\npatients with HGG and 76 patients with LGG). The median age of the 240 patients with age data available is 60.31\\nyears. The dataset is shown in Figure 6. Four MRI sequencesâ€”T1-weighted, post-contrast-enhanced T1-weighted\\n(T1C), T2-weighted (T2), and T2 fluid-attenuated inversion recovery (FLAIR) volumesâ€”are included in each patientâ€™s'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='(T1C), T2-weighted (T2), and T2 fluid-attenuated inversion recovery (FLAIR) volumesâ€”are included in each patientâ€™s\\nMRI scan collection. The dataset underwent skull-striping as preprocessing, was interpolated to a uniform isotropic\\nresolution of 1 mm3, and registered to SRI24 space with a dimension of 240 240 155. Four labels are included in the\\ndatasetâ€™s annotations: background, gadolinium-enhancing tumor, peri-tumoral edema, and the center of the necrotic and\\nnon-enhancing tumor. The whole tumor region is represented by the area denoted by the final three of the four labels.\\nIn order to use the suggested methodology in this study [35, 36], we randomly selected 20 slices from each patientâ€™s\\naxial plane MRI scan that included the tumor site while maintaining the T1, T1C, and T2 channels for each slice. We\\nselected the T1, T1C, and T2 channels from a total of four channels based on the outcomes of the initial trials. The'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='selected the T1, T1C, and T2 channels from a total of four channels based on the outcomes of the initial trials. The\\npre-trained AlexNet requires three-channel input. The 6,700 2D 3-channel slice dataset was further divided into a test\\nset (203 patients), a validation set (66 patients), and a training set (66 patients). The ratio of HGG patients to LGG\\npatients in each of the three cohorts is the same as it is for the entire dataset. Slices with LGG tumors were all labeled\\nas 0, whereas slices with HGG tumors were all labeled as 1. To suit the pre-trained CNN, the pictures were scaled down\\nfrom 240*240 pixels to 224*224 pixels.\\n7A PREPRINT - AUGUST 31, 2024\\nFigure 6: BRATS Dataset [34]\\n.\\n3.2 MRI Pre-processing\\nPre-processing MR images is a critical step in any workflow for quantitative analysis. These preprocessing steps might\\nbe made up of many procedures, each of which aims to either enhance the quality of the image or standardize its'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='be made up of many procedures, each of which aims to either enhance the quality of the image or standardize its\\ngeometrical and intensity patterns [37]. Preprocessing steps that were used in an experiment will be mentioned in this\\nparagraph.\\nThe data were registered to SRI24 space with a dimension of 240 240 155, interpolated to a uniform isotropic resolution\\nof 1 mm3, and preprocessed using skull-striping. The dataset has four labels as part of its annotations: background,\\ngadolinium-enhancing tumor, peri-tumoral edema, and necrotic and non-enhancing tumor core. The entire tumor region\\nis indicated by the final three labels on the four labels.\\n3.3 Framework for Active Learning Based on Transfer Learning knowledge\\nA CNN must be trained from scratch (with random initialization), which takes a lot more time and computing power\\nthan using a CNN that has already been pre-trained on a sizable dataset [38, 39, 40]. Finetuning and freezing are the two'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='than using a CNN that has already been pre-trained on a sizable dataset [38, 39, 40]. Finetuning and freezing are the two\\nprimary transfer learning possibilities. In fine-tuning, the weights and biases of a pre-trained CNN are adopted in place\\nof random initialization, and a traditional training procedure is then carried out on the target dataset. We think of the\\npre-trained model CNN layers as a static feature extractor in the freezing case. In this case, we allow the fully connected\\nlayers to be modified across the target dataset while authors freeze the weights and biases of the convolutional layers\\nwe want to use. The convolutional layers do not have to be the only frozen layers. Any subset of convolutional or fully\\nconnected layers can be chosen as the frozen layers; however, it is customary to only freeze the shallower convolutional\\nlevels. In this study, the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) dataset [41], which contains'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='levels. In this study, the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) dataset [41], which contains\\nreal-world pictures, is used to pre-train the CNNs.The authors choose fine-tuning as our transfer learning approach\\nsince the ImageNet dataset and our target medical picture domain are very different from one another.\\nThe authors chose the pre-trained AlexNet and modified it on the BRATS 19 dataset to lower the annotation cost. Three\\nmax-pooling layers, three fully connected layers, and five convolutional layers make up AlexNet. The depth of AlexNet\\nis suitable for classifying brain tumors and is significantly shallower than other benchmark CNNs (such as ResNet and\\nVGG), which promotes faster convergence and requires fewer processing resources.\\nIn this study, the authors provide a novel transfer learning-based active learning framework to lower the annotation cost'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='In this study, the authors provide a novel transfer learning-based active learning framework to lower the annotation cost\\nwhile retaining the stability and robustness of CNN efficacy for classifying brain tumors. The procedure for our active\\nlearning is shown in Figure 7. Authors assume that there are labeled and unlabeled subgroups in the training dataset.\\nFinding the most insightful examples throughout the whole training set is the objective; these samples may or may not\\ncoincide with the labeled training subset. There are four steps in the workflow: 1) Authors chose 30% of the training\\ndata at random for the labeled training set, assuming the other 70% of the samples were unlabeled. The pre-trained\\nAlexNet was then fine-tuned using the 30% labeled training subset, and the learning rate was adjusted to different levels\\n(i.e., 0.001, 0.0005, and 0.0001). This phase allowed us to get three improved CNNs. 2) To determine the classification'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='(i.e., 0.001, 0.0005, and 0.0001). This phase allowed us to get three improved CNNs. 2) To determine the classification\\nprobabilities of each sample in the complete training dataset, we employed these fine-tuned CNNs.There is no need for\\nlabels in this phase because the CNNs solely use forward propagation to calculate outputs. 3) After each training sample\\ngenerated three predicted possibilities in step 2, the calculation is done by pairwise KL divergence and individual\\nentropy. The entropy and KL divergence of each sample is added to create the uncertainty score. An uncertainty score\\nlist for the full training dataset was achieved using this method. 4) Authors picked 30% of the training cohort, which\\n8A PREPRINT - AUGUST 31, 2024\\nFigure 7: Workflow of the suggested active learning framework based on transfer learning [35]\\n.\\nFigure 8: Performance of CNN on samples from various uncertainty intervals [35]\\n.'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='Figure 7: Workflow of the suggested active learning framework based on transfer learning [35]\\n.\\nFigure 8: Performance of CNN on samples from various uncertainty intervals [35]\\n.\\nwas made up of the most informative data, and sorted the uncertainty score list in descending order. This chosen subset\\nneeded to be labeled, therefore it was utilized to improve a pre-trained AlexNet.The maximum training size required is\\n30 + 30= 60% (40% reduction in training size) of the whole training cohort if there was no overlap between the initially\\nlabeled training subset (30%) and the most informative subset identified (30%). The maximum training size required is\\njust 30% (70% reduction in training size) of the whole training cohort if the most informative samples identified are\\nprecisely identical to the original labeled training subset (30%). In other words, this suggested transfer learning-based\\nactive learning architecture can save between 40 and 70% of annotation costs (on average, 55%).'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='active learning architecture can save between 40 and 70% of annotation costs (on average, 55%).\\n3.4 Case Report and Analysis\\nOn a different test dataset of 66 patients, using this suggested strategy, the model produced an area under the receiver\\noperating characteristic (ROC) curve (AUC) of 82.89%, which was 2.92% higher than the baseline AUC while saving at\\nleast 40% of the labeling cost. The authors built a balanced dataset and ran the same process on it to further investigate\\nthe robustness of our strategy. The modelâ€™s AUC of 82%, compared to the baselineâ€™s AUC of 78.48%, confirms the\\nstability and robustness of this suggested transfer learning enhanced with an active learning framework while drastically\\ndecreasing the amount of training data.\\nThe authors of Results of Using Transfer Learning show that transfer learning is a successful strategy and enhances\\nour base models. The top 10% of certain and unsure cases are shown in AUC Results of selecting a different range'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='our base models. The top 10% of certain and unsure cases are shown in AUC Results of selecting a different range\\nof uncertainty distribution to be uninformative, and hence leaving them out allows the models to generalize more\\neffectively. We will conduct experiments to demonstrate how our uncertainty sampling strategy enhances the baseline\\n9A PREPRINT - AUGUST 31, 2024\\nwith a sample size fixed at 30% in AUC Results of the Uncertainty Sampling Method. Finally, we will illustrate the\\nfollowing in the AUC results of the Uncertainty Sampling Method on a balanced dataset: 1) Our sampling technique\\nworks whether the dataset is balanced or unbalanced. 2) Our sampling strategyâ€™s improvement of the baseline is neither\\naccidental nor the product of chance.\\nThe amount of labeled training data needed may be drastically decreased while retaining a high level of tumor\\nclassification accuracy using a transfer learning-based active learning architecture. As shown in Figure 8 the top 30%'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='classification accuracy using a transfer learning-based active learning architecture. As shown in Figure 8 the top 30%\\nof definite instances or the top 30% of uncertain examples are chosen to lower the AUC findings for the validation\\n(and test) group. We removed the top 10% (most uncertainty scores) and bottom 10% (lowest uncertainty total score)\\nsamples to find outliers only with the lowest training values. According to Fig. reftrain.JPG, the uncertainty range\\n/ -40% boosts AUC values by 12.51% when compared to the band of 0-30%. Similar to the interval of 70-100 percent,\\nthe ambiguity range of 60-90 percent increases AUC by 7.72 percent.\\n4 Conclusion\\nThe goal of medical imaging is to identify diseases and gain insights into the functioning of organs and anatomy.\\nVarious medical procedures, such as identifying lung tumors, spinal deformities, and artery stenosis detection, can be'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='Various medical procedures, such as identifying lung tumors, spinal deformities, and artery stenosis detection, can be\\ncarried out through medical imaging. Image processing techniques such as enhancement, segmentation, and denoising,\\nas well as machine learning methods, can improve the performance of medical imaging. Machine learning algorithms\\ncan classify or measure image properties optimally if given enough labeled data, which is a challenge for medical\\nimages due to a lack of labeled data. To address this challenge, the chapter discusses the active learning concept, which\\ninvolves using machine learning to train on relevant data and improve accuracy for quick medical diagnosis, specifically\\nfor medical MR images.\\nReferences\\n[1] Eyal Bercovich and Marcia C Javitt. Medical imaging: from roentgen to the digital revolution, and beyond.\\nRambam Maimonides medical journal, 9(4), 2018.'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='for medical MR images.\\nReferences\\n[1] Eyal Bercovich and Marcia C Javitt. Medical imaging: from roentgen to the digital revolution, and beyond.\\nRambam Maimonides medical journal, 9(4), 2018.\\n[2] Andrea Gallamini, Colette Zwarthoed, and Anna Borra. Positron emission tomography (pet) in oncology.Cancers,\\n6(4):1821â€“1889, 2014.\\n[3] Bradley J Erickson, Panagiotis Korfiatis, Zeynettin Akkus, and Timothy L Kline. Machine learning for medical\\nimaging. Radiographics, 37(2):505â€“515, 2017.\\n[4] Sajedul Talukder, Sai Puppala, and Ismail Hossain. Federated learning-based contraband detection within airport\\nbaggage x-rays. Journal of Computing Sciences in Colleges, 38(3):218â€“218, 2022.\\n[5] Sai Puppala, Ismail Hossain, and Sajedul Talukder. Towards federated learning based contraband detection within\\nairport baggage x-rays. In 2022 IEEE International Conference on Machine Learning and Applied Network\\nTechnologies (ICMLANT), pages 1â€“6. IEEE, 2022.'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='airport baggage x-rays. In 2022 IEEE International Conference on Machine Learning and Applied Network\\nTechnologies (ICMLANT), pages 1â€“6. IEEE, 2022.\\n[6] Ismail Hossain, Sai Puppala, and Sajedul Talukder. Collaborative differentially private federated learning\\nframework for the prediction of diabetic retinopathy. In 2023 IEEE 2nd International Conference on AI in\\nCybersecurity (ICAIC), pages 1â€“6. IEEE, 2023.\\n[7] Sajedul Talukder, Sai Puppala, and Ismail Hossain. Prediction of childhood and pregnancy lead poisoning using\\ndeep learning. Journal of Computing Sciences in Colleges, 38(3):219â€“219, 2022.\\n[8] Sai Puppala, Ismail Hossain, and Sajedul Talukder. Machine learning and sentiment analysis for predicting\\nenvironmental lead toxicity in children at the zip code level. In 2023 IEEE 2nd International Conference on AI in\\nCybersecurity (ICAIC), pages 1â€“6. IEEE, 2023.\\n[9] Angona Biswas and Md Saiful Islam. Mri brain tumor classification technique using fuzzy c-means clustering and'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='Cybersecurity (ICAIC), pages 1â€“6. IEEE, 2023.\\n[9] Angona Biswas and Md Saiful Islam. Mri brain tumor classification technique using fuzzy c-means clustering and\\nartificial neural network. In International Conference on Artificial Intelligence for Smart Community: AISC 2020,\\n17â€“18 December, Universiti Teknologi Petronas, Malaysia, pages 1005â€“1012. Springer, 2022.\\n[10] Suren Makaju, PWC Prasad, Abeer Alsadoon, AK Singh, and A Elchouemi. Lung cancer detection using ct scan\\nimages. Procedia Computer Science, 125:107â€“114, 2018.\\n[11] Martin J Willemink, Wojciech A Koszek, Cailin Hardell, Jie Wu, Dominik Fleischmann, Hugh Harvey, Les R\\nFolio, Ronald M Summers, Daniel L Rubin, and Matthew P Lungren. Preparing medical imaging data for machine\\nlearning. Radiology, 295(1):4â€“15, 2020.\\n10A PREPRINT - AUGUST 31, 2024\\n[12] Jahanzaib Latif, Chuangbai Xiao, Azhar Imran, and Shanshan Tu. Medical imaging using machine learning'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='learning. Radiology, 295(1):4â€“15, 2020.\\n10A PREPRINT - AUGUST 31, 2024\\n[12] Jahanzaib Latif, Chuangbai Xiao, Azhar Imran, and Shanshan Tu. Medical imaging using machine learning\\nand deep learning algorithms: a review. In 2019 2nd International conference on computing, mathematics and\\nengineering technologies (iCoMET), pages 1â€“5. IEEE, 2019.\\n[13] Isabella Castiglioni, Leonardo Rundo, Marina Codari, Giovanni Di Leo, Christian Salvatore, Matteo Interlenghi,\\nFrancesca Gallivanone, Andrea Cozzi, Natascha Claudia Dâ€™Amico, and Francesco Sardanelli. Ai applications to\\nmedical images: From machine learning to deep learning. Physica Medica, 83:9â€“24, 2021.\\n[14] KC Santosh. Ai-driven tools for coronavirus outbreak: need of active learning and cross-population train/test\\nmodels on multitudinal/multimodal data. Journal of medical systems, 44:1â€“5, 2020.\\n[15] Tonmoy Hossain, Fairuz Shadmani Shishir, Mohsena Ashraf, MD Abdullah Al Nasim, and Faisal Muhammad'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='models on multitudinal/multimodal data. Journal of medical systems, 44:1â€“5, 2020.\\n[15] Tonmoy Hossain, Fairuz Shadmani Shishir, Mohsena Ashraf, MD Abdullah Al Nasim, and Faisal Muhammad\\nShah. Brain tumor detection using convolutional neural network. In2019 1st international conference on advances\\nin science, engineering and robotics technology (ICASERT), pages 1â€“6. IEEE, 2019.\\n[16] Faisal Muhammad Shah, Tonmoy Hossain, Mohsena Ashraf, Fairuz Shadmani Shishir, MD Abdullah Al Nasim,\\nand Md Hasanul Kabir. Brain tumor segmentation techniques on medical images-a review. International Journal\\nof Scientific & Engineering Research, 10(2):1514â€“1525, 2019.\\n[17] Tonmoy Hossain, Fairuz Shadmani Shishir, Mohsena Ashraf, MD Abdullah Al Nasim4&, and Faisal Muhammad\\nShah. Brain tumor detection using convolutional neural network.\\n[18] R Pugalenthi, MP Rajakumar, J Ramya, and V Rajinikanth. Evaluation and classification of the brain tumor mri'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='Shah. Brain tumor detection using convolutional neural network.\\n[18] R Pugalenthi, MP Rajakumar, J Ramya, and V Rajinikanth. Evaluation and classification of the brain tumor mri\\nusing machine learning technique. Journal of Control Engineering and Applied Informatics, 21(4):12â€“21, 2019.\\n[19] Nazmul Karim, Niluthpol Chowdhury Mithun, Abhinav Rajvanshi, Han-pang Chiu, Supun Samarasekera, and\\nNazanin Rahnavard. C-sfda: A curriculum learning aided self-training framework for efficient source free domain\\nadaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages\\n24120â€“24131, 2023.\\n[20] A Sivasangari, P Ajitha, JS Vimali, Jithina Jose, and S Gowri. Breast cancer detection using machine learning. In\\nMobile Computing and Sustainable Informatics: Proceedings of ICMCSI 2021, pages 693â€“702. Springer, 2022.\\n[21] Nishita Sinha, Puneet Sharma, and Deepak Arora. Prediction model for breast cancer detection using machine'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='[21] Nishita Sinha, Puneet Sharma, and Deepak Arora. Prediction model for breast cancer detection using machine\\nlearning algorithms. In Computational Methods and Data Engineering: Proceedings of ICMDE 2020, Volume 1,\\npages 431â€“440. Springer, 2021.\\n[22] Yuan Xu, Yuxin Wang, Jie Yuan, Qian Cheng, Xueding Wang, and Paul L Carson. Medical breast ultrasound\\nimage segmentation by machine learning. Ultrasonics, 91:1â€“9, 2019.\\n[23] Bernhard Kainz, Mattias P Heinrich, Antonios Makropoulos, Jonas Oppenheimer, Ramin Mandegaran, Shrinivasan\\nSankar, Christopher Deane, Sven Mischkewitz, Fouad Al-Noor, Andrew C Rawdin, et al. Non-invasive diagnosis\\nof deep vein thrombosis from ultrasound imaging with machine learning. NPJ Digital Medicine, 4(1):137, 2021.\\n[24] Ali M Hasan, Mohammed M Al-Jawad, Hamid A Jalab, Hadil Shaiba, Rabha W Ibrahim, and Alaâ€™a R AL-\\nShamasneh. Classification of covid-19 coronavirus, pneumonia and healthy lungs in ct scans using q-deformed'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='Shamasneh. Classification of covid-19 coronavirus, pneumonia and healthy lungs in ct scans using q-deformed\\nentropy and deep learning features. Entropy, 22(5):517, 2020.\\n[25] Vruddhi Shah, Rinkal Keniya, Akanksha Shridharani, Manav Punjabi, Jainam Shah, and Ninad Mehendale.\\nDiagnosis of covid-19 using ct scan images and deep learning techniques. Emergency radiology, 28:497â€“505,\\n2021.\\n[26] MD Nasim, Aditi Dhali, Faria Afrin, Noshin Tasnim Zaman, and Nazmul Karim. The prominence of artificial\\nintelligence in covid-19. arXiv preprint arXiv:2111.09537, 2021.\\n[27] Ankita Shelke, Madhura Inamdar, Vruddhi Shah, Amanshu Tiwari, Aafiya Hussain, Talha Chafekar, and Ninad\\nMehendale. Chest x-ray classification using deep learning for automated covid-19 screening.SN computer science,\\n2(4):300, 2021.\\n[28] Tripti Goel, R Murugan, Seyedali Mirjalili, and Deba Kumar Chakrabartty. Optconet: an optimized convolutional'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='2(4):300, 2021.\\n[28] Tripti Goel, R Murugan, Seyedali Mirjalili, and Deba Kumar Chakrabartty. Optconet: an optimized convolutional\\nneural network for an automatic diagnosis of covid-19. Applied Intelligence, 51:1351â€“1366, 2021.\\n[29] Rachna Jain, Meenu Gupta, Soham Taneja, and D Jude Hemanth. Deep learning based detection and analysis of\\ncovid-19 on chest x-ray images. Applied Intelligence, 51:1690â€“1700, 2021.\\n[30] Manal Tantawi, Rezq Thabet, Ahmad M Sayed, Omer El-emam, and Gaber Abd El bake. Bone x-rays classification\\nand abnormality detection. In Internet of Thingsâ€”Applications and Future: Proceedings of ITAF 2019 , pages\\n277â€“286. Springer, 2020.\\n[31] Justin Ker, Lipo Wang, Jai Rao, and Tchoyoson Lim. Deep learning applications in medical image analysis. Ieee\\nAccess, 6:9375â€“9389, 2017.\\n11A PREPRINT - AUGUST 31, 2024\\n[32] Md Abdullah Al Nasim, Abdullah Al Munem, Maksuda Islam, Md Aminul Haque Palash, Md Mahim Anjum'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='Access, 6:9375â€“9389, 2017.\\n11A PREPRINT - AUGUST 31, 2024\\n[32] Md Abdullah Al Nasim, Abdullah Al Munem, Maksuda Islam, Md Aminul Haque Palash, Md Mahim Anjum\\nHaque, and Faisal Muhammad Shah. Brain tumor segmentation using enhanced u-net model with empirical\\nanalysis. In 2022 25th International Conference on Computer and Information Technology (ICCIT) , pages\\n1027â€“1032. IEEE, 2022.\\n[33] Spyridon Bakas, Hamed Akbari, Aristeidis Sotiras, Michel Bilello, Martin Rozycki, Justin S Kirby, John B\\nFreymann, Keyvan Farahani, and Christos Davatzikos. Advancing the cancer genome atlas glioma mri collections\\nwith expert segmentation labels and radiomic features. Scientific data, 4(1):1â€“13, 2017.\\n[34] Bjoern H Menze, Andras Jakab, Stefan Bauer, Jayashree Kalpathy-Cramer, Keyvan Farahani, Justin Kirby, Yuliya\\nBurren, Nicole Porz, Johannes Slotboom, Roland Wiest, et al. The multimodal brain tumor image segmentation\\nbenchmark (brats). IEEE transactions on medical imaging, 34(10):1993â€“2024, 2014.'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='Burren, Nicole Porz, Johannes Slotboom, Roland Wiest, et al. The multimodal brain tumor image segmentation\\nbenchmark (brats). IEEE transactions on medical imaging, 34(10):1993â€“2024, 2014.\\n[35] Ruqian Hao, Khashayar Namdar, Lin Liu, and Farzad Khalvati. A transfer learningâ€“based active learning\\nframework for brain tumor classification. Frontiers in Artificial Intelligence, 4:635766, 2021.\\n[36] Hossain Tonmoy, Shishir Fairuz Shadmani, Ashraf Mohsena, MD Al Nasim Abdullah, and Muhammad Shah\\nFaisal. Brain tumor detection using convolutional neural network. In 2019 1st International Conference on\\nAdvances in Science, Engineering and Robotics Technology (ICASERT), IEEE, pages 1â€“6, 2019.\\n[37] JosÃ© V ManjÃ³n. Mri preprocessing. Imaging Biomarkers: Development and Clinical Integration, pages 53â€“63,\\n2017.\\n[38] Jiuxiang Gu, Zhenhua Wang, Jason Kuen, Lianyang Ma, Amir Shahroudy, Bing Shuai, Ting Liu, Xingxing Wang,'),\n",
       " Document(metadata={'arxiv_id': '2306.01827v2', 'title': 'Active Learning on Medical Image', 'section': 'body', 'authors': 'Angona Biswas, MD Abdullah Al Nasim, Md Shahin Ali'}, page_content='2017.\\n[38] Jiuxiang Gu, Zhenhua Wang, Jason Kuen, Lianyang Ma, Amir Shahroudy, Bing Shuai, Ting Liu, Xingxing Wang,\\nGang Wang, Jianfei Cai, et al. Recent advances in convolutional neural networks.Pattern recognition, 77:354â€“377,\\n2018.\\n[39] Zewen Li, Fan Liu, Wenjie Yang, Shouheng Peng, and Jun Zhou. A survey of convolutional neural networks:\\nanalysis, applications, and prospects. IEEE transactions on neural networks and learning systems, 2021.\\n[40] Rikiya Yamashita, Mizuho Nishio, Richard Kinh Gian Do, and Kaori Togashi. Convolutional neural networks: an\\noverview and application in radiology. Insights into imaging, 9:611â€“629, 2018.\\n[41] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej\\nKarpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge.International\\njournal of computer vision, 115:211â€“252, 2015.\\n12'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'title_abstract', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='Title: DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model\\n\\nAbstract: Large-scale, big-variant, high-quality data are crucial for developing robust and successful deep-learning models for medical applications since they potentially enable better generalization performance and avoid overfitting. However, the scarcity of high-quality labeled data always presents significant challenges. This paper proposes a novel approach to address this challenge by developing controllable diffusion models for medical image synthesis, called DiffBoost. We leverage recent diffusion probabilistic models to generate realistic and diverse synthetic medical image data that preserve the essential characteristics of the original medical images by incorporating edge information of objects to guide the synthesis process. In our approach, we ensure that the synthesized samples adhere to medically relevant constraints and preserve the underlying structure of imaging data. Due to the random sampling process by the diffusion model, we can generate an arbitrary number of synthetic images with diverse appearances. To validate the effectiveness of our proposed method, we conduct an extensive set of medical image segmentation experiments on multiple datasets, including Ultrasound breast (+13.87%), CT spleen (+0.38%), and MRI prostate (+7.78%), achieving significant improvements over the baseline segmentation methods. The promising results demonstrate the effectiveness of our \\\\textcolor{black}{DiffBoost} for medical image segmentation tasks and show the feasibility of introducing a first-ever text-guided diffusion model for general medical image segmentation tasks. With carefully designed ablation experiments, we investigate the influence of various data augmentations, hyper-parameter settings, patch size for generating random merging mask settings, and combined influence with different network architectures. Source code are available at https://github.com/NUBagciLab/DiffBoost.'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2024 1\\nDiffBoost: Enhancing Medical Image\\nSegmentation via Text-Guided Diffusion Model\\nZheyuan Zhang, Lanhong Y ao, Bin Wang, Debesh Jha, Gorkem Durak, Elif Keles, Alpay Medetalibeyoglu,\\nUlas Bagci\\nAbstractâ€” Large-scale, big-variant, high-quality data are\\ncrucial for developing robust and successful deep-learning\\nmodels for medical applications since they potentially en-\\nable better generalization performance and avoid overfit-\\nting. However, the scarcity of high-quality labeled data al-\\nways presents significant challenges. This paper proposes\\na novel approach to address this challenge by developing\\ncontrollable diffusion models for medical image synthesis,\\ncalled DiffBoost. We leverage recent diffusion probabilistic\\nmodels to generate realistic and diverse synthetic medi-\\ncal image data that preserve the essential characteristics\\nof the original medical images by incorporating edge in-'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='models to generate realistic and diverse synthetic medi-\\ncal image data that preserve the essential characteristics\\nof the original medical images by incorporating edge in-\\nformation of objects to guide the synthesis process. In\\nour approach, we ensure that the synthesized samples\\nadhere to medically relevant constraints and preserve the\\nunderlying structure of imaging data. Due to the random\\nsampling process by the diffusion model, we can generate\\nan arbitrary number of synthetic images with diverse ap-\\npearances. To validate the effectiveness of our proposed\\nmethod, we conduct an extensive set of medical image\\nsegmentation experiments on multiple datasets, including\\nUltrasound breast (+13.87%), CT spleen (+0.38%), and MRI\\nprostate (+7.78%), achieving significant improvements over\\nthe baseline segmentation methods. The promising results\\ndemonstrate the effectiveness of our DiffBoost for medical\\nimage segmentation tasks and show the feasibility of intro-'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='the baseline segmentation methods. The promising results\\ndemonstrate the effectiveness of our DiffBoost for medical\\nimage segmentation tasks and show the feasibility of intro-\\nducing a first-ever text-guided diffusion model for general\\nmedical image segmentation tasks. With carefully designed\\nablation experiments, we investigate the influence of var-\\nious data augmentations, hyper-parameter settings, patch\\nsize for generating random merging mask settings, and\\ncombined influence with different network architectures.\\nSource code with checkpoints are available at https://\\ngithub.com/NUBagciLab/DiffBoost.\\nIndex Termsâ€” Medical Image Segmentation, Image Syn-\\nthesis, Data Augmentation, Score-based Generative Mod-\\nels, Diffusion Models\\nI. I NTRODUCTION\\nThe recent surge of artificial intelligence (AI) / deep\\nlearning in medical image analysis has revolutionized the\\nfield. However, achieving optimal performance with these\\ntechniques often hinges on access to large-scale, high-quality'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='learning in medical image analysis has revolutionized the\\nfield. However, achieving optimal performance with these\\ntechniques often hinges on access to large-scale, high-quality\\nannotated datasets. Unfortunately, the medical domain faces\\na significant challenge â€“ data scarcity . This scarcity stems\\nThis work was supported by the NIH NCI R01-CA246704 and\\nNIH/NIDDK #U01 DK127384-02S1.\\nAll authors belong to Machine & Hybrid Intelligence Lab in Northwest-\\nern University, Chicago IL 60611, USA\\nCorresponding author: Ulas Bagci (ulas.bagci@northwestern.edu)\\nfrom several factors: (i) acquiring and annotating high-quality\\nmedical images is a time-consuming, labor-intensive, and\\nexpensive endeavor. (ii) Collaboration in data sharing and\\ncollection can be limited by privacy concerns and ethical\\nconsiderations. (iii) The challenge is amplified in the context\\nof rare diseases, where data availability is even more limited.\\nThe data scarcity is surely a bottleneck, impeding the'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='considerations. (iii) The challenge is amplified in the context\\nof rare diseases, where data availability is even more limited.\\nThe data scarcity is surely a bottleneck, impeding the\\nfull potential of deep learning in medical image analysis.\\nHence, the medical imaging community continuously explores\\ninnovative solutions to overcome this hurdle and unlock further\\nadvancements. Some potential solutions for this important\\nproblem are the following: data augmentation, transfer learn-\\ning and domain adaptation, federated learning, and lightweight\\ndeep learning architectures. In this study, our proposed strategy\\nfor getting larger and more diverse data is based on a new data\\naugmentation strategy with a generative AI model, specifically\\na stable-diffusion model, and we apply a downstream task\\nof segmentation to explore its efficacy on one of the most\\nimportant medical image analysis tasks - segmentation.\\nData augmentation is a widely adopted approach to alle-'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='of segmentation to explore its efficacy on one of the most\\nimportant medical image analysis tasks - segmentation.\\nData augmentation is a widely adopted approach to alle-\\nviate the issue of limited annotated data by expanding the\\ntraining dataset by generating new samples [1]. This fosters\\nimproved learning and generalization capabilities in deep\\nlearning models by enriching the dataset with diverse and\\nrepresentative examples and mitigating the risk of overfitting.\\nData augmentation techniques can be broadly categorized\\ninto two groups: transformation-based methods and generative\\nmethods. Both categories aim to expand the available training\\ndata by generating new samples that maintain the essential\\nvisual clues of the original data.\\nTransformation-based data augmentation methods involve\\nthe application of basic transformations to the original data\\nsamples [2]. These techniques are relatively simple and com-\\nputationally efficient. Some common traditional methods in-'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='the application of basic transformations to the original data\\nsamples [2]. These techniques are relatively simple and com-\\nputationally efficient. Some common traditional methods in-\\nclude spatial level: Rotation, Scaling, Translation, Flipping,\\nand Intensity level: Contrast Adjustment and gamma Correc-\\ntion. Traditional data augmentation can provide explainable\\nand reliable augmented data with low computation costs,\\nwhich has been widely accepted as an essential element in\\nmedical AI applications [3], [4]. While traditional methods\\nare easy to implement and computationally less demanding,\\nthey may not be sufficient to capture the complex variations\\nand high-dimensional relationships present in medical images,\\nparticularly in the context of diverse patient populations [5].\\nGenerative data augmentation methods, on the other hand,\\narXiv:2310.12868v2  [cs.CV]  14 Dec 20242 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2024'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='Generative data augmentation methods, on the other hand,\\narXiv:2310.12868v2  [cs.CV]  14 Dec 20242 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2024\\nemploy advanced models, such as adversarial image attacking\\nand other generative AI models to synthesize more complex\\nand realistic synthetic samples [6]. These methods offer the\\npotential to capture the intricate variations and relationships\\nwithin medical imaging data. (i) Adversarial attack-based data\\naugmentation involves generating perturbed versions of the\\noriginal images that can deceive the model into making incor-\\nrect predictions. These perturbations are carefully designed to\\nmaintain the visual appearance of the original images while\\ncausing the model to misclassify them [7]â€“[9]. (ii) Generative\\nAI models: Until recently, the most widely used generative\\nmodel for the medical imaging was the Generative Adversarial\\nNetworks (GANs). GANs consist of two neural networks, a'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='AI models: Until recently, the most widely used generative\\nmodel for the medical imaging was the Generative Adversarial\\nNetworks (GANs). GANs consist of two neural networks, a\\ngenerator, and a discriminator trained in tandem. The generator\\ncreates synthetic images, while the discriminator assesses the\\nrealism of these images by comparing them to the original\\ndata. Through this adversarial process, the generator learns to\\nproduce increasingly realistic samples, which can be utilized\\nfor data augmentation. It is also worth noting that most\\nadversarial attack models conduct data augmentation during\\nsegmentation training, while the generative models are trained\\nbefore training the segmentation.\\nMore recently, denoising diffusion probabilistic models\\n(DDPM) represent a novel topic in generative AI, showing\\nimpressive performance in high-quality image synthesis [10],\\n[11], surpassing GAN models. By simulating a stochastic\\nreverse diffusion process, the diffusion models gradually trans-'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='impressive performance in high-quality image synthesis [10],\\n[11], surpassing GAN models. By simulating a stochastic\\nreverse diffusion process, the diffusion models gradually trans-\\nform an initial noise sample into a realistic data sample\\nthrough a series of denoising steps. The forward process in\\ndiffusion models gradually adds noise to the data, step by step,\\nuntil the data (image) becomes pure noise. The main goal of\\nthe diffusion model is to learn the reverse process to denoise\\nthe corrupted samples and recover the original data [9], [10].\\nBy leveraging a denoising score-matching objective, the model\\nlearns a denoising function that estimates the gradient of the\\ndata distributionâ€™s log density. The sampling process starts\\nwith an initial noise sample and iteratively refines it by\\napplying the learned denoising function at each time step\\nt, following the inverse noise schedule. At each step, the\\nmodel estimates the gradient of the log-density and updates'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='applying the learned denoising function at each time step\\nt, following the inverse noise schedule. At each step, the\\nmodel estimates the gradient of the log-density and updates\\nthe current sample accordingly. After a predefined number of\\nreverse diffusion steps, an initial noise sample is iteratively\\nrefined according to the learned diffusion process, resulting\\nin high-quality synthetic samples that resemble the original\\ndata [10].\\nCompared with the GAN methods, diffusion models have\\nseveral advantages [12]. First, GANs are known for their\\ntraining instability, which arises from the adversarial min-max\\noptimization process between the generator and discriminator\\nnetworks. This can lead to issues such as mode collapse and\\nvanishing gradients. In contrast, diffusion models employ a\\ndenoising score-matching objective, a more stable and well-\\nbehaved optimization problem. This leads to more stable\\ntraining and better convergence. Second, diffusion models have'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='denoising score-matching objective, a more stable and well-\\nbehaved optimization problem. This leads to more stable\\ntraining and better convergence. Second, diffusion models have\\ndemonstrated the ability to generate high-quality samples with\\nsharp and detailed features. This is critical for medical image\\nanalysis tasks, where the generated samples are in need to be\\nboth visually realistic and medically relevant. GANs, on the\\nother hand, can sometimes produce samples with noticeable\\nartifacts or unrealistic features. Third, in diffusion models, the\\nsampling process is performed through a series of reverse\\ndiffusion steps that gradually refine an initial noise sample.\\nThis process can be controlled by adjusting the number of\\nsteps, noise schedule, and other parameters, allowing for fine-\\ngrained control over the generated samples. This is essential\\nfor generating samples with varying degrees of complexity\\nand diversity. In contrast, GANs typically provide less explicit'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='grained control over the generated samples. This is essential\\nfor generating samples with varying degrees of complexity\\nand diversity. In contrast, GANs typically provide less explicit\\ncontrol over the sampling process. Finally, due to the denoising\\nobjective, diffusion models can be more robust to overfitting\\nthan GANs and avoid mode collapse. This is particularly\\nimportant when working with limited data, as is often the case\\nin medical image analysis tasks [13].\\nIn this study, we propose a text-guided diffusion model-\\nbased (DDPM) data augmentation approach, called DiffBoost,\\nto enhance the performance of downstream medical image\\nsegmentation tasks by generating reliable and medically rele-\\nvant synthetic images to be used in training of segmentation\\nalgorithm. The proposed approach consists of the following\\nsteps: i) Pretraining on large medical datasets: We begin\\nby pretraining a diffusion model on RadImageNet [14], a\\ncomprehensive medical image dataset. ii) Fine-tuning on'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='steps: i) Pretraining on large medical datasets: We begin\\nby pretraining a diffusion model on RadImageNet [14], a\\ncomprehensive medical image dataset. ii) Fine-tuning on\\ndownstream task: Following the pretraining, the diffusion\\nmodel is fine-tuned on a smaller dataset specific to the tar-\\nget downstream task (segmentation). This adaptation process\\nallows our model to account for unique characteristics and\\nvariations present in the task-specific data, resulting in more\\nrelevant synthetic samples for data augmentation. iii) Integra-\\ntion with Downstream Task Training: To enrich our training\\ndata, we leverage a fine-tuned diffusion model to generate new\\nsamples. These synthetic samples incorporate text and edge\\ninformation for guidance. During model training, a carefully\\ndesigned combination loss ensures both real and synthetic\\nsamples contribute equally, fostering better generalization for\\nthe target task.\\nWe validate the effectiveness of the proposed method Diff-'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='designed combination loss ensures both real and synthetic\\nsamples contribute equally, fostering better generalization for\\nthe target task.\\nWe validate the effectiveness of the proposed method Diff-\\nBoost on various datasets, including breast ultrasound, spleen\\nCT, and prostate MRI, with extensive experiments. We employ\\nablation experiments for a systematic exploration of the impact\\nof data augmentation ratios, hyperparameter tuning, patch size\\nselection within the random merging mask generation process,\\nand the interplay between different network architectures.\\nThrough this analysis, we gain valuable insights into the\\nindividual and combined contributions of these components\\nto model performance on segmentation tasks.\\nII. R ELATED WORK\\nStable Diffusion Model : Rombach et al. [11] applies the\\ndiffusion model in the latent space of pre-trained autoencoders,\\nenabling training on limited computational resources while re-\\ntaining their quality and flexibility. Cross-attention layers turn'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='diffusion model in the latent space of pre-trained autoencoders,\\nenabling training on limited computational resources while re-\\ntaining their quality and flexibility. Cross-attention layers turn\\ndiffusion models into powerful, flexible generators with vari-\\nous conditioning inputs, such as text or bounding boxes. The\\nCLIP [15] builds a strong connection between the image and\\ntext representation through large-scale text-image pair training.\\nThe combination of CLIP and Stable diffusion enables us toAUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING 3\\ndirectly generate high-quality and meaningful images from the\\ntext, as shown in [11]. Based on the pre-trained large-scale\\ntext-to-image stable diffusion model, many interesting appli-\\ncations have been proposed in computer vision, like Textual\\nInversion [16], Subject Driven Generation [17], Pix2Pix Text\\nInstruct Translation [18]. Building upon existing text-to-image'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='cations have been proposed in computer vision, like Textual\\nInversion [16], Subject Driven Generation [17], Pix2Pix Text\\nInstruct Translation [18]. Building upon existing text-to-image\\ndiffusion models, [19] incorporated additional information like\\nedge maps, segmentation information, and key points. This\\nallows for greater control over the generated image, ensuring it\\nreflects the provided structural details through the conditioning\\nprocess.\\nDiffusion Models in Medical Applications : Diffusion\\nmodels have emerged as a powerful generative approach for\\nmedical image synthesis and augmentation. This innovative\\ntechnique offers a promising avenue for enhancing the per-\\nformance of diverse medical image analysis tasks [20]. These\\nmodels have been applied to various data modalities already,\\nfrom CT to MRI and from 2D to 3D images [21]â€“[26].\\nDiffusion models are also used for image-to-image translation\\napplications such as generating CT scans from MRIs [27]â€“'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='from CT to MRI and from 2D to 3D images [21]â€“[26].\\nDiffusion models are also used for image-to-image translation\\napplications such as generating CT scans from MRIs [27]â€“\\n[29]. The diffusion models are used to extract meaningful deep\\nrepresentation features through the reconstruction process too\\n[30], [31]. The conclusions of such studies show that diffusion\\nalgorithms can be useful for medical image segmentation\\napplications, as evidenced in [32]â€“[34]. For a popular topic\\nin brain imaging, authors in [35] introduce Med-DDPM for\\n3D Brain MRI synthesis, further helping with improving\\nsegmentation accuracy from 65.31% to 66.75% in terms of\\nDice score.\\nIn a very recent work by [36], authors introduce high-\\nquality sampling to enhance the effectiveness of data aug-\\nmentation. Beyond conventional use, diffusion models are\\napplied to medical image reconstruction [37], denoising [38],\\nand anomaly detection [39], [40]. Last, but not least, authors'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='mentation. Beyond conventional use, diffusion models are\\napplied to medical image reconstruction [37], denoising [38],\\nand anomaly detection [39], [40]. Last, but not least, authors\\nin [41] introduce Diffusion-Based semantic polyp synthesis,\\nenhancing polyp segmentation models to be comparable with\\nreal endoscopic images. While existing work explores medical\\nimage augmentation, leveraging text-based guidance with a\\npixel-level aligned diffusion model remains under investigated.\\nOur work tackles this gap by exploring this approach from\\nvarious perspectives.\\nIII. M ETHODS\\nIn this work, we present a text guided diffusion based\\nmedical image data augmentation approach, DiffBoost, aimed\\nat generating reliable and medically relevant synthetic imaging\\ndata to improve the performance of medical image segmen-\\ntation. Three major steps of our algorithm are illustrated in\\nFigure 1. After introducing the background of DDPM, each\\nof these steps is described below in detail.'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='tation. Three major steps of our algorithm are illustrated in\\nFigure 1. After introducing the background of DDPM, each\\nof these steps is described below in detail.\\nA. Denoising Diffusion Probabilistic Models\\nDenoising Diffusion Probabilistic Models (DDPMs) define\\nthe forward noise process q with x0 âˆ¼ q(x0) representing the\\ntarget data distribution, which produces latent x1 through xT\\nby adding Gaussian noise at time t as follows:\\nq(x1, ..., xT |x0) :=\\nTY\\nt=1\\nq(xt|xtâˆ’1), (1)\\nq(xt|xtâˆ’1) := N (xt;\\np\\n1 âˆ’ Î²txtâˆ’1, Î²tI), (2)\\nwhere Î²t âˆˆ (0, 1) represents the variance schedule across\\ndiffusion steps and I is the identity matrix. With Î±t := 1 âˆ’ Î²t\\nand Â¯Î±t :=Qt\\ns=0 Î±s, we can readily sample an arbitrary step\\nof noised latent samples from the x0 according to Equation 2:\\nxt = âˆšÂ¯Î±tx0 +\\nâˆš\\n1 âˆ’ Â¯Î±tÏµ. (3)\\nWhen T is sufficiently large according to the schedule of Î²t, Â¯Î±\\nwill be nearly 0, and xt will approximately follow the isotropic\\nGaussian Distribution. Thus, our goal is to approximate the'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='When T is sufficiently large according to the schedule of Î²t, Â¯Î±\\nwill be nearly 0, and xt will approximately follow the isotropic\\nGaussian Distribution. Thus, our goal is to approximate the\\nreverse process q(xtâˆ’1|xt) such that we can use such process\\nto generate a sample q(x0) from xT âˆ¼ N (0, I). However,\\nsince q(xtâˆ’1|xt) is not directly tractable, we introduce a neural\\nnetwork ÂµÎ¸(xt, t) to approximate it as follows:\\np(x0, ..., xT âˆ’1|xT ) :=\\nTY\\nt=1\\np(xtâˆ’1|xt), (4)\\npÎ¸(xtâˆ’1|xt) := N (xtâˆ’1; ÂµÎ¸(xt, t), Î£Î¸(xt, t)). (5)\\nWhile this approach is promising, Ho et al. [10] shows that\\ndirect parameterization of ÂµÎ¸(xt, t) may lead to a worse per-\\nformance. Instead, predicting a noise Ïµ using a noise prediction\\nnetwork ÏµÎ¸(xt, t) and generating ÂµÎ¸(xt, t) may be a better\\nsolution:\\nÂµÎ¸(xt, t) = 1âˆšÎ±t\\n\\x12\\nxt âˆ’ Î²tâˆš1 âˆ’ Â¯Î±t\\nÏµÎ¸(xt, t)\\n\\x13\\n. (6)\\nTo train this noise prediction network, we optimize the\\nvariational lower bound (VLB) on the negative log-likelihood\\nas follows:'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='solution:\\nÂµÎ¸(xt, t) = 1âˆšÎ±t\\n\\x12\\nxt âˆ’ Î²tâˆš1 âˆ’ Â¯Î±t\\nÏµÎ¸(xt, t)\\n\\x13\\n. (6)\\nTo train this noise prediction network, we optimize the\\nvariational lower bound (VLB) on the negative log-likelihood\\nas follows:\\nLvlb := L0 + L1 + ... + LT âˆ’1 + LT , , (7)\\nL0 := âˆ’ log pÎ¸(x0|x1), (8)\\nLtâˆ’1 := DKL (q(xtâˆ’1|xt, x0), pÎ¸(xtâˆ’1|xt)) , (9)\\nLT := DKL (q(xT |x0), p(xT )) , (10)\\nwhere DKL represents the KL divergence between two Gaus-\\nsian distributions. By reparameterizing Equation 7, [10] pro-\\nposes a simplified objective as follows:\\nLsimple = Et,x0,Ïµ\\n\\x02\\n||Ïµ âˆ’ ÏµÎ¸(xt, t)||2\\x03\\n, (11)\\nwhere J. Song et al. [42] demonstrates that this loss objective\\nconnects with score-based generative networks. Following this\\ntraining setup, R. Rombach et al. [11] conducts the diffusion\\ntraining in latent feature space z where z = E(x), Ëœx = D(z)\\nand E, D represents the encoder and decoder in autoencoder\\nsetup, respectively. This powerful design enables us to sample\\nhigh-quality and high-resolution images within a reasonable'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='and E, D represents the encoder and decoder in autoencoder\\nsetup, respectively. This powerful design enables us to sample\\nhigh-quality and high-resolution images within a reasonable\\ntime without extensive computation in generating large-size\\nimages directly. The capabilities of diffusion models are en-\\nhanced as powerful and flexible generators f by incorporating\\ncross-attention layers from various conditioning inputs, such\\nas text or bounding boxes, into the model architecture for4 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2024\\nStep 1: Pre-Training\\nStep 2: Fine-tuning\\nStep 3: Downstream Task (Segmentation) \\nâ€œMRI,prostateâ€â€œUS,breastâ€ â€œCT,spleenâ€\\nInput\\nImages\\nEdge \\nImages\\nPrompts\\nlossExtract\\nFine-tune\\nAdd\\nMask\\nOutput \\nSegmentation\\nMask\\nEdge \\nImage\\nPromptâ€œUS,breast,enhance contrastâ€\\nâ€œMRI,prostate,darkened imageâ€\\nâ€œCT,spleen,median filterâ€\\nOriginal Data\\nSampling\\nTrain\\nAugmented Data\\nExtract\\nAdd \\nRandom \\nVariations\\nAdd \\nMask\\nSegmentor\\n(eg. UNet, â€¦)\\nPretrained ControlNet'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='â€œMRI,prostate,darkened imageâ€\\nâ€œCT,spleen,median filterâ€\\nOriginal Data\\nSampling\\nTrain\\nAugmented Data\\nExtract\\nAdd \\nRandom \\nVariations\\nAdd \\nMask\\nSegmentor\\n(eg. UNet, â€¦)\\nPretrained ControlNet\\nFine-tuned \\nPretrained \\nControlNet\\nEdge condition block\\nText condition block\\nEdge condition block\\nText condition block\\nFrozen Block\\nInput\\nImages\\nEdge \\nImages\\nPrompts\\nloss\\nTrain\\nSynthesized ImagesControlNet\\nEdge condition block\\nText condition block\\nâ€œCT,abdomen,\\nnormalâ€\\nâ€œMRI,brain,\\nnormalâ€\\nâ€œUS,\\nkidneyâ€\\nExtract\\nRandom Fusion\\nSynthesized Images\\nFig. 1. Our proposed approach involves three stages: (a) training a diffusion model on a comprehensive radiology imaging dataset (RadImageNet),\\n(b) fine-tuning the pre-trained model on a task-specific dataset, allowing for adaptation to the unique characteristics of each target task, and (c)\\nutilizing the fine-tuned model for downstream task training, integrating the synthetic samples (generated during data augmentation) to enhance'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='utilizing the fine-tuned model for downstream task training, integrating the synthetic samples (generated during data augmentation) to enhance\\ngeneralization and performance in the target task (segmentation).\\ncontroling the generation steps. The ÏµÎ¸(xt, c, t) is optimized\\naccording to the objective loss in Equation 11, where c\\nrepresents various conditioning inputs such as the text prompts\\nwith CLIP encoding [15].\\nIn this work, we design the noise prediction ÏµÎ¸(xt, ct, ce, t)\\nnetwork with two types of conditioning input: text ct and\\nedge information ce. Our detailed network architecture is\\nshown in Figure 1. Leveraging the benefits of large-scale pre-\\ntrained text-to-image stable diffusion models while mitigating\\nunnecessary computational resource burden, we implement\\na branch design comprising the original branch for stable\\ndiffusion with text conditioning input and an auxiliary branch\\nincorporating additional text conditioning input. To ensure'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='a branch design comprising the original branch for stable\\ndiffusion with text conditioning input and an auxiliary branch\\nincorporating additional text conditioning input. To ensure\\nthe generation of grayscale medical images as opposed to\\nRGB images, we incorporate a cross-channel average block\\nin the final output stage of the model (Figure 1a). With\\nthis novel approach, we enable grayscale image generation\\nwithout requiring a complete network re-architecture. This\\ndesign optimization streamlines the training process, leading\\nto efficient resource allocation and ultimately culminating in\\nimproved overall model performance.\\nB. Pre-training on RadImageNet\\nCurrently, most text-to-image diffusion models rely on vast\\ndatasets of natural images for training. This lack of focus on\\nlarge medical datasets presents a clear gap that needs to be\\naddressed. The significant disparity between diffusion models'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='datasets of natural images for training. This lack of focus on\\nlarge medical datasets presents a clear gap that needs to be\\naddressed. The significant disparity between diffusion models\\ntrained on natural images and those designed for medicalAUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING 5\\nimages presents a critical challenge. Our research addresses\\nthis gap by proposing the adaptation of well-trained natural\\nimage diffusion models for application in the medical field.\\nIn other words, we first train (fine-tune) the diffusion model\\non a large-scale medical image dataset, RadImageNet, using\\npre-trained checkpoints from stable diffusion algorithm rather\\nthan training from scratch [19]. This pre-trained model serves\\nas a foundation for subsequent adaptation to segmentation\\ntasks, effectively bridging the gap between the two domains.\\nRadImageNet is a comprehensive, large-scale medical imaging\\ndataset comprising a diverse array of images from various'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='tasks, effectively bridging the gap between the two domains.\\nRadImageNet is a comprehensive, large-scale medical imaging\\ndataset comprising a diverse array of images from various\\nmodalities, such as magnetic resonance imaging (MRI), com-\\nputed tomography (CT), and ultrasound (US) [14]. RadIma-\\ngeNet encompasses 11 anatomical regions, including CT scans\\nof the chest, abdomen, and pelvis, MRI scans of the ankle,\\nfoot, knee, hip, shoulder, brain, spine, abdomen, and pelvis\\nas US scans of the abdomen, pelvis, and thyroid gland. In\\ntotal, RadImageNet constitutes a collection of 1.35 million\\nradiologic images, offering a diverse and extensive resource for\\ntraining one robust medical image generation diffusion model.\\nIn generating free training prompts using RadImageNet, we\\nemployed a triplet: a combination of data modality , organ\\nname, and category name . For instance, a single CT scan\\nimage of the abdomen with arterial pathology would utilize'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='employed a triplet: a combination of data modality , organ\\nname, and category name . For instance, a single CT scan\\nimage of the abdomen with arterial pathology would utilize\\nprompts in the form of â€œCT, Abdomen, Arterial Pathology.â€\\nSimilarly, a single MRI image of the hip with Chondral\\npathology would use the prompt of â€œMR, Hip, Chondral\\npathologyâ€. This methodology ensures a systematic and con-\\nsistent generation of free prompts across various medical imag-\\ning contexts while maintaining accurate medical terminology\\nguiding precise medical structure generation. As discussed\\nearlier, our approach utilizes both text and edge conditioning\\ninputs. To generate the edge conditioning input from RadIm-\\nageNet, we employ the Holistically-Nested Edge Detection\\n(HED) algorithm proposed by [43]. HED is a sophisticated\\ndeep learning-based edge detection technique that efficiently\\nand accurately identifies image boundaries. By leveraging the'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='(HED) algorithm proposed by [43]. HED is a sophisticated\\ndeep learning-based edge detection technique that efficiently\\nand accurately identifies image boundaries. By leveraging the\\nextracted hidden features, we can generate precise edges not\\nonly for the skull but also for soft-tissue organs like the liver,\\nspleen, or pancreas. HED learns rich hierarchical representa-\\ntions under the guidance of deep supervision, rendering it a\\nhighly accurate tool for directing the generation of realistic\\nand meaningful medical images.\\nTraining details: We then adopt the stable diffusion model\\nimplementation with pre-training checkpoints in large-scale\\ntext-to-image datasets [11], [19]. With text and edge condi-\\ntioning inputs, the diffusion model is trained on RadImageNet\\nusing the AdamW optimizer. The training process is conducted\\nutilizing 6 NVIDIA RTX A6000 GPUs, each equipped with\\n48GB memory and a batch size of 384 in total (48 for each'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='using the AdamW optimizer. The training process is conducted\\nutilizing 6 NVIDIA RTX A6000 GPUs, each equipped with\\n48GB memory and a batch size of 384 in total (48 for each\\nunder the DDP setting). The entire training procedure takes\\napproximately seven days to complete, with a learning rate\\nset at 10âˆ’5. We release all checkpoints and provide sample\\nresults from our proposed model in Figure 2, showing that real\\nand generated images share similar anatomical information\\nregardless of potential intensity differences.\\nC. Finetuning on Downstream Task\\nWhile training a relatively large model offers advantages, it\\nmay not fully capture the intricacies of the data distribution\\nspecific to each medical application. Particularly, the pre-\\ntrained model might lack the capability to generate specific\\nsegmentation targets, which is desired in the segmentation\\ntask. To address this limitation and increase the modelâ€™s\\nadaptability to diverse medical tasks, we fine-tune the pre-'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='segmentation targets, which is desired in the segmentation\\ntask. To address this limitation and increase the modelâ€™s\\nadaptability to diverse medical tasks, we fine-tune the pre-\\ntrained large-scale text-to-image medical generation stable\\ndiffusion model. The fine-tuning process enables the model\\nto learn task-specific features and variations relevant to the\\nsegmentation task at hand. This tailored approach leads to the\\ngeneration of more pertinent synthetic samples for data aug-\\nmentation, consequently enhancing the overall performance.\\nThis process not only enhances the applicability of the model\\nacross a wide range of medical tasks but also ensures a higher\\ndegree of consistency and accuracy in the generated samples.\\nAs a result, the fine-tuned diffusion model is better equipped\\nto contribute to more reliable generated augmentation data and\\nimproved performance and generalization in various medical\\nimage analysis tasks.\\nTraining details: During the fine-tuning stages, we (con-'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='improved performance and generalization in various medical\\nimage analysis tasks.\\nTraining details: During the fine-tuning stages, we (con-\\ntinue to) incorporate both text and edge conditioning inputs to\\nensure accurate medical image generation. In contrast to the\\npre-training on RadImageNet, we specifically incorporate the\\nedge derived from the segmentation mask into the generation\\ncondition here. This approach is taken to ensure the accu-\\nrate representation of anatomical structures for segmentation\\ntargets. Leveraging the robust initial starting point obtained\\nfrom the pre-trained model, we utilize a single NVIDIA RTX\\nA6000 GPU for each subtask training, maintaining a batch size\\nof 48 while employing the AdamW optimizer. The learning\\nrate is set at 10âˆ’6, and the fine-tuning process is performed\\nover 100 epochs on the training set. To prevent potential data\\nleakage, we exclusively use image-text pairs from the training\\nset for fine-tuning the generation diffusion model. By adopting'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='over 100 epochs on the training set. To prevent potential data\\nleakage, we exclusively use image-text pairs from the training\\nset for fine-tuning the generation diffusion model. By adopting\\nthis cautious approach, we minimize the risk of inadvertently\\nincorporating information from validation or test sets, thereby\\nensuring a more reliable evaluation of the modelâ€™s performance\\nand generalization capabilities.\\nD. Training of Downstream Task - Segmentation\\nIn this study, we primarily concentrated on segmentation\\ntasks as a downstream application for assessing the efficacy\\nof our data augmentation approach. The previous fine-tuned\\ndiffusion models generate new synthetic samples for data\\naugmentation by adding augmentation text caug, like â€œen-\\nhanced contrastâ€, and â€œhigh resolution,â€ with the original text\\nct as conditioning input. By introducing these augmentation\\ntexts, we can generate more diverse data for promoting better\\ngeneralization and performance. At the same time, these'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='ct as conditioning input. By introducing these augmentation\\ntexts, we can generate more diverse data for promoting better\\ngeneralization and performance. At the same time, these\\nsamples are created with text and edge information guidance\\nto ensure that they represent the target distribution and exhibit\\nmeaningful attributes relevant to the downstream task. The\\ngenerated synthetic samples were combined with the original\\ntraining data during downstream task (segmentation) training.\\nThis integration allows the model to learn from both real6 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2024\\nMR, hip, normal\\n CT, abdomen, normal\\n MR, ankle, osseous disruption\\n MR, brain, normal\\n US, thyroid\\n MR, spine, disc pathology\\nOriginalEdgePix2PixOurs\\nFig. 2. In the provided illustrations, the initial row presents example original images sourced from RadImageNet. Middle row indicate edge maps'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='MR, spine, disc pathology\\nOriginalEdgePix2PixOurs\\nFig. 2. In the provided illustrations, the initial row presents example original images sourced from RadImageNet. Middle row indicate edge maps\\nof the original images to be used in diffusion process to enhance the visual quality. The last two rows includes sample images generated by the\\nPix2Pix GAN model and fine-tuned ControlNet across diverse modalities. Both the original and synthesized images maintain congruent anatomical\\nstructures, albeit there may be disparities in intensity.\\nand augmented samples, effectively increasing the diversity\\nand volume of the training data. The choice of network\\narchitecture is not confined to the specific training procedure\\npresented in this study, as our method is compatible with\\nvarious segmentation loss designs.\\nAlgorithm 1 Training procedure for segmentation\\nGiven a trained diffusion model D, and (x0, ct, ce, y) represents\\nthe image, text, edge, and label pairs in the training set'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='Algorithm 1 Training procedure for segmentation\\nGiven a trained diffusion model D, and (x0, ct, ce, y) represents\\nthe image, text, edge, and label pairs in the training set\\nTraining target: any arbitrary segmentation network C with seg-\\nmentation loss function l with loss balance hyper-parameter Î±\\n1: Augment x0 by n times Ïµ âˆ¼ N (0; 1)\\nxi = D(Ïµi, ct + caugi , ce), i âˆ¼ 1, . . . , n\\n2: for t = 1, . . . , epochs do\\n3: Random choose i âˆ¼ 1, . . . , n\\n4: m = generate-random-patch(Î±, patch size )\\n5: loss = l(C(m Â· x0 + (1 âˆ’ m) Â· xi), y)\\n6: optimizer.zero grad()\\n7: loss.backward()\\n8: optimizer.step()\\n9: end for\\n10: return C\\nDuring the model training, we integrated both real and\\nsynthetic samples by employing a randomly generated patch\\nmask for each image, determined by a hyper-parameter Î± and\\na pre-specified resolution. For instance, given a dimension of\\n384Ã—384, as is customary in our studies, and a patch size\\nof 64, we would derive a random matrix with a dimension of'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='a pre-specified resolution. For instance, given a dimension of\\n384Ã—384, as is customary in our studies, and a patch size\\nof 64, we would derive a random matrix with a dimension of\\n384/64Ã—384/64 from a uniform distribution ranging between 0\\nand 1. Every patch with values lower than a specific threshold\\nÎ± is converted to 1, while all others become 0. This simplified\\nmatrix is then resized back to its original dimensions using a\\nnearest-neighbor interpolation technique. Intuitively, whenÎ± is\\nlarge, more patches will be set as 1, and the combined (mixed)\\nimage will be more similar to the original samples, and when\\nÎ± is small, more patches will be set as 0 and the combined\\n(mixed) image will be more similar to the generated samples.\\nThis procedure enables the fusion of real and synthetic images\\nat the patch level. Thus, the hyperparameter Î± governs the\\nmixing ratio, with higher values favoring the inclusion of real\\nimage patches and lower values incorporating more synthetic'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='at the patch level. Thus, the hyperparameter Î± governs the\\nmixing ratio, with higher values favoring the inclusion of real\\nimage patches and lower values incorporating more synthetic\\npatches into the final representation. Figure 4 illustrates the\\ninfluence of Î± including the extreme cases when Î± = 0\\n(meaning that patches are from generated samples only) nand\\nÎ± = 1 (meaning that patches are from original images only).\\nSuch strategy ensures that the model is not predisposed to\\ngenuine or artificial data, fostering improved generalization\\nand resultant performance. We employed an ablation study to\\nsystematically investigate the impact of different hyperparam-AUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING 7\\nOriginal\\n Edge\\n Augment Sample 1\\n Augment Sample 2\\n Augment Sample 3\\n Augment Sample 4\\nFig. 3. Example augmented samples are illustrated in the last four columns while the first and second columns show original MRI, CT, and US'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='Augment Sample 2\\n Augment Sample 3\\n Augment Sample 4\\nFig. 3. Example augmented samples are illustrated in the last four columns while the first and second columns show original MRI, CT, and US\\nimages and their edge maps, respectively. Augmented samples show notable variances in intensity distribution (diversity) while they retain the\\nstructural integrity.\\neters and image resolution choices on model performance(s).\\nFor a comprehensive understanding of the training algorithm\\nitself, please refer to Algorithm 1.\\nIV. R ESULTS\\nA. Diffusion Model for RadImageNet\\nThe pre-trained diffusion model demonstrated a promising\\nperformance on the RadImageNet for data generation. We\\nmeasured the quality of the generated synthetic images using\\nthe Mean Absolute Error (MAE), Mean Squared Error (MSE),\\nRoot Mean Square Error (RMSE), and MultiScale Structural\\nSimilarity Metrics (MS-SSIM). Prevalent metrics used in\\nnatural image generation, such as Frechet Inception Distance'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='Root Mean Square Error (RMSE), and MultiScale Structural\\nSimilarity Metrics (MS-SSIM). Prevalent metrics used in\\nnatural image generation, such as Frechet Inception Distance\\n(FID) and Inception Score (IS), are underpinned by a feature\\nextractor pre-trained on the ImageNet dataset. The inherent\\ncharacteristics and complexities of medical images, which\\nare fundamentally different from natural images, render these\\nmetrics less effective in accurately assessing the performance\\nof our approach.\\nWe compared our diffusion-based generation methods with\\nthe widely adopted conditional GAN: Pix2Pix image transla-\\ntion [44]. Pix2Pix takes the same edge information as guidance\\nand reconstructs the original input image. The diffusion model\\nachieved an MAE score of 0.087, an MSE of 0.023, an RMSE\\nof 0.144, an SSIM of 0.636, and an MS-SSIM of 0.636 while\\nthe Pix2Pix GAN method achieved an MAE score of 0.147,\\nan MSE of 0.0538, an RMSE of 0.2219, an SSIM of 0.4583,'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='of 0.144, an SSIM of 0.636, and an MS-SSIM of 0.636 while\\nthe Pix2Pix GAN method achieved an MAE score of 0.147,\\nan MSE of 0.0538, an RMSE of 0.2219, an SSIM of 0.4583,\\nand an MS-SSIM of 0.5325, as shown in Table I, indicating\\na high fidelity and diversity of the generated medical images.\\nThese results suggest that the diffusion model is effective in\\ngenerating realistic and meaningful medical images, which\\nin turn can enhance the performance of downstream medical\\nimage analysis tasks. Figure 3 indicates several sampling\\nresults when combined with text-guidance.\\nIt should be noted that our focus in this study is on generat-\\ning controlled, clinically relevant variations with text prompts.\\nIn other words, our work aims to strike a balance between the\\ntwo (segmentation and data diversity), not maximizing one\\nover another. One may conclude that Pix2Pix may generate\\nmore diverse images regarding MS-SSIM. However, it should\\nbe noted that Pix2Pix does not take text prompts as input and'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='over another. One may conclude that Pix2Pix may generate\\nmore diverse images regarding MS-SSIM. However, it should\\nbe noted that Pix2Pix does not take text prompts as input and\\ncan hardly be used to generate more diverse data with semantic\\ndiversity for enhancing segmentation performance. The MS-\\nSSIM metric primarily measures pixel-level similarity, which\\ncan be high even when images exhibit meaningful semantic\\nvariations. Our modelâ€™s higher MS-SSIM scores indicate better\\nstructural consistency with the ground truth, not necessarily\\nless meaningful diversity.\\nB. Performance on Downstream Tasks\\nWe validate our modelâ€™s performance on multiple datasets\\nwith limited sample sizes, including ultrasound, CT, and MRI\\nmodalities across various organs like breast [45], spleen [46],\\nand prostate [46]. For 3D datasets like CT spleen or MRI\\nprostate, we split them into 2D with a slice depth of one.\\nWe use the standard AttentionUNet [47] as the segmenta-'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='and prostate [46]. For 3D datasets like CT spleen or MRI\\nprostate, we split them into 2D with a slice depth of one.\\nWe use the standard AttentionUNet [47] as the segmenta-\\ntion backbone with MONAI implementation [48], and in\\nthe ablation studies, we investigate the combination effects\\nbetween DiffBoost and various segmentation backbones. For\\ncomparison, we investigated different traditional medical aug-\\nmentation methods, including spatial transforms like Ran-\\ndom Rotate, Random Scale, and Random Mirror, Random8 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2024\\nAlpha: 0.0\\n Alpha: 0.2\\n Alpha: 0.4\\n Alpha: 0.6\\n Alpha: 0.8\\n Alpha: 1.0\\nFig. 4. The hyper-parameter Î± determines the combination balance between original and augmented samples at the patch level.\\nTABLE I\\nPERFORMANCE ON THE RadImageNet DATASET GENERATION . DATA RANGE IS BETWEEN 0-1 FOR COMPUTATION .\\nMAE â†“ MSE â†“ RMSE â†“ SSIM â†‘ MS-SSIM â†‘'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='TABLE I\\nPERFORMANCE ON THE RadImageNet DATASET GENERATION . DATA RANGE IS BETWEEN 0-1 FOR COMPUTATION .\\nMAE â†“ MSE â†“ RMSE â†“ SSIM â†‘ MS-SSIM â†‘\\nPix2Pix [44] 0.1469 Â± 0.0556 0.0538 Â± 0.0366 0.2219 Â± 0.0676 0.4583 Â± 0.1105 0.5325 Â± 0.1557\\nOurs 0.0873 Â± 0.0363 0.0229 Â± 0.0163 0.1441 Â± 0.0463 0.6356 Â± 0.1252 0.6666 Â± 0.1491\\nResolution, and intensity transform like Random Contrast,\\nRandom Gamma, Random Brightness, Random Noise, and\\nthe Deep Stack transform, which indicates the combination\\nof all previous transforms implemented in nnUNet [4]. We\\nhave chosen AdamW as optimizer, and all experiments were\\nconducted under 3-fold cross-validation. Model performance\\nwas comprehensively assessed using two categories of metrics:\\nregion-level metrics such as Dice coefficient (Dice), Precision,\\nand Recall, and shape-centric metrics like the 95% Hausdorff\\nDistance (HD95) and Average Symmetric Surface Distance\\n(ASSD). This dual-metric approach facilitates a detailed and'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='and Recall, and shape-centric metrics like the 95% Hausdorff\\nDistance (HD95) and Average Symmetric Surface Distance\\n(ASSD). This dual-metric approach facilitates a detailed and\\nrigorous evaluation of our modelâ€™s capabilities.\\nTable II demonstrates the superiority of DiffBoost compared\\nto other data augmentation techniques. Notably, DiffBoost\\nachieves significant improvements in Dice coefficient, sug-\\ngesting that DiffBoost effectively guides the model towards\\nlearning more robust and intensity-independent features, par-\\nticularly the morphology (shape and structure) of the target\\norgans. Beyond the average performance boost, DiffBoost also\\nleads to a reduction in standard deviation across datasets.\\nThis decreased variability implies that the model trained with\\nDiffBoost captures features more consistently. This robustness\\nis further supported by the observed reduction in HD95\\ndistance across datasets. This metric measures the average'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='DiffBoost captures features more consistently. This robustness\\nis further supported by the observed reduction in HD95\\ndistance across datasets. This metric measures the average\\ndistance between the predicted segmentation boundary and the\\nground truth, indicating that DiffBoost generates segmentation\\nmasks with more precise and consistent shapes for anatomical\\nstructures. These findings highlight DiffBoostâ€™s potential as\\na powerful tool for achieving robust medical image segmen-\\ntation, particularly in tasks where consistent and accurate\\nidentification of anatomical structures is critical.\\nDiffBoost demonstrates significant improvements in seg-\\nmentation performance, particularly for challenging tasks like\\nprostate MRI and breast ultrasound segmentation. The Dice\\ncoefficient increases from 78.46% to 84.56% (7.8% improve-\\nment) for prostate MRI and from 62.92% to 71.65% (13.87%\\nimprovement) for breast cancer segmentation. These results'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='coefficient increases from 78.46% to 84.56% (7.8% improve-\\nment) for prostate MRI and from 62.92% to 71.65% (13.87%\\nimprovement) for breast cancer segmentation. These results\\nhighlight DiffBoostâ€™s ability to enhance model performance in\\ncomplex segmentation scenarios where structural information\\nis crucial. For less complex tasks like spleen CT segmentation,\\nwhere the baseline approach already achieves a high accuracy\\n(Dice coefficient of 94.42%), the improvement from DiffBoost\\nwas marginal (94.78%). This suggests that for tasks with\\nwell-defined features and high baseline performance, data\\naugmentation methods may have a less pronounced effect. Itâ€™s\\nimportant to note that combining multiple data augmentation\\ntechniques (like DeepStack) doesnâ€™t always guarantee better\\nperformance compared to a single, well-designed approach\\n(as shown in previous works [49]â€“[51]). DiffBoost stands out\\nby consistently delivering noteworthy advancements even for'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='performance compared to a single, well-designed approach\\n(as shown in previous works [49]â€“[51]). DiffBoost stands out\\nby consistently delivering noteworthy advancements even for\\ntasks with already strong baseline performance.\\nBeyond the quantitative comparisons, we conducted a qual-\\nitative analysis (Figure 5) to visually assess the performance\\nof our method. Interestingly, DiffBoost leads to a noticeableAUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING 9\\nTABLE II\\nCOMPARING WITH OTHER DATA AUGMENTATION METHODS ON ULTRASOUND , CT, AND MRI MODALITIES ACROSS VARIOUS ORGANS , INCLUDING\\nBREAST CANCER , SPLEEN , AND PROSTATE SEGMENTATION DATASETS . WE CAN OBSERVE SUPERIOR PERFORMANCE OF THE DIFFBOOST\\nTECHNIQUE OVER OTHER DATA AUGMENTATION METHODS .\\nTask Task 1: Prostate MRI Segmentation (Sample Size: 32 )\\nMethod Dice â†‘ Precision â†‘ Recall â†‘ HD95 (mm) â†“ ASSD (mm) â†“\\nBaseline 78.46 Â± 10.36 77.72 Â± 11.89 81.35 Â± 13.11 11.93 Â± 8.04 3.12 Â± 1.32'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='Task Task 1: Prostate MRI Segmentation (Sample Size: 32 )\\nMethod Dice â†‘ Precision â†‘ Recall â†‘ HD95 (mm) â†“ ASSD (mm) â†“\\nBaseline 78.46 Â± 10.36 77.72 Â± 11.89 81.35 Â± 13.11 11.93 Â± 8.04 3.12 Â± 1.32\\nRandomContrast 81.23 Â± 8.79 82.13 Â± 10.25 82.10 Â± 12.07 10.41 Â± 6.31 2.59 Â± 1.00\\nRandomGamma 79.30 Â± 9.05 79.37 Â± 11.15 82.17 Â± 11.98 11.65 Â± 7.07 2.84 Â± 1.44\\nRandomBrightness 79.18 Â± 8.07 80.97 Â± 10.02 79.81 Â± 12.54 11.10 Â± 9.07 2.91 Â± 1.18\\nRandomNoise 80.68 Â± 7.29 80.34 Â± 10.38 82.55 Â± 10.33 12.04 Â± 9.43 2.68 Â± 1.06\\nRandomResolution 78.63 Â± 8.54 80.82 Â± 10.85 78.79 Â± 12.56 12.22 Â± 11.25 2.85 Â± 1.33\\nRandomMirror 79.23 Â± 11.32 85.08 Â± 9.93 76.19 Â± 15.94 9.31 Â± 5.46 2.55 Â± 1.18\\nRandomRotate 82.12 Â± 9.03 85.62 Â± 8.33 80.53 Â± 12.67 8.47 Â± 7.47 2.30 Â± 1.39\\nRandomScale 83.11 Â± 7.29 84.59 Â± 9.26 83.18 Â± 10.53 9.64 Â± 7.84 2.37 Â± 1.06\\nDeepStack 78.97 Â± 9.40 81.86 Â± 10.21 78.99 Â± 14.33 9.32 Â± 4.61 2.7 Â± 1.17\\nDiffBoost (ours) 84.56 Â± 6.69 86.70 Â± 6.50 83.98 Â± 10.53 7.75 Â± 8.88 2.06 Â± 1.40'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='DeepStack 78.97 Â± 9.40 81.86 Â± 10.21 78.99 Â± 14.33 9.32 Â± 4.61 2.7 Â± 1.17\\nDiffBoost (ours) 84.56 Â± 6.69 86.70 Â± 6.50 83.98 Â± 10.53 7.75 Â± 8.88 2.06 Â± 1.40\\nTask Task 2: Spleen CT Segmentation (Sample Size: 41)\\nMethod Dice â†‘ Precision â†‘ Recall â†‘ HD95 (mm) â†“ ASSD (mm) â†“\\nBaseline 94.42 Â± 2.76 95.12 Â± 2.84 93.92 Â± 4.67 5.18 Â± 4.43 0.94 Â± 0.68\\nRandomContrast 94.29 Â± 2.55 95.15 Â± 3.43 93.66 Â± 4.29 8.18 Â± 12.58 1.35 Â± 1.38\\nRandomGamma 94.69 Â± 1.98 95.37 Â± 2.80 94.15 Â± 3.31 5.59 Â± 5.42 1.14 Â± 1.43\\nRandomBrightness 93.84 Â± 2.82 94.17 Â± 3.83 93.77 Â± 4.68 6.57 Â± 6.04 1.23 Â± 1.04\\nRandomNoise 93.84 Â± 3.28 94.59 Â± 3.71 93.44 Â± 5.53 6.32 Â± 5.15 1.29 Â± 0.94\\nRandomResolution 93.99 Â± 3.01 94.66 Â± 4.22 93.68 Â± 5.04 7.84 Â± 7.74 1.29 Â± 1.12\\nRandomMirror 91.95 Â± 4.89 92.92 Â± 5.05 91.52 Â± 7.46 22.89 Â± 38.69 3.23 Â± 3.30\\nRandomRotate 93.76 Â± 4.46 95.49 Â± 3.17 92.6 Â± 7.65 6.15 Â± 6.27 1.17 Â± 1.01\\nRandomScale 93.98 Â± 2.65 94.82 Â± 4.11 93.44 Â± 4.21 5.96 Â± 5.39 1.09 Â± 0.81'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='RandomRotate 93.76 Â± 4.46 95.49 Â± 3.17 92.6 Â± 7.65 6.15 Â± 6.27 1.17 Â± 1.01\\nRandomScale 93.98 Â± 2.65 94.82 Â± 4.11 93.44 Â± 4.21 5.96 Â± 5.39 1.09 Â± 0.81\\nDeepStack 93.66 Â± 3.36 93.73 Â± 4.60 93.99 Â± 5.55 8.3 Â± 8.28 1.39 Â± 1.15\\nDiffBoost (ours) 94.78 Â± 1.95 94.14 Â± 3.13 95.55 Â± 2.68 4.88 Â± 3.83 0.9 Â± 0.55\\nTask Task 3: Breast Cancer Ultrasound Segmentation (Sample Size: 147)\\nMethod Dice â†‘ Precision â†‘ Recall â†‘ HD95 (pixel) â†“ ASSD (pixel) â†“\\nBaseline 62.92 Â± 25.79 63.34 Â± 31.15 76.59 Â± 24.50 138.19 Â± 118.30 38.20 Â± 38.20\\nRandomContrast 64.81 Â± 27.36 65.83 Â± 30.89 73.93 Â± 27.96 123.24 Â± 105.32 34.96 Â± 33.83\\nRandomGamma 66.43 Â± 25.11 67.66 Â± 29.26 75.40 Â± 25.51 117.96 Â± 113.57 33.63 Â± 34.13\\nRandomBrightness 63.65 Â± 27.58 65.56 Â± 31.79 73.42 Â± 26.95 125.58 Â± 111.85 37.63 Â± 39.78\\nRandomNoise 66.24 Â± 25.52 67.29 Â± 30.73 75.79 Â± 22.92 132.05 Â± 120.47 35.65 Â± 34.14\\nRandomResolution 67.89 Â± 25.64 68.15 Â± 29.01 77.03 Â± 23.88 127.33 Â± 124.93 34.42 Â± 37.11'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='RandomNoise 66.24 Â± 25.52 67.29 Â± 30.73 75.79 Â± 22.92 132.05 Â± 120.47 35.65 Â± 34.14\\nRandomResolution 67.89 Â± 25.64 68.15 Â± 29.01 77.03 Â± 23.88 127.33 Â± 124.93 34.42 Â± 37.11\\nRandomMirror 70.15 Â± 25.55 70.85 Â± 28.77 78.45 Â± 25.14 102.08 Â± 105.71 29.83 Â± 36.53\\nRandomRotate 69.15 Â± 27.57 69.99 Â± 30.95 77.87 Â± 25.20 118.18 Â± 129.85 34.33 Â± 44.07\\nRandomScale 69.15 Â± 26.05 68.96 Â± 29.15 79.24 Â± 25.84 113.74 Â± 117.50 32.01 Â± 36.97\\nDeepStack 63.85 Â± 25.63 63.64 Â± 30.05 76.33 Â± 23.94 144.79 Â± 120.06 37.94 Â± 38.56\\nDiffBoost (ours) 71.65 Â± 24.52 70.22 Â± 27.89 81.91 Â± 22.39 95.18 Â± 103.41 26.98 Â± 32.55\\nimprovement in segmentation reliability, especially for chal-\\nlenging tasks like ultrasound breast cancer segmentation. This\\ncan be attributed to the modelâ€™s focus on structural information\\nbeyond just intensity distribution. As a result, the model\\nexcels at delineating suspicious regions with greater accuracy.\\nThese qualitative findings strongly support the effectiveness'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='beyond just intensity distribution. As a result, the model\\nexcels at delineating suspicious regions with greater accuracy.\\nThese qualitative findings strongly support the effectiveness\\nof DiffBoost as a data augmentation approach. By generating\\nmeaningful and realistic synthetic medical images, DiffBoost\\ncan significantly improve the performance of downstream\\nmedical image segmentation tasks.\\nV. A BLATION EXPERIMENTS\\nWe conducted extensive ablation experiments to investigate\\nthe influence of different data augmentation ratios, hyper-\\nparameter settings, patch size settings, and conjunction effects\\nwith various network architectures on the prostate MRI seg-\\nmentation dataset.\\nA. Influence of Augmentation Ratio\\nThe data augmentation ratio, defined as n in Algorithm 1\\nfor how many samples we generated from the original images,\\ncan have a notable impact on the final performance of the\\nmodel. As one might expect, a higher data augmentation'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='for how many samples we generated from the original images,\\ncan have a notable impact on the final performance of the\\nmodel. As one might expect, a higher data augmentation\\nratio can lead to more robust performance, although the\\nmarginal gains may diminish as the ratio increases. A higher\\naugmentation ratio can also increase computational demands,\\nwhich is undesirable for diffusion models due to their already\\nresource-intensive nature. Therefore, it is crucial to strike an\\nappropriate balance between the data augmentation ratio and\\ncomputational efficiency when employing diffusion models\\nfor medical imaging applications. As shown in Figure 6(a),\\nobservations gleaned from our analysis indicate a notable\\nperformance increase when the data augmentation ratio is\\nrelatively small. However, when this ratio exceeds a factor of\\n10, the marginal performance enhancements begin to plateau,\\nexhibiting a limited increase and some degree of fluctuation.'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='relatively small. However, when this ratio exceeds a factor of\\n10, the marginal performance enhancements begin to plateau,\\nexhibiting a limited increase and some degree of fluctuation.\\nThis suggests that a higher data augmentation ratio does10 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2024\\nGround Truth\\n Baseline\\n RandomContrast\\n RandomNoise\\n RandomRotate\\n RandomScale\\n DeepStack\\n DiffBoost (Ours)\\nFig. 5. Visual comparison of segmentation performance over some other augmentation methods. DiffBoost enables the model to generate a more\\nconsistent shape of the anatomical structure and outperforms other data augmentation methods.\\n0 3 5 10 20 30 40 50\\nAugmentation Ratio\\n78\\n79\\n80\\n81\\n82\\n83\\n84\\n85Dice Coefficient\\nBaseline\\n(a) Influence of Augmentation Ratio\\n0 0.2 0.3 0.4 0.5 0.6 0.8 1\\n Setting\\n78\\n79\\n80\\n81\\n82\\n83\\n84\\n85Dice Coefficient\\nBaseline\\n(b) Influence of  Setting\\n1 32 64 96 128 384\\nPatch Size Setting\\n78\\n79\\n80\\n81\\n82\\n83\\n84\\n85Dice Coefficient\\nBaseline'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='0 0.2 0.3 0.4 0.5 0.6 0.8 1\\n Setting\\n78\\n79\\n80\\n81\\n82\\n83\\n84\\n85Dice Coefficient\\nBaseline\\n(b) Influence of  Setting\\n1 32 64 96 128 384\\nPatch Size Setting\\n78\\n79\\n80\\n81\\n82\\n83\\n84\\n85Dice Coefficient\\nBaseline\\n(c) Influence of Patch Size\\nFig. 6. (a) Influence of data augmentation ratios on segmentation performance. The segmentation performance increases significantly as the data\\naugmentation ratio increases. The marginal performance gain is limited once the ratio is larger than 10. (b) Influence of hyper-parameter Î± setting\\non segmentation performance. The optimal performance of the model can be adversely affected by the settings for Î± if it is either excessively high\\nor low. A high-performance plateau exists within the wide middle range. (c) Influence of patch size on segmentation performance. Regardless of\\npatch size setting, DiffBoost can achieve significant advancement over the baseline method.\\nnot necessarily translate to proportionally larger performance'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='patch size setting, DiffBoost can achieve significant advancement over the baseline method.\\nnot necessarily translate to proportionally larger performance\\nimprovements, and the marginal performance gain might be\\nlimited compared with a large computation burden.\\nB. Influence of Hyper-parameter Î±\\nThe hyper-parameter Î± plays a significant role in our frame-\\nwork, enabling us to control the balance between the contribu-AUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING 11\\ntions of real training samples and those synthetically generated\\nby our diffusion model. Specifically, Î± acts as a weighting\\nfactor in our pre-designed combination loss function, where a\\nlarger Î± assigns more importance to the real samples, and a\\nsmaller Î± increases the influence of the generated samples. In\\nthis ablation study, we explore the impact of various hyper-\\nparameters on the final performance metrics. As expected,\\nwhen Î± is set exceedingly high, the model leans towards the'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='this ablation study, we explore the impact of various hyper-\\nparameters on the final performance metrics. As expected,\\nwhen Î± is set exceedingly high, the model leans towards the\\noriginal samples, resulting in a marginal increase in perfor-\\nmance. Conversely, an excessively low Î± would cause the\\nmodel to overly concentrate on synthetic samples, adversely\\naffecting performance due to the lack of guidance from real\\nsamples. This anticipated trend is explicitly corroborated by\\nFigure 6 (b), which presents a high-performance plateau when\\nthe data augmentation ratio resides within the wide range of\\n0.2-0.8.\\nC. Influence of Patch Size\\nThe choice of patch size is critical in our experiments as it\\ndictates the spatial granularity at which real and synthetic sam-\\nples are combined. In cases where the patch size matches the\\nimage size, the selection between real and synthetic samples\\noccurs on a case-by-case basis. Conversely, when the patch'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='ples are combined. In cases where the patch size matches the\\nimage size, the selection between real and synthetic samples\\noccurs on a case-by-case basis. Conversely, when the patch\\nsize is set to 1, selection occurs at the pixel level. Empirical\\nfindings in Figure 6 (c) suggest that the DiffBoost significantly\\nimproves over the baselines, irrespective of the patch size\\nsetting. Moreover, optimal performance was noted when the\\npatch size was set to a median spatial level, illustrating\\nthe importance of carefully balancing fine-grained and broad\\nspatial representations.\\nD. Influence of Network Architecture\\nTo ensure the general applicability of proposed DiffBoost,\\nit is important to investigate its performance in conjunction\\nwith various network architectures, as structural differences\\nmay impact the final performance. To this end, we conducted\\nan ablation study to assess the combined influence of dif-\\nferent network architectures and the DiffBoost method on'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='may impact the final performance. To this end, we conducted\\nan ablation study to assess the combined influence of dif-\\nferent network architectures and the DiffBoost method on\\nthe prostate MRI segmentation tasks. Besides the backbone\\nAttentionUNet in the previous section, we include traditional\\nCNN structures like basic UNet [52], Residual UNet [53],\\nResNet50 UNet [54], and the recent transformer structure like\\nSwinUNETR [55]. This comprehensive analysis, as shown\\nin Figure 7, demonstrates the robustness and adaptability of\\nDiffBoost across a diverse range of architectural designs,\\nfurther supporting its utility in a wide array of medical imaging\\nsegmentation applications.\\nVI. D ISCUSSION AND CONCLUSION\\nIn this study, we proposed a diffusion model-based data aug-\\nmentation approach, called DiffBoost, for enhancing the per-\\nformance of medical image segmentation. The method demon-\\nstrated promising results in generating anatomy-meaningful'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='mentation approach, called DiffBoost, for enhancing the per-\\nformance of medical image segmentation. The method demon-\\nstrated promising results in generating anatomy-meaningful\\nsynthetic images conditioned on text and edge information.\\nOur study has some limitations too. For instance, our current\\ntext input is a combination of several categorical labels. We\\nBasic UNet Residual UNet AttentionUNet ResNet50UNet SwinUNETR\\nSegmentation Backbones\\n70\\n72\\n74\\n76\\n78\\n80\\n82\\n84\\n86\\n88Dice Coefficient\\nInfluence on Different Segmentation Backbone\\nBaseline\\nDiffBoost\\nFig. 7. Influence of backbone architectures on segmentation results.\\nDiffBoost achieves a promising and consistent improvement regardless\\nof the choice of different backbones.\\nhave not tested how the model will perform under natural texts,\\nlike â€Can you help to generate a CT image of a spleen. â€ , and\\nhow the performance will differ compared to categorical label\\nbased generation. We envision that it should be possible to'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='like â€Can you help to generate a CT image of a spleen. â€ , and\\nhow the performance will differ compared to categorical label\\nbased generation. We envision that it should be possible to\\ngenerate similar results under natural text input too because\\nthe recent surge in techniques for aligning medical text data\\nwith large-scale models presents a unique opportunity allowing\\nfor the utilization of more natural language descriptions when\\ngenerating medical images. In some cases, like in our study,\\nsimple categorical conditions or other input forms may already\\nsuffice to guide the segmentation process effectively. On the\\nother hand, natural text input can be more useful when a\\nsegmentation task requires specific instructions and provides\\nmuch more flexible input.\\nAnother limitation is that employing the edge map (Con-\\ntrolNet) as a condition on image generation can limit the\\nanatomical variation; hence, influencing the characteristics of'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='Another limitation is that employing the edge map (Con-\\ntrolNet) as a condition on image generation can limit the\\nanatomical variation; hence, influencing the characteristics of\\nthe generated images. However, ControlNet can also be viewed\\nas providing boundary information derived from the gener-\\nated texture. If ControlNet overly emphasizes or constrains\\ncertain features, there might occur a reduction in diversity\\nin generated samples, affecting the segmentation results. This\\nis not the case in our experiments, luckily, reflecting the\\nfact that ControlNet is not excessively limiting the diversity,\\nor anatomical variations in the generated samples, thanks to\\nparameter control in the edge locations in hierarchical ways.\\nThis potential limitation can be substantial if true edges are\\nused instead of edgemaps. It is because edgemaps are used\\nat different sensitivity levels while it is hard to control the\\nsensitivity of the true edges for different images, resolutions,\\nand body regions.'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='at different sensitivity levels while it is hard to control the\\nsensitivity of the true edges for different images, resolutions,\\nand body regions.\\nOne may wonder if higher-fidelity synthetic images (than\\nthose already generated by our method) are truly necessary\\nfor successful segmentation. Based on our findings and limited\\nstudies in the literature, the answer is â€no.â€ High-fidelity syn-\\nthetic scans are not always necessary for successful medical\\nimage segmentation. High accuracy can still be obtained with\\nlower-fidelity synthetic images as long as latent space features\\nare informative. However, we also acknowledge that higher-\\nquality synthetic images can boost the segmentation results12 IEEE TRANSACTIONS ON MEDICAL IMAGING, VOL. XX, NO. XX, XXXX 2024\\nfurther. It should also be noted that the optimal approach for\\ngenerating high-fidelity images will depend on various factors,\\nincluding the complexity of the anatomy and the availability\\nof large-scale real-world data.'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='generating high-fidelity images will depend on various factors,\\nincluding the complexity of the anatomy and the availability\\nof large-scale real-world data.\\nFrom the visual results, we observed mixed facts about\\nthe quality of boundary information in soft and bone tissue\\nlocations. For instance, as shown in Figure 2, the skull in\\nMRI can lead to clean edge guidance. Similarly, generated\\nimages from the text prompt â€œCT, abdomen, normalâ€ provide\\nclear boundaries even from soft tissues (i.e., liver, pancreas,\\nand spleen) due to the involvement of Holistically Nested\\nedge detection method. We have observed the same behavior\\nin prostate MRI too (as shown in Figure 3). However, there\\nwere cases where the soft tissue boundaries were not clear in\\nthe generated images. As a result, we did not have a clear\\nconsensus whether soft tissue generation is always inferior to\\nother tissues or not.\\nOther than pix2pix, we did not benchmark other GAN-'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='the generated images. As a result, we did not have a clear\\nconsensus whether soft tissue generation is always inferior to\\nother tissues or not.\\nOther than pix2pix, we did not benchmark other GAN-\\nbased methods like StyleGAN [56] because our study pri-\\nmarily underscores the feasibility of diffusion model-based\\ndata augmentation for segmentation tasks; the complexity\\nof tailoring these techniques to each medical dataset in our\\nproblem is computationally prohibitive and digressing from\\nthe main focus of this paper. Nevertheless, comprehensive\\ncomparison and analysis of generative AI algorithms from\\nthree broad classes (GANs, V AEs, Diffusions) can be studied\\nas a complementary to our current study. Yet, this does not\\nundermine our core findings.\\nOur immediate future work will focus on (i) exploring tech-\\nniques to accelerate the sampling process within the diffusion\\nmodel for improved efficiency, (ii) investigating methods to in-'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='Our immediate future work will focus on (i) exploring tech-\\nniques to accelerate the sampling process within the diffusion\\nmodel for improved efficiency, (ii) investigating methods to in-\\ncorporate text information more powerfully, allowing for even\\nfiner control over the generated images, and (iii) expanding\\nimage patterns beyond edge information to integrate broader\\ncontextual information from the text descriptions, potentially\\nleading to richer and more nuanced synthetic images.\\nREFERENCES\\n[1] P. Chlap, H. Min, N. Vandenberg, J. Dowling, L. Holloway, and A. Ha-\\nworth, â€œA review of medical image data augmentation techniques for\\ndeep learning applications,â€ Journal of Medical Imaging and Radiation\\nOncology, vol. 65, no. 5, pp. 545â€“563, 2021.\\n[2] D. A. Van Dyk and X.-L. Meng, â€œThe art of data augmentation,â€ Journal\\nof Computational and Graphical Statistics , vol. 10, no. 1, pp. 1â€“50,\\n2001.\\n[3] L. Zhang, X. Wang, D. Yang, T. Sanford, S. Harmon, B. Turkbey, B. J.'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='of Computational and Graphical Statistics , vol. 10, no. 1, pp. 1â€“50,\\n2001.\\n[3] L. Zhang, X. Wang, D. Yang, T. Sanford, S. Harmon, B. Turkbey, B. J.\\nWood, H. Roth, A. Myronenko, D. Xuet al., â€œGeneralizing deep learning\\nfor medical image segmentation to unseen domains via deep stacked\\ntransformation,â€ IEEE transactions on medical imaging , vol. 39, no. 7,\\npp. 2531â€“2540, 2020.\\n[4] F. Isensee, P. F. Jaeger, S. A. Kohl, J. Petersen, and K. H. Maier-\\nHein, â€œnnU-Net: A self-configuring method for deep learning-based\\nbiomedical image segmentation,â€ Nature methods , vol. 18, no. 2, pp.\\n203â€“211, 2021.\\n[5] F. Garcea, A. Serra, F. Lamberti, and L. Morra, â€œData augmentation for\\nmedical imaging: A systematic literature review,â€ Computers in Biology\\nand Medicine, p. 106391, 2022.\\n[6] H.-C. Shin, N. A. Tenenholtz, J. K. Rogers, C. G. Schwarz, M. L.\\nSenjem, J. L. Gunter, K. P. Andriole, and M. Michalski, â€œMedical image\\nsynthesis for data augmentation and anonymization using generative'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='Senjem, J. L. Gunter, K. P. Andriole, and M. Michalski, â€œMedical image\\nsynthesis for data augmentation and anonymization using generative\\nadversarial networks,â€ in Proceedings of the International Conference\\non Simulation and Synthesis in Medical Imaging , 2018, pp. 1â€“11.\\n[7] I. J. Goodfellow, J. Shlens, and C. Szegedy, â€œExplaining and harnessing\\nadversarial examples,â€ arXiv preprint arXiv:1412.6572 , 2014.\\n[8] Z. Zhang, B. Wang, L. Yao, U. Demir, D. Jha, I. B. Turkbey, B. Gong,\\nand U. Bagci, â€œDomain generalization with adversarial intensity attack\\nfor medical image segmentation,â€ arXiv preprint arXiv:2304.02720 ,\\n2023.\\n[9] C. Chen, C. Qin, H. Qiu, C. Ouyang, S. Wang, L. Chen, G. Tarroni,\\nW. Bai, and D. Rueckert, â€œRealistic adversarial data augmentation for mr\\nimage segmentation,â€ in Proceedings of the Medical Image Computing\\nand Computer Assisted Intervention (MICCAI), 2020 , 2020, pp. 667â€“\\n677.\\n[10] J. Ho, A. Jain, and P. Abbeel, â€œDenoising diffusion probabilistic models,â€'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='and Computer Assisted Intervention (MICCAI), 2020 , 2020, pp. 667â€“\\n677.\\n[10] J. Ho, A. Jain, and P. Abbeel, â€œDenoising diffusion probabilistic models,â€\\nProceedings of the Advances in Neural Information Processing Systems ,\\nvol. 33, pp. 6840â€“6851, 2020.\\n[11] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer, â€œHigh-\\nresolution image synthesis with latent diffusion models,â€ in Proceedings\\nof the IEEE/CVF Conference on Computer Vision and Pattern Recogni-\\ntion, 2022, pp. 10 684â€“10 695.\\n[12] F.-A. Croitoru, V . Hondru, R. T. Ionescu, and M. Shah, â€œDiffusion\\nmodels in vision: A survey,â€ IEEE Transactions on Pattern Analysis\\nand Machine Intelligence , 2023.\\n[13] P. Dhariwal and A. Nichol, â€œDiffusion models beat gans on image syn-\\nthesis,â€ Proceedings of the Advances in Neural Information Processing\\nSystems, vol. 34, pp. 8780â€“8794, 2021.\\n[14] X. Mei, Z. Liu, P. M. Robson, B. Marinelli, M. Huang, A. Doshi,\\nA. Jacobi, C. Cao, K. E. Link, T. Yang et al., â€œRadimagenet: An open'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='Systems, vol. 34, pp. 8780â€“8794, 2021.\\n[14] X. Mei, Z. Liu, P. M. Robson, B. Marinelli, M. Huang, A. Doshi,\\nA. Jacobi, C. Cao, K. E. Link, T. Yang et al., â€œRadimagenet: An open\\nradiologic deep learning research dataset for effective transfer learning,â€\\nRadiology: Artificial Intelligence, vol. 4, no. 5, p. e210315, 2022.\\n[15] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal,\\nG. Sastry, A. Askell, P. Mishkin, J. Clark et al., â€œLearning transferable\\nvisual models from natural language supervision,â€ in Proceedings of the\\nInternational conference on machine learning , 2021, pp. 8748â€“8763.\\n[16] R. Gal, Y . Alaluf, Y . Atzmon, O. Patashnik, A. H. Bermano,\\nG. Chechik, and D. Cohen-Or, â€œAn image is worth one word: Person-\\nalizing text-to-image generation using textual inversion,â€ arXiv preprint\\narXiv:2208.01618, 2022.\\n[17] N. Ruiz, Y . Li, V . Jampani, Y . Pritch, M. Rubinstein, and K. Aberman,\\nâ€œDreambooth: Fine tuning text-to-image diffusion models for subject-'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='arXiv:2208.01618, 2022.\\n[17] N. Ruiz, Y . Li, V . Jampani, Y . Pritch, M. Rubinstein, and K. Aberman,\\nâ€œDreambooth: Fine tuning text-to-image diffusion models for subject-\\ndriven generation,â€ arXiv preprint arXiv:2208.12242 , 2022.\\n[18] T. Brooks, A. Holynski, and A. A. Efros, â€œInstructpix2pix: Learning\\nto follow image editing instructions,â€ arXiv preprint arXiv:2211.09800,\\n2022.\\n[19] L. Zhang and M. Agrawala, â€œAdding conditional control to text-to-image\\ndiffusion models,â€ arXiv preprint arXiv:2302.05543 , 2023.\\n[20] A. Kazerouni, E. K. Aghdam, M. Heidari, R. Azad, M. Fayyaz,\\nI. Hacihaliloglu, and D. Merhof, â€œDiffusion models for medical image\\nanalysis: A comprehensive survey,â€ arXiv preprint arXiv:2211.07804 ,\\n2022.\\n[21] L. Jiang, Y . Mao, X. Chen, X. Wang, and C. Li, â€œCoLa-Diff: Conditional\\nLatent Diffusion Model for Multi-Modal MRI Synthesis,â€ arXiv preprint\\narXiv:2303.14081, 2023.\\n[22] W. Peng, E. Adeli, Q. Zhao, and K. M. Pohl, â€œGenerating realistic'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='Latent Diffusion Model for Multi-Modal MRI Synthesis,â€ arXiv preprint\\narXiv:2303.14081, 2023.\\n[22] W. Peng, E. Adeli, Q. Zhao, and K. M. Pohl, â€œGenerating realistic\\n3d brain mris using a conditional diffusion probabilistic model,â€ arXiv\\npreprint arXiv:2212.08034, 2022.\\n[23] M. Iskandar, H. Mannering, Z. Sun, J. Matthew, H. Kerdegari, L. Peralta,\\nand M. Xochicale, â€œTowards realistic ultrasound fetal brain imaging\\nsynthesis,â€ arXiv preprint arXiv:2304.03941 , 2023.\\n[24] R. Mach Â´aË‡cek, L. Mozaffari, Z. Sepasdar, S. Parasa, P. Halvorsen, M. A.\\nRiegler, and V . Thambawita, â€œMask-conditioned latent diffusion for gen-\\nerating gastrointestinal polyp images,â€ arXiv preprint arXiv:2304.05233,\\n2023.\\n[25] G. M Â¨uller-Franzes, J. M. Niehues, F. Khader, S. T. Arasteh, C. Haar-\\nburger, C. Kuhl, T. Wang, T. Han, S. Nebelung, J. N. Kather et al. ,\\nâ€œDiffusion probabilistic models beat gans on medical images,â€ arXiv\\npreprint arXiv:2212.07501, 2022.'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='burger, C. Kuhl, T. Wang, T. Han, S. Nebelung, J. N. Kather et al. ,\\nâ€œDiffusion probabilistic models beat gans on medical images,â€ arXiv\\npreprint arXiv:2212.07501, 2022.\\n[26] H. A. Bedel and T. C Â¸ ukur, â€œDreamr: Diffusion-driven counterfactual\\nexplanation for functional mri,â€ arXiv preprint arXiv:2307.09547, 2023.\\n[27] Y . Li, H.-C. Shao, X. Liang, L. Chen, R. Li, S. Jiang, J. Wang, and\\nY . Zhang, â€œZero-shot medical image translation via frequency-guided\\ndiffusion models,â€ arXiv preprint arXiv:2304.02742 , 2023.\\n[28] Q. Lyu and G. Wang, â€œConversion between ct and mri images using\\ndiffusion and score-matching models,â€arXiv preprint arXiv:2209.12104,\\n2022.\\n[29] M. Â¨Ozbey, S. U. Dar, H. A. Bedel, O. Dalmaz, S Â¸. Â¨Ozturk, A. G Â¨ungÂ¨or,\\nand T. C Â¸ ukur, â€œUnsupervised medical image translation with adversarial\\ndiffusion models,â€ arXiv preprint arXiv:2207.08208 , 2022.AUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING 13'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='diffusion models,â€ arXiv preprint arXiv:2207.08208 , 2022.AUTHOR et al.: PREPARATION OF PAPERS FOR IEEE TRANSACTIONS ON MEDICAL IMAGING 13\\n[30] J. Xu, S. Liu, A. Vahdat, W. Byeon, X. Wang, and S. De Mello, â€œOpen-\\nvocabulary panoptic segmentation with text-to-image diffusion models,â€\\narXiv preprint arXiv:2303.04803 , 2023.\\n[31] D. Baranchuk, A. V oynov, I. Rubachev, V . Khrulkov, and A. Babenko,\\nâ€œLabel-efficient semantic segmentation with diffusion models,â€ in Pro-\\nceedings of the International Conference on Learning Representations ,\\n2022.\\n[32] J. Wu, R. Fu, H. Fang, Y . Zhang, and Y . Xu, â€œMedsegdiff-v2: Diffusion\\nbased medical image segmentation with transformer,â€ arXiv preprint\\narXiv:2301.11798, 2023.\\n[33] F. Bieder, J. Wolleb, A. Durrer, R. Sandk Â¨uhler, and P. C. Cattin, â€œDif-\\nfusion models for memory-efficient processing of 3d medical images,â€\\narXiv preprint arXiv:2303.15288 , 2023.\\n[34] A. Rahman, J. M. J. Valanarasu, I. Hacihaliloglu, and V . M. Patel,'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='fusion models for memory-efficient processing of 3d medical images,â€\\narXiv preprint arXiv:2303.15288 , 2023.\\n[34] A. Rahman, J. M. J. Valanarasu, I. Hacihaliloglu, and V . M. Patel,\\nâ€œAmbiguous medical image segmentation using diffusion models,â€arXiv\\npreprint arXiv:2304.04745, 2023.\\n[35] Z. Dorjsembe, H.-K. Pao, S. Odonchimed, and F. Xiao, â€œConditional\\ndiffusion models for semantic 3d medical image synthesis,â€ arXiv\\npreprint arXiv:2305.18453, 2023.\\n[36] S. Shao, X. Yuan, Z. Huang, Z. Qiu, S. Wang, and K. Zhou, â€œDiffuse-\\nexpand: Expanding dataset for 2d medical image segmentation using\\ndiffusion models,â€ arXiv preprint arXiv:2304.13416 , 2023.\\n[37] Z.-X. Cui, C. Cao, S. Liu, Q. Zhu, J. Cheng, H. Wang, Y . Zhu, and\\nD. Liang, â€œSelf-score: Self-supervised learning on score-based models\\nfor mri reconstruction,â€ arXiv preprint arXiv:2209.00835 , 2022.\\n[38] T. Xiang, M. Yurt, A. B. Syed, K. Setsompop, and A. Chaudhari, â€œDDM\\n2: Self-Supervised Diffusion MRI Denoising with Generative Diffusion'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='[38] T. Xiang, M. Yurt, A. B. Syed, K. Setsompop, and A. Chaudhari, â€œDDM\\n2: Self-Supervised Diffusion MRI Denoising with Generative Diffusion\\nModels,â€ arXiv preprint arXiv:2302.03018 , 2023.\\n[39] J. Wolleb, F. Bieder, R. Sandk Â¨uhler, and P. C. Cattin, â€œDiffusion models\\nfor medical anomaly detection,â€ in Proceedings of the Medical Image\\nComputing and Computer Assisted Intervention (MICCAI), 2022 , 2022,\\npp. 35â€“45.\\n[40] C. I. Bercea, M. Neumayr, D. Rueckert, and J. A. Schnabel, â€œMask,\\nstitch, and re-sample: Enhancing robustness and generalizability in\\nanomaly detection through automatic diffusion models,â€ arXiv preprint\\narXiv:2305.19643, 2023.\\n[41] Z. Dorjsembe, H.-K. Pao, and F. Xiao, â€œPolyp-ddpm: Diffusion-based\\nsemantic polyp synthesis for enhanced segmentation,â€ arXiv preprint\\narXiv:2402.04031, 2024.\\n[42] Y . Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and\\nB. Poole, â€œScore-based generative modeling through stochastic differ-'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='arXiv:2402.04031, 2024.\\n[42] Y . Song, J. Sohl-Dickstein, D. P. Kingma, A. Kumar, S. Ermon, and\\nB. Poole, â€œScore-based generative modeling through stochastic differ-\\nential equations,â€ arXiv preprint arXiv:2011.13456 , 2020.\\n[43] S. Xie and Z. Tu, â€œHolistically-nested edge detection,â€ in Proceedings of\\nthe IEEE international conference on computer vision , 2015, pp. 1395â€“\\n1403.\\n[44] P. Isola, J.-Y . Zhu, T. Zhou, and A. A. Efros, â€œImage-to-image translation\\nwith conditional adversarial networks,â€ in Proceedings of the IEEE\\nconference on computer vision and pattern recognition, 2017, pp. 1125â€“\\n1134.\\n[45] W. Al-Dhabyani, M. Gomaa, H. Khaled, and A. Fahmy, â€œDataset of\\nbreast ultrasound images,â€ Data in brief , vol. 28, p. 104863, 2020.\\n[46] M. Antonelli, A. Reinke, S. Bakas, K. Farahani, A. Kopp-Schneider,\\nB. A. Landman, G. Litjens, B. Menze, O. Ronneberger, R. M. Summers\\net al. , â€œThe medical segmentation decathlon,â€ Nature communications,\\nvol. 13, no. 1, p. 4128, 2022.'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='B. A. Landman, G. Litjens, B. Menze, O. Ronneberger, R. M. Summers\\net al. , â€œThe medical segmentation decathlon,â€ Nature communications,\\nvol. 13, no. 1, p. 4128, 2022.\\n[47] O. Oktay, J. Schlemper, L. L. Folgoc, M. Lee, M. Heinrich, K. Misawa,\\nK. Mori, S. McDonagh, N. Y . Hammerla, B. Kainz et al. , â€œAtten-\\ntion u-net: Learning where to look for the pancreas,â€ arXiv preprint\\narXiv:1804.03999, 2018.\\n[48] M. J. Cardoso, W. Li, R. Brown, N. Ma, E. Kerfoot, Y . Wang, B. Murrey,\\nA. Myronenko, C. Zhao, D. Yang et al., â€œMonai: An open-source frame-\\nwork for deep learning in healthcare,â€ arXiv preprint arXiv:2211.02701,\\n2022.\\n[49] E. D. Cubuk, B. Zoph, D. Mane, V . Vasudevan, and Q. V . Le, â€œAutoaug-\\nment: Learning augmentation strategies from data,â€ in Proceedings of\\nthe IEEE/CVF conference on computer vision and pattern recognition ,\\n2019, pp. 113â€“123.\\n[50] E. D. Cubuk, B. Zoph, J. Shlens, and Q. V . Le, â€œRandaugment:\\nPractical automated data augmentation with a reduced search space,â€'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='2019, pp. 113â€“123.\\n[50] E. D. Cubuk, B. Zoph, J. Shlens, and Q. V . Le, â€œRandaugment:\\nPractical automated data augmentation with a reduced search space,â€\\nin Proceedings of the IEEE/CVF conference on computer vision and\\npattern recognition workshops , 2020, pp. 702â€“703.\\n[51] S. G. M Â¨uller and F. Hutter, â€œTrivialaugment: Tuning-free yet state-of-the-\\nart data augmentation,â€ in Proceedings of the IEEE/CVF international\\nconference on computer vision , 2021, pp. 774â€“782.\\n[52] T. Falk, D. Mai, R. Bensch, Â¨O. C Â¸ ic Â¸ek, A. Abdulkadir, Y . Marrakchi,\\nA. BÂ¨ohm, J. Deubner, Z. JÂ¨ackel, K. Seiwald et al., â€œU-net: deep learning\\nfor cell counting, detection, and morphometry,â€ Nature methods, vol. 16,\\nno. 1, pp. 67â€“70, 2019.\\n[53] E. Kerfoot, J. Clough, I. Oksuz, J. Lee, A. P. King, and J. A. Schnabel,\\nâ€œLeft-ventricle quantification using residual u-net,â€ in Proceedings of\\nthe Statistical Atlases and Computational Models of the Heart , 2019,\\npp. 371â€“380.'),\n",
       " Document(metadata={'arxiv_id': '2310.12868v2', 'title': 'DiffBoost: Enhancing Medical Image Segmentation via Text-Guided Diffusion Model', 'section': 'body', 'authors': 'Zheyuan Zhang, Lanhong Yao, Bin Wang'}, page_content='â€œLeft-ventricle quantification using residual u-net,â€ in Proceedings of\\nthe Statistical Atlases and Computational Models of the Heart , 2019,\\npp. 371â€“380.\\n[54] K. He, X. Zhang, S. Ren, and J. Sun, â€œDeep residual learning for image\\nrecognition,â€ in Proceedings of the IEEE conference on computer vision\\nand pattern recognition , 2016, pp. 770â€“778.\\n[55] A. Hatamizadeh, V . Nath, Y . Tang, D. Yang, H. R. Roth, and D. Xu,\\nâ€œSwin unetr: Swin transformers for semantic segmentation of brain\\ntumors in mri images,â€ in Proceedings of the International MICCAI\\nBrainlesion Workshop, 2021, pp. 272â€“284.\\n[56] T. Karras, S. Laine, and T. Aila, â€œA style-based generator architecture\\nfor generative adversarial networks,â€ in Proceedings of the IEEE/CVF\\nconference on computer vision and pattern recognition, 2019, pp. 4401â€“\\n4410.'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'title_abstract', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='Title: Cross-dimensional transfer learning in medical image segmentation with deep learning\\n\\nAbstract: Over the last decade, convolutional neural networks have emerged and advanced the state-of-the-art in various image analysis and computer vision applications. The performance of 2D image classification networks is constantly improving and being trained on databases made of millions of natural images. However, progress in medical image analysis has been hindered by limited annotated data and acquisition constraints. These limitations are even more pronounced given the volumetry of medical imaging data. In this paper, we introduce an efficient way to transfer the efficiency of a 2D classification network trained on natural images to 2D, 3D uni- and multi-modal medical image segmentation applications. In this direction, we designed novel architectures based on two key principles: weight transfer by embedding a 2D pre-trained encoder into a higher dimensional U-Net, and dimensional transfer by expanding a 2D segmentation network into a higher dimension one. The proposed networks were tested on benchmarks comprising different modalities: MR, CT, and ultrasound images. Our 2D network ranked first on the CAMUS challenge dedicated to echo-cardiographic data segmentation and surpassed the state-of-the-art. Regarding 2D/3D MR and CT abdominal images from the CHAOS challenge, our approach largely outperformed the other 2D-based methods described in the challenge paper on Dice, RAVD, ASSD, and MSSD scores and ranked third on the online evaluation platform. Our 3D network applied to the BraTS 2022 competition also achieved promising results, reaching an average Dice score of 91.69% (91.22%) for the whole tumor, 83.23% (84.77%) for the tumor core, and 81.75% (83.88%) for enhanced tumor using the approach based on weight (dimensional) transfer. Experimental and qualitative results illustrate the effectiveness of our methods for multi-dimensional medical image segmentation.'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='Cross-dimensional transfer learning in medical image\\nsegmentation with deep learning\\nHicham Messaoudia,âˆ—, Ahror Belaidb,c, Douraied Ben Salemd,e, Pierre-Henri Conzed,f\\naLaboratory of Medical Informatics (LIMED), Faculty of Technology, University of Bejaia, 06000 Bejaia,\\nAlgeria\\nbLaboratory of Medical Informatics (LIMED), Faculty of Exact Sciences, University of Bejaia, 06000\\nBejaia, Algeria\\ncData Science & Applications Research Unit - CERIST, 06000, Bejaia, Algeria\\ndLaboratory of Medical Information Processing (LaTIM) UMR 1101, Inserm, 29200, Brest, France\\neNeuroradiology Department, University Hospital of Brest, 29200, Brest, France\\nfIMT Atlantique, 29200, Brest, France\\nAbstract\\nOver the last decade, convolutional neural networks have emerged and advanced the\\nstate-of-the-art in various image analysis and computer vision applications. The perfor-\\nmance of 2D image classification networks is constantly improving and being trained'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='state-of-the-art in various image analysis and computer vision applications. The perfor-\\nmance of 2D image classification networks is constantly improving and being trained\\non databases made of millions of natural images. Conversely, in the field of medical\\nimage analysis, the progress is also remarkable but has mainly slowed down due to\\nthe relative lack of annotated data and besides, the inherent constraints related to the\\nacquisition process. These limitations are even more pronounced given the volume-\\ntry of medical imaging data. In this paper, we introduce an e fficient way to transfer\\nthe efficiency of a 2D classification network trained on natural images to 2D, 3D uni-\\nand multi-modal medical image segmentation applications. In this direction, we de-\\nsigned novel architectures based on two key principles: weight transfer by embedding\\na 2D pre-trained encoder into a higher dimensional U-Net, and dimensional transfer'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='signed novel architectures based on two key principles: weight transfer by embedding\\na 2D pre-trained encoder into a higher dimensional U-Net, and dimensional transfer\\nby expanding a 2D segmentation network into a higher dimension one. The proposed\\nnetworks were tested on benchmarks comprising di fferent modalities: MR, CT, and\\nultrasound images. Our 2D network ranked first on the CAMUS challenge dedicated\\nto echo-cardiographic data segmentation and surpassed the state-of-the-art. Regard-\\ning 2D/3D MR and CT abdominal images from the CHAOS challenge, our approach\\nlargely outperformed the other 2D-based methods described in the challenge paper on\\nDice, RA VD, ASSD, and MSSD scores and ranked third on the online evaluation plat-\\nform. Our 3D network applied to the BraTS 2022 competition also achieved promis-\\ning results, reaching an average Dice score of 91.69% (91.22%) for the whole tumor,\\n83.23% (84.77%) for the tumor core and 81.75% (83.88%) for enhanced tumor using'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='ing results, reaching an average Dice score of 91.69% (91.22%) for the whole tumor,\\n83.23% (84.77%) for the tumor core and 81.75% (83.88%) for enhanced tumor using\\nthe approach based on weight (dimensional) transfer. Experimental and qualitative re-\\nsults illustrate the e ffectiveness of our methods for multi-dimensional medical image\\nsegmentation.\\nKeywords: Medical image segmentation, transfer learning, convolutional neural\\nnetworks, cross-dimensional transfer.\\nThe final version of the article is available at this link. August 1, 2023\\narXiv:2307.15872v1  [eess.IV]  29 Jul 2023H. Messaoudi et al. 2\\n1. Introduction\\nDeep learning owes its exponential success to the evolution of technological equip-\\nment, learning algorithms, and the availability of large datasets. The approaches that\\nare derived from this have evolved the state-of-the-art in several fields such as image\\nclassification (Li et al., 2014), image segmentation (Rehman et al., 2020, Minaee et al.,'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='are derived from this have evolved the state-of-the-art in several fields such as image\\nclassification (Li et al., 2014), image segmentation (Rehman et al., 2020, Minaee et al.,\\n2021), image annotation (Shin et al., 2016) and computer-aided diagnosis (Suk et al.,\\n2013). Currently, in computer vision, the use of convolutional neural networks (CNN)\\nis predominant (Zegour et al., 2023). This is mainly due to their ability to discover hier-\\narchical representations of data, making them perfectly suitable for any image-related\\ntask. Medical imaging has been widely used over the past years for early cancer detec-\\ntion (Takiddin et al., 2021, Shah et al., 2022), drug development (Liang et al., 2020),\\nand treatment care (Iqbal et al., 2021). Accurate image segmentation plays a pivotal\\nrole in cancer diagnosis and can improve long-term survival rates. However, the direct\\ninterpretation of a large set of medical images can be tedious. The need for automation'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='role in cancer diagnosis and can improve long-term survival rates. However, the direct\\ninterpretation of a large set of medical images can be tedious. The need for automation\\nis more than necessary, especially with the emergence of high-dimensional data.\\nHowever, deep learning networks require a large amount of data to be able to gen-\\neralize effectively. In medical imaging, there is still a latent lack of high-quality an-\\nnotated data. This is mainly due to the established regulations imposed in di fferent\\ncountries (Willemink et al., 2020) and the significant cost of manual annotation, the\\nlatter being even more critical when dealing with uni- or multi-modal 3D medical data.\\nIn addition, manual annotation is error-prone and time-consuming at large scale, and\\nthe development of clinically meaningful, accurate, and automatic segmentation sys-\\ntems is even more crucial.\\nOne of the most promising strategies towards robust medical image segmentation'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='the development of clinically meaningful, accurate, and automatic segmentation sys-\\ntems is even more crucial.\\nOne of the most promising strategies towards robust medical image segmentation\\ndeals with transfer learning (Bozinovski, 2020). Its core concept is based on the re-\\nuse of networks pre-trained on a specific task from another application in order to\\nsignificantly save computational resources, accelerate convergence during training and\\nimprove the network efficiency. The underlying assumption is that many extracted fea-\\ntures, especially low-level ones, are usually shared between di fferent image types and\\ntasks. Most research in 3D segmentation is currently focused on improving the train-\\ning process and centered on the exploration of encoder-decoder network architectures\\ncapability (Jiang et al., 2020, Zhao et al., 2020). However, very few studies (Starke\\net al., 2020, Merino et al., 2021) have been carried out on the re-use of pre-trained 2D'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='capability (Jiang et al., 2020, Zhao et al., 2020). However, very few studies (Starke\\net al., 2020, Merino et al., 2021) have been carried out on the re-use of pre-trained 2D\\nnetworks for 3D medical image segmentation, and none to our knowledge has explored\\nthe performance of cross-dimensional transfer learning. The latter designation refers\\nto the use of a sequence of (m + n)-dimensional weights, extrapolated from the weights\\nof a pre-trained m-dimensional network, for the initialization of a segment or a whole\\n(m + n)-dimensional network.\\nIn this paper, we propose several novel network architectures for 2D, 3D uni- and\\nmulti-modal medical image segmentation. First, we develop a 2D U-Net-like archi-\\ntecture using a pre-trained 2D encoder, intended for the segmentation of 2D echo-\\nâˆ—Corresponding author.\\nEmail address: hicham.messaoudi@univ-bejaia.dz (Hicham Messaoudi)H. Messaoudi et al. 3\\ncardiographic data and 3D abdominal organs. We then present the second network of'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='âˆ—Corresponding author.\\nEmail address: hicham.messaoudi@univ-bejaia.dz (Hicham Messaoudi)H. Messaoudi et al. 3\\ncardiographic data and 3D abdominal organs. We then present the second network of\\nour weight transfer learning, which consists of the incorporation of a pre-trained 2D\\nsegmentation network into the core of a 3D U-Net-like architecture. In addition, we\\npropose the dimensional transfer learning which is an extrapolation of 3D weights from\\n2D encoder weights. These weights are then used to initialize the encoder of a 3D U-\\nNet-like architecture. These two networks are designed for the segmentation of 3D uni-\\nand multi-modal data of brain tumors. In this work, the baseline 2D encoder chosen\\nin our experiments is the E fficientNet (Tan and Le, 2019) architecture, but any other\\nclassification network could be considered. Our main contributions are summarized\\nbelow1:\\nâ€¢ Proposal of a variety of ways to re-use a 2D classifier network for 2D /3D seg-\\nmentation purposes.'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='classification network could be considered. Our main contributions are summarized\\nbelow1:\\nâ€¢ Proposal of a variety of ways to re-use a 2D classifier network for 2D /3D seg-\\nmentation purposes.\\nâ€¢ The re-use principles are generic with respect to the choice of the classifier and\\nits application.\\nâ€¢ Exploitation of these networks on several medical imaging modalities including\\nultrasound, magnetic resonance (MR), and computed tomography (CT) images.\\nâ€¢ Evaluation and validation of the proposed networks on various benchmarks in-\\ncluding BraTS, CAMUS, and CHAOS, with promising results.\\n1.1. Related works\\nCurrently, CNN architectures have largely conquered the field of image segmen-\\ntation, which can be defined as the classification of a set of pixels or voxels accord-\\ning to a semantic criterion chosen beforehand. Thus, several architectures based on\\ndifferent approaches have been designed, including the encoder-decoder architecture'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='ing to a semantic criterion chosen beforehand. Thus, several architectures based on\\ndifferent approaches have been designed, including the encoder-decoder architecture\\nwhich remains dominant and constitutes the basis of the majority of the currently pro-\\nposed frameworks. In medical image segmentation, the aim is to automatically or\\nsemi-automatically delineate healthy anatomical or pathological tissues for a variety of\\npurposes ranging from simple assisted diagnosis to image-guided surgery.\\nA well-known network architecture and the most popular one for these applications\\nis the U-Net (Ronneberger et al., 2015) architecture. This network is relatively fast and\\nmitigates the limitations due to data scarcity issues. Its architecture is based on an en-\\ncoder, which is a contracting path used for the compression and interpretation of the\\ndata into an internal latent representation. The latter is linked to a symmetrical decoder'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='coder, which is a contracting path used for the compression and interpretation of the\\ndata into an internal latent representation. The latter is linked to a symmetrical decoder\\nwhich is applied to regain spatial coherence. Motivated by scene understanding appli-\\ncations, SegNet (Badrinarayanan et al., 2017) appeared after and is made of an encoder\\narchitecture that is topologically similar to VGG-16 and uses skip-connections as in\\nU-Net. A major distinction between them is that SegNet uses less memory since skip-\\nconnections from the encoder derive from the compressed feature maps by pooling\\nlayers whereas U-Net uses full-scale feature maps. The attention U-Net (Oktay et al.,\\n1The code and additional resources are available at this link.H. Messaoudi et al. 4\\n2018b) introduces a novel attention gate (AG) model, motivated by the empirical ob-\\nservation of its usefulness in extracting relevant features and its ability to automatically'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='2018b) introduces a novel attention gate (AG) model, motivated by the empirical ob-\\nservation of its usefulness in extracting relevant features and its ability to automatically\\nlearn to focus on target structures of varying morphology. U-Net++ (Zhou et al., 2020)\\nis an architecture that aims at improving the vanilla U-Net and takes mainly advantage\\nof redesigned skip-connections, where the feature maps of the encoder are enriched by\\nintermediate convolution layers. A deep supervision scheme was also used for better\\noptimization of the learning process of the decoder and to get an earlier representa-\\ntion of output predictions. As an improvement to U-Net ++, U-Net3+ was developed\\n(Huang et al., 2020) based on full-scale skip-connections to explore su fficient infor-\\nmation and better estimate both organ position and morphology. The architecture was\\ndesigned with reduced parameter complexity compared to U-Net ++ to improve the\\ncomputational efficiency.'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='mation and better estimate both organ position and morphology. The architecture was\\ndesigned with reduced parameter complexity compared to U-Net ++ to improve the\\ncomputational efficiency.\\nFurthermore, during the di fferent segmentation-based competitions, many archi-\\ntectures have been developed to tackle a variety of challenges. In the brain tumor\\nsegmentation competition (BraTS) 2020 (Bakas et al., 2017, 2018, Menze et al., 2015)\\nand to enable diversity in prediction results, Henry et al. (2021) presented two net-\\nworks with similar architecture inspired by the 3D U-Net (C Â¸ ic Â¸ek et al., 2016) using\\ntwo different training approaches. The initial convolutional block was set to 48 chan-\\nnels for better use of the original spatial information before compression. They used\\ntwo sets of networks along with a deep supervision scheme and a stochastic weight\\naveraging (Izmailov et al., 2018) approach to improve the predictive power of their net-'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='two sets of networks along with a deep supervision scheme and a stochastic weight\\naveraging (Izmailov et al., 2018) approach to improve the predictive power of their net-\\nworks. Another 3D U-Net-based architecture was developed by Ahmad et al. (2021)\\nwith reduced parametric complexity and residual Inception blocks to learn multi-scale\\ncontexts. Dense connections were also used with a constrained growth-rate to limit un-\\nnecessary features. Dilated convolutions were employed in the encoder to expand the\\nsize of the receptive field in the feature maps leading to promising results. Yuan (2021)\\nintroduced SA-Net, a 3D U-Net inspired architecture that replaces the long-range skip\\nconnections between the same scale with full-scale skip-connections to make full use\\nof original feature maps. SA-Net came with a dynamic scale attention mechanism that\\nensures a correct appreciation of the significance of each feature map.'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='of original feature maps. SA-Net came with a dynamic scale attention mechanism that\\nensures a correct appreciation of the significance of each feature map.\\nHowever, the previously described networks are usually randomly trained, which\\ncan be sub-optimal in terms of computation time and performance, and for tasks in-\\nvolving a limited amount of annotated data. The weight initialization scheme used\\nbeing a critical aspect for the generalization of deep neural networks, several works\\nhave shown a way to overcome those problems through the use of pre-trained network\\nweights arising from larger datasets. The fact that these encoders were not trained on\\na closer domain to the targeted task does not prevent the clear improvement of training\\nand test scores. The superiority of this approach has been empirically confirmed when\\ncompared to the use of random initialization. With TernausNet, a classical U-Net along\\nwith incorporated pre-trained VGG-11, Iglovikov and Shvets (2018) showed a signif-'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='compared to the use of random initialization. With TernausNet, a classical U-Net along\\nwith incorporated pre-trained VGG-11, Iglovikov and Shvets (2018) showed a signif-\\nicant improvement in network performance compared to using random initialization\\non aerial imaging data (Maggiori et al., 2017). In the second version of TernausNet,\\nthe authors replaced the encoder with a pre-trained ABN WideResnet-38 (Bul `o et al.,\\n2017) and showed superior results (Iglovikov et al., 2018) in comparison to other meth-\\nods employed on DeepGlobe-CVPR (Demir et al., 2018), a challenge dedicated to the\\nsegmentation, classification, and detection of satellite images. Conze et al. (2020) ex-H. Messaoudi et al. 5\\ntended this idea to the medical field by using a VGG-16 pre-trained on ImageNet (Deng\\net al., 2009) as part of the encoder of a U-Net architecture (v16U-Net). The results\\nshowed a substantial gain in performance over classical U-Net and fully randomly ini-'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='et al., 2009) as part of the encoder of a U-Net architecture (v16U-Net). The results\\nshowed a substantial gain in performance over classical U-Net and fully randomly ini-\\ntialized v16U-Net for pathological shoulder muscle MR segmentation purposes. The\\nsame findings arose with a deeper network based on VGG-19 in (Conze et al., 2021)\\nfor abdominal multi-organ (liver, kidneys, spleen) segmentation from CT scans.\\nFigure 1: Graphical summary of the proposed approaches and architectures. From top to bottom : weight\\ntransfer learning (WTL) approach with the proposed 2D (Omnia-Net) and 3D (DS-Net) segmentation\\nnetworks. Below is the dimensional transfer learning (DTL) approach along with an instance of its\\napplication using the proposed DX-Net.\\nOne of the most promising strategies in network modeling developed in recent years\\ndeals with the introduction of a scaling method that uniformly adjusts depth, width,\\nand resolution dimensions by using a compound coe fficient. Therefore, a new fam-'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='deals with the introduction of a scaling method that uniformly adjusts depth, width,\\nand resolution dimensions by using a compound coe fficient. Therefore, a new fam-\\nily of architectures referred to as E fficietNet (Tan and Le, 2019) was created through\\nneural architecture search (Zoph and Le, 2016) and demonstrated outstanding accu-\\nracy and parameter e fficiency on image classification tasks. These networks are also\\nhighly effective when transferred to an image segmentation context and several works\\nhave been carried out on this path. Yang et al. (2021) proposed E fficientU-Net++ for\\nthe segmentation of melanoma skin lesions, which is a combination of a U-Net ++\\nand a pre-trained E fficientNet with re-designed skip-connections for the aggregation\\nof features from varying scales. E fficientUNet++ showed higher scores than U-Net\\nor U-Net++ in that particular task. Huynh and Boutry (2020) used the same strategy'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='of features from varying scales. E fficientUNet++ showed higher scores than U-Net\\nor U-Net++ in that particular task. Huynh and Boutry (2020) used the same strategy\\nfor the detection and segmentation of artifacts and diseases in endoscopy images and\\nreplaced the U-Net++ encoder with an EfficientNet towards more accurate feature ex-\\ntraction. They also highlighted the influence of test-time augmentation (TTA) on the\\nimprovement of segmentation results.\\nWang et al. (2021) introduced EAR-U-Net for automatic liver segmentation in CT,\\nwhich is a 2D U-Net like architecture using E fficientNet-B4 for the encoder part andH. Messaoudi et al. 6\\nMethod Network Domain Dataset Ranking\\nWeight Transfer\\nLearning\\nOmnia-Net\\nDS-Net\\n2D echo-cardiography\\n3D abdominal\\n3D multi-modal\\nbrain tumors\\nCAMUS\\nCHAOS\\nBraTS\\n1st\\n3rd\\n-\\nDimensional Transfer\\nLearning DX-Net 3D multi-modal\\nbrain tumors BraTS -\\nTable 1: Summary of the different proposed approaches and networks and their ranking according to the'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='CHAOS\\nBraTS\\n1st\\n3rd\\n-\\nDimensional Transfer\\nLearning DX-Net 3D multi-modal\\nbrain tumors BraTS -\\nTable 1: Summary of the different proposed approaches and networks and their ranking according to the\\ndataset used and their domain of application.\\nan attention gate scheme in the skip-connections to discriminate the useful features\\nfrom the irrelevant ones. With the aim of developing a fully-automated and e fficient\\nCOVID-19 detection system, Chowdhury et al. (2021) proposed the ECOVNet archi-\\ntecture, which uses an E fficientNet with weights pre-trained on ImageNet. Its classi-\\nfication performance was promising and showed a clear improvement using di fferent\\nensemble strategies along with a visualization technique that is provided to highlight\\nthe data features related to class distinction. In our previous work (Messaoudi et al.,\\n2021), by including the E fficientNet architecture as part of the encoding branch, we'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='the data features related to class distinction. In our previous work (Messaoudi et al.,\\n2021), by including the E fficientNet architecture as part of the encoding branch, we\\nproposed an asymmetric 3D U-Net architecture for the segmentation of brain tumors.\\nThe first layers of the encoder were devoted to reducing the depth dimension to fit the\\n2D EfficientNet input. Experimental results on validation and test data showed that the\\nproposed method achieves promising performance.\\nHowever, the weights produced by EfficientNet in a supervised training context can\\nbe improved, as shown by Xie et al. (2020) through noisy student training, which is a\\nsemi-supervised learning technique that, unlike knowledge distillation (Romero et al.,\\n2015, Hinton et al., 2015), uses a student network of a same or larger size with added\\nnoise. In order to take advantage of the weights of existing pre-trained 2D networks,\\nMerino et al. (2021) proposed to derive their 3D architectural versions for classifica-'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='noise. In order to take advantage of the weights of existing pre-trained 2D networks,\\nMerino et al. (2021) proposed to derive their 3D architectural versions for classifica-\\ntion purposes. The comparison between 2D and 3D classification networks, with and\\nwithout transferred weights showed a significant improvement when extrapolating 3D\\nweights from 2D, up to a score difference ratio of 18% for the 3D version of the Incep-\\ntion ResNet architecture.\\nThe limitations of the above-mentioned methods for image segmentation are that\\nthey either require a large amount of data to train a network from scratch or that they\\nare limited in terms of performance using the 2D weights transfer. In this work, we\\npropose a novel method for 2D and 3D medical image segmentation that overcomes\\nthese limitations. In Sect. 2, we describe the proposed approaches followed by the\\narchitectures derived from them. We start with the weight transfer learning, where'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='these limitations. In Sect. 2, we describe the proposed approaches followed by the\\narchitectures derived from them. We start with the weight transfer learning, where\\nwe introduce the Omnia-Net architecture for 2D echo-cardiographic segmentation and\\n3D segmentation of abdominal organs (CT, MR), along with the DS-Net architecture\\nfor brain tumor segmentation in a 3D multi-modal setting. In the same context, we\\nintroduce the dimensional transfer learning using the DX-Net architecture. In Sect. 3,\\nwe display the results obtained through statistical and graphical evidences to show the\\neffectiveness of our approaches. We discuss the findings and report our interpretations\\nbefore concluding in Sect. 4.H. Messaoudi et al. 7\\n2. Material and methods\\nWe introduce in this section the two proposed transfer learning principles that are:\\nweight and dimensional transfer learning, from which three networks are derived. The'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='2. Material and methods\\nWe introduce in this section the two proposed transfer learning principles that are:\\nweight and dimensional transfer learning, from which three networks are derived. The\\nfirst one aims at re-using the weights of a 2D classification network for 2D medical\\nimage segmentation tasks and can be extended to adjacent problems. The second net-\\nwork is defined as the integration of a 2D segmentation architecture into a 3D one\\nand is intended for 3D brain tumor segmentation. The last one can be defined as the\\n3D transformation of 2D weights from a classification network, and their use for the\\ninitialization of the encoder of a 3D U-Net-like network (Tab. 1).\\n2.1. Weight transfer learning\\nIn the following section, we introduce the weight transfer learning (WTL) ap-\\nproach, which can be formulated as a generalization of standard transfer learning. The\\nWTL approach is based on the re-use of a pre-trained network on a di fferent task or'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='proach, which can be formulated as a generalization of standard transfer learning. The\\nWTL approach is based on the re-use of a pre-trained network on a di fferent task or\\nparadigm without any alteration of its weights in the process. This can be performed\\nthrough classical transfer learning such as the re-use of a 2D network on a di fferent\\nclassification task, or its incorporation into a network designed to address a di fferent\\nproblem (as in the case for segmentation networks with pre-trained encoders), or even\\nits incorporation into a higher dimensional network\\nFigure 2: Schematic illustration of the proposed 2D network architecture (Omnia-Net).\\nWe introduce the use of a pre-trained 2D classification network, as an encoder of\\na U-Net-like architecture, which we refer to as Omnia-Net due to its suitability for\\nuse in both 2D and 3D contexts and its maintained performance in various modalities.\\nThe Omnia-Net encoder architecture is slightly modified to incorporate a convolutional'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='use in both 2D and 3D contexts and its maintained performance in various modalities.\\nThe Omnia-Net encoder architecture is slightly modified to incorporate a convolutional\\nblock to take advantage of the full-scale characteristics of the input images as shown\\nin Fig. 2. We evaluate its e fficiency and validate its potential for the segmentation\\nof 2D echo-cardiographic data from the CAMUS dataset (Leclerc et al., 2019) and\\nin the context of 3D abdominal organ MR and CT segmentation using the CHAOS\\ndataset (Kavur et al., 2019, 2021). Note that only the encoder part of Omnia-Net is pre-\\ntrained. Finally, we incorporate Omnia-Net into a higher dimensional architecture, the\\ndimensionally-stacked network (DS-Net) as shown in Fig. 4. The DS-Net is intended\\nfor 3D multi-modal brain tumor segmentation. We assess its performance using the\\nBraTS 2022 dataset (Baid et al., 2021).H. Messaoudi et al. 8\\n2.1.1. 2D echo-cardiographic image segmentation'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='for 3D multi-modal brain tumor segmentation. We assess its performance using the\\nBraTS 2022 dataset (Baid et al., 2021).H. Messaoudi et al. 8\\n2.1.1. 2D echo-cardiographic image segmentation\\nTo further support the empirical evidence of the generalizability power of pre-\\ntrained networks over randomly initialized ones, we validate our network using several\\ndatasets. CAMUS is the first used for this purpose. It is the largest publicly available\\nfully-annotated 2D echo-cardiographic dataset. It consists of clinical images acquired\\nfrom 500 different patients with di fferent chamber views using optimized acquisition\\ntechniques to enable the evaluation of left ventricular ejection fraction measurements\\nalong with a wide variability of acquisition settings.\\nFigure 3: Visualizations of ED (top) and ES (down) samples of the CAMUS test set. From left to right:\\necho-cardiographic data and its corresponding predicted annotation by Omnia-Net. The epicardium and'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='echo-cardiographic data and its corresponding predicted annotation by Omnia-Net. The epicardium and\\nendocardium of the left ventricle and left atrium wall are shown respectively in blue, red, and green.\\nThe dataset is divided into images containing end diastole (ED) and end systole\\n(ES) phases for both 2 and 4 chambers views, each data including information relative\\nto the characteristics and quality of the images. Three cardiologists (O1, O2, O3) man-\\nually annotated the test set, while O1 manually annotated the entire dataset (train and\\ntest). Also, the challenge organizers did not stop at the representation of inter-observer\\nvariability but were also interested in the intra-observer variability by providing man-\\nual annotation of O1â€™s test set at 7-month interval (O1a, O1b). Samples of the test set\\ncan be seen in Fig. 3.\\nFor deep learning methods, U-Net 1 and U-Net 2 (Leclerc et al., 2019) were trained'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='ual annotation of O1â€™s test set at 7-month interval (O1a, O1b). Samples of the test set\\ncan be seen in Fig. 3.\\nFor deep learning methods, U-Net 1 and U-Net 2 (Leclerc et al., 2019) were trained\\nby the challenge creators. Regarding these two variants of the original U-Net, one is\\noptimized for speed with 2M parameters and the other one for accuracy with 17.5M\\nparameters. U-Net 1 is used as a segmentation module in the anatomically constrained\\nneural network (ACNN) (Oktay et al., 2018a). The stacked Hourglass network (Newell\\net al., 2016) has also been used for comparison purposes, it included a set of 3 encoder-\\ndecoder networks along with a deep supervision scheme. The final segmentation resultsH. Messaoudi et al. 9\\nwere defined by the output of the third network. The architecture of the U-Net ++\\nhas been adapted to obtain the best results on this particular dataset. The same data\\npre-processing and post-processing methods have been applied to all the mentioned'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='has been adapted to obtain the best results on this particular dataset. The same data\\npre-processing and post-processing methods have been applied to all the mentioned\\nnetworks (Leclerc et al., 2019), contrary to our methodology where no post-processing\\nis employed.\\nIn our work, we focus on using the E fficientNet-B0 as a baseline. More complex\\nversions of the network could also be used, taking into account the adaptation of both\\nresolution and regularization (dropout, more aggressive data augmentation) schemes.\\nWe opt for the implementation of (Wightman et al., 2022) for the encoder part. Each\\nlayer of the decoder is a succession of nearest neighbor interpolation, concatenation,\\ntwo convolutional layers with a kernel size of 3, each followed by a batch normalization\\nand ReLU activation. For data pre-processing, we opt for an independent normalization\\nof each sample of the dataset from which we subtract the mean of the image, divided'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='and ReLU activation. For data pre-processing, we opt for an independent normalization\\nof each sample of the dataset from which we subtract the mean of the image, divided\\nby the standard deviation. The output activation function of the network is a Softmax\\nin our case.\\nFigure 4: Schematic illustration of the proposed DS-Net architecture. The intra-slice encoder as well as the\\ndecoder consists of a succession of convolutional blocks with SiLU-activated instance normalization.\\n2.1.2. 3D abdominal organ MR segmentation\\nDuring the last years, several deep learning methods have been proposed for the\\nsegmentation of abdominal organs. The most common approaches are based on U-Net\\n(Ronneberger et al., 2015), fully convolutional networks (Shelhamer et al., 2017) or\\nMask R-CNN (Jin et al., 2021). These methods show a very good performance in terms\\nof segmentation quality but require large amounts of annotated data to achieve this level'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='Mask R-CNN (Jin et al., 2021). These methods show a very good performance in terms\\nof segmentation quality but require large amounts of annotated data to achieve this level\\nof performance. Recently, the CHAOS dataset (Kavur et al., 2021) has been created to\\nassess the performance of automatic organ segmentation algorithms on abdominal CT\\nand MR images. It contains annotated abdominal organ data from 80 healthy patients,\\ncomposed of 40 CT and MR data consisting of two di fferent sequences (T1 and T2).\\nTwo samples from the test set can be seen in Fig. 5. The CHAOS challenge is divided\\ninto 5 tasks. Task 1 is a multi-modal binary segmentation of the liver, the goal being\\nto create a system able to take as inputs CT or MR images of di fferent sequences and\\nto infer their correct masks using a single system. Tasks 2 and 3 are binary CT and\\nMR liver segmentation tasks, respectively. Task 5 represents a generalization of task 3'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='to infer their correct masks using a single system. Tasks 2 and 3 are binary CT and\\nMR liver segmentation tasks, respectively. Task 5 represents a generalization of task 3\\nto other organs (left and right kidneys, spleen). Task 4 is a generalization of task 1 to\\nother organs. In this context, the CT images include only the liver class, while the MR\\nimages include the four abdominal organs.H. Messaoudi et al. 10\\nFigure 5: Visualizations on two samples from the CHAOS training set (Kavur et al., 2021). From left to\\nright: liver, right and left kidney, and spleen in MR T1-in and T1-out modalities and their corresponding\\ndelineations are shown respectively in red, yellow, blue, and green.\\nCommonly, using a 2D network to segment higher dimensional data may seem\\ncounter-intuitive due mainly to the loss of spatial coherence. Nevertheless, Isensee\\net al. (2017) showed a clear performance degradation using a 3D network for the seg-'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='counter-intuitive due mainly to the loss of spatial coherence. Nevertheless, Isensee\\net al. (2017) showed a clear performance degradation using a 3D network for the seg-\\nmentation of anisotropic data. In addition, several 2D architectures during the CHAOS\\ncompetition were employed, as is the case for Conze et al. (2021) using a condi-\\ntional generative adversarial network and Ernst et al., with a modified Attention 2D\\nU-Net (Abraham and Khan, 2018) along with multi-scaled input image pyramid to en-\\nhance feature representation. Pham et al used a different approach with an architecture\\ncomposed of three modules (auto-encoder, Hourglass network, 2D U-Net), this scheme\\nbeing used to enhance the organ localization capabilities of the automated approach.\\nThese methods have all performed more or less e fficiently, some better than others on\\nthe different challenge tasks. This shows the e ffectiveness of these approaches in spe-'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='These methods have all performed more or less e fficiently, some better than others on\\nthe different challenge tasks. This shows the e ffectiveness of these approaches in spe-\\ncific instances but does not really show the particular element that made them perform\\nbetter on certain tasks when compared to others.\\n2.1.3. 3D multi-modal brain tumor MR segmentation\\nBrain tumor segmentation is a crucial task for monitoring patient diagnosis and cor-\\nrect disease isolation. In order to establish a concrete context for the listing and ranking\\nof the various algorithms and approaches proposed for this purpose, the multi-modal\\nbrain tumor segmentation (BraTS) 2022 dataset (Baid et al., 2021) was created. The\\ndataset includes a large number of low- and high-grade gliomas data obtained from\\nroutine multi-institutional multi-modal MR imaging scans, subdivided into training,\\nvalidation, and test sets. Each data volume consists of 4 multi-modal scans which are'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='routine multi-institutional multi-modal MR imaging scans, subdivided into training,\\nvalidation, and test sets. Each data volume consists of 4 multi-modal scans which are\\nnative (T1), post-contrast T1-weighted (T1ce), T2-weighted (T2), and T2 Fluid Atten-H. Messaoudi et al. 11\\nFigure 6: Schematic illustration of the proposed DX-Net architecture. T-Conv and IN respectively stand for\\ntransposed convolution (Shelhamer et al., 2016) and instance normalization (Ulyanov et al., 2016).\\nuated Inversion Recovery (FLAIR) volumes. The di fferent regions considered for the\\nevaluation include: the enhancing tumor (ET) which is depicted by the areas that show\\nhyper-intensity in T1ce when compared to T1, the tumor core (TC) which is the part\\nthat is typically resected surgically and includes the ET region as well as the necrotic\\n(fluid-filled) and non-enhancing (solid) regions of the tumor core (NET/Ncr), the latter\\nbeing hypo-intense in T1ce when compared to T1 and the whole tumor (WT) which is'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='(fluid-filled) and non-enhancing (solid) regions of the tumor core (NET/Ncr), the latter\\nbeing hypo-intense in T1ce when compared to T1 and the whole tumor (WT) which is\\nthe combined TC region and the peritumoral edema (ED), that can be identified by the\\nhyper-intense signal in FLAIR. Training samples can be seen in Fig. 7.\\nAs shown in Fig. 4, two phases comprise the encoding and decoding processes.\\nFirst, we encode 3D data into 2D data, preserving the original dimensions of height and\\nwidth and just compressing the depth to a limited number of channels. Second, the data\\nis being processed to be adapted as an input to the 2D EfficientNet, which is the second\\nencoding stage. The output of the 2D pre-trained encoder is a tensor with a batch size\\nof 4 and 3 channels which is transmitted to a 2D decoder consisting at each level of\\nan upsampling layer processing a nearest neighbor interpolation followed by a skip-'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='of 4 and 3 channels which is transmitted to a 2D decoder consisting at each level of\\nan upsampling layer processing a nearest neighbor interpolation followed by a skip-\\nconnection and two blocks comprising a convolutional layer, a batch normalization\\nlayer, and a ReLU activation function. The output of the 2D decoder is not activated\\nand is transmitted after adequate transformation to the 3D decoder. The latter follows\\nthe same structure as the 2D decoder, except that it uses instance normalization layers\\ninstead of batch normalization, which is considered much more adequate given the\\nused batch size for the original data.\\n2.2. Dimensional transfer learning\\nIn this section, instead of processing data as 2D axial images, we directly em-\\nploy volumetric medical images including multiple channels in a multi-modal image\\nsegmentation context. We propose a novel image segmentation architecture based on\\ndimensional transfer learning referred to as Dimensionally-eXpanded network (DX-'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='segmentation context. We propose a novel image segmentation architecture based on\\ndimensional transfer learning referred to as Dimensionally-eXpanded network (DX-\\nNet). As displayed in Fig. 6, DX-Net is a U-Net-like architecture including a modifiedH. Messaoudi et al. 12\\nFigure 7: Visualizations on the 440th MR T2 data of the BraTS 2022 training set (Baid et al., 2021). From\\ntop to bottom: axial, sagittal, and coronal views are shown with their corresponding ground truth. Edema is\\nshown in green, enhancing tumor in yellow, and necrosis/non-enhancing tumor in red.\\n3D EfficientNet encoder. The initialization of its weights is done using the weights of\\nits 2D equivalent. Each set of weights is translated into 3D by concatenating the 2D\\nweights repeatedly over the 3D depth. The 2D weights are drawn from a noisy-student\\ntraining (Xie et al., 2020) on natural images (ImageNet). The particular choice of these'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='weights repeatedly over the 3D depth. The 2D weights are drawn from a noisy-student\\ntraining (Xie et al., 2020) on natural images (ImageNet). The particular choice of these\\nweights is motivated by our belief that a network with solid performance on natural im-\\nage classification will be better suited to perform feature extraction as an encoder for\\nan image segmentation task. Therefore, we hypothesize a performance transfer across\\nadjacent tasks. The DX-Net decoder, on the other hand, is randomly initialized and\\nis described in much more detail in the following section. DX-Net is intended for 3D\\nmulti-modal medical image segmentation. We evaluate its performance using the 3D\\nMR brain tumor segmentation dataset (BraTS).\\n2.2.1. 3D multi-modal brain tumor MR segmentation\\nMotivated by the favorable results obtained by the weight transfer of pre-trained 2D\\nencoders in the context of the segmentation of higher dimensional medical images, weH. Messaoudi et al. 13'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='Motivated by the favorable results obtained by the weight transfer of pre-trained 2D\\nencoders in the context of the segmentation of higher dimensional medical images, weH. Messaoudi et al. 13\\npropose in this part a straightforward approach to re-use the weights of those networks.\\nIt consists of the transformation of pre-trained classification network 2D weights into\\n3D. The produced weights are used to initialize the equivalent 3D architecture param-\\neters. This process has already been applied in the context of classification (Merino\\net al., 2021) and yields excellent results when compared to a random initialization, the\\ndifference being that our objective is to use this dimensional transfer approach in an\\nimage segmentation context, and also apply the weights resulting from a noisy student\\ntraining, instead of those acquired through a classical training on ImageNet.\\nThe field of 3D medical data segmentation already su ffers from a latent lack of'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='training, instead of those acquired through a classical training on ImageNet.\\nThe field of 3D medical data segmentation already su ffers from a latent lack of\\nannotated data and costly computational power. Therefore, this approach represents\\na favorable opportunity to drastically reduce the cost and training time of higher di-\\nmensional convolutional networks. In the network depicted in Fig. 6, the first block is\\na convolution layer followed by an instance normalization and a Sigmoid linear unit\\n(SiLU) activation function. The main idea is to capture the spatial features of the in-\\nputs before down-sampling in order to improve the reconstruction of the segmented\\nregions through skip-connections. The remaining part of the encoder is a 3D version\\nof EfficientNet-B0, the choice of the architecture being mainly motivated by the low\\nresolution of MR data. For the initialization of this part of the network, we use the'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='of EfficientNet-B0, the choice of the architecture being mainly motivated by the low\\nresolution of MR data. For the initialization of this part of the network, we use the\\nnoisy student weights of its 2D counterpart. The transformation of these weights into\\n3D is performed by projecting the information of the 2D learnable parameters onto\\nthe depth of their 3D counterpart. The batch normalization layers are all replaced by\\ninstance normalization layers since we are using a batch size of 1. Each block of the\\ndecoder comprises a transposed 3D convolution layer followed by a succession of two\\nblocks consisting of 3 Ã—3 convolution layer, instance normalization, and SiLU activa-\\ntion. The output layer is followed by a Sigmoid activation to binarize the predictions.\\nThe training strategy is the same as the one used in the weight transfer section.\\n3. Results and discussion\\nIn this section, we introduce the results of the methods mentioned in Sect. 2 by\\nfollowing the same order of presentation.'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='3. Results and discussion\\nIn this section, we introduce the results of the methods mentioned in Sect. 2 by\\nfollowing the same order of presentation.\\n3.1. Weight transfer learning\\nWe start by reporting the results obtained with the weight transfer learning approach\\non 2D echo-cardiographic and 3D abdominal data as well as 3D multi-modal brain\\ntumor data, while discussing the obtained results.\\n3.1.1. 2D echo-cardiographic image segmentation\\nIn the original paper (Leclerc et al., 2019), CAMUS is presented as a variable\\nimage quality dataset. In our work, we show experimentally that data quality has little\\nto no impact on the performance of model optimization as long as the data is uniformly\\nannotated.\\nSeveral metrics are used to evaluate the performance of the proposed methods, on\\nimage segmentation and volume estimation of multi-chamber view ultrasound images.H. Messaoudi et al. 14\\nManual annotation : Training set - Patient 009 ED 2Ch.'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='image segmentation and volume estimation of multi-chamber view ultrasound images.H. Messaoudi et al. 14\\nManual annotation : Training set - Patient 009 ED 2Ch.\\nBest : Test set - Patient 029 ED 4Ch - Dice Scores : Endo : 0.97. Epi : 0.98. LA :\\n0.93.\\nWorse : Test set - Patient 050 ED 2Ch - Dice Scores : Endo : 0.92. Epi : 0.92. LA :\\n0.68.\\nFigure 8: Qualitative results on CAMUS dataset (Leclerc et al., 2019). Cases were selected as manual\\nannotations from the training set, best and worst from the test set. Within each row: input data 2- or\\n4-chambers view at end-diastolic (ED) phase and their corresponding annotations on the right where\\nepicardium and endocardium of the left ventricle and left atrium wall are shown respectively in blue, red,\\nand green.\\nDice score is a standard metric for evaluating the performance of image segmentation\\nmethods. The Dice score is defined as follows:\\nDice(A, B) = 2|A âˆ© B|\\n|A| + |B| , (1)'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='and green.\\nDice score is a standard metric for evaluating the performance of image segmentation\\nmethods. The Dice score is defined as follows:\\nDice(A, B) = 2|A âˆ© B|\\n|A| + |B| , (1)\\nwhere |.| denotes the cardinality, A the segmentation mask of the ground truth andB the\\npredicted segmentation mask. The Dice score ranges between 0 and 1, with a higher\\nvalue indicating better performance. In addition, the mean absolute distance (MAD) is\\nused for performance evaluation. For ai âˆˆ A and bi âˆˆ B, MAD(A, B) is given by:H. Messaoudi et al. 15\\nMAD(A, B) = 1\\nN\\nNX\\ni=1\\n|ai âˆ’ bi|, (2)\\nwhere N refers to the total number of samples, and |.| the absolute value. Lower MAD\\nvalues indicate a more accurate segmentation. The Hausdor ffdistance (HD) is also\\nused to evaluate the performance of segmentation methods. HD is defined as:\\nHD(A, B) = max(h(A, B), h(B, A)) (3)\\nwhere :\\nh(A, B) = max\\naâˆˆA\\nmin\\nbâˆˆB\\nâˆ¥a âˆ’ bâˆ¥ (4)\\nA smaller HD value indicates a better similarity between predicted segmentation results'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='HD(A, B) = max(h(A, B), h(B, A)) (3)\\nwhere :\\nh(A, B) = max\\naâˆˆA\\nmin\\nbâˆˆB\\nâˆ¥a âˆ’ bâˆ¥ (4)\\nA smaller HD value indicates a better similarity between predicted segmentation results\\nand ground truth.\\nMethods\\nED ES\\nLVEndo LVEpi LVEndo LVEpi\\nDice MAD HD Dice MAD HD Dice MAD HD Dice MAD HD\\nO1avsO2 0.919 2.2 6.0 0.913 3.5 8.0 0.873 2.7 6.6 0.890 3.9 8.6\\nO1avsO3 0.886 3.3 8.2 0.943 2.3 6.5 0.823 4.0 8.8 0.931 2.4 6.4\\nO2vsO3 0.921 2.3 6.3 0.922 3.0 7.4 0.888 2.6 6.9 0.885 3.9 8.4\\nO1avsO1b 0.945 1.4 4.6 0.957 1.7 5.0 0.930 1.3 4.5 0.951 1.7 5.0\\nSRF 0.895 2.8 11.2 0.914 3.2 13.0 0.848 3.6 11.6 0.901 3.5 13.0\\nBEASM-fully 0.879 3.3 9.2 0.895 3.9 10.6 0.826 3.8 9.9 0.880 4.2 11.2\\nBEASM-semi 0.920 2.2 6.0 0.917 3.2 8.2 0.861 3.1 7.7 0.900 3.5 9.2\\nU-Net 1 0.934 1.7 5.5 0.951 1.9 5.9 0.905 1.8 5.7 0.943 2.0 6.1\\nU-Net 2 0.939 1.6 5.3 0.954 1.7 6.0 0.916 1.6 5.5 0.945 1.9 6.1\\nACNN 0.932 1.7 5.8 0.950 1.9 6.4 0.903 1.9 6.0 0.942 2.0 6.3\\nSHN 0.934 1.7 5.6 0.951 1.9 5.7 0.906 1.8 5.8 0.944 2.0 6.0'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='U-Net 2 0.939 1.6 5.3 0.954 1.7 6.0 0.916 1.6 5.5 0.945 1.9 6.1\\nACNN 0.932 1.7 5.8 0.950 1.9 6.4 0.903 1.9 6.0 0.942 2.0 6.3\\nSHN 0.934 1.7 5.6 0.951 1.9 5.7 0.906 1.8 5.8 0.944 2.0 6.0\\nU-Net++ 0.927 1.8 6.5 0.945 2.1 7.2 0.904 1.8 6.3 0.939 2.1 7.1\\nOmnia-Net (ours)0.945 1.5 4.8 0.956 1.8 5.4 0.922 1.6 4.8 0.951 1.8 5.9\\nTable 2: Comparison of image segmentation scores of non-deep and deep learning methods from the\\nCAMUS challenge and the proposed Omnia-Net architecture on CAMUS test set (Leclerc et al., 2019).\\nLVEn: Endocardial contour of the left ventricle; LVEp: Epicardial contour of the left ventricle; ED: End\\ndiastole; ES: End systole; Dice : Dice Score; MAD: Mean Absolute Distance; HD: HausdorffDistance.\\nThe values in bold refer to the best performance for each measure.\\nDuring training, we reserve 80% of the dataset while randomly allocating 20% for\\nthe validation set and we also employ a 5-fold cross-validation scheme to perform an'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='During training, we reserve 80% of the dataset while randomly allocating 20% for\\nthe validation set and we also employ a 5-fold cross-validation scheme to perform an\\noptimal training such that the network can take advantage of all existing characteris-\\ntics in the training set. The Nadam optimizer is employed, which is none other than\\nAdam with Nesterov accelerated gradient. This choice is motivated by the fact that\\nit is theoretically and in most cases empirically superior to Adam (Ruder, 2017). We\\nopt for an initial learning rate of 3 Ã— 10âˆ’4 which is reduced at each epoch by 5% until\\nit reaches 1 Ã— 10âˆ’5, the latter being the critical learning rate defining the end of the\\ntraining. We chose to use a compound loss function (Jadon, 2020) combining Dice\\nand binary cross-entropy (BCE), which is adequate for tasks that require dealing with\\nunbalanced classes (see Eq. 5).H. Messaoudi et al. 16\\nLloss = 1 âˆ’ 2\\nN\\nNX\\nn=0\\nPI\\ni=0 yi,ngi,n\\nPI\\ni=0 yi,n + PI\\ni=0 gi,n + Ïµ\\nâˆ’\\n1\\nN\\nNX\\nn=0\\nIX\\ni=0'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='unbalanced classes (see Eq. 5).H. Messaoudi et al. 16\\nLloss = 1 âˆ’ 2\\nN\\nNX\\nn=0\\nPI\\ni=0 yi,ngi,n\\nPI\\ni=0 yi,n + PI\\ni=0 gi,n + Ïµ\\nâˆ’\\n1\\nN\\nNX\\nn=0\\nIX\\ni=0\\nyi,n log (gi,n) + (1 âˆ’ yi,n) log (1 âˆ’ gi,n) ,\\n(5)\\nwhere N is the number of classes, I the total number of spatial coordinates, yi,n is the\\nSigmoid activated output of the network and gi,n the binary ground truth with respect\\nto class n and ith data coordinate. Ïµ is a small constant to avoid divisions by zero.\\nMoreover, we do not use any additional data for network training as we restrict our\\nstudy to the provided one and we do not perform any data augmentation due to material\\nand time constraints. To measure the reliability of the network during the training, we\\nreserve a random validation set defined as 20% of the dataset being thus large enough\\nto ensure the correct estimation of the model generalization.\\nMethods Corr . coefficient\\nLVEF LVEDV LVESV\\nO1a vs O2 0.801 0.940 0.956\\nO1a vs O3 0.646 0.895 0.860\\nO2 vs O3 0.569 0.926 0.916'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='to ensure the correct estimation of the model generalization.\\nMethods Corr . coefficient\\nLVEF LVEDV LVESV\\nO1a vs O2 0.801 0.940 0.956\\nO1a vs O3 0.646 0.895 0.860\\nO2 vs O3 0.569 0.926 0.916\\nO1a vs O1b 0.896 0.978 0.981\\nSRF 0.465 0.755 0.827\\nBEASM-fully 0.731 0.704 0.713\\nBEASM-semi 0.790 0.886 0.880\\nU-Net 1 0.791 0.947 0.955\\nU-Net 2 0.823 0.954 0.964\\nACNN 0.799 0.945 0.947\\nSHN 0.770 0.943 0.938\\nU-Net++ 0.789 0.946 0.952\\nOmnia-Net (ours) 0.896 0.980 0.974\\nTable 3: Comparison of volume estimation scores of non-deep and deep learning methods from the\\nCAMUS challenge and the proposed Omnia-Net architecture on CAMUS test set Leclerc et al. (2019). The\\nvalues in bold refer to the best performance for each measure.\\nFor comparison purposes, we draw directly on the scores established in the original\\nchallenge. The non-deep learning methods that obtained the best results during the\\nCETUS challenge (Bernard et al., 2016) are also included (Pedrosa et al., 2017). The'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='challenge. The non-deep learning methods that obtained the best results during the\\nCETUS challenge (Bernard et al., 2016) are also included (Pedrosa et al., 2017). The\\nresults that can be seen in Tab. 2 and Tab. 3 are obtained from the online evaluation\\nplatform1 on the test set, for which the ground truth is not available to the general\\npublic. As can be seen in Tab. 2, our network clearly outperforms state-of-the-art\\nmethods for endocardial (+0.6% for Dice, +0.1 for MAD, +0.5 for HD) and epicardial\\n(+0.2% for Dice, +0.3 for HD) left ventricle contour delineations in the end diastole\\n1http://camus.creatis.insa-lyon.fr/challenge/H. Messaoudi et al. 17\\nScores Team NameMean Score DICE DICE Score RA VD (%) RA VD Score ASSD (mm) ASSD Score MSSD (mm) MSSD Score\\nTask 1'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='1http://camus.creatis.insa-lyon.fr/challenge/H. Messaoudi et al. 17\\nScores Team NameMean Score DICE DICE Score RA VD (%) RA VD Score ASSD (mm) ASSD Score MSSD (mm) MSSD Score\\nTask 1\\nLimed (ours)73.62Â±12.66 0.96Â±0.02 96.08Â±1.82 2.78Â±2.28 50.10Â±33.21 1.45Â±1.61 90.36Â±10.76 27.50Â±23.28 57.92Â±25.43OvGUMEMoRIAL 55.78Â±19.20 0.88Â±0.15 83.14Â±28.16 13.84Â±30.26 24.67Â±31.15 11.86Â±65.73 76.31Â±21.13 57.45Â±67.52 31.29Â±26.01PKDIA 50.66Â±23.95 0.85Â±0.26 84.15Â±28.45 6.65Â±6.83 21.66Â±30.35 9.77Â±23.94 75.84Â±28.76 46.56Â±45.02 42.28Â±27.05IITKGP-KLIV 40.34Â±20.25 0.72Â±0.31 60.64Â±44.95 9.87Â±16.27 24.38Â±32.20 11.85Â±16.87 50.48Â±37.71 95.43Â±53.17 7.22Â±18.68\\nTask 2'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='Task 2\\nLimed (ours)82.98Â±6.120.98Â±0.0098.08Â±0.3 1.48Â±0.95 70.31Â±19.00.67Â±0.12 95.54Â±0.79 19.2Â±12.69 68.0Â±21.14PKDIA 82.46Â±8.47 0.98Â±0.00 97.79Â±0.43 1.32Â±1.302 73.6Â±26.440.89Â±0.36 94.06Â±2.37 21.89Â±13.94 64.38Â±20.17OvGUMEMoRIAL 61.13Â±19.72 0.90Â±0.21 90.18Â±21.25 9 x 103Â±4 x 103 44.35Â±35.63 4.89Â±12.05 81.03Â±20.46 55.99Â±38.47 28.96Â±26.73IITKGP-KLIV 55.35Â±17.58 0.92Â±0.22 91.51Â±21.54 8.36Â±21.62 30.41Â±27.12 27.55Â±114.04 81.97Â±21.88 102.37Â±110.9 17.50Â±21.79\\nTask 3\\nLimed (ours)72.88Â±12.15 0.95Â±0.02 95.29Â±1.55 3.12Â±2.38 46.16Â±31.92 1.42Â±0.96 90.55Â±6.37 25.37Â±17.17 59.51Â±23.54PKDIA 70.71Â±6.40 0.94Â±0.01 94.47Â±1.38 3.53Â±2.14 41.8Â±24.85 1.56Â±0.68 89.58Â±4.54 26.06Â±8.20 56.99Â±12.73OvGUMEMoRIAL 41.15Â±21.61 0.81Â±0.15 64.94Â±37.25 49.89Â±71.57 10.12Â±14.66 5.78Â±4.59 64.54Â±24.43 54.47Â±24.16 25.01Â±20.13IITKGP-KLIV 34.69Â±8.49 0.63Â±0.07 46.45Â±1.44 6.09Â±6.05 43.89Â±27.02 13.11Â±3.65 40.66Â±9.35 85.24Â±23.37 7.77Â±12.81\\nTask 4'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='Task 4\\nLimed (ours)61.80Â±16.12 0.90Â±0.0685.45Â±20.837.6Â±5.48 25.29Â±24.71 2.98Â±2.8180.63Â±18.2429.71Â±21.2255.81Â±26.5PKDIA 49.63Â±23.25 0.88Â±0.2185.46Â±25.528.43Â±7.77 18.97Â±29.67 6.37Â±18.9682.09Â±23.9633.17Â±38.9356.64Â±29.11OvGUMEMoRIAL 43.15Â±13.88 0.85Â±0.16 79.10Â±29.51 5 x 103Â±5 x 104 12.07Â±23.83 5.22Â±12.43 73.00Â±21.83 74.09Â±52.44 22.16Â±26.82IITKGP-KLIV 35.33Â±17.79 0.63Â±0.36 50.14Â±46.58 13.51Â±20.33 15.17Â±27.32 16.69Â±19.87 40.46Â±38.26 130.3Â±67.59 8.39Â±22.29\\nTask 5\\nLimed (ours)69.32Â±8.540.93Â±0.03 91.76Â±6.82 7.67Â±4.97 22.38Â±19.49 1.46Â±1.5 91.19Â±6.45 18.83Â±11.11 71.94Â±12.14PKDIA 66.46Â±5.810.93Â±0.02 92.97Â±1.78 6.91Â±3.27 28.65Â±18.05 1.43Â±0.5990.44Â±3.96 20.1Â±5.90 66.71Â±9.38OvGUMEMoRIAL 44.34Â±14.92 0.79Â±0.15 64.37Â±32.19 76.64Â±122.44 9.45Â±11.98 4.56Â±3.15 71.11Â±18.22 42.93Â±17.86 39.48Â±16.67IITKGP-KLIV 25.63Â±5.64 0.56Â±0.06 41.91Â±11.16 13.38Â±11.2 11.74Â±11.08 18.7Â±6.11 35.92Â±8.71 114.51Â±45.63 11.65Â±13.00\\nAverage'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='Average\\nLimed (ours)72.12Â±11.12 0.94Â±0.03 93.33Â±6.26 4.53Â±3.21 42.85Â±25.67 1.6Â±1.4 89.65Â±8.52 24.12Â±17.09 62.63Â±21.75PKDIA 63.98Â±13.58 0.92Â±0.10 90.97Â±11.51 5.37Â±4.26 36.94Â±25.87 4.0Â±8.9 86.40Â±12.72 29.56Â±22.4 57.4Â±19.69OvGUMEMoRIAL 49.11Â±17.87 0.85Â±0.16 76.35Â±29.67 2.8 x 103Â±1 x 104 20.13Â±23.45 6.46Â±19.59 73.20Â±21.21 56.99Â±40.09 29.38Â±23.27IITKGP-KLIV 38.27Â±13.95 0.69Â±0.20 58.13Â±25.13 10.24Â±15.09 25.12Â±24.95 17.6Â±32.11 49.90Â±23.18 105.57Â±60.13 10.51Â±17.71\\nTable 4: Comparison between scores of the 2D networks used in CHAOS challenge, the metrics values are\\ndrawn using the CHAOS online evaluation platform. The given values represent the prediction scores\\nobtained using our method on each task along with the average scores on all tasks. The best results are\\ngiven in bold. ASSD : Average symmetric surface distance; MSSD : Maximum symmetric surface distance;\\nRA VD : Relative absolute volume difference.\\nphase. This improvement is also noticeable in the end systole phase for endocardial'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='RA VD : Relative absolute volume difference.\\nphase. This improvement is also noticeable in the end systole phase for endocardial\\n(+0.6% for Dice, +0.7 for HD) and epicardial ( +0.6% for Dice, +0.1 for MAD, +0.1\\nfor HD) left ventricle contour delineations.\\nIn Tab. 3, the correlation coefficient shows ostentatiously the gain in performance\\nintroduced by Omnia-Net for the ejection fraction (+7.3%), end-diastolic (+2.6%), and\\nend-systolic (+1%) ventricular volume estimation. The closeness of Omnia-Net scores\\nto those of inter- and intra-observer contours and volumes estimation is also pertinent\\nand shows that the method can already be valuable for clinical use. We acknowledge\\nthat the training process can be further enhanced and even expanded, given the fact that\\nthe network is trained on less than 50 epochs. Nevertheless, it proves the generalization\\ncapability of our network with a limited number of epochs and its ability to accurately'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='the network is trained on less than 50 epochs. Nevertheless, it proves the generalization\\ncapability of our network with a limited number of epochs and its ability to accurately\\nreproduce manual annotations with high fidelity, as can be seen in Fig. 8.\\n3.1.2. 3D abdominal organ MR and CT segmentation\\nIn this section, we evaluate the proposed method on the five tasks of the CHAOS\\nchallenge. Network training is done using the 2D axial slices of the CT and MR vol-\\numes. We use the same image size throughout the challenge tasks (512 Ã— 512). We\\nemploy several data augmentations including scaling, rotation, translation, shearing,\\nwindow width/level, and additive Gaussian noise with a probability of occurrence of\\n0.5 for each. The B4 version of the E fficientNet is preferred, thus making a compro-\\nmise between efficiency and computational complexity. It is also more appropriate for\\nthe selected image size. Regarding the parameters optimization, we exploit the same'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='mise between efficiency and computational complexity. It is also more appropriate for\\nthe selected image size. Regarding the parameters optimization, we exploit the same\\ncompound loss function described in Sect. 3.1.1.\\nOur approach is based on relatively the same architecture as described in Sect. 2.1.1,\\nthe only modification made is the substitution of the encoder by the B4 version. We\\nemploy the Nadam optimizer with an initial learning rate of 10 âˆ’3 to avoid an auto-\\nrestriction of the evolution of the model optimization to a local minimum. We opt for\\na learning rate decay method based on the reduction of the learning rate by 5% at eachH. Messaoudi et al. 18\\nFigure 9: Visualizations on two samples from the CHAOS test set. Top to bottom: CT test data and their\\ncorresponding mask predicted by Omnia-Net.\\nepoch until reaching a minimal learning rate which in this case is at a level of 10 âˆ’5.\\nThis method aims at accelerating the convergence of the network in the first epochs of'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='epoch until reaching a minimal learning rate which in this case is at a level of 10 âˆ’5.\\nThis method aims at accelerating the convergence of the network in the first epochs of\\nthe training. The reduction of the learning rate to a critical minimum value mitigates\\nthe appearance of oscillations at later stages of the training which can occur when using\\na constant and relatively high learning rate.\\nIn our experiments, we use a single network throughout the challenge tasks without\\nusing additional data during training. There are two different modalities in the CHAOS\\ndataset, CT and MR (T1-DUAL and T2-SPIR sequences). We use a unique 2D network\\nfor the 5 di fferent tasks and compare our results to the 2D networks that participated\\nin the 5 tasks which are depicted in the challenge paper. As shown in Tab. 4, our\\nnetwork obtains the best averaged scores and particularly stands out in the multi-modal\\nsetting of Task 1, where we can see that our network shows a remarkable generalization'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='network obtains the best averaged scores and particularly stands out in the multi-modal\\nsetting of Task 1, where we can see that our network shows a remarkable generalization\\nperformance with a gap of more than 11% on the Dice score metric and more than 17\\non the average metric, and this when compared to the best scores of the 2D networks\\nshown in the CHAOS challenge paper. This can be due to several factors: we used\\nlarger image sizes (512Ã—512) contrary to the OvGUMEMoRIAL (128x128), IITKGP-\\nKLIV (256 Ã—256) and PKDIA (256 Ã—256 MR, 512 Ã—512 CT) teams which allowed a\\nmore stable training and better use of the learning potential of the network through the\\nuse of a constant input image resolution. It is also worth mentioning that we used a\\nbatch size of 8 unlike the others and the Nadam optimizer which performs better than\\nthe traditional Adam for computer vision tasks.\\nThe network we used is a key element of the performance of our method. We em-'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='the traditional Adam for computer vision tasks.\\nThe network we used is a key element of the performance of our method. We em-\\nployed the EfficientNet as an encoder whose transfer power is no longer to be proven,\\nwe nevertheless added a convolution layer with 16 filters at the beginning of the en-H. Messaoudi et al. 19\\nFigure 10: Visualizations on three samples from the CHAOS test set (Task 5). From top to bottom: liver,\\nright and left kidney, and spleen in T1-in, T1-out, and T2-SPIR along with their according delineation\\nprediction by Omnia-Net, are shown respectively in red, yellow, blue, and green.\\ncoder aiming at improving the performance of full-scale feature extraction. The output\\nof this layer is then sent through skip-connection to the end of the decoder as shown in\\nFig. 2 to allow a better characterization of the segmented elements and to avoid any loss\\nof information or noise that could be generated by the processing of data through the'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='Fig. 2 to allow a better characterization of the segmented elements and to avoid any loss\\nof information or noise that could be generated by the processing of data through the\\nnetwork. This was mainly motivated by the fact that the first layer of the E fficientNet\\ndirectly compresses the input signal and does not take advantage of the full-scale infor-\\nmation. Tab. 4 also illustrates the networkâ€™s ability to achieve a high performance that\\nis generalizable to binary and multi-class cases, in both single and multiple modalities,\\nwhich favors its adoption for real-life systems.\\n3.1.3. 3D multi-modal brain tumor MR segmentation\\nOur segmentation approach follows an encoder-decoder CNN-based architecture\\nwith an embedded pre-trained encoder as introduced in (Messaoudi et al., 2021). We\\nperform a standard pre-processing strategy and crop the image in order to retain only\\nthe voxels containing the brain region and use a patch size of 128 Ã—128Ã—128 for train-H. Messaoudi et al. 20'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='perform a standard pre-processing strategy and crop the image in order to retain only\\nthe voxels containing the brain region and use a patch size of 128 Ã—128Ã—128 for train-H. Messaoudi et al. 20\\ning. We then normalize with z-score the non-zero voxels of each input data channel\\nindependently and use a batch size of 1 to fit GPU memory constraints.\\nDice Score WT TC ET\\nMean 91.69 83.23 81.75\\nMedian 93.45 93.10 89.17\\n25 Quartile 89.51 83.89 81.98\\n75 Quartile 96.09 96.34 94.11\\nTable 5: DS-Net segmentation scores on the BraTS 2022 validation set.\\nInstead of minimizing the loss function using the errors obtained from the NET/Ncr,\\nED, and ET classes, we follow a region-based strategy in which the regions of interest\\nbecome TC, WT, and ET. These regions are the ones that are scored by the challenge\\nevaluation platform. In doing so, we use a Sigmoid activation function instead of Soft-\\nmax at the end of the network since the new target classes are no longer independent.'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='evaluation platform. In doing so, we use a Sigmoid activation function instead of Soft-\\nmax at the end of the network since the new target classes are no longer independent.\\nWe train our network for 90 epochs and binarize the predicted outputs using a threshold\\nof 0.5 and as for CarrÂ´e et al. (2022), we keep the ET unaltered and extract the NET/Ncr\\nfrom the TC and ED from the WT through logical operations.\\nFor network training, we use the sum of Dice and binary cross-entropy loss as\\na loss function, which operates on three class labels: TC, WT, and ET. CNNs often\\nshow a relative improvement during the training process by performing an alteration\\nof the learning rate. Inspired by the step decay strategy of He et al. (2015) and the\\ncosine annealing strategy of Loshchilov and Hutter (2016), we train our network with\\na constant learning rate of 10 âˆ’3 without calculating the validation score until training'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='cosine annealing strategy of Loshchilov and Hutter (2016), we train our network with\\na constant learning rate of 10 âˆ’3 without calculating the validation score until training\\nscore becomes greater than 0.85. After 40 epochs, we use the cosine annealing strategy\\nfor the rest of the training with a minimum learning rate of 10âˆ’5. We opt for the use of\\nthe Nadam optimizer with coefficients betas set to (0.95, 0.99) along with LookAhead\\n(Zhang et al., 2019). The number of fast weights updates is set to 6, and the magnitude\\nof the final parameters (also called LookAhead parameter) is set to 0.5.\\nTaking into account that many LGG data show an absence of enhancing tumor\\nclass, thus resulting in a high probability of a false positive that can be generated from\\nthe network during inference, we adopt a thresholding strategy targeting the ET class.\\nIf the ET volume is lower than a defined threshold, we ensure that the corresponding'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='the network during inference, we adopt a thresholding strategy targeting the ET class.\\nIf the ET volume is lower than a defined threshold, we ensure that the corresponding\\nvoxels are belonging to the NET/Ncr class in order for them to be calculated as part of\\nthe whole and core tumor.\\nThe results shown in Tab. 5 underpin the performance of our method on the valida-\\ntion set. All segmentation results are evaluated on the BraTS 2022 challenge platform.\\nThe relatively low score of enhancing tumor when compared to other classes is mainly\\ndue to a binary score calculation system of the BraTS evaluation platform that penal-\\nizes the whole predicted class with the worst score if the latter includes a false positive\\nvoxel.\\nThe scores are relatively better for other classes. Our network more easily dis-\\ntinguishes peritumoral edema and NET /Ncr volumes as can be seen in Fig. 11 which\\ndepicts the segmentation results over some cases from the validation set. It can be'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='tinguishes peritumoral edema and NET /Ncr volumes as can be seen in Fig. 11 which\\ndepicts the segmentation results over some cases from the validation set. It can be\\nseen that our network can generate convincing results, even on some small volumesH. Messaoudi et al. 21\\nwhich can be due to the exploitation of the second dimension leading to a better fea-\\nture extraction of the intra-slice information and thus showing the effectiveness of our\\napproach on brain tumor segmentation problems.\\nFigure 11: Visualizations on the 00015 patient T2 data from the BraTS 2022 validation set. From top to\\nbottom: Axial, sagittal and coronal views are shown with their corresponding DS-Net segmentation result.\\nEdema is shown in green, enhancing tumor in yellow, and necrosis/non-enhancing tumor in red.\\n3.2. Dimensional transfer learning\\n3.2.1. 3D multi-modal brain tumor MR segmentation\\nThe two networks, DS-Net and DX-Net, are both designed for 3D image segmenta-'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='3.2. Dimensional transfer learning\\n3.2.1. 3D multi-modal brain tumor MR segmentation\\nThe two networks, DS-Net and DX-Net, are both designed for 3D image segmenta-\\ntion tasks. They di ffer in their encoding and decoding processes, with DX-Net having\\na 3D pre-trained encoder and a 3D decoder, and DS-Net having a 3D encoder followed\\nby a 2D pre-trained encoder, a 2D decoder, and a 3D decoder. The training process\\nundertaken for the DX-Net is similar to the one followed for DS-Net. From Tab. 5,\\nwe can see the mean Dice score of each region computed by the online evaluation\\nplatform. From it, we can derive that the whole tumor region is more easily detected\\nby both networks. However, if we look at the TC and ET scores for both networks,\\nwe can see that the peritumoral edema is much better segmented by DS-Net, this canH. Messaoudi et al. 22\\nBest : BraTS22 Validation 00190, TC= 0.9930, WT = 0.9883, ET = 0.9748\\n25th Percentile : BraTS22 Validation 01719, TC= 0.7868, WT = 0.7927, ET = 1.0'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='Best : BraTS22 Validation 00190, TC= 0.9930, WT = 0.9883, ET = 0.9748\\n25th Percentile : BraTS22 Validation 01719, TC= 0.7868, WT = 0.7927, ET = 1.0\\nMedian : BraTS22 Validation 00462, TC= 0.9569, WT = 0.9047, ET = 0.8865\\n75th Percentile : BraTS22 Validation 00553, TC= 0.9709, WT = 0.9452, ET =\\n0.9324\\nWorst : BraTS22 Validation 00213, TC = 0.0 , WT = 0.2394 , ET = 0.0\\nFigure 12: DX-Net qualitative results. Cases were selected as best, worst, median and 75th and 25th\\npercentile. Within each row, the raw T2 image is shown to the left, the T1ce image in the middle and on\\noverlay with the generated segmentation on the T1ce image is shown on the right. Edema is shown in green,\\nenhancing tumor in yellow and necrosis/non-enhancing tumor on red.H. Messaoudi et al. 23\\nbe deduced by looking at the superiority of the mean Dice of ET and TC for DX-Net\\nwhen compared to that of the DS-Net, despite that DS-Net shows a higher score for the'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='be deduced by looking at the superiority of the mean Dice of ET and TC for DX-Net\\nwhen compared to that of the DS-Net, despite that DS-Net shows a higher score for the\\nwhole tumor region which indicates a much better delineation of the peritumoral edema\\nclass produced by DS-Net. Nevertheless, the DX-Net shows a clear superiority in the\\ndiscrimination of the tumor core and enhancing tumor region. We assume that the un-\\nderlying cause could be a better exploitation of volumetric features, and this is through\\nthe application of a deeper and better initialized 3D encoder. DX-Net post-processed\\npredictions are shown in Fig. 12.\\nThe use of a fully 3D network, as in DX-Net, allows for the capture of authentic\\nvolumetric features in the image, which can lead to improved segmentation, especially\\nfor difficult areas in the image. This is because 3D convolution layers can learn the\\ninter-slice relationships within the depth dimension using the generated pre-trained 3D'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='for difficult areas in the image. This is because 3D convolution layers can learn the\\ninter-slice relationships within the depth dimension using the generated pre-trained 3D\\nweights, allowing for a more reliable representation of the input image. On the other\\nhand, DS-Netâ€™s approach of first encoding the 3D data into 2D data and then processing\\nit with a 2D pre-trained encoder, may be slower and sub-optimal for configurations\\nrequiring fast training as compared to DX-Net. This is evident from the training time,\\nwhere DS-Net takes 2 seconds per image to train, while DX-Net takes 1.5 seconds\\nper image on an NVIDIA Tesla P100 GPU. However, this trade-o ffin computational\\nefficiency may come at the cost of not fully capturing the intra-slice information as\\nefficiently as by DS-Net.\\nDice Score WT TC ET\\nMean 91.22 84.77 83.88\\nMedian 94.15 93.46 89.74\\n25 Quartile 90.08 85.56 83.49\\n75 Quartile 96.46 96.48 94.92\\nTable 6: DX-Net segmentation scores on the BraTS 2022 validation set.'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='Dice Score WT TC ET\\nMean 91.22 84.77 83.88\\nMedian 94.15 93.46 89.74\\n25 Quartile 90.08 85.56 83.49\\n75 Quartile 96.46 96.48 94.92\\nTable 6: DX-Net segmentation scores on the BraTS 2022 validation set.\\nFurthermore, the networks could be significantly improved by extending the train-\\ning time and also benefiting from a more aggressive data augmentation scheme. The\\nlearning process also depends mostly on the annotations performed, these last ones can\\nundermine the proper generalizability of the network, especially when the task is com-\\nplex and error-prone. The networks are trained for less than 100 epochs and without\\nextensive post-processing. The particular strength of the DX-Net is its relatively low\\ncomputational power consumption and its ability to accurately retain and reproduce\\nexpert annotations, which might motivate its use in a clinical context.\\n4. Conclusion\\nIn this work, we introduced two transfer learning paradigms. First, we presented the'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='expert annotations, which might motivate its use in a clinical context.\\n4. Conclusion\\nIn this work, we introduced two transfer learning paradigms. First, we presented the\\nweight transfer learning, an efficient approach to re-use the weights of a pre-trained 2D\\nclassifier network by embedding it in a network of the same or higher dimension. We\\nderived from it two network architectures: Omnia-Net, a 2D network which is intended\\nfor the segmentation of 2D echo-cardiographic and 3D MR and CT data and DS-Net,H. Messaoudi et al. 24\\na 3D network embedding a 2D architecture that allows the exploitation of intra-slice\\ninformation from brain tumor images.\\nThe second proposed approach is the dimensional transfer learning, which is based\\non the 3D weights extrapolation of a pre-trained 2D network. DX-Net is a network\\nderived from this approach. It is a U-Net-like architecture using a 3D E fficientNet\\nencoder. The latter is initialized with extrapolated 3D weights from the 2D EfficientNet'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='derived from this approach. It is a U-Net-like architecture using a 3D E fficientNet\\nencoder. The latter is initialized with extrapolated 3D weights from the 2D EfficientNet\\nweights, which are pre-trained on ImageNet following a noisy-student strategy.\\nEmpirical results showed that our approaches clearly outperform state-of-the-art\\nmethods. Omnia-Net ranked first in the CAMUS challenge, which involved addressing\\nvariable image quality, multi-chamber views, and multi-phase context. This network\\nproduced convincing results and can already be used in a clinical context. On the\\nCHAOS challenge, our approach was ranked 3 rd with the same network and got bet-\\nter results than all methods using 2D networks in the challenge. DS-Net and DX-Net\\nshowed promising results and achieve competitive performance. Our future perspec-\\ntives will be turned towards the investigation of other methods to handle weight trans-'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='showed promising results and achieve competitive performance. Our future perspec-\\ntives will be turned towards the investigation of other methods to handle weight trans-\\nformation from 2D to higher dimensions in the context of dimensional transfer learning\\nto achieve better performance conservation in higher dimensions and enhanced image\\nsegmentation quality.\\n5. Declaration of interests\\nThe authors declare that they have no known competing financial interests or per-\\nsonal relationships that could have appeared to influence the work reported in this pa-\\nper.\\n6. Acknowledgment\\nThis work is sponsored by the General Directorate for Scientific Research and\\nTechnological Development, Ministry of Higher Education and Scientific Research\\n(DGRSDT), Algeria.\\n7. Funding\\nThis research did not receive any specific grant from funding agencies in the public,\\ncommercial, or not-for-profit sectors.\\nReferences\\nAbraham, N., Khan, N.M., 2018. A novel focal tversky loss function with improved'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='commercial, or not-for-profit sectors.\\nReferences\\nAbraham, N., Khan, N.M., 2018. A novel focal tversky loss function with improved\\nattention u-net for lesion segmentation. CoRR abs /1810.07842. URL: http://\\narxiv.org/abs/1810.07842, arXiv:1810.07842.\\nAhmad, P., Jin, H., Alroobaea, R., Qamar, S., Zheng, R., Alnajjar, F., Aboudi, F.,\\n2021. MH UNet: A multi-scale hierarchical based architecture for medical image\\nsegmentation. IEEE Access 9, 148384â€“148408. URL: https://doi.org/10.\\n1109/access.2021.3122543, doi:10.1109/access.2021.3122543.H. Messaoudi et al. 25\\nBadrinarayanan, V ., Kendall, A., Cipolla, R., 2017. SegNet: A deep convolutional\\nencoder-decoder architecture for image segmentation. IEEE Transactions on Pattern\\nAnalysis and Machine Intelligence 39, 2481â€“2495. URL: https://doi.org/10.\\n1109/tpami.2016.2644615, doi:10.1109/tpami.2016.2644615.\\nBaid, U., Ghodasara, S., Bilello, M., Mohan, S., Calabrese, E., Colak, E., Farahani,'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='1109/tpami.2016.2644615, doi:10.1109/tpami.2016.2644615.\\nBaid, U., Ghodasara, S., Bilello, M., Mohan, S., Calabrese, E., Colak, E., Farahani,\\nK., Kalpathy-Cramer, J., Kitamura, F.C., Pati, S., Prevedello, L.M., Rudie, J.D.,\\nSako, C., Shinohara, R.T., Bergquist, T., Chai, R., Eddy, J.A., Elliott, J., Reade, W.,\\nSchaffter, T., Yu, T., Zheng, J., Annotators, B., Davatzikos, C., Mongan, J., Hess, C.,\\nCha, S., Villanueva-Meyer, J.E., Freymann, J.B., Kirby, J.S., Wiestler, B., Crivellaro,\\nP., Colen, R.R., Kotrotsou, A., Marcus, D.S., Milchenko, M., Nazeri, A., Fathallah-\\nShaykh, H.M., Wiest, R., Jakab, A., Weber, M., Mahajan, A., Menze, B.H., Flanders,\\nA.E., Bakas, S., 2021. The RSNA-ASNR-MICCAI brats 2021 benchmark on brain\\ntumor segmentation and radiogenomic classification. CoRR abs/2107.02314. URL:\\nhttps://arxiv.org/abs/2107.02314, arXiv:2107.02314.\\nBakas, S., Akbari, H., Sotiras, A., Bilello, M., Rozycki, M., Kirby, J.S., Frey-'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='https://arxiv.org/abs/2107.02314, arXiv:2107.02314.\\nBakas, S., Akbari, H., Sotiras, A., Bilello, M., Rozycki, M., Kirby, J.S., Frey-\\nmann, J.B., Farahani, K., Davatzikos, C., 2017. Advancing the cancer genome\\natlas glioma MRI collections with expert segmentation labels and radiomic fea-\\ntures. Scientific Data 4. URL: https://doi.org/10.1038/sdata.2017.117,\\ndoi:10.1038/sdata.2017.117.\\nBakas, S., Reyes, M., et Al., A.J., 2018. Identifying the best machine learning al-\\ngorithms for brain tumor segmentation, progression assessment, and overall sur-\\nvival prediction in the BRATS challenge. CoRR abs /1811.02629. URL: http:\\n//arxiv.org/abs/1811.02629, arXiv:1811.02629.\\nBernard, O., Bosch, J.G., Heyde, B., Alessandrini, M., Barbosa, D., Camarasu-Pop,\\nS., Cervenansky, F., Valette, S., Mirea, O., Bernier, M., Jodoin, P.M., Domingos,\\nJ.S., Stebbing, R.V ., Keraudren, K., Oktay, O., Caballero, J., Shi, W., Rueckert,'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content=\"S., Cervenansky, F., Valette, S., Mirea, O., Bernier, M., Jodoin, P.M., Domingos,\\nJ.S., Stebbing, R.V ., Keraudren, K., Oktay, O., Caballero, J., Shi, W., Rueckert,\\nD., Milletari, F., Ahmadi, S.A., Smistad, E., Lindseth, F., van Stralen, M., Wang,\\nC., Smedby, O., Donal, E., Monaghan, M., Papachristidis, A., Geleijnse, M.L.,\\nGalli, E., D'hooge, J., 2016. Standardized evaluation system for left ventricular\\nsegmentation algorithms in 3d echocardiography. IEEE Transactions on Medical\\nImaging 35, 967â€“977. URL: https://doi.org/10.1109/tmi.2015.2503890,\\ndoi:10.1109/tmi.2015.2503890.\\nBozinovski, S., 2020. Reminder of the first paper on transfer learning in neural net-\\nworks, 1976. Informatica 44. URL: https://doi.org/10.31449/inf.v44i3.\\n2828, doi:10.31449/inf.v44i3.2828.\\nBul`o, S.R., Porzi, L., Kontschieder, P., 2017. In-place activated batchnorm for memory-\\noptimized training of dnns. CoRR abs /1712.02616. URL: http://arxiv.org/\\nabs/1712.02616, arXiv:1712.02616.\"),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='Bul`o, S.R., Porzi, L., Kontschieder, P., 2017. In-place activated batchnorm for memory-\\noptimized training of dnns. CoRR abs /1712.02616. URL: http://arxiv.org/\\nabs/1712.02616, arXiv:1712.02616.\\nCarrÂ´e, A., Deutsch, E., Robert, C., 2022. Automatic brain tumor segmentation\\nwith a bridge-unet deeply supervised enhanced with downsampling pooling com-\\nbination, atrous spatial pyramid pooling, squeeze-and-excitation and EvoNorm,H. Messaoudi et al. 26\\nin: Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries.\\nSpringer International Publishing, pp. 253â€“266. URL: https://doi.org/10.\\n1007/978-3-031-09002-8_23 , doi:10.1007/978-3-031-09002-8_23 .\\nChowdhury, N.K., Kabir, M.A., Rahman, M.M., Rezoana, N., 2021. ECOVNet: a\\nhighly effective ensemble based deep learning model for detecting COVID-19. PeerJ\\nComputer Science 7, e551. URL: https://doi.org/10.7717/peerj-cs.551,\\ndoi:10.7717/peerj-cs.551.\\nC Â¸ ic Â¸ek,Â¨O., Abdulkadir, A., Lienkamp, S.S., Brox, T., Ronneberger, O.,'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='Computer Science 7, e551. URL: https://doi.org/10.7717/peerj-cs.551,\\ndoi:10.7717/peerj-cs.551.\\nC Â¸ ic Â¸ek,Â¨O., Abdulkadir, A., Lienkamp, S.S., Brox, T., Ronneberger, O.,\\n2016. 3d u-net: Learning dense volumetric segmentation from sparse anno-\\ntation. CoRR abs /1606.06650. URL: http://arxiv.org/abs/1606.06650,\\narXiv:1606.06650.\\nConze, P.H., Brochard, S., Burdin, V ., Sheehan, F.T., Pons, C., 2020. Healthy ver-\\nsus pathological learning transferability in shoulder muscle MRI segmentation using\\ndeep convolutional encoder-decoders. Computerized Medical Imaging and Graphics\\n83, 101733.\\nConze, P.H., Kavur, A.E., Cornec-Le Gall, E., Gezer, N.S., Le Meur, Y ., Selver, M.A.,\\nRousseau, F., 2021. Abdominal multi-organ segmentation with cascaded convo-\\nlutional and adversarial deep networks. Artificial Intelligence in Medicine 117,\\n102109.\\nDemir, I., Koperski, K., Lindenbaum, D., Pang, G., Huang, J., Basu, S., Hughes, F.,'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='lutional and adversarial deep networks. Artificial Intelligence in Medicine 117,\\n102109.\\nDemir, I., Koperski, K., Lindenbaum, D., Pang, G., Huang, J., Basu, S., Hughes, F.,\\nTuia, D., Raskar, R., 2018. Deepglobe 2018: A challenge to parse the earth through\\nsatellite images. CoRR abs /1805.06561. URL: http://arxiv.org/abs/1805.\\n06561, arXiv:1805.06561.\\nDeng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L., 2009. Imagenet: A large-\\nscale hierarchical image database, in: 2009 IEEE conference on computer vision and\\npattern recognition, Ieee. pp. 248â€“255.\\nHe, K., Zhang, X., Ren, S., Sun, J., 2015. Deep residual learning for image recog-\\nnition. CoRR abs /1512.03385. URL: http://arxiv.org/abs/1512.03385,\\narXiv:1512.03385.\\nHenry, T., CarrÂ´e, A., Lerousseau, M., Estienne, T., Robert, C., Paragios, N., Deutsch,\\nE., 2021. Brain tumor segmentation with self-ensembled, deeply-supervised 3d u-net\\nneural networks: A BraTS 2020 challenge solution, in: Brainlesion: Glioma, Mul-'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='E., 2021. Brain tumor segmentation with self-ensembled, deeply-supervised 3d u-net\\nneural networks: A BraTS 2020 challenge solution, in: Brainlesion: Glioma, Mul-\\ntiple Sclerosis, Stroke and Traumatic Brain Injuries. Springer International Publish-\\ning, pp. 327â€“339. URL: https://doi.org/10.1007/978-3-030-72084-1_30 ,\\ndoi:10.1007/978-3-030-72084-1_30 .\\nHinton, G., Vinyals, O., Dean, J., 2015. Distilling the knowledge in a neural network.\\narXiv:1503.02531.\\nHuang, H., Lin, L., Tong, R., Hu, H., Zhang, Q., Iwamoto, Y ., Han, X., Chen, Y .W.,\\nWu, J., 2020. Unet 3+: A full-scale connected unet for medical image segmentation.\\narXiv:2004.08790.H. Messaoudi et al. 27\\nHuynh, L.D., Boutry, N., 2020. A u-net ++ with pre-trained e fficientnet backbone\\nfor segmentation of diseases and artifacts in endoscopy images and videos, in:\\nAli, S., Daul, C., Rittscher, J., Stoyanov, D., Grisan, E. (Eds.), Proceedings of the\\n2nd International Workshop and Challenge on Computer Vision in Endoscopy, En-'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='Ali, S., Daul, C., Rittscher, J., Stoyanov, D., Grisan, E. (Eds.), Proceedings of the\\n2nd International Workshop and Challenge on Computer Vision in Endoscopy, En-\\ndoCV@ISBI 2020, Iowa City, Iowa, USA, 3rd April 2020, CEUR-WS.org. pp. 13â€“\\n17. URL: http://ceur-ws.org/Vol-2595/endoCV2020_paper_id_11.pdf.\\nIglovikov, V ., Seferbekov, S., Buslaev, A., Shvets, A., 2018. Ternausnetv2: Fully\\nconvolutional network for instance segmentation, in: 2018 IEEE /CVF Conference\\non Computer Vision and Pattern Recognition Workshops (CVPRW), pp. 228â€“2284.\\ndoi:10.1109/CVPRW.2018.00042.\\nIglovikov, V .I., Shvets, A.A., 2018. Ternausnet: U-net with vgg11 encoder pre-trained\\non imagenet for image segmentation. ArXiv abs/1801.05746.\\nIqbal, M.J., Javed, Z., Sadia, H., Qureshi, I.A., Irshad, A., Ahmed, R., Malik, K.,\\nRaza, S., Abbas, A., Pezzani, R., Sharifi-Rad, J., 2021. Clinical applications\\nof artificial intelligence and machine learning in cancer diagnosis: looking into'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='Raza, S., Abbas, A., Pezzani, R., Sharifi-Rad, J., 2021. Clinical applications\\nof artificial intelligence and machine learning in cancer diagnosis: looking into\\nthe future. Cancer Cell International 21. URL: https://doi.org/10.1186/\\ns12935-021-01981-1 , doi:10.1186/s12935-021-01981-1 .\\nIsensee, F., Jaeger, P., Full, P.M., Wolf, I., Engelhardt, S., Maier-Hein, K.H., 2017.\\nAutomatic cardiac disease assessment on cine-mri via time-series segmentation and\\ndomain specific features. CoRR abs/1707.00587. URL: http://arxiv.org/abs/\\n1707.00587.\\nIzmailov, P., Podoprikhin, D., Garipov, T., Vetrov, D.P., Wilson, A.G., 2018. Averaging\\nweights leads to wider optima and better generalization. CoRR abs /1803.05407.\\nURL: http://arxiv.org/abs/1803.05407.\\nJadon, S., 2020. A survey of loss functions for semantic segmentation, in: 2020 IEEE\\nConference on Computational Intelligence in Bioinformatics and Computational Bi-\\nology (CIBCB), pp. 1â€“7. doi: 10.1109/CIBCB48159.2020.9277638.'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='Conference on Computational Intelligence in Bioinformatics and Computational Bi-\\nology (CIBCB), pp. 1â€“7. doi: 10.1109/CIBCB48159.2020.9277638.\\nJiang, Z., Ding, C., Liu, M., Tao, D., 2020. Two-stage cascaded u-net: 1st place\\nsolution to BraTS challenge 2019 segmentation task, in: Brainlesion: Glioma, Mul-\\ntiple Sclerosis, Stroke and Traumatic Brain Injuries. Springer International Publish-\\ning, pp. 231â€“241. URL: https://doi.org/10.1007/978-3-030-46640-4_22 ,\\ndoi:10.1007/978-3-030-46640-4_22 .\\nJin, J., Song, M.H., Kim, S.D., Jin, D., 2021. Mask r-cnn models to purify medical\\nimages of training sets, in: 2021 International Conference on e-Health and Bioengi-\\nneering (EHB), pp. 1â€“4. doi: 10.1109/EHB52898.2021.9657741.\\nKavur, A.E., Gezer, N.S., BarÄ±s Â¸, M., Aslan, S., Conze, P.H., Groza, V ., et al., 2021.\\nChaos challenge-combined (CT-MR) healthy abdominal organ segmentation. Med-\\nical Image Analysis 69, 101950.H. Messaoudi et al. 28'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='Chaos challenge-combined (CT-MR) healthy abdominal organ segmentation. Med-\\nical Image Analysis 69, 101950.H. Messaoudi et al. 28\\nKavur, A.E., Selver, M.A., Dicle, O., BarÃ…, M., Gezer, N.S., 2019. CHAOS\\n- Combined (CT-MR) Healthy Abdominal Organ Segmentation Challenge Data.\\nURL: https://doi.org/10.5281/zenodo.3362844, doi: 10.5281/zenodo.\\n3362844.\\nLeclerc, S., Smistad, E., Pedrosa, J., Ostvik, A., Cervenansky, F., Espinosa, F., Es-\\npeland, T., Berg, E.A.R., Jodoin, P.M., Grenier, T., Lartizien, C., Dhooge, J., Lovs-\\ntakken, L., Bernard, O., 2019. Deep learning for segmentation using an open\\nlarge-scale dataset in 2d echocardiography. IEEE Transactions on Medical Imag-\\ning 38, 2198â€“2210. URL: https://doi.org/10.1109/tmi.2019.2900516,\\ndoi:10.1109/tmi.2019.2900516.\\nLi, Q., Cai, W., Wang, X., Zhou, Y ., Feng, D.D., Chen, M., 2014. Medical im-\\nage classification with convolutional neural network, in: 2014 13th International'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='doi:10.1109/tmi.2019.2900516.\\nLi, Q., Cai, W., Wang, X., Zhou, Y ., Feng, D.D., Chen, M., 2014. Medical im-\\nage classification with convolutional neural network, in: 2014 13th International\\nConference on Control Automation Robotics & Vision (ICARCV), pp. 844â€“848.\\ndoi:10.1109/ICARCV.2014.7064414.\\nLiang, G., Fan, W., Luo, H., Zhu, X., 2020. The emerging roles of artificial intelligence\\nin cancer drug development and precision therapy. Biomedicine & Pharmacother-\\napy 128, 110255. URL: https://doi.org/10.1016/j.biopha.2020.110255,\\ndoi:10.1016/j.biopha.2020.110255.\\nLoshchilov, I., Hutter, F., 2016. SGDR: stochastic gradient descent with\\nrestarts. CoRR abs /1608.03983. URL: http://arxiv.org/abs/1608.03983,\\narXiv:1608.03983.\\nMaggiori, E., Tarabalka, Y ., Charpiat, G., Alliez, P., 2017. Can semantic labeling meth-\\nods generalize to any city? the inria aerial image labeling benchmark, in: 2017 IEEE\\nInternational Geoscience and Remote Sensing Symposium (IGARSS), pp. 3226â€“'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='ods generalize to any city? the inria aerial image labeling benchmark, in: 2017 IEEE\\nInternational Geoscience and Remote Sensing Symposium (IGARSS), pp. 3226â€“\\n3229. doi: 10.1109/IGARSS.2017.8127684.\\nMenze, B.H., Jakab, A., et Al, S.B., 2015. The multimodal brain tumor image seg-\\nmentation benchmark (BRATS). IEEE Transactions on Medical Imaging 34, 1993â€“\\n2024. URL: https://doi.org/10.1109/tmi.2014.2377694, doi: 10.1109/\\ntmi.2014.2377694.\\nMerino, I., Azpiazu, J., Remazeilles, A., Sierra, B., 2021. 3d convolutional neural\\nnetworks initialized from pretrained 2d convolutional neural networks for classifi-\\ncation of industrial parts. Sensors 21, 1078. URL: https://doi.org/10.3390/\\ns21041078, doi:10.3390/s21041078.\\nMessaoudi, H., Belaid, A., Allaoui, M.L., Zetout, A., Allili, M.S., SouhilTliba, Salem,\\nD.B., Conze, P.H., 2021. E fficient embedding network for 3d brain tumor segmen-\\ntation, in: Crimi, A., Bakas, S. (Eds.), Brainlesion: Glioma, Multiple Sclerosis,'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='D.B., Conze, P.H., 2021. E fficient embedding network for 3d brain tumor segmen-\\ntation, in: Crimi, A., Bakas, S. (Eds.), Brainlesion: Glioma, Multiple Sclerosis,\\nStroke and Traumatic Brain Injuries, Springer International Publishing, Cham. pp.\\n252â€“262. URL: https://doi.org/10.1007/978-3-030-72084-1_23 , doi:10.\\n1007/978-3-030-72084-1_23 .H. Messaoudi et al. 29\\nMinaee, S., Boykov, Y .Y ., Porikli, F., Plaza, A.J., Kehtarnavaz, N., Terzopoulos, D.,\\n2021. Image segmentation using deep learning: A survey. IEEE Transactions on Pat-\\ntern Analysis and Machine Intelligence , 1â€“1URL: https://doi.org/10.1109/\\ntpami.2021.3059968, doi:10.1109/tpami.2021.3059968.\\nNewell, A., Yang, K., Deng, J., 2016. Stacked hourglass networks for human pose\\nestimation, in: Computer Vision â€“ ECCV 2016. Springer International Publish-\\ning, pp. 483â€“499. URL: https://doi.org/10.1007/978-3-319-46484-8_29 ,\\ndoi:10.1007/978-3-319-46484-8_29 .'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content=\"estimation, in: Computer Vision â€“ ECCV 2016. Springer International Publish-\\ning, pp. 483â€“499. URL: https://doi.org/10.1007/978-3-319-46484-8_29 ,\\ndoi:10.1007/978-3-319-46484-8_29 .\\nOktay, O., Ferrante, E., Kamnitsas, K., Heinrich, M., Bai, W., Caballero, J., Cook,\\nS.A., de Marvao, A., Dawes, T., O'Regan, D.P., Kainz, B., Glocker, B., Rueck-\\nert, D., 2018a. Anatomically constrained neural networks (ACNNs): Application\\nto cardiac image enhancement and segmentation. IEEE Transactions on Medical\\nImaging 37, 384â€“395. URL: https://doi.org/10.1109/tmi.2017.2743464,\\ndoi:10.1109/tmi.2017.2743464.\\nOktay, O., Schlemper, J., Folgoc, L.L., Lee, M., Heinrich, M., Misawa, K., Mori,\\nK., McDonagh, S., Hammerla, N.Y ., Kainz, B., Glocker, B., Rueckert, D., 2018b.\\nAttention u-net: Learning where to look for the pancreas. arXiv:1804.03999.\\nPedrosa, J., Barbosa, D., Heyde, B., Schnell, F., Rosner, A., Claus, P., D'hooge, J.,\"),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content=\"Attention u-net: Learning where to look for the pancreas. arXiv:1804.03999.\\nPedrosa, J., Barbosa, D., Heyde, B., Schnell, F., Rosner, A., Claus, P., D'hooge, J.,\\n2017. Left ventricular myocardial segmentation in 3-d ultrasound recordings: Effect\\nof different endocardial and epicardial coupling strategies. IEEE Transactions on Ul-\\ntrasonics, Ferroelectrics, and Frequency Control 64, 525â€“536. URL:https://doi.\\norg/10.1109/tuffc.2016.2638080, doi:10.1109/tuffc.2016.2638080.\\nRehman, M.U., Cho, S., Kim, J.H., Chong, K.T., 2020. BU-net: Brain tumor segmen-\\ntation using modified u-net architecture. Electronics 9, 2203. URL: https://doi.\\norg/10.3390/electronics9122203, doi:10.3390/electronics9122203.\\nRomero, A., Ballas, N., Kahou, S.E., Chassang, A., Gatta, C., Bengio, Y ., 2015. Fit-\\nnets: Hints for thin deep nets. arXiv:1412.6550.\\nRonneberger, O., Fischer, P., Brox, T., 2015. U-net: Convolutional networks\\nfor biomedical image segmentation, in: Lecture Notes in Computer Science.\"),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='nets: Hints for thin deep nets. arXiv:1412.6550.\\nRonneberger, O., Fischer, P., Brox, T., 2015. U-net: Convolutional networks\\nfor biomedical image segmentation, in: Lecture Notes in Computer Science.\\nSpringer International Publishing, pp. 234â€“241. URL: https://doi.org/10.\\n1007/978-3-319-24574-4_28 , doi:10.1007/978-3-319-24574-4_28 .\\nRuder, S., 2017. An overview of gradient descent optimization algorithms.\\narXiv:1609.04747.\\nShah, S.M., Khan, R.A., Arif, S., Sajid, U., 2022. Artificial intelligence for\\nbreast cancer analysis: Trends & directions. Computers in Biology and Medicine\\n142, 105221. URL: https://doi.org/10.1016/j.compbiomed.2022.105221,\\ndoi:10.1016/j.compbiomed.2022.105221.\\nShelhamer, E., Long, J., Darrell, T., 2016. Fully convolutional networks for seman-\\ntic segmentation. CoRR abs /1605.06211. URL: http://arxiv.org/abs/1605.\\n06211.H. Messaoudi et al. 30\\nShelhamer, E., Long, J., Darrell, T., 2017. Fully convolutional networks for semantic'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='tic segmentation. CoRR abs /1605.06211. URL: http://arxiv.org/abs/1605.\\n06211.H. Messaoudi et al. 30\\nShelhamer, E., Long, J., Darrell, T., 2017. Fully convolutional networks for semantic\\nsegmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence 39,\\n640â€“651. doi: 10.1109/TPAMI.2016.2572683.\\nShin, H.C., Roberts, K., Lu, L., Demner-Fushman, D., Yao, J., Summers, R.M., 2016.\\nLearning to read chest x-rays: Recurrent neural cascade model for automated image\\nannotation, in: 2016 IEEE Conference on Computer Vision and Pattern Recognition\\n(CVPR), pp. 2497â€“2506. doi: 10.1109/CVPR.2016.274.\\nStarke, S., Leger, S., Zwanenburg, A., Leger, K., Lohaus, F., Linge, A., Schreiber, A.,\\nKalinauskaite, G., Tinhofer, I., Guberina, N., Guberina, M., Balermpas, P., von der\\nGrÂ¨un, J., Ganswindt, U., Belka, C., Peeken, J.C., Combs, S.E., Boeke, S., Zips, D.,\\nRichter, C., Troost, E.G.C., Krause, M., Baumann, M., L Â¨ock, S., 2020. 2d and 3d'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='GrÂ¨un, J., Ganswindt, U., Belka, C., Peeken, J.C., Combs, S.E., Boeke, S., Zips, D.,\\nRichter, C., Troost, E.G.C., Krause, M., Baumann, M., L Â¨ock, S., 2020. 2d and 3d\\nconvolutional neural networks for outcome modelling of locally advanced head and\\nneck squamous cell carcinoma. Scientific Reports 10. URL: https://doi.org/\\n10.1038/s41598-020-70542-9 , doi:10.1038/s41598-020-70542-9 .\\nSuk, H.I., , Lee, S.W., Shen, D., 2013. Latent feature representation with stacked\\nauto-encoder for AD /MCI diagnosis. Brain Structure and Function 220, 841â€“\\n859. URL: https://doi.org/10.1007/s00429-013-0687-3 , doi: 10.1007/\\ns00429-013-0687-3 .\\nTakiddin, A., Schneider, J., Yang, Y ., Abd-Alrazaq, A., Househ, M., 2021. Artificial\\nintelligence for skin cancer detection: Scoping review. Journal of Medical Internet\\nResearch 23, e22934. URL: https://doi.org/10.2196/22934, doi:10.2196/\\n22934.\\nTan, M., Le, Q.V ., 2019. E fficientnet: Rethinking model scaling for convolutional'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='Research 23, e22934. URL: https://doi.org/10.2196/22934, doi:10.2196/\\n22934.\\nTan, M., Le, Q.V ., 2019. E fficientnet: Rethinking model scaling for convolutional\\nneural networks. CoRR abs /1905.11946. URL: http://arxiv.org/abs/1905.\\n11946, arXiv:1905.11946.\\nUlyanov, D., Vedaldi, A., Lempitsky, V .S., 2016. Instance normalization: The missing\\ningredient for fast stylization. CoRR abs /1607.08022. URL: http://arxiv.org/\\nabs/1607.08022, arXiv:1607.08022.\\nWang, J., Zhang, X., Lv, P., Zhou, L., Wang, H., 2021. Ear-u-net: E fficient-\\nnet and attention-based residual u-net for automatic liver segmentation in ct.\\narXiv:2110.01014.\\nWightman, R., Soare, A., Arora, A., Ha, C., Reich, C., Raw, N., Kaczmarzyk, J.,\\nMrT23, , Mike, SeeFun, Contrastive, Rizin, M., Hyeongchan Kim, Kertsz, C.,\\nDushyant Mehta, Cucurull, G., Kushajveer Singh, , Han, Tatsunami, Y ., Lavin,\\nA., Juntang Zhuang, Hollemans, M., Sameni, S., Shults, V ., Wang, X., Yonghye'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='Dushyant Mehta, Cucurull, G., Kushajveer Singh, , Han, Tatsunami, Y ., Lavin,\\nA., Juntang Zhuang, Hollemans, M., Sameni, S., Shults, V ., Wang, X., Yonghye\\nKwon, Uchida, Y ., Zhong, Z., Comar, Kim, T., 2022. rwightman /pytorch-image-\\nmodels: Maxxvit (coatnet, maxvit, and related experimental weights). URL:\\nhttps://zenodo.org/record/4414861, doi:10.5281/ZENODO.4414861.\\nWillemink, M.J., Koszek, W.A., Hardell, C., Wu, J., Fleischmann, D., Harvey, H.,\\nFolio, L.R., Summers, R.M., Rubin, D.L., Lungren, M.P., 2020. Preparing medicalH. Messaoudi et al. 31\\nimaging data for machine learning. Radiology 295, 4â€“15. URL: https://doi.\\norg/10.1148/radiol.2020192224, doi:10.1148/radiol.2020192224.\\nXie, Q., Luong, M.T., Hovy, E., Le, Q.V ., 2020. Self-training with noisy student im-\\nproves imagenet classification, in: 2020 IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition (CVPR), pp. 10684â€“10695. doi: 10.1109/CVPR42600.\\n2020.01070.'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='proves imagenet classification, in: 2020 IEEE/CVF Conference on Computer Vision\\nand Pattern Recognition (CVPR), pp. 10684â€“10695. doi: 10.1109/CVPR42600.\\n2020.01070.\\nYang, C.H., Ren, J.H., Huang, H.C., Chuang, L.Y ., Chang, P.Y ., 2021. Deep hybrid\\nconvolutional neural network for segmentation of melanoma skin lesion. Compu-\\ntational Intelligence and Neuroscience 2021, 1â€“15. URL: https://doi.org/10.\\n1155/2021/9409508, doi:10.1155/2021/9409508.\\nYuan, Y ., 2021. Automatic brain tumor segmentation with scale attention network,\\nin: Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries.\\nSpringer International Publishing, pp. 285â€“294. URL: https://doi.org/10.\\n1007/978-3-030-72084-1_26 , doi:10.1007/978-3-030-72084-1_26 .\\nZegour, R., Belaid, A., Ognard, J., Salem, D.B., 2023. Convolutional neural networks-\\nbased method for skin hydration measurements in high resolution MRI. Biomedical\\nSignal Processing and Control 81, 104491. URL: https://doi.org/10.1016/j.'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='based method for skin hydration measurements in high resolution MRI. Biomedical\\nSignal Processing and Control 81, 104491. URL: https://doi.org/10.1016/j.\\nbspc.2022.104491, doi:10.1016/j.bspc.2022.104491.\\nZhang, M.R., Lucas, J., Hinton, G.E., Ba, J., 2019. Lookahead optimizer: k steps\\nforward, 1 step back. CoRR abs /1907.08610. URL: http://arxiv.org/abs/\\n1907.08610, arXiv:1907.08610.\\nZhao, Y .X., Zhang, Y .M., Liu, C.L., 2020. Bag of tricks for 3d MRI brain tumor seg-\\nmentation, in: Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain\\nInjuries. Springer International Publishing, pp. 210â€“220. URL:https://doi.org/\\n10.1007/978-3-030-46640-4_20 , doi:10.1007/978-3-030-46640-4_20 .\\nZhou, Z., Siddiquee, M.M.R., Tajbakhsh, N., Liang, J., 2020. UNe ++t: Redesigning\\nskip connections to exploit multiscale features in image segmentation. IEEE Trans-\\nactions on Medical Imaging 39, 1856â€“1867. URL: https://doi.org/10.1109/\\ntmi.2019.2959609, doi:10.1109/tmi.2019.2959609.'),\n",
       " Document(metadata={'arxiv_id': '2307.15872v1', 'title': 'Cross-dimensional transfer learning in medical image segmentation with deep learning', 'section': 'body', 'authors': 'Hicham Messaoudi, Ahror Belaid, Douraied Ben Salem'}, page_content='actions on Medical Imaging 39, 1856â€“1867. URL: https://doi.org/10.1109/\\ntmi.2019.2959609, doi:10.1109/tmi.2019.2959609.\\nZoph, B., Le, Q.V ., 2016. Neural architecture search with reinforcement learn-\\ning. CoRR abs /1611.01578. URL: http://arxiv.org/abs/1611.01578,\\narXiv:1611.01578.'),\n",
       " Document(metadata={'arxiv_id': '2403.18233v1', 'title': 'Benchmarking Image Transformers for Prostate Cancer Detection from Ultrasound Data', 'section': 'title_abstract', 'authors': 'Mohamed Harmanani, Paul F. R. Wilson, Fahimeh Fooladgar'}, page_content='Title: Benchmarking Image Transformers for Prostate Cancer Detection from Ultrasound Data\\n\\nAbstract: PURPOSE: Deep learning methods for classifying prostate cancer (PCa) in ultrasound images typically employ convolutional networks (CNNs) to detect cancer in small regions of interest (ROI) along a needle trace region. However, this approach suffers from weak labelling, since the ground-truth histopathology labels do not describe the properties of individual ROIs. Recently, multi-scale approaches have sought to mitigate this issue by combining the context awareness of transformers with a CNN feature extractor to detect cancer from multiple ROIs using multiple-instance learning (MIL). In this work, we present a detailed study of several image transformer architectures for both ROI-scale and multi-scale classification, and a comparison of the performance of CNNs and transformers for ultrasound-based prostate cancer classification. We also design a novel multi-objective learning strategy that combines both ROI and core predictions to further mitigate label noise. METHODS: We evaluate 3 image transformers on ROI-scale cancer classification, then use the strongest model to tune a multi-scale classifier with MIL. We train our MIL models using our novel multi-objective learning strategy and compare our results to existing baselines. RESULTS: We find that for both ROI-scale and multi-scale PCa detection, image transformer backbones lag behind their CNN counterparts. This deficit in performance is even more noticeable for larger models. When using multi-objective learning, we can improve performance of MIL, with a 77.9% AUROC, a sensitivity of 75.9%, and a specificity of 66.3%. CONCLUSION: Convolutional networks are better suited for modelling sparse datasets of prostate ultrasounds, producing more robust features than transformers in PCa detection. Multi-scale methods remain the best architecture for this task, with multi-objective learning presenting an effective way to improve performance.'),\n",
       " Document(metadata={'arxiv_id': '2403.18233v1', 'title': 'Benchmarking Image Transformers for Prostate Cancer Detection from Ultrasound Data', 'section': 'body', 'authors': 'Mohamed Harmanani, Paul F. R. Wilson, Fahimeh Fooladgar'}, page_content='Benchmarking Image Transformers for Prostate Cancer\\nDetection from Ultrasound Data\\nMohamed Harmanani 1, Paul F. R. Wilson 1, Fahimeh Fooladgar 2, Amoon Jamzad 1,\\nMahdi Gilany 1, Minh Nguyen Nhat To 2, Brian Wodlinger 3, Purang Abolmaesumi 2, and\\nParvin Mousavi 1\\n1Queenâ€™s University, Kingston, Canada\\n2University of British Columbia, Vancouver, Canada\\n3Exact Imaging, Markham, Canada\\nABSTRACT\\nPURPOSE: Deep learning methods for classifying prostate cancer in ultrasound images typically employ con-\\nvolutional networks to detect cancer in small regions of interest (ROI) along a needle trace region. Recently,\\nmulti-scale approaches have combined the context awareness of Transformers with a convolutional feature extrac-\\ntor to detect cancer from multiple ROIs. In this work, we present a detailed study of several Image Transformer\\narchitectures for both ROI-scale and multi-scale classification, and a comparison of the performance of CNNs'),\n",
       " Document(metadata={'arxiv_id': '2403.18233v1', 'title': 'Benchmarking Image Transformers for Prostate Cancer Detection from Ultrasound Data', 'section': 'body', 'authors': 'Mohamed Harmanani, Paul F. R. Wilson, Fahimeh Fooladgar'}, page_content='architectures for both ROI-scale and multi-scale classification, and a comparison of the performance of CNNs\\nand Transformers for ultrasound-based prostate cancer classification. METHODS: We use a dataset of 6607\\nprostate biopsy cores extracted from 693 patients at 5 distinct clinical centers. We evaluate 3 vision trans-\\nformers on ROI-scale cancer classification then use the strongest model to tune a multi-scale classifier using\\nmulti-objective learning. We compare our results in both settings to a baseline convolutional architecture typi-\\ncally used in computer vision tasks. We evaluate all our models using nested k-fold cross-validation. RESULTS:\\nOur core-wise multi-objective model achieves a 77.9% AUROC, a sensitivity of 75.9%, and a specificity of 66.3%,\\na considerable improvement over the baseline. CONCLUSION: We conclude that the combined use of Image\\nTransformers and multi-objective learning has the potential to improve performance in prostate cancer classifi-'),\n",
       " Document(metadata={'arxiv_id': '2403.18233v1', 'title': 'Benchmarking Image Transformers for Prostate Cancer Detection from Ultrasound Data', 'section': 'body', 'authors': 'Mohamed Harmanani, Paul F. R. Wilson, Fahimeh Fooladgar'}, page_content='Transformers and multi-objective learning has the potential to improve performance in prostate cancer classifi-\\ncation from ultrasound.\\nKeywords: Vision transformers, multi-objective learning, prostate cancer, ultrasound\\n1. PURPOSE\\nEarly and accurate diagnosis of prostate cancer (PCa) is crucial in order to improve the chances of successful\\ntreatment. The standard method used to diagnose PCa is the histopathological annotation of biopsy tissue\\nretrieved from the patient under the guidance of Trans-rectal ultrasound (TRUS). Because of the low sensitivity\\nof conventional ultrasound in identifying prostate lesions, 1 TRUS-guided biopsy is usually systematic in nature:\\na number of biopsy cores are sampled from different locations in the prostate. This is in contrast to targeted\\nbiopsy, where tissue is sampled from a specific suspicious tissue location. Systematic biopsy carries significant'),\n",
       " Document(metadata={'arxiv_id': '2403.18233v1', 'title': 'Benchmarking Image Transformers for Prostate Cancer Detection from Ultrasound Data', 'section': 'body', 'authors': 'Mohamed Harmanani, Paul F. R. Wilson, Fahimeh Fooladgar'}, page_content='biopsy, where tissue is sampled from a specific suspicious tissue location. Systematic biopsy carries significant\\nrisks of adverse effects due to the large number of biopsy samples that need to be retrieved. As such, improving\\nthe performance of targeted biopsy has the potential to decrease the likelihood of biopsy-related risks and\\ncomplications.\\nMicro-ultrasound is a newly developed technology that allows visualization of tissue microstructures at much\\nhigher resolutions than conventional ultrasound. As such, this imaging modality is a prime candidate for training\\ndeep learning models to detect prostate cancer in ultrasound images. Typically, deep learning is used during\\ntargeted biopsy to classify small regions of interest (ROI) across a needle trace region. 2 This approach has seen\\nsome measure of success, but still struggles with a number of issues. For instance, ROI-scale PCa detection'),\n",
       " Document(metadata={'arxiv_id': '2403.18233v1', 'title': 'Benchmarking Image Transformers for Prostate Cancer Detection from Ultrasound Data', 'section': 'body', 'authors': 'Mohamed Harmanani, Paul F. R. Wilson, Fahimeh Fooladgar'}, page_content='some measure of success, but still struggles with a number of issues. For instance, ROI-scale PCa detection\\nsuffers from weak labelling: ground-truth histopathology labels describe tissue properties of the entire biopsy\\ncore, and ROI labels are only an approximation of the true distribution of cancer in the core. Moreover, ROI-scale\\nSend correspondance to: 22mh5@queensu.ca\\n1\\narXiv:2403.18233v1  [eess.IV]  27 Mar 2024models do not consider the broader contextual information encoded in multiple overlapping patches as clinicians\\ntypically do.\\nMulti-scale methods to PCa detection from ultrasound have recently been proposed as a solution to these\\nproblems, and have been shown to outperform ROI-scale classifiers. 3 Recent work has shown that using a\\nTransformer model to leverage contextual information in multiple patches belonging to the same core can increase\\nthe performance of deep learning-based PCa detection. In this work, we explore the effectiveness of Vision'),\n",
       " Document(metadata={'arxiv_id': '2403.18233v1', 'title': 'Benchmarking Image Transformers for Prostate Cancer Detection from Ultrasound Data', 'section': 'body', 'authors': 'Mohamed Harmanani, Paul F. R. Wilson, Fahimeh Fooladgar'}, page_content='the performance of deep learning-based PCa detection. In this work, we explore the effectiveness of Vision\\nTransformers as feature extractors in both ROI-scale and multi-scale contexts. We perform a detailed study of\\nseveral architectures in the hopes of improving the performance of multi-scale PCa detection. Furthermore, we\\nintroduce a novel learning objective that takes advantage of both core-scale and ROI-scale predictions to improve\\nmulti-scale Transformer models for PCa detection.\\n2. MATERIALS AND METHODS\\n2.1 Data Collection & Processing\\nWe use data collected from 693 patients who underwent prostate biopsy in five centers under the guidance of\\nTrans-rectal ultrasound (TRUS). The biopsy cores are extracted using the ExactVu micro-ultrasound system. 4\\nRaw Radio Frequency (RF) ultrasound images of the tissue are saved immediately before the biopsy gun is\\nfired, and the needle trace region is approximately determined using the angle and depth of the penetration. The'),\n",
       " Document(metadata={'arxiv_id': '2403.18233v1', 'title': 'Benchmarking Image Transformers for Prostate Cancer Detection from Ultrasound Data', 'section': 'body', 'authors': 'Mohamed Harmanani, Paul F. R. Wilson, Fahimeh Fooladgar'}, page_content='fired, and the needle trace region is approximately determined using the angle and depth of the penetration. The\\nextracted biopsy cores are then analyzed histopathologically to determine the Gleason score and the approximate\\npercentage of cancer present. We have 6607 total cores, with 86.7% of the dataset being non-cancerous. To\\nmitigate the imbalance of the labels, we undersample the benign cores during training in order to ensure the\\ndataset has an equal amount of benign and cancerous cores. Using the known angle and depth of the needleâ€™s\\npenetration, we determine a rectangular trace region.\\nRegions of interest (ROIs) of dimension 5 Ã—5 are extracted from the areas where the needle trace region overlaps\\nwith the prostate mask and are labelled 0 (benign) or 1 (cancer). Finally, each ROI is reshaped to 256 Ã— 256\\nusing linear interpolation, instance-normalized by computing its mean and standard deviation, and rescaled to'),\n",
       " Document(metadata={'arxiv_id': '2403.18233v1', 'title': 'Benchmarking Image Transformers for Prostate Cancer Detection from Ultrasound Data', 'section': 'body', 'authors': 'Mohamed Harmanani, Paul F. R. Wilson, Fahimeh Fooladgar'}, page_content='using linear interpolation, instance-normalized by computing its mean and standard deviation, and rescaled to\\nthe range (0, 1). To build our dataset, we extract 55 patches along the needle region of each core and compile\\nthem into training, validation, and test sets. We make sure to keep cores and patches from the same patient in\\nthe same set to avoid any risk of data leakage.\\n2.2 Self-supervised Pre-training\\nWe use Variance-Invariance-Covariance Regularization 5 (VICReg) to pre-train our models. VICReg works in\\nthe following manner: given an ultrasound image x, two stochastic data augmentations t1 and t2 are applied\\nto the image to yield two distinct views of x, denoted x1 and x2. For example, the image may be randomly\\nrotated, scaled, cropped, or distorted. 5 Afterwards, an encoder model is used to extract vector representations\\nh1 and h2 from each view, which are then projected into a latent space by a MLP network, where the following\\nself-supervised loss function is applied:'),\n",
       " Document(metadata={'arxiv_id': '2403.18233v1', 'title': 'Benchmarking Image Transformers for Prostate Cancer Detection from Ultrasound Data', 'section': 'body', 'authors': 'Mohamed Harmanani, Paul F. R. Wilson, Fahimeh Fooladgar'}, page_content='h1 and h2 from each view, which are then projected into a latent space by a MLP network, where the following\\nself-supervised loss function is applied:\\nLVICReg(z1, z2) = Î»s(z1, z2) + Âµ[v(z1) + v(z2)] + Î½[(c(z1) + c(z2)],\\nwhere z1 and z2 are the projections of h1 and h2, and Î», Âµ, Î½ are tunable hyperparameters. Following prior\\nwork, we use Î» = 25 , Âµ = 25 , Î½ = 1. 2, 3, 5 s, v, and c designate the invariance, variance, and covariance loss\\nfunctions respectively. The invariance loss is given by the mean squared error loss (MSE), the variance loss\\nmaintains the variance of features across batches, and the covariance loss minimizes feature redundancy through\\ncross-correlations projected features. 2, 5\\n2.3 Supervised Finetuning\\nWe compare several transformer architectures on the task of cancer detection on a single ROI. To that end,\\nwe use a standard Vision Transformer 6 (ViT) architecture, a Compact Convolutional Transformer 7 (CCT), and'),\n",
       " Document(metadata={'arxiv_id': '2403.18233v1', 'title': 'Benchmarking Image Transformers for Prostate Cancer Detection from Ultrasound Data', 'section': 'body', 'authors': 'Mohamed Harmanani, Paul F. R. Wilson, Fahimeh Fooladgar'}, page_content='we use a standard Vision Transformer 6 (ViT) architecture, a Compact Convolutional Transformer 7 (CCT), and\\na Pyramid Vision Transformer 8 (PvT). We choose a modified ResNet18 9 as our ROI-scale baseline, with only\\none sequence of convolutions and batch normalization in each residual block. This reduction in the number of\\nparameters mitigates overfitting and is associated with an increase in performance. 2\\n2Figure 1: Multi-scale classification of prostate cancer across the whole biopsy core using BERT and multi-\\nobjective learning.\\nWe pre-train each model described above using VICReg, then finetune them by loading the self-supervised\\nweights and training the model with an additional MLP classifier attached. We train the model to detect cancer\\nin individual ROIs, then match each ROI to their corresponding core. We then aggregate the predictions made\\nfor each patch in the core and compute their average to produce the final output of the core.'),\n",
       " Document(metadata={'arxiv_id': '2403.18233v1', 'title': 'Benchmarking Image Transformers for Prostate Cancer Detection from Ultrasound Data', 'section': 'body', 'authors': 'Mohamed Harmanani, Paul F. R. Wilson, Fahimeh Fooladgar'}, page_content='for each patch in the core and compute their average to produce the final output of the core.\\nWe then take the 2 best models with the best performance and use them as a backbone to extract feature\\nrepresentations of all the ROIs in each core. We first project each feature representation to a 72 Ã— 1 vector and\\ntrain a 12-layer BERT10 classifier to produce a prediction for the entire core given this sequence of features. The\\nweights of the model are updated using cross-entropy loss.\\n2.3.1 Multi-objective learning\\nIn addition to BERT sequence classification, we re-train the MLP layer used in Section 2.3 to assign a prediction\\nto each ROI in the core. We then use those predictions as input to a second cross-entropy loss function. The\\nfinal multi-objective loss can be summarized as follows:\\nLMO = Î³LCE(Ë†y, y) + (1 âˆ’ Î³)LCE(Ë† z, z),\\nwhere LCE designates the cross-entropy loss function, Ë†y is a core-wise prediction, y is the label given to the entire'),\n",
       " Document(metadata={'arxiv_id': '2403.18233v1', 'title': 'Benchmarking Image Transformers for Prostate Cancer Detection from Ultrasound Data', 'section': 'body', 'authors': 'Mohamed Harmanani, Paul F. R. Wilson, Fahimeh Fooladgar'}, page_content='LMO = Î³LCE(Ë†y, y) + (1 âˆ’ Î³)LCE(Ë† z, z),\\nwhere LCE designates the cross-entropy loss function, Ë†y is a core-wise prediction, y is the label given to the entire\\ncore, Ë† zis a vector containing predictions for each patch, and z is a vector containing the labels for all the patches\\n(all of which are equal to y). 0 â‰¤ Î³ â‰¤ 1 is a tunable hyperparameter that is used to weigh each component of\\nthe multi-objective loss. This workflow is illustrated in full in Figure 1.\\n3. RESULTS\\nWe evaluate both ROI-scale and multi-scale methods using nested k-fold cross validation, which divides the\\ndataset into 5 folds each with an equal number of patients from each center. Each fold is used for testing once,\\n3Backbone Finetuning AUROC Bal. Accuracy Sensitivity Specificity\\nROI-scale methods:\\nResNet18 Linear 76.1 Â± 3.48 69.0 Â± 2.51 65.3 Â± 5.71 73.6 Â± 3.88\\nViT Linear 66.8 Â± 2.66 62.8 Â± 2.91 69.1 Â± 3.87 54.6 Â± 2.09\\nCCT Linear 74.1 Â± 3.93 68.1 Â± 2.80 70.6 Â± 7.04 65.5 Â± 4.46'),\n",
       " Document(metadata={'arxiv_id': '2403.18233v1', 'title': 'Benchmarking Image Transformers for Prostate Cancer Detection from Ultrasound Data', 'section': 'body', 'authors': 'Mohamed Harmanani, Paul F. R. Wilson, Fahimeh Fooladgar'}, page_content='ResNet18 Linear 76.1 Â± 3.48 69.0 Â± 2.51 65.3 Â± 5.71 73.6 Â± 3.88\\nViT Linear 66.8 Â± 2.66 62.8 Â± 2.91 69.1 Â± 3.87 54.6 Â± 2.09\\nCCT Linear 74.1 Â± 3.93 68.1 Â± 2.80 70.6 Â± 7.04 65.5 Â± 4.46\\nPvT Linear 73.5 Â± 3.70 67.4 Â± 2.82 71.4 Â± 9.56 63.4 Â± 4.63\\nMulti-scale methods:\\nResNet18 BERT 76.6 Â± 2.20 64.4 Â± 1.50 63.4 Â± 23.6 65.4 Â± 25.9\\nResNet18 BERT + MO âˆ— 77.9 Â± 2.53 71.1 Â± 6.02 75.9 Â± 11.7 66.3 Â± 19.7\\nCCT BERT 71.2 Â± 3.79 61.5 Â± 3.36 66.5 Â± 27.1 56.5 Â± 22.4\\nCCT BERT + MO âˆ— 71.6 Â± 3.01 63.4 Â± 3.21 53.3 Â± 13.9 73.4 Â± 8.98\\nTable 1: Comparing the performance of various ROI-based and core-based methods for prostate cancer detection\\nwith the remaining folds used for training and validation respectively. This ensures we obtain realistic estimates\\nof the modelâ€™s performance by evaluating on a wider variety of testing data. We compute the AUROC, sensitivity,\\nand specificity of each model on each of the 5 test folds and average their performance. These results are shown\\nin Table 1.'),\n",
       " Document(metadata={'arxiv_id': '2403.18233v1', 'title': 'Benchmarking Image Transformers for Prostate Cancer Detection from Ultrasound Data', 'section': 'body', 'authors': 'Mohamed Harmanani, Paul F. R. Wilson, Fahimeh Fooladgar'}, page_content='and specificity of each model on each of the 5 test folds and average their performance. These results are shown\\nin Table 1.\\nAmong our ROI-level models, all 3 vision transformer variants fall short of exceeding the baseline ResNet model\\nin overall performance, with a difference of âˆ’2% and âˆ’0.9% in AUROC and Balanced Accuracy for the CCT\\nmodel. The standard ViT model drastically fails to keep up with the other ROI-scale models, with an AUROC\\nof 66 .8%, and an accuracy of 62 .8%. Interestingly, the ResNet model obtains the lowest sensitivity among all\\nROI models, at 65.3%, lower than that of ViT, CCT, and PvT. CCT and PvT obtain the highest sensitivities at\\n70.6% and 71 .4% respectively. This suggests that Transformer-based models are more likely to output a cancer\\nprediction than not, whereas the ResNet baseline is more conservative, prioritizing negative predictions.\\nOur MIL models show an improvement over single-instance ROI baselines, especially when using a ResNet18'),\n",
       " Document(metadata={'arxiv_id': '2403.18233v1', 'title': 'Benchmarking Image Transformers for Prostate Cancer Detection from Ultrasound Data', 'section': 'body', 'authors': 'Mohamed Harmanani, Paul F. R. Wilson, Fahimeh Fooladgar'}, page_content='Our MIL models show an improvement over single-instance ROI baselines, especially when using a ResNet18\\nbackbone, with an AUROC exceeding that of any previous ROI baseline (+0 .5% over ROI-scale ResNet18,\\n+2.5% over CCT). Although the balanced accuracy of the MIL architecture is significantly smaller than that\\nof its ROI-scale counterparts, choosing a threshold different than 0.5 at the output layer results in increased\\nperformance more in line with the AUROC. On the other hand, the MIL architecture with a CCT backbone fails\\nto perform adequately, with a 5 .1% decrease in AUROC when compared to the baseline ResNet18 and a 3 .1%\\ndecrease when compared to the CCT model. We believe this shows that in the context of ultrasound-based PCa\\ndetection on a small dataset, convolutional features are more robust than Transformer embeddings. This would\\nexplain the performance of the ROI-scale models, as the 2 highest performing models use convolutions as part\\nof their feature extraction.'),\n",
       " Document(metadata={'arxiv_id': '2403.18233v1', 'title': 'Benchmarking Image Transformers for Prostate Cancer Detection from Ultrasound Data', 'section': 'body', 'authors': 'Mohamed Harmanani, Paul F. R. Wilson, Fahimeh Fooladgar'}, page_content='explain the performance of the ROI-scale models, as the 2 highest performing models use convolutions as part\\nof their feature extraction.\\nFinally, MIL models trained with multi-objective loss outperform the ones trained with simple cross-entropy,\\nwith a 1 .3% improvement in AUROC for the ResNet18+BERT architecture, and a modest 0 .4% improvement\\nfor CCT+BERT. As such, the multi-objective ResNet18+BERT obtains the highest performance metrics across\\nall models, with an AUROC of 77 .9%, a balanced accuracy of 71 .1%, and a sensitivty of 75 .9%, a significant\\nimprovement over all baselines. The CCT+BERT model continues to perform poorly even with multi-objective\\nloss, failing to exceed its single-instance ROI counterpart (âˆ’2.5% AUROC, âˆ’4.7% Bal. Acc.). Hence, while CCT\\nmodels perform reasonably well on ROI-scale PCa detection, they do not translate as well to a multi-scale setting.'),\n",
       " Document(metadata={'arxiv_id': '2403.18233v1', 'title': 'Benchmarking Image Transformers for Prostate Cancer Detection from Ultrasound Data', 'section': 'body', 'authors': 'Mohamed Harmanani, Paul F. R. Wilson, Fahimeh Fooladgar'}, page_content='models perform reasonably well on ROI-scale PCa detection, they do not translate as well to a multi-scale setting.\\nThese results suggest that ResNet18 models are the most effective feature extractors for our micro-ultrasound\\ndataset.\\n4. CONCLUSION\\nWe conclude that given a small dataset of prostate ultrasounds such as ours, feature representations learned by\\na Transformer backbone are insufficient to exceed convolutional baselines in performance. One possible reason\\nâˆ—Multi-Objective loss function\\n4for this could be the better parameter efficiency afforded by convolutional layers, which mitigates overfitting. It\\nis also possible that convolutions are better suited for modeling features extracted from prostate ultrasounds,\\nresulting in stronger performance. Finally, we find that using multi-objective learning to combine both ROI and\\ncore loss functions improves performance sufficiently to beat both the ROI-scale baseline and the multi-scale'),\n",
       " Document(metadata={'arxiv_id': '2403.18233v1', 'title': 'Benchmarking Image Transformers for Prostate Cancer Detection from Ultrasound Data', 'section': 'body', 'authors': 'Mohamed Harmanani, Paul F. R. Wilson, Fahimeh Fooladgar'}, page_content='core loss functions improves performance sufficiently to beat both the ROI-scale baseline and the multi-scale\\nstate of the art method for PCa detection from ultrasound.\\nACKNOWLEDGMENTS\\nWe thank the Natural Sciences and Engineering Research Council of Canada (NSERC) and the Canadian Insti-\\ntutes of Health Research (CIHR) for supporting our work. Parvin Mousavi is supported by the CIFAR AI Chair\\nand the Vector Institute. Brian Wodlinger is Vice President of Clinical and Engineering at Exact Imaging. All\\npatient data was used with informed consent and approval of institutional ethics boards.\\nREFERENCES\\n[1] Ahmed, H. U., Bosaily, A. E.-S., Brown, L. C., Gabe, R., Kaplan, R., Parmar, M. K., Collaco-Moraes, Y.,\\nWard, K., Hindley, R. G., Freeman, A., et al., â€œDiagnostic accuracy of multi-parametric mri and trus biopsy\\nin prostate cancer (promis): a paired validating confirmatory study,â€ The Lancet 389(10071), 815â€“822\\n(2017).'),\n",
       " Document(metadata={'arxiv_id': '2403.18233v1', 'title': 'Benchmarking Image Transformers for Prostate Cancer Detection from Ultrasound Data', 'section': 'body', 'authors': 'Mohamed Harmanani, Paul F. R. Wilson, Fahimeh Fooladgar'}, page_content='in prostate cancer (promis): a paired validating confirmatory study,â€ The Lancet 389(10071), 815â€“822\\n(2017).\\n[2] Wilson, P. F., Gilany, M., Jamzad, A., Fooladgar, F., To, M. N. N., Wodlinger, B., Abolmaesumi, P.,\\nand Mousavi, P., â€œSelf-supervised learning with limited labeled data for prostate cancer detection in high\\nfrequency ultrasound,â€ arXiv preprint arXiv:2211.00527 (2022).\\n[3] Gilany, M., Wilson, P., Perera-Ortega, A., Jamzad, A., To, M. N. N., Fooladgar, F., Wodlinger, B., Abol-\\nmaesumi, P., and Mousavi, P., â€œTrusformer: improving prostate cancer detection from micro-ultrasound\\nusing attention and self-supervision,â€ International Journal of Computer Assisted Radiology and Surgery ,\\n1â€“8 (2023).\\n[4] Rohrbach, D., Wodlinger, B., Wen, J., Mamou, J., and Feleppa, E., â€œHigh-frequency quantitative ultrasound\\nfor imaging prostate cancer using a novel micro-ultrasound scanner,â€Ultrasound in medicine & biology 44(7),\\n1341â€“1354 (2018).'),\n",
       " Document(metadata={'arxiv_id': '2403.18233v1', 'title': 'Benchmarking Image Transformers for Prostate Cancer Detection from Ultrasound Data', 'section': 'body', 'authors': 'Mohamed Harmanani, Paul F. R. Wilson, Fahimeh Fooladgar'}, page_content='for imaging prostate cancer using a novel micro-ultrasound scanner,â€Ultrasound in medicine & biology 44(7),\\n1341â€“1354 (2018).\\n[5] Bardes, A., Ponce, J., and LeCun, Y., â€œVicreg: Variance-invariance-covariance regularization for self-\\nsupervised learning,â€ arXiv preprint arXiv:2105.04906 (2021).\\n[6] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M.,\\nMinderer, M., Heigold, G., Gelly, S., et al., â€œAn image is worth 16x16 words: Transformers for image\\nrecognition at scale,â€ arXiv preprint arXiv:2010.11929 (2020).\\n[7] Hassani, A., Walton, S., Shah, N., Abuduweili, A., Li, J., and Shi, H., â€œEscaping the big data paradigm\\nwith compact transformers,â€ arXiv preprint arXiv:2104.05704 (2021).\\n[8] Wang, W., Xie, E., Li, X., Fan, D.-P., Song, K., Liang, D., Lu, T., Luo, P., and Shao, L., â€œPyramid\\nvision transformer: A versatile backbone for dense prediction without convolutions,â€ in [ Proceedings of the'),\n",
       " Document(metadata={'arxiv_id': '2403.18233v1', 'title': 'Benchmarking Image Transformers for Prostate Cancer Detection from Ultrasound Data', 'section': 'body', 'authors': 'Mohamed Harmanani, Paul F. R. Wilson, Fahimeh Fooladgar'}, page_content='vision transformer: A versatile backbone for dense prediction without convolutions,â€ in [ Proceedings of the\\nIEEE/CVF international conference on computer vision ], 568â€“578 (2021).\\n[9] He, K., Zhang, X., Ren, S., and Sun, J., â€œDeep residual learning for image recognition,â€ in [ Proceedings of\\nthe IEEE conference on computer vision and pattern recognition ], 770â€“778 (2016).\\n[10] Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K., â€œBert: Pre-training of deep bidirectional transformers\\nfor language understanding,â€ arXiv preprint arXiv:1810.04805 (2018).\\n5'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'title_abstract', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='Title: Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images\\n\\nAbstract: We propose a novel attention gate (AG) model for medical image analysis that automatically learns to focus on target structures of varying shapes and sizes. Models trained with AGs implicitly learn to suppress irrelevant regions in an input image while highlighting salient features useful for a specific task. This enables us to eliminate the necessity of using explicit external tissue/organ localisation modules when using convolutional neural networks (CNNs). AGs can be easily integrated into standard CNN models such as VGG or U-Net architectures with minimal computational overhead while increasing the model sensitivity and prediction accuracy. The proposed AG models are evaluated on a variety of tasks, including medical image classification and segmentation. For classification, we demonstrate the use case of AGs in scan plane detection for fetal ultrasound screening. We show that the proposed attention mechanism can provide efficient object localisation while improving the overall prediction performance by reducing false positives. For segmentation, the proposed architecture is evaluated on two large 3D CT abdominal datasets with manual annotations for multiple organs. Experimental results show that AG models consistently improve the prediction performance of the base architectures across different datasets and training sizes while preserving computational efficiency. Moreover, AGs guide the model activations to be focused around salient regions, which provides better insights into how model predictions are made. The source code for the proposed AG models is publicly available.'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='Attention Gated Networks:\\nLearning to Leverage Salient Regions in Medical Images\\nJo Schlempera,1, Ozan Oktaya,b,1, Michiel Schaapb, Mattias Heinrichc,\\nBernhard Kainza, Ben Glockera, Daniel Rueckerta\\naBioMedIA, Imperial College London, SW7 2AZ, London, UK\\nbHeartFlow, Redwood City, CA 94063, USA\\ncMedical Informatics, University of Luebeck, DE\\nAbstract\\nWe propose a novel attention gate (AG) model for medical image analysis that\\nautomatically learns to focus on target structures of varying shapes and sizes.\\nModels trained with AGs implicitly learn to suppress irrelevant regions in an input\\nimage while highlighting salient features useful for a speciï¬c task. This enables\\nus to eliminate the necessity of using explicit external tissue/organ localisation\\nmodules when using convolutional neural networks (CNNs). AGs can be easily\\nintegrated into standard CNN models such as VGG or U-Net architectures with\\nminimal computational overhead while increasing the model sensitivity and'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='integrated into standard CNN models such as VGG or U-Net architectures with\\nminimal computational overhead while increasing the model sensitivity and\\nprediction accuracy. The proposed AG models are evaluated on a variety of\\ntasks, including medical image classiï¬cation and segmentation. For classiï¬cation,\\nwe demonstrate the use case of AGs in scan plane detection for fetal ultrasound\\nscreening. We show that the proposed attention mechanism can provide eï¬ƒcient\\nobjectlocalisationwhileimprovingtheoverallpredictionperformancebyreducing\\nfalse positives. For segmentation, the proposed architecture is evaluated on two\\nlarge 3D CT abdominal datasets with manual annotations for multiple organs.\\nExperimental results show that AG models consistently improve the prediction\\nperformance of the base architectures across diï¬€erent datasets and training\\nsizes while preserving computational eï¬ƒciency. Moreover, AGs guide the model'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='performance of the base architectures across diï¬€erent datasets and training\\nsizes while preserving computational eï¬ƒciency. Moreover, AGs guide the model\\nactivations to be focused around salient regions, which provides better insights\\ninto how model predictions are made. The source code for the proposed AG\\nmodels is publicly available.\\nKeywords: Fully Convolutional Networks, Image Classiï¬cation, Localisation,\\nSegmentation, Soft Attention, Attention Gates\\n1The corresponding authors contributed equally.\\n2Â©2019. This manuscript version is made available under the CC-BY-NC-ND 4.0 license\\n(http://creativecommons.org/licenses/by-nc-nd/4.0/)\\nAccepted for Medical Image Analysis (Special Issue) January 23, 2019\\narXiv:1808.08114v2  [cs.CV]  20 Jan 20191. Introduction\\nAutomated medical image analysis has been extensively studied in the medical\\nimaging community due to the fact that manual labelling of large amounts of'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='Automated medical image analysis has been extensively studied in the medical\\nimaging community due to the fact that manual labelling of large amounts of\\nmedical images is a tedious and error-prone task. Accurate and reliable solutions\\nare required to increase clinical work ï¬‚ow eï¬ƒciency and support decision making\\nthrough fast and automatic extraction of quantitative measurements.\\nWith the advent of convolutional neural networks (CNNs), near-radiologist\\nlevel performance can be achieved in automated medical image analysis tasks\\nincluding classiï¬cation of Alzheimerâ€™s disease (Sarraf et al., 2017), skin lesions\\n(Esteva et al., 2017; Kawahara and Hamarneh, 2016) and echo-cardiogram views\\n(Madani et al., 2018), lung nodule detection in CT/X-ray (Liao et al., 2017; Zhu\\net al., 2018) and cardiac MR segmentation (Bai et al., 2017). An extensive list of\\napplications can be found in (Litjens et al., 2017; Zaharchuk et al., 2018). High'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='et al., 2018) and cardiac MR segmentation (Bai et al., 2017). An extensive list of\\napplications can be found in (Litjens et al., 2017; Zaharchuk et al., 2018). High\\nrepresentation power, fast inference, and weight sharing properties have made\\nCNNs the de facto standard for image classiï¬cation and segmentation.\\nMethods for existing applications rely heavily on multi-stage, cascaded CNNs\\nwhen the target organs show large inter-patient variation in terms of shape\\nand size. Cascaded frameworks extract a region of interest (ROI) and make\\ndense predictions on that particular ROI. The application areas include cardiac\\nMRI (Khened et al., 2018), cardiac CT (Payer et al., 2017), abdominal CT\\n(Roth et al., 2017, 2018) segmentation, and lung CT nodule detection (Liao\\net al., 2017). However, this approach leads to excessive and redundant use of\\ncomputational resources and model parameters; for instance, similar low-level\\nfeatures are repeatedly extracted by all models within the cascade.'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='computational resources and model parameters; for instance, similar low-level\\nfeatures are repeatedly extracted by all models within the cascade.\\nTo address this general problem, we propose a simple and yet eï¬€ective solution,\\nnamed attention gates(AGs). By incorporating AGs into standard CNN models,\\nmodel parameters and intermediate feature maps are expected to be utilised more\\neï¬ƒciently while minimising the necessity of cascaded models to solve localisation\\nand classiï¬cation tasks separately. In more detail, AGs automatically learn to\\nfocus on target structures without additional supervision. At test time, these\\ngates generate soft region proposals implicitly on-the-ï¬‚y and highlight salient\\nfeatures useful for a speciï¬c task. In return, the proposed AGs improve model\\nsensitivity and accuracy for global and dense label predictions by suppressing\\nfeature activations in irrelevant regions. In this way, the necessity of using an'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='sensitivity and accuracy for global and dense label predictions by suppressing\\nfeature activations in irrelevant regions. In this way, the necessity of using an\\nexternal organ localisation module can be eliminated while maintaining the high\\nprediction accuracy. In addition, they do not introduce signiï¬cant computational\\noverhead and do not require a large number of model parameters as in the\\ncase of multi-model frameworks. CNN models with AGs can be trained from\\nscratch in a standard way similar to the training of fully convolutional network\\n(FCN) models. Similar attention mechanisms have been proposed for natural\\nimage classiï¬cation (Jetley et al., 2018) and captioning (Anderson et al., 2017)\\nto perform adaptive feature pooling, where model predictions are conditioned\\nonly on a subset of selected image regions. In this paper, we generalise this\\ndesign and propose image-grid based gating that allows attention coeï¬ƒcients to\\nbe speciï¬c to local regions.'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='only on a subset of selected image regions. In this paper, we generalise this\\ndesign and propose image-grid based gating that allows attention coeï¬ƒcients to\\nbe speciï¬c to local regions.\\n2We demonstrate the performance of AG in real-time fetal ultrasound scan\\nplane detection and CT pancreas segmentation. The ï¬rst task is challenging due\\nto low interpretability of the images and localising the object of interest is key\\nto successful classiï¬cation of the plane. To this end, we incorporate AGs into a\\nvariant of a VGG network, termed AG-Sononet, to demonstrate that attention\\nmechanism can automatically localise the object of interest and improve the\\noverall classiï¬cation performance. The second task of pancreas segmentation\\nis challenging due to low tissue contrast and large variability in organ shape\\nand size. Moreover, we extend a standard U-Net architecture (Attention U-Net).\\nWe choose to evaluate our implementation on two commonly used benchmarks:'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='and size. Moreover, we extend a standard U-Net architecture (Attention U-Net).\\nWe choose to evaluate our implementation on two commonly used benchmarks:\\nTCIA PancreasCT-82 (Roth et al., 2016) and multi-class abdominalCT-150.\\nThe results show that AGs consistently improve prediction accuracy across\\ndiï¬€erent datasets and training sizes while achieving state-of-the-art performance\\nwithout requiring multiple CNN models.\\n1.1. Related Work\\nAttention Gates: AGs are commonly used in classiï¬cation tasks such as\\nin the analysis of citation graphs (VeliÄkoviÄ‡ et al., 2017) and natural images\\n(Jetley et al., 2018; Wang et al., 2017a). Similarly in the context of natural\\nlanguage processing (NLP), such as image captioning (Anderson et al., 2017) and\\nmachine translation (Bahdanau et al., 2014; Luong et al., 2015; Shen et al., 2017;\\nVaswani et al., 2017), there have been several use cases of soft-attention models\\nto eï¬ƒciently use the given context information. In particular, given a sequence of'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='Vaswani et al., 2017), there have been several use cases of soft-attention models\\nto eï¬ƒciently use the given context information. In particular, given a sequence of\\ntext and a current word, a task is to extract a next word in a sentence generation\\nor translation. The idea of attention mechanisms is to generate acontext vector\\nwhich assigns weights on the input sequence. Thus, the signal highlights the\\nsalient feature of the sequence conditioned on the current word while suppressing\\nthe irrelevant counter-parts, making the prediction more contextualised.\\nInitial work on attention modelling has explored salient image regions by\\ninterpreting gradient of output class scores with respect to the input image.\\nTrainable attention, on the other hand, is enforced by design and categorised as\\nhard- and soft-attention. Hard attention (Mnih et al., 2014), e.g. iterative region\\nproposal and cropping, is often non-diï¬€erentiable and relies on reinforcement'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='hard- and soft-attention. Hard attention (Mnih et al., 2014), e.g. iterative region\\nproposal and cropping, is often non-diï¬€erentiable and relies on reinforcement\\nlearning for parameter updates, which makes model training more diï¬ƒcult.\\nYpsilantis and Montana (2017) used recursive hard-attention to detect anomalies\\nin chest X-ray scans. Contrarily, soft attention is probabilistic, end-to-end\\ndiï¬€erentiable, and utilises standard back-propagation without need for posterior\\nsampling. For instance, additive soft attention is used in sentence-to-sentence\\ntranslation (Bahdanau et al., 2014; Shen et al., 2017) and more recently applied\\nto image classiï¬cation (Jetley et al., 2018; Wang et al., 2017a).\\nIn computer vision, attention mechanisms are applied to a variety of problems,\\nincluding image classiï¬cation (Jetley et al., 2018; Wang et al., 2017a; Zhao et al.,\\n2017), segmentation (Ren and Zemel, 2016), action recognition (Liu et al., 2017;'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='including image classiï¬cation (Jetley et al., 2018; Wang et al., 2017a; Zhao et al.,\\n2017), segmentation (Ren and Zemel, 2016), action recognition (Liu et al., 2017;\\nPei et al., 2016; Wang et al., 2017b), image captioning (Lu et al., 2016; Xu\\net al., 2015), and visual question answering (Nam et al., 2016; Yang et al.,\\n2015). Hu et al. (2017) used channel-wise attention to highlight important\\n3feature dimensions, which was the top-performer in the ILSVRC 2017 image\\nclassiï¬cation challenge. Similarly, non-local self attention was used by Wang\\net al. (2017b) to capture long range dependencies.\\nIn the context of medical image analysis, attention models have been exploited\\nfor medical report generation (Zhang et al., 2017a,b) as well as joint image and\\ntext classiï¬cation (Wang et al., 2018). However, for standard medical image\\nclassiï¬cation, despite often the information to be classiï¬ed are extremely localised,\\nonly a handful of works use attention mechanisms (Guan et al., 2018; Pesce'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='classiï¬cation, despite often the information to be classiï¬ed are extremely localised,\\nonly a handful of works use attention mechanisms (Guan et al., 2018; Pesce\\net al., 2017). In these methods, either bounding box labels are available to guide\\nthe attention, or local context is extracted by a hard-attention model (i.e. region\\nproposal followed by hard-cropping).\\n2D Ultrasound Scan Plane Detection: Fetal ultrasound screening is an\\nimportant diagnostic protocol to detect abnormal fetal development. During\\nscreening examination, multiple anatomically standardised (NHS Screening\\nProgrammes, 2015) scan planes are used to obtain biometric measurements as\\nwell as identifying abnormalities such as lesions. Ultrasound suï¬€ers from low\\nsignal-to-noise ratio and image artefacts. As such, diagnostic accuracy and\\nreproducibility is limited and requires a high level of expert knowledge and\\ntraining. In the past, several approaches were proposed (Chen et al., 2015;'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='reproducibility is limited and requires a high level of expert knowledge and\\ntraining. In the past, several approaches were proposed (Chen et al., 2015;\\nYaqub et al., 2015), however, they are computationally expensive and cannot be\\ndeployed for the real-time application. More recently, Baumgartner et al. (2016)\\nproposed a CNN architecture calledSononet. It achieves very good performance\\nin real-time plane detection, retrospective frame retrieval (retrieving the most\\nrelevant frame) and weakly supervised object localisation. However, it suï¬€ers\\nfrom low recall value in diï¬€erentiating diï¬€erent planar views of the cardiac\\nchambers, which requires the method to be able to exploit the subtle diï¬€erences\\nin the local structure and it makes the problem challenging.\\nPancreas Segmentation in 3D-CT Images: Early work on pancreas\\nsegmentation from abdominal CT used statistical shape models (Cerrolaza\\net al., 2016; Saito et al., 2016) or multi-atlas techniques (Oda et al., 2017;'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='segmentation from abdominal CT used statistical shape models (Cerrolaza\\net al., 2016; Saito et al., 2016) or multi-atlas techniques (Oda et al., 2017;\\nWolz et al., 2013). In particular, atlas approaches beneï¬t from implicit shape\\nconstraints enforced by propagation of manual annotations. However, in public\\nbenchmarks such as the TCIA dataset (Roth et al., 2016), Dice similarity\\ncoeï¬ƒcients (DSC) for atlas-based frameworks are relatively low, ranging from\\n69.6% to 73.9% (Oda et al., 2017; Wolz et al., 2013). A classiï¬cation based\\nframework was proposed by Zografos et al. (2015) to remove the dependency of\\natlas to image registration. Recently, cascaded multi-stage CNN models (Roth\\net al., 2017, 2018; Zhou et al., 2017) have been proposed to address the problem.\\nHere, an initial coarse-level model (e.g. U-Net or Regression Forest) is used to\\nobtain a ROI and then a cropped ROI is used for segmentation reï¬nement by a'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='Here, an initial coarse-level model (e.g. U-Net or Regression Forest) is used to\\nobtain a ROI and then a cropped ROI is used for segmentation reï¬nement by a\\nsecond model. Similarly, combinations of 2D-FCN and recurrent neural network\\n(RNN) models are utilised by Cai et al. (2017) to exploit dependencies between\\nadjacent axial slices. These approaches achieve state-of-the-art performance\\nin the TCIA benchmark (81.2%âˆ’ 82.4% DSC). Without using a cascaded\\nframework, the performance drops between2.0 and 4.4 DSC points. Recently,\\nYu et al. (2017) proposed an iterative two-stage model that recursively updates\\n4local and global predictions, and both models are trained end-to-end. Besides\\nstandard FCNs, dense connections (Gibson et al., 2017) and sparse convolutions\\n(Heinrich et al., 2018; Heinrich and Oktay, 2017) have been applied to the CT\\npancreas segmentation problem. Dense connections and sparse kernels reduce\\ncomputational complexity by requiring less number of non-zero parameters.'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='pancreas segmentation problem. Dense connections and sparse kernels reduce\\ncomputational complexity by requiring less number of non-zero parameters.\\n1.2. Contributions\\nIn this paper, we propose a novel soft-attention gating module that can be\\nutilised in CNN based standard image analysis models for dense label predictions.\\nAdditionally, we explore the beneï¬t of AGs to medical image analysis, in partic-\\nular, in the context of image classiï¬cation and segmentation. The contributions\\nof this work can be summarised as follows:\\nâ€¢ We take the attention approach proposed by Jetley et al. (2018) a step\\nfurther by proposing grid-based gating that allows attention gates to be\\nmore speciï¬c to local regions. This improves performance compared to\\ngating based on a global feature vector. Moreover, our approach is not\\nonly limited to adaptive pooling (Jetley et al., 2018) but can be also used\\nfor dense predictions as in segmentation networks.'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='gating based on a global feature vector. Moreover, our approach is not\\nonly limited to adaptive pooling (Jetley et al., 2018) but can be also used\\nfor dense predictions as in segmentation networks.\\nâ€¢ We propose one of the ï¬rst use cases of soft-attention in a feed-forward\\nCNN model applied to a medical imaging task that is end-to-end trainable.\\nThe proposed attention gates can replace hard-attention approaches used\\nin image classiï¬cation (Ypsilantis and Montana, 2017) and external organ\\nlocalisation models in image segmentation frameworks (Khened et al.,\\n2018; Oda et al., 2017; Roth et al., 2017, 2018). This also eliminates the\\nneed for any bounding box labels and backpropagation-based saliency map\\ngeneration used by Baumgartner et al. (2016).\\nâ€¢ For classiï¬cation, we apply the proposed model to real-time fetal ultrasound\\nscan plane detection and show its superior classiï¬cation performance over\\nthe baseline approach. We show that attention maps can used for fast'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='scan plane detection and show its superior classiï¬cation performance over\\nthe baseline approach. We show that attention maps can used for fast\\n(weakly-supervised) object localisation, demonstrating that the attended\\nfeatures indeed correlate with the anatomy of interest.\\nâ€¢ For segmentation, an extension to the standard U-Net model is proposed\\nthat provides increased sensitivity without the need of complicated heuris-\\ntics, while not sacriï¬cing speciï¬city. We demonstrate that accuracy im-\\nprovements when using U-Net are consistent across diï¬€erent imaging\\ndatasets and training sizes.\\nâ€¢ We demonstrate that the proposed attention mechanism provides ï¬ne-\\nscale attention maps that can be visualised, with minimal computational\\noverhead, which helps with interpretability of predictions.\\n5Figure 1: Axial (a) and sagittal (f) views of a 3DCT scan, (b,g) attention coeï¬ƒcients,\\nimage feature activations before (c,h) and after attention gating (d,e,i,j). Similarly, (k-n)'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='5Figure 1: Axial (a) and sagittal (f) views of a 3DCT scan, (b,g) attention coeï¬ƒcients,\\nimage feature activations before (c,h) and after attention gating (d,e,i,j). Similarly, (k-n)\\nvisualise the gating on a coarse scale skip connection. The ï¬ltered feature activations\\n(d,e,i,j) are collected from multiple AGs, where a subset of organs is selected by each\\ngate and activations consistently correspond to speciï¬c structures across diï¬€erent scans.\\n2. Methodology\\n2.1. Convolutional Neural Network\\nCNNs are now the state-of-the-art method for many tasks including classiï¬-\\ncation , localisation and segmentation (Bai et al., 2017; Kamnitsas et al., 2017,\\n2018; Lee et al., 2015; Litjens et al., 2017; Long et al., 2015; Ronneberger et al.,\\n2015; Roth et al., 2017, 2018; Xie and Tu, 2015; Zaharchuk et al., 2018). CNNs\\noutperform traditional approaches in medical image analysis while being an order\\nof magnitude faster than, e.g., graph-cut and multi-atlas segmentation techniques'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='outperform traditional approaches in medical image analysis while being an order\\nof magnitude faster than, e.g., graph-cut and multi-atlas segmentation techniques\\n(Wolz et al., 2013). The success of CNNs is attributed to the fact that (I) domain\\nspeciï¬c image features are learnt using stochastic gradient descent (SGD) optimi-\\nsation, (II) learnt kernels are shared across all pixels, and (III) image convolution\\noperations exploit the structural information in medical images in an optimal\\nfashion. However, it remains diï¬ƒcult to reduce false-positive predictions for small\\nobjects that show large shape variability. In such cases, in order to improve the\\naccuracy, current frameworks (Guan et al., 2018; Khened et al., 2018; Roth et al.,\\n2017, 2018) rely on additional preceding object localisation models to simplify\\nthe task into separate localisation and subsequent classiï¬cation/segmentation\\nsteps, or guide the localisation using weak labels (Pesce et al., 2017). Here, we'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='the task into separate localisation and subsequent classiï¬cation/segmentation\\nsteps, or guide the localisation using weak labels (Pesce et al., 2017). Here, we\\ndemonstrate that the same objective can be achieved by integrating attention\\ngates (AGs) in a standard CNN model. This does not require the training of\\nmultiple models and a large number of extra model parameters. In contrast to\\nthe localisation model in multi-stage CNNs, AGs progressively suppress feature\\nresponses in irrelevant background regions without the requirement to crop a\\nROI between networks.\\n6 \\n x x x \\n x x x \\nReLU \\nx \\n \\nResampler\\nx x \\n \\nFigure 2: Schematic of the proposed additive attention gate (AG). Input features (xl) are\\nscaled with attention coeï¬ƒcients (Î±) computed in AG. Spatial regions are selected by analysing\\nboth the activations and contextual information provided by the gating signal (g) which is\\ncollected from a coarser scale. Grid resampling of attention coeï¬ƒcients is performed using'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='both the activations and contextual information provided by the gating signal (g) which is\\ncollected from a coarser scale. Grid resampling of attention coeï¬ƒcients is performed using\\ntrilinear interpolation.\\n2.2. Attention Gate Module\\nWe now introduceAttention Gate(AG), which is a mechanism which can\\nbe incorporated in any existing CNN architecture. Letxl ={xl\\ni}n\\ni=1 be the\\nactivation map of a chosen layerlâˆˆ{ 1,...,L }, where eachxl\\ni represents the pixel-\\nwise feature vector of lengthFl (i.e. the number of channels). For eachxl\\ni, AG\\ncomputes coeï¬ƒcientsÎ±l ={Î±l\\ni}n\\n=1, whereÎ±l\\niâˆˆ [0, 1], in order to identify salient\\nimageregionsandprunefeatureresponsestopreserveonlytheactivationsrelevant\\nto the speciï¬c task as shown in Figure 1. The output of AG isË†xl ={Î±l\\nixl\\ni}n\\ni=1,\\nwhere each feature vector is scaled by the corresponding attention coeï¬ƒcient.\\nThe attention coeï¬ƒcientsÎ±l\\ni are computed as follows: In standard CNN'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='ixl\\ni}n\\ni=1,\\nwhere each feature vector is scaled by the corresponding attention coeï¬ƒcient.\\nThe attention coeï¬ƒcientsÎ±l\\ni are computed as follows: In standard CNN\\narchitectures, to capture a suï¬ƒciently large receptive ï¬eld and thus, semantic\\ncontextual information, the feature-map is gradually downsampled. The features\\non the coarse spatial grid level identify location of the target objects and model\\ntheir relationship at global scale. Letgâˆˆ RFg be such global feature vector and\\nprovide information to AGs to disambiguate task-irrelevant feature content in\\nxl\\ni. The idea is to consider eachxl\\ni and g jointly to attend the features at each\\nscale l that are most relevant to the objective being minimised.\\nThere are two commonly used attention types: multiplicative (Luong et al.,\\n2015) and additive attention (Bahdanau et al., 2014). The former is faster to\\ncompute and more memory-eï¬ƒcient in practice since it can be implemented as a'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='2015) and additive attention (Bahdanau et al., 2014). The former is faster to\\ncompute and more memory-eï¬ƒcient in practice since it can be implemented as a\\nmatrix multiplication. However, additive attention is experimentally shown to\\nbe performing better for large dimensional input features (Britz et al., 2017).\\nFor this reason, we use the latter to obtain the gating coeï¬ƒcient as can be seen\\nin Figure 2, which is formulated as follows:\\nql\\natt,i = ÏˆT(\\nÏƒ1 ( W T\\nx xl\\ni + W T\\ng g + bxg )\\n)\\n+bÏˆ (1)\\nÎ±l =Ïƒ2(ql\\natt(xl, g ; Î˜att) ), (2)\\nwhere Ïƒ1(x) is an element-wise nonlinearity (e.g. rectiï¬ed linear-unit) andÏƒ2(x)\\nis a normalisation function. For example, one can apply sigmoid to restrict\\nthe range to[0, 1], or one can apply softmax operationÎ±l\\ni = eql\\natt,i/âˆ‘\\nieql\\natt,i\\n7such that the attention map sums to 1. AG is therefore characterised by\\na set of parametersÎ˜att containing: linear transformations Wxâˆˆ RFlÃ—Fint,\\nWgâˆˆ RFgÃ—Fint, Ïˆâˆˆ RFintÃ—1 and bias termsbÏˆâˆˆ R , bxgâˆˆ RFint. The linear'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='a set of parametersÎ˜att containing: linear transformations Wxâˆˆ RFlÃ—Fint,\\nWgâˆˆ RFgÃ—Fint, Ïˆâˆˆ RFintÃ—1 and bias termsbÏˆâˆˆ R , bxgâˆˆ RFint. The linear\\ntransformations are computed using channel-wise1Ã— 1Ã— 1 convolutions.\\nWenotethatAGparameterscanbetrainedwiththestandardback-propagation\\nupdates without a need for sampling based optimisation methods as used in\\nhard-attention (Mnih et al., 2014). While AG does not require auxiliary loss\\nfunction to optimise, we found that using deep-supervision (Lee et al., 2015)\\nencourages the intermediate feature-maps to be semantically discriminative at\\neach image scale. This ensures that attention units, at diï¬€erent scales, have an\\nability to inï¬‚uence the responses to a large range of image foreground content.\\nWe therefore prevent dense predictions from being reconstructed from small\\nsubsets of gated feature-maps.\\n2.2.1. Multi-dimensional Attention\\nIn case of where multiple semantic classes are present in the image, one can'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='subsets of gated feature-maps.\\n2.2.1. Multi-dimensional Attention\\nIn case of where multiple semantic classes are present in the image, one can\\nlearn multi-dimensional attention coeï¬ƒcients. This is inspired by the approach\\nof Shen et al. (2017), where multi-dimensional attention coeï¬ƒcients are used\\nto learn sentence embeddings. Thus, each AG learns to focus on a subset of\\ntarget structures. In case of multi-dimensional AGs, eachÎ±l corresponds to a\\nvector and produce Ë†xl = [Î±l\\n(1)âŠ™ xl,...,Î± l\\n(m)âŠ™ xl] where Î±l\\n(k) is k-th sub AG\\nandâŠ™ is element-wise multiplication operation. In each sub-AG, complementary\\ninformation is extracted and fused to deï¬ne the output of skip connection.\\n2.2.2. Gating Signal and Grid Attention\\nAs the gating signalg must encode global information from large spatial\\ncontext, it is usually obtained from the coarsest scale activation map. For\\nexample in classiï¬cation, one could use the activation map just before the ï¬nal'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='context, it is usually obtained from the coarsest scale activation map. For\\nexample in classiï¬cation, one could use the activation map just before the ï¬nal\\nsoftmax layer. In the context of medical imaging, however, since most objects\\nof interest are highly localised, ï¬‚attening may have the disadvantage of losing\\nimportant spatial context. In fact, in many cases a few max-pooling operations\\nare suï¬ƒcient to infer the global context without explicitly using the global\\npooling. Therefore, we propose agrid attention mechanism. The idea is to use\\nthe coarse scale feature map before any ï¬‚attening is done. For example, given\\nan input tensor size ofFlÃ—HxÃ—Wx, afterr max pooling operations, the tensor\\nsize is reduced toFgÃ—HgÃ—Wg =FgÃ—Hx/(2r)Ã—Wy/(2r). To generate the\\nattention map, we can either downsample or upsample the coarse grid to match\\nthe spatial resolution ofxl. In this way, the attention mechanism has more\\nï¬‚exibility in terms of what to focus on a regional basis. For upsampling, we'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='the spatial resolution ofxl. In this way, the attention mechanism has more\\nï¬‚exibility in terms of what to focus on a regional basis. For upsampling, we\\nchose to use bilinear upsampling. Note that the upsampling can be replaced by\\na learnable weight, however, we did not opt for this for the sake of simplicity.\\nFor segmentation, one can directly use the coarsest activation map as the gating\\nsignal.\\n8Figure 3: A block diagram of the proposed Attention U-Net segmentation model. Input image\\nis progressively ï¬ltered and downsampled by factor of2 at each scale in the encoding part\\nof the network (e.g.H4 =H1/8). Nc denotes the number of classes. Attention gates (AGs)\\nï¬lter the features propagated through the skip connections. Schematic of the AGs is shown\\nin Figure 2. Feature selectivity in AGs is achieved by use of contextual information (gating)\\nextracted in coarser scales.\\n2.2.3. Backward Pass through Attention Gates\\nInformation extracted from coarse scale is used in gating to disambiguate'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='extracted in coarser scales.\\n2.2.3. Backward Pass through Attention Gates\\nInformation extracted from coarse scale is used in gating to disambiguate\\nirrelevant and noisy responses in input feature-maps. For instance, in the\\nU-Net architecture, gating is performed on skip connections right before the\\nconcatenation to merge only relevant activations. Additionally, AGs ï¬lter the\\nneuron activations during the forward pass as well as during the backward pass.\\nGradients originating from background regions are down weighted during the\\nbackward pass. This allows model parameters in shallower layers to be updated\\nmostly based on spatial regions that are relevant to a given task. The update\\nrule for convolution parameters in layerlâˆ’ 1 can be formulated as follows:\\nâˆ‚(Ë†xl\\ni)\\nâˆ‚ (Î¦lâˆ’1) = âˆ‚\\n(\\nÎ±l\\nif(xlâˆ’1\\ni ; Î¦lâˆ’1)\\n)\\nâˆ‚ (Î¦lâˆ’1) =Î±l\\ni\\nâˆ‚(f(xlâˆ’1\\ni ; Î¦lâˆ’1))\\nâˆ‚ (Î¦lâˆ’1) + âˆ‚(Î±l\\ni)\\nâˆ‚ (Î¦lâˆ’1)xl\\ni (3)\\nwhere the ï¬rst gradient term on the right-hand side is scaled withÎ±l\\ni.\\n2.3. Attention Gates for Segmentation'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='i ; Î¦lâˆ’1)\\n)\\nâˆ‚ (Î¦lâˆ’1) =Î±l\\ni\\nâˆ‚(f(xlâˆ’1\\ni ; Î¦lâˆ’1))\\nâˆ‚ (Î¦lâˆ’1) + âˆ‚(Î±l\\ni)\\nâˆ‚ (Î¦lâˆ’1)xl\\ni (3)\\nwhere the ï¬rst gradient term on the right-hand side is scaled withÎ±l\\ni.\\n2.3. Attention Gates for Segmentation\\nIn this work, we build our attention-gated segmentation model on top of a\\nstandard 3D U-Net architecture. U-Nets are commonly used for image segmen-\\ntation tasks because of their good performance and eï¬ƒcient use of GPU memory.\\nThe latter advantage is mainly linked to extraction of image features at multiple\\nimage scales. Coarse feature-maps capture contextual information and highlight\\nthe category and location of foreground objects. Feature-maps extracted at\\nmultiple scales are later merged through skip connections to combine coarse-\\nand ï¬ne-level dense predictions as shown in Figure 3. The proposed AGs are\\n9Figure 4: The schematics of the proposed attention-gated classiï¬cation model,AG-Sononet.\\nThe proposed attention units are incorporated in layer 11 and layer 14. The attention maps'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='9Figure 4: The schematics of the proposed attention-gated classiï¬cation model,AG-Sononet.\\nThe proposed attention units are incorporated in layer 11 and layer 14. The attention maps\\nare summed along the spatial axes, resulting in vectors withFli features. The vectors are\\ncombined using fully connected layers at aggregation stage to yield ï¬nal predictions.\\nincorporated into the standard U-Net architecture to highlight salient features\\nthat are passed through the skip connections. For AGs, we chose sigmoid activa-\\ntion function for normalisation:Ïƒ2(x) = 1\\n1+exp(âˆ’x). While in image captioning\\n(Anderson et al., 2017) and classiï¬cation (Jetley et al., 2018) tasks, the softmax\\nactivation function is used to normalise the attention coeï¬ƒcientsÏƒ2, however,\\nsequential use of softmax yields sparser activations at the output. For dense\\nprediction task, we empirically observed that sigmoid resulted in better training\\nconvergence for the AG parameters.\\n2.4. Attention Gates for Classiï¬cation'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='prediction task, we empirically observed that sigmoid resulted in better training\\nconvergence for the AG parameters.\\n2.4. Attention Gates for Classiï¬cation\\nFor attention-gated classiï¬cation model, we choseSononet (Baumgartner\\net al., 2016) to be our base architecture, which is a variant of VGG network\\n(Simonyan and Zisserman, 2014). The diï¬€erence is that Sononet can be decoupled\\ninto feature extraction module and adaptation module. In the adaptation module,\\nthe number of channels are ï¬rst reduced to the number of target classesC.\\nSubsequently, the spatial information is ï¬‚attened via channel-wise global average\\npooling. Finally, a softmax operation is applied to the resulting vector and the\\nentry with maximum activation is selected as the prediction. As the network\\nis constrained to classify based on the reduced vector, the network is forced to\\nextract the most salient features for each class.\\nThe proposed attention mechanism is incorporated in the Sononet architec-'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='extract the most salient features for each class.\\nThe proposed attention mechanism is incorporated in the Sononet architec-\\nture to better exploit local information. In the modiï¬ed architecture, termed\\nAttention-Gated Sononet (AG-Sononet), we remove the adaptation module. The\\nï¬nal layer of the feature extraction module is used as gridded global feature map\\ng. We apply the proposed attention mechanism to layer 11 and 14 just before\\npooling, We empirically found that attention gates were less eï¬€ective if applied to\\nthe earliest layer. We speculate that this is because ï¬rst few layers only represent\\nlow-level features, which is not discriminative yet to be attended. The proposed\\narchitecture is shown in Figure 4. After the attention coeï¬ƒcients{Î±l\\ni}n\\ni=1 are\\nobtained, the weighted average over the spatial axes is computed, yielding a\\nvector of lengthFl at scalel: Ëœxl =âˆ‘n\\ni=1Î±l\\nixl\\ni. In addition, we also perform'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='i}n\\ni=1 are\\nobtained, the weighted average over the spatial axes is computed, yielding a\\nvector of lengthFl at scalel: Ëœxl =âˆ‘n\\ni=1Î±l\\nixl\\ni. In addition, we also perform\\n10the global average pooling on the coarsest scale representation. The prediction\\nis given by ï¬tting a fully connected layer on the concatenated feature vector\\n{Ëœxl1, Ëœxl2, Ëœxl3} (e.g. l1 = 11,l 2 = 14,l 3 = 17). We note that for AG-sononet,\\nwe normalised the attention coeï¬ƒcients asÎ±l\\ni = (Î±l\\niâˆ’Î±l\\nmin/âˆ‘\\nj(Î±l\\njâˆ’Î±l\\nmin)),\\nwhere Î±l\\nmin = minjÎ±l\\nj, as we realised that softmax output was often too sparse,\\nmaking the prediction more challenging.\\nGiven the attended feature vectors at diï¬€erent scales, we highlight that the\\naggregation strategy is ï¬‚exible and that it can be adjusted depending on the\\ntarget problem. We empirically observed that a combination of deep-supervision\\n(Lee et al., 2015) for each scale followed by ï¬ne-tuning using a new FC layer\\nï¬tted on the concatenated vector gave the best performance.'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='(Lee et al., 2015) for each scale followed by ï¬ne-tuning using a new FC layer\\nï¬tted on the concatenated vector gave the best performance.\\n3. Experiments and Results\\nThe proposed AG model is modular and independent of application type;\\nas such it can be easily adapted for pixel and image level classiï¬cation tasks.\\nTo demonstrate its applicability to image classiï¬cation and segmentation, we\\nevaluate the proposed attention based FCN models on challenging abdominal CT\\nmulti-label segmentation and 2D ultrasound image plane classiï¬cation problems.\\nIn particular, pancreas boundary delineation is a diï¬ƒcult task due to shape-\\nvariability and poor tissue contrast, similarly image quality and subject variability\\nintroduce challenges in 2D-US image classiï¬cation. Our models are compared\\nagainst the standard 3D U-Net and Sononet in terms of model prediction\\nperformance, model capacity, computation time, and memory requirements.\\n3.1. Evaluation Datasets'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='against the standard 3D U-Net and Sononet in terms of model prediction\\nperformance, model capacity, computation time, and memory requirements.\\n3.1. Evaluation Datasets\\nIn this section, we present the image datasets used in classiï¬cation and\\nsegmentation experiments.\\n3.1.1. 3D-CT Abdominal Image Datasets\\nFor the experiments, two diï¬€erent CT abdominal datasets are used: (I) 150\\nabdominal 3D CT scans acquired from patients diagnosed with gastric cancer\\n(CT-150). In all images, the pancreas, liver, and spleen boundaries were semi-\\nautomatically delineated by three trained researchers and manually veriï¬ed by a\\nclinician. The same dataset is used by Roth et al. (2017) to benchmark the U-Net\\nmodel in pancreas segmentation. (II) The second dataset3 (CT-82) consists of\\n82 contrast enhanced 3D CT scans with pancreas manual annotations performed\\nslice-by-slice. This dataset (NIH-TCIA) (Roth et al., 2016) is publicly available'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='82 contrast enhanced 3D CT scans with pancreas manual annotations performed\\nslice-by-slice. This dataset (NIH-TCIA) (Roth et al., 2016) is publicly available\\nand commonly used to benchmark CT pancreas segmentation frameworks. The\\nimages from both datasets are downsampled to isotropic2.00 mm resolution due\\nto the large image size and hardware memory limitations.\\n3https://wiki.cancerimagingarchive.net/display/Public/Pancreas-CT\\n113.1.2. 2D Fetal Ultrasound Image Dataset\\nOur dataset consisted of 2694 2D ultrasound examinations of volunteers with\\ngestational ages between 18 and 22 weeks. The dataset contains 13 types of\\nstandard scan planes and background, complying the standard speciï¬ed in the\\nUK National Health Service (NHS) fetal anomaly screening programme (FASP)\\nhandbook (NHS Screening Programmes, 2015). The standard scan planes are:\\nBrain (Cb.), Brain (Tv.), Proï¬le, Lips, Abdominal, Kidneys, Femur, Spine\\n(Cor.), Spine (Sag.), 4CH, 3VV, RVOT, LVOT. The dataset further includes'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='Brain (Cb.), Brain (Tv.), Proï¬le, Lips, Abdominal, Kidneys, Femur, Spine\\n(Cor.), Spine (Sag.), 4CH, 3VV, RVOT, LVOT. The dataset further includes\\nlarge portions of frames which contains anatomies that are not part of the scan\\nplane, labelled as â€œbackgroundâ€. The details of the image acquisition protocol as\\nwell as how scan plane labels are obtained can be found in (Baumgartner et al.,\\n2016). The data was cropped to central208Ã— 272 to prevent the network from\\nlearning the surrounding annotations shown in the ultrasound scan screen.\\n3.2. Model Training and Implementation Details\\nThe datasets used in this manuscript contain large class imbalance issue that\\nneeds to be addressed. For ultrasound dataset, due to the nature of screening\\nprocess, the background label dominates the dataset. To address this, we used\\na weighted sampling strategy, where we matched the probability of sampling\\none of the foreground labels to the probability of sampling a background label.'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='a weighted sampling strategy, where we matched the probability of sampling\\none of the foreground labels to the probability of sampling a background label.\\nFor the segmentation models, the class imbalance problem is tackled using the\\nSorensen-Dice loss (Drozdzal et al., 2016; Milletari et al., 2016) deï¬ned over all\\nsemantic classes. Dice loss is experimentally shown to be less sensitive to class\\nimbalance in segmentation tasks.\\nFor both tasks, batch-normalisation, deep-supervision (Lee et al., 2015), and\\nstandard data-augmentation techniques (aï¬ƒne transformations, axial ï¬‚ips, ran-\\ndom crops) are used in training attention and baseline networks. Intensity values\\nare linearly scaled to obtain a normal distributionN(0, 1). For classiï¬cation\\nmodels, we empirically found that optimising with Stochastic Gradient Descent\\nwith Nesterov momentum (Ï = 0.9) worked the best. The initial learning rate\\nwas set to 0.1, which was subsequently reduced by a factor of 0.1 for every 100'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='with Nesterov momentum (Ï = 0.9) worked the best. The initial learning rate\\nwas set to 0.1, which was subsequently reduced by a factor of 0.1 for every 100\\nepoch. We also used a warm-start learning rate of 0.01 for the ï¬rst 5 epochs.\\nFor segmentation models, we used Adam withÎ± = 10âˆ’4,Î² 1 = 0.9,Î² 2 = 0.999.\\nThe batch size for the Sononet models was set to 64. However, for the 3D-CT\\nsegmentation models, gradient updates are computed using small batch sizes of\\n2 to 4 samples. For larger segmentation networks, gradient averaging is used\\nover multiple forward and backward passes. This is mainly because we propose a\\n3D-model to capture suï¬ƒcient semantic context in contrast to the state-of-the-art\\nCNN segmentation frameworks (Cai et al., 2017; Roth et al., 2018). Gating\\nparameters are initialised so that attention gates pass through feature vectors at\\nall spatial locations. Moreover, we do not require multiple training stages as in'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='parameters are initialised so that attention gates pass through feature vectors at\\nall spatial locations. Moreover, we do not require multiple training stages as in\\nhard-attention based approaches therefore simplifying the training procedure.\\n3.2.1. Implementation Details:\\nThe architecture for AG-sononet is shown in Fig. 4. The parameters for\\nAG-Sononet was initialised using a partially trained Sononet. We compare our\\n12Table 1: Multi-class CT abdominal segmentation results obtained on theCT-150 dataset: The\\nresults are reported in terms of Dice score (DSC) and mesh surface to surface distances (S2S).\\nThese distances are reported only for the pancreas segmentations. The proposed Attention\\nU-Net model is benchmarked against the standard U-Net model for diï¬€erent training and\\ntesting splits. Inference time (forward pass) of the models are computed for input tensor of\\nsize 160 Ã— 160 Ã— 96. Statistically signiï¬cant results are highlighted in bold font.\\nMethod U-Net Att U-Net U-Net Att U-Net'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='size 160 Ã— 160 Ã— 96. Statistically signiï¬cant results are highlighted in bold font.\\nMethod U-Net Att U-Net U-Net Att U-Net\\nTrain/Test Split 120/30 120/30 30/120 30/120\\nPancreas DSC 0.814 Â±0.116 0.840Â±0.087 0.741Â±0.137 0.767Â±0.132\\nPancreas Precision 0.848 Â±0.110 0.849 Â±0.098 0.789 Â±0.176 0.794Â±0.150\\nPancreas Recall 0.806 Â±0.126 0.841Â±0.092 0.743Â±0.179 0.762Â±0.145\\nPancreas S2S Dist (mm) 2.358 Â±1.464 1.920Â±1.284 3.765Â±3.452 3.507 Â±3.814\\nSpleen DSC 0.962 Â±0.013 0.965 Â±0.013 0.935 Â±0.095 0.943Â±0.092\\nKidney DSC 0.963 Â±0.013 0.964 Â±0.016 0.951 Â±0.019 0.954 Â±0.021\\nNumber of Params 5.88 M 6.40 M 5.88 M 6.40 M\\nInference Time 0.167 s 0.179 s 0.167 s 0.179 s\\n(a) (b) (c) (d)\\nFigure 5: (a-b) The ground-truth pancreas segmentation, (c) U-Net and (d) Attention\\nU-Net. The missed dense predictions by U-Net are highlighted with red arrows.\\nmodels with diï¬€erent capacities, with the initial number of features 8, 16 and 32.\\nFor U-net and Attention U-net, the initial number of features is set toF1 = 8,'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='models with diï¬€erent capacities, with the initial number of features 8, 16 and 32.\\nFor U-net and Attention U-net, the initial number of features is set toF1 = 8,\\nwhich is doubled after every max-pooling operation. Our implementation using\\nPyTorch (Paszke et al., 2017) is publicly available4.\\n3.3. 3D-CT Abdominal Image Segmentation Results\\nThe proposed Attention U-Net model is benchmarked against the standard\\nU-Net (Ronneberger et al., 2015) on multi-class abdominal CT segmentation. We\\nuse CT-150 dataset for both training (120) and testing (30). The corresponding\\nDice scores (DSC) and surface distances (S2S) are given in Table 1. The results\\non pancreas predictions demonstrate that attention gates (AGs) increase recall\\nvalues (p = .005) by improving the modelâ€™s expression power as it relies on\\nAGs to localise foreground pixels. The diï¬€erence between predictions obtained\\nwith these two models are qualitatively compared in Figure 5. In the second'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='AGs to localise foreground pixels. The diï¬€erence between predictions obtained\\nwith these two models are qualitatively compared in Figure 5. In the second\\nexperiment, the same models are trained with fewer training images (30) to show\\nthat the performance improvement is consistent and signiï¬cant for diï¬€erent sizes\\n4https://github.com/ozan-oktay/Attention-Gated-Networks\\n13Table 2: Segmentation experiments onCT-150 dataset are repeated with higher capacity\\nU-Net models to demonstrate the eï¬ƒciency of the attention models with similar or less network\\ncapacity. The additional ï¬lters in the U-Net model are distributed uniformly across all the\\nlayers. Segmentation results for the pancreas are reported in terms of dice score, precision,\\nrecall, surface distances. The models are trained with the same train/test data splits (120/30).\\nMethod # of Pars DSC Precision Recall S2S Dist (mm) Run Time\\nU-Net 6.44 M .821 Â±.119 .849 Â±.111 .814 Â±.125 2.383 Â±1.918 .191 s'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='Method # of Pars DSC Precision Recall S2S Dist (mm) Run Time\\nU-Net 6.44 M .821 Â±.119 .849 Â±.111 .814 Â±.125 2.383 Â±1.918 .191 s\\nU-Net 10.40 M .825 Â±.104 .861 Â±.082 .807 Â±.121 2.202 Â±1.144 .222 s\\nTable 3: Pancreas segmentation results obtained on the TCIA Pancreas-CT Dataset (Roth\\net al., 2016). The dataset contains in total 82 scans which are split into training (61) and\\ntesting (21) sets. The corresponding results are obtained before (BFT) and after ï¬ne tuning\\n(AFT) and also training the models from scratch (SCR). Statistically signiï¬cant results are\\nhighlighted in bold font.\\nMethod Dice Score Precision Recall S2S Dist (mm)\\nBFT\\nU-Net 0.690 Â±0.132 0.680 Â±0.109 0.733 Â±0.190 6.389 Â±3.900\\nAttention U-Net 0.712Â±0.110 0.693Â±0.115 0.751Â±0.149 5.251 Â±2.551\\nAFT\\nU-Net 0.820 Â±0.043 0.824 Â±0.070 0.828 Â±0.064 2.464 Â±0.529\\nAttention U-Net 0.831Â±0.038 0.825Â±0.073 0.840Â±0.053 2.305 Â±0.568\\nSCR\\nU-Net 0.815 Â±0.068 0.815 Â±0.105 0.826 Â±0.062 2.576 Â±1.180'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='AFT\\nU-Net 0.820 Â±0.043 0.824 Â±0.070 0.828 Â±0.064 2.464 Â±0.529\\nAttention U-Net 0.831Â±0.038 0.825Â±0.073 0.840Â±0.053 2.305 Â±0.568\\nSCR\\nU-Net 0.815 Â±0.068 0.815 Â±0.105 0.826 Â±0.062 2.576 Â±1.180\\nAttention U-Net 0.821 Â±0.057 0.815 Â±0.093 0.835Â±0.057 2.333 Â±0.856\\nof training data (p =.01). For both approaches, we observe a performance drop\\non spleen DSC as the training size is reduced. The drop is less signiï¬cant with\\nthe proposed framework. For kidney segmentation, the models achieve similar\\naccuracy since the tissue contrast is higher.\\nIn Table 1, we also report the number of trainable parameters for both\\nmodels. We observe that by adding 8% extra capacity to the standard U-Net,\\nthe performance can be improved by 2-3% in terms of DSC. For a fair comparison,\\nwe also train higher capacity U-Net models and compare against the proposed\\nmodel with smaller network size. The results shown in Table 2 demonstrate that\\nthe addition of AGs contributes more than simply increasing model capacity'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='model with smaller network size. The results shown in Table 2 demonstrate that\\nthe addition of AGs contributes more than simply increasing model capacity\\n(uniformly) across all layers of the network (p = .007). Therefore, additional\\ncapacity should be used for AGs to localise tissues, in cases when AGs are used\\nto reduce the redundancy of training multiple, individual models.\\n3.3.1. Comparison to State-of-the-Art CT Abdominal Segmentation Frameworks\\nThe proposed architecture is evaluated on the public TCIA CT Pancreas\\nbenchmark to compare its performance with state-of-the-art methods. Initially,\\nthe models trained onCT-150 dataset are directly applied toCT-82 dataset to\\nobserve the applicability of the two models on diï¬€erent datasets. The correspond-\\ning results (BFT) are given in Table 3. U-Net model outperforms traditional\\natlas techniques (Wolz et al., 2013) although it was trained on a disjoint dataset.\\nMoreover, the attention model performs consistently better in pancreas segmen-'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='atlas techniques (Wolz et al., 2013) although it was trained on a disjoint dataset.\\nMoreover, the attention model performs consistently better in pancreas segmen-\\n14Table 4: State-of-the-art CT pancreas segmentation methods that are based on single and\\nmultiple CNN models. The listed segmentation frameworks are evaluated on the same public\\nbenchmark (CT-82) using diï¬€erent number of training and testing images. Similarly, the FCN\\napproach proposed in (Roth et al., 2017) is benchmarked onCT-150 although it is trained on\\nan external dataset (Ext).\\nMethod Dataset Pancreas DSC Train/Test # Folds\\nHierarchical 3D FCN (Roth et al., 2017)CT-150 82 .2 Â±10.2 Ext/150 -\\nDense-Dilated FCN (Gibson et al., 2017)CT-82& Synapse5 66.0 Â±10.0 63 /9 5-CV\\n2D U-Net (Heinrich et al., 2018) CT-82 75 .7 Â±9.0 66/16 5-CV\\nHN 2D FCN Stage-1(Roth et al., 2018) CT-82 76 .8 Â±11.1 62/20 4-CV\\nHN 2D FCN Stage-2(Roth et al., 2018) CT-82 81 .2 Â±7.3 62/20 4-CV\\n2D FCN (Cai et al., 2017) CT-82 80 .3 Â±9.0 62/20 4-CV'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='HN 2D FCN Stage-1(Roth et al., 2018) CT-82 76 .8 Â±11.1 62/20 4-CV\\nHN 2D FCN Stage-2(Roth et al., 2018) CT-82 81 .2 Â±7.3 62/20 4-CV\\n2D FCN (Cai et al., 2017) CT-82 80 .3 Â±9.0 62/20 4-CV\\n2D FCN + RNN (Cai et al., 2017) CT-82 82 .3 Â±6.7 62/20 4-CV\\nSingle Model 2D FCN (Zhou et al., 2017)CT-82 75 .7 Â±10.5 62/20 4-CV\\nMulti-Model 2D FCN (Zhou et al., 2017)CT-82 82 .2 Â±5.7 62/20 4-CV\\ntation across diï¬€erent datasets. These models are later ï¬ne-tuned (AFT) on\\na subset of TCIA dataset (61 train, 21 test). The output nodes corresponding\\nto spleen and kidney are excluded from the output softmax computation, and\\nthe gradient updates are computed only for the background and pancreas labels.\\nThe results in Table 3 and 4 show improved performance compared to concate-\\nnated multi-model CNN approaches (Cai et al., 2017; Roth et al., 2018; Zhou\\net al., 2017) due to additional training data and richer semantic information (e.g.'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='nated multi-model CNN approaches (Cai et al., 2017; Roth et al., 2018; Zhou\\net al., 2017) due to additional training data and richer semantic information (e.g.\\nspleen labels). Additionally, we trained the two models from scratch (SCR) with\\n61 training images randomly selected from theCT-82 dataset. Similar to the\\nresults onCT-150 dataset, AGs improve the segmentation accuracy and lower\\nthe surface distances (p =.03) due to increased recall rate of pancreas pixels\\n(p =.09).\\nResults from state-of-the-art CT pancreas segmentation models are sum-\\nmarised in Table 4 for comparison purposes. Since the models are trained on\\nthe same training dataset, this comparison gives an insight on how the attention\\nmodel compares to the relevant literature. It is important to note that, post-\\nprocessing (e.g. using conditional random ï¬eld) is not utilised in our framework\\nas the experiments mainly focus on quantiï¬cation of performance improvement'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='processing (e.g. using conditional random ï¬eld) is not utilised in our framework\\nas the experiments mainly focus on quantiï¬cation of performance improvement\\nbrought by AGs in an isolated setting. Similarly, residual and dense connections\\ncan be used as in (Gibson et al., 2017) in conjunction with AGs to improve the\\nsegmentation results. In that regard, our 3D Attention U-Net model performs\\nsimilar to the state-of-the-art, despite the input images are downsampled to\\nlower resolution. More importantly, our approach signiï¬cantly improves the\\nresults compared to single-model based segmentation frameworks (see Table\\n4). We do not require multiple CNN models to localise and segment object\\nboundaries. Lastly, we performed5-fold cross-validation on theCT-82 dataset\\nusing the Attention U-Net for a better comparison, which achieved81.48Â± 6.23\\nDSC for pancreas labels.\\n3.4. 2D Fetal Ultrasound Image Classiï¬cation Results'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='using the Attention U-Net for a better comparison, which achieved81.48Â± 6.23\\nDSC for pancreas labels.\\n3.4. 2D Fetal Ultrasound Image Classiï¬cation Results\\nThe dataset was split to training (122, 233), validation (30, 553) and testing\\n(38, 243) frames on subject basis. For evaluation, we used macro-averaged preci-\\n15Table 5: Test results for standard scan plane detection. Number of initial ï¬lters is denoted by\\nthe postï¬x â€œ-nâ€. Time taken for forward (Fwd) and backward (Bwd) passes were recorded in\\nmilliseconds.\\nMethod Accuracy F1 Precision Recall Fwd/Bwd ( ms) #Param\\nSononet-8 0.969 0.899 0.878 0.922 1.36/2.60 0.16M\\nAG-Sononet-8 0.977 0.922 0.916 0.929 1.92/3.47 0.18M\\nSononet-16 0.977 0.923 0.916 0.931 1.45/3.92 0.65M\\nAG-Sononet-16 0.978 0.929 0.924 0.934 1.94/5.13 0.70M\\nSononet-32 0.979 0.931 0.924 0.938 2.40/6.72 2.58M\\nAG-Sononet-32 0.980 0.933 0.931 0.935 2.92/8.68 2.79M\\nTable 6: Class-wise performance for AG-Sononet-8. In bracket shows the improvement over'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='Sononet-32 0.979 0.931 0.924 0.938 2.40/6.72 2.58M\\nAG-Sononet-32 0.980 0.933 0.931 0.935 2.92/8.68 2.79M\\nTable 6: Class-wise performance for AG-Sononet-8. In bracket shows the improvement over\\nSononet-8. Bold highlights the improvement more than 0.02.\\nPrecision Recall F1\\nBrain (Cb.) 0.988 (-0.002) 0.982 (-0.002) 0.985 (-0.002)\\nBrain (Tv.) 0.980 (0.003) 0.990 (0.002) 0.985 (0.003)\\nProï¬le 0.953 (0.055) 0.962 (0.009) 0.958 (0.033)\\nLips 0.976 (0.029) 0.956 (-0.003) 0.966 (0.013)\\nAbdominal 0.963 (0.011) 0.961 (0.007) 0.962 (0.009)\\nKidneys 0.863 (0.054) 0.902 (0.003) 0.882 (0.030)\\nFemur 0.975 (0.019) 0.976 (-0.005) 0.975 (0.007)\\nSpine (Cor.) 0.935 (0.049) 0.979 (0.000) 0.957 (0.026)\\nSpine (Sag.) 0.936 (0.055) 0.979 (-0.012) 0.957 (0.024)\\n4CH 0.943 (0.035) 0.970 (0.007) 0.956 (0.022)\\n3VV 0.694 (0.050) 0.722 (-0.014) 0.708 (0.021)\\nRVOT 0.691 (0.029) 0.705(0.044) 0.698(0.036)\\nLVOT 0.925 (0.022) 0.933(0.027) 0.929(0.024)\\nBackground 0.995 (-0.001) 0.992 (0.007) 0.993 (0.003)'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='3VV 0.694 (0.050) 0.722 (-0.014) 0.708 (0.021)\\nRVOT 0.691 (0.029) 0.705(0.044) 0.698(0.036)\\nLVOT 0.925 (0.022) 0.933(0.027) 0.929(0.024)\\nBackground 0.995 (-0.001) 0.992 (0.007) 0.993 (0.003)\\nsion, recall, F1, overall accuracy, the number of parameters and execution speed,\\nsummarised in Table 5.\\nIn general, AG-Sononet improves the results over Sononet at all capacity\\nlevels. In particular, AG-Sononet achieves higher precision. AG-Sononet reduces\\nfalse positive examples because the gating mechanism suppresses background\\nnoise and forces the network to make the prediction based on class-speciï¬c\\nfeatures. As the capacity of Sononet is increased, the gap between the methods\\nare tightened, but we note that the performance of AG-Sononet is also close to\\nthe one of Sononet with double the capacity. In Table 6, we show the class-wise\\nF1, precision and recall values for AG-Sononet-8, where the improvement over\\nSononet is indicated in brackets. We see that the precision increased by around'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='F1, precision and recall values for AG-Sononet-8, where the improvement over\\nSononet is indicated in brackets. We see that the precision increased by around\\n5% for kidney, proï¬e and spines. For the most challenging cardiac views, we see\\non average 3% improvement for 4CH and 3VV (p< 0.05).\\n3.5. Attention Map Analysis\\nThe attention coeï¬ƒcients of the proposed U-Net model, which are obtained\\nfrom 3D-CT test images, are visualised with respect to training epochs (see\\nFigure 6). We commonly observe that AGs initially have a uniform distribution\\nand pass features at all spatial locations. This is gradually updated and localised\\ntowards the targeted organ boundaries. Additionally, at coarser scales AGs\\n16Figure 6: The ï¬gure shows the attention coeï¬ƒcients (Î±ls2, Î±ls3) across diï¬€erent training\\nepochs (3, 6, 10, 60, 150). The images are extracted from sagittal and axial planes of a 3D\\nabdominal CT scan from the testing dataset. The model gradually learns to focus on the'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='epochs (3, 6, 10, 60, 150). The images are extracted from sagittal and axial planes of a 3D\\nabdominal CT scan from the testing dataset. The model gradually learns to focus on the\\npancreas, kidney, and spleen.\\nprovide a rough outline of organs which are gradually reï¬ned at ï¬ner resolutions.\\nMoreover, by training multiple AGs at each image scale, we observe that each\\nAG learns to focus on a particular subset of organs.\\n3.5.1. Object Localisation using Attention Maps\\nWith the proposed architecture, the localisation maps can obtained for almost\\nno additional computational cost. In Figure 7, we show the attention maps of\\nAG-Sononet across diï¬€erent subjects, together the red bounding box annotation\\ngenerated using the attention maps (see Appendix for the heuristics). We see\\nthat the network consistently focuses on the object of interest, consistent with\\nthe blue ground truth annotation. We note, however, attention map outlines the'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='that the network consistently focuses on the object of interest, consistent with\\nthe blue ground truth annotation. We note, however, attention map outlines the\\ndiscriminant region; in particular, it does not necessarily coincide with the entire\\nobject. Nevertheless, as it does not guided backpropagation for localisation (a\\nstrategy in (Baumgartner et al., 2016)), attention models are advantageous for\\nthe real-time applications.\\n4. Discussion\\nIn this work, we considered soft-attention mechanism and discussed how to\\nincorporate this idea into segmentation and scan plane detection frameworks\\nto better exploit local structures in CT abdominal and fetal ultrasound images.\\nIn particular, we highlighted several aspects: gridded attention mechanisms, a\\nnormalisation strategy for the attention map, and aggregation strategies. We\\nempirically observed and reported that using soft-max as the activation function\\ntends to generate a map that is sparsely activated and is overly sensitive to local'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='empirically observed and reported that using soft-max as the activation function\\ntends to generate a map that is sparsely activated and is overly sensitive to local\\nintensity changes. The latter is problematic as in ultrasound imaging, image\\nquality is often low. In the classiï¬cation setting, We found that dividing the\\nactivations by the sum of the activations helped generate attention map with\\nlarger contextual support. As demonstrated in the segmentation framework,\\nSigmoid function is a good alternative as it only normalises the range and allows\\n17Figure 7: Examples of the obtained attention map and generated bounding boxes (red) from\\nAG-Sononet-FT across diï¬€erent subjects. The ground truth annotation is shown in blue. The\\ndetected region highly agrees with the object of interest.\\nmore information to ï¬‚ow. However, we found that training is non-trivial due to\\nthe gradient saturation problem.\\nWe noted that training the attention-mechanism was slightly more complex'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='more information to ï¬‚ow. However, we found that training is non-trivial due to\\nthe gradient saturation problem.\\nWe noted that training the attention-mechanism was slightly more complex\\nthan the standard network architecture. In particular, we observed that the\\nstrategy employed to aggregate the attention maps at diï¬€erent scales aï¬€ects\\nboth the learning of the attention mechanism itself and hence the performance.\\nHaving a loss term deï¬ned at each scale ensures that the network learns to\\nattend at each scale. We observed that ï¬rst training the network at each scale\\nseparately, followed by ï¬ne-tuning was the most stable approach to get the\\noptimal performance.\\nThere is a vast body of literature in machine learning exploring diï¬€erent\\ngating architectures. For example, highway networks (Greï¬€ et al., 2016) make\\nuse of residual connections around the gate block to allow better gradient back-\\npropagationandslightlysofterattentionmechanisms. Althoughoursegmentation'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='use of residual connections around the gate block to allow better gradient back-\\npropagationandslightlysofterattentionmechanisms. Althoughoursegmentation\\nexperiments with residual connections have not provided any signiï¬cant perfor-\\nmance improvement, future work will focus on this aspect to obtain a better\\ntraining behaviour.\\nLastly, we note that the presented quantitative comparisons between the At-\\ntention 3D-Unet and state-of-the-art 2D cascaded models might not be suï¬ƒcient\\nenough to draw a ï¬nal conclusion, as the proposed approach takes advantage of\\n18rich contextual information in all spatial dimensions. On the other hand, the\\n2D models utilise the high resolution information present in axial CT planes\\nwithout any downsampling. We think that with the advent of improved GPU\\ncomputation power and memory, larger capacity 3D-CT segmentation models\\ncan be trained with larger image grids without the need for image downsampling.'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='computation power and memory, larger capacity 3D-CT segmentation models\\ncan be trained with larger image grids without the need for image downsampling.\\nIn this regard, future research will focus more and more on deploying 3D models,\\nand the performance of Attention U-Net can be further enhanced by utilising\\nï¬ne resolution input batches without any additional heuristics.\\n5. Conclusion\\nIn this work we proposed a novel and modular attention gate model that can\\nbe easily incorporated into existing segmentation and classiï¬cation architectures.\\nOur approach can eliminate the necessity of applying an external object locali-\\nsation model by implicitly learning to highlight salient regions in input images.\\nMoreover, in a classiï¬cation setting, AGs leverage the salient information to\\nperform task adaptive feature pooling operation.\\nWe applied the proposed attention model to standard scan plane detection\\nduring fetal ultrasound screening and showed that it improves overall results,'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='We applied the proposed attention model to standard scan plane detection\\nduring fetal ultrasound screening and showed that it improves overall results,\\nespecially precision, with much less parameters. This was done by generating\\nthe gating signal to pinpoint local as well as global information that is useful\\nfor the classiï¬cation. Similarly, experimental results on CT segmentation task\\ndemonstrate that the proposed AGs are highly beneï¬cial for tissue/organ identi-\\nï¬cation and localisation. This is particularly true for variable small size organs\\nsuch as the pancreas, and similar behaviour is observed in image classiï¬cation\\ntasks.\\nAdditionally, AGs allow one to generate ï¬ne-grained attention map that can\\nbe exploited for object localisation. We envisage that the proposed soft-attention\\nmodule could support explainable deep learning, which is a vital research area\\nfor medical imaging analysis.\\nReferences\\nAnderson P, He X, Buehler C, Teney D, Johnson M, Gould S, Zhang L. Bottom-'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='module could support explainable deep learning, which is a vital research area\\nfor medical imaging analysis.\\nReferences\\nAnderson P, He X, Buehler C, Teney D, Johnson M, Gould S, Zhang L. Bottom-\\nup and top-down attention for image captioning and vqa. arXiv preprint\\narXiv:170707998 2017;.\\nBahdanau D, Cho K, Bengio Y. Neural machine translation by jointly learning\\nto align and translate. arXiv preprint arXiv:14090473 2014;.\\nBai W, Sinclair M, Tarroni G, Oktay O, Rajchl M, Vaillant G, Lee AM, Aung\\nN, Lukaschuk E, Sanghvi MM, et al. Human-level cmr image analysis with\\ndeep fully convolutional networks. arXiv preprint arXiv:171009289 2017;.\\nBaumgartner CF, Kamnitsas K, Matthew J, Fletcher TP, Smith S, Koch LM,\\nKainz B, Rueckert D. Real-time detection and localisation of fetal standard\\nscan planes in 2d freehand ultrasound. arXiv preprint arXiv:161205601 2016;.\\n19Britz D, Goldie A, Luong MT, Le Q. Massive exploration of neural machine'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='scan planes in 2d freehand ultrasound. arXiv preprint arXiv:161205601 2016;.\\n19Britz D, Goldie A, Luong MT, Le Q. Massive exploration of neural machine\\ntranslation architectures. arXiv preprint arXiv:170303906 2017;.\\nCai J, Lu L, Xie Y, Xing F, Yang L. Improving deep pancreas segmentation in\\nCT and MRI images via recurrent neural contextual learning and direct loss\\nfunction. In: MICCAI. 2017. .\\nCerrolaza JJ, Summers RM, Linguraru MG. Soft multi-organ shape models\\nvia generalized PCA: A general framework. In: MICCAI. Springer; 2016. p.\\n219â€“28.\\nChen H, Dou Q, Ni D, Cheng JZ, Qin J, Li S, Heng PA. Automatic fetal\\nultrasound standard plane detection using knowledge transferred recurrent\\nneural networks. In: International Conference on Medical Image Computing\\nand Computer-Assisted Intervention. Springer; 2015. p. 507â€“14.\\nDrozdzal M, Vorontsov E, Chartrand G, Kadoury S, Pal C. The importance of\\nskip connections in biomedical image segmentation. In: Deep Learning and'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='Drozdzal M, Vorontsov E, Chartrand G, Kadoury S, Pal C. The importance of\\nskip connections in biomedical image segmentation. In: Deep Learning and\\nData Labeling for Medical Applications. Springer; 2016. p. 179â€“87.\\nEsteva A, Kuprel B, Novoa RA, Ko J, Swetter SM, Blau HM, Thrun S.\\nDermatologist-level classiï¬cation of skin cancer with deep neural networks.\\nNature 2017;542(7639):115.\\nGibson E, Giganti F, Hu Y, Bonmati E, Bandula S, Gurusamy K, Davidson\\nBR, Pereira SP, Clarkson MJ, Barratt DC. Towards image-guided pancreas\\nand biliary endoscopy: Automatic multi-organ segmentation on abdominal ct\\nwith dense dilated networks. In: International Conference on Medical Image\\nComputing and Computer-Assisted Intervention. Springer; 2017. p. 728â€“36.\\nGreï¬€ K, Srivastava RK, Schmidhuber J. Highway and residual networks learn\\nunrolled iterative estimation. arXiv preprint arXiv:161207771 2016;.\\nGuan Q, Huang Y, Zhong Z, Zheng Z, Zheng L, Yang Y. Diagnose like a'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='unrolled iterative estimation. arXiv preprint arXiv:161207771 2016;.\\nGuan Q, Huang Y, Zhong Z, Zheng Z, Zheng L, Yang Y. Diagnose like a\\nradiologist: Attention guided convolutional neural network for thorax disease\\nclassiï¬cation. arXiv preprint arXiv:180109927 2018;.\\nHeinrich MP, Blendowski M, Oktay O. TernaryNet: Faster deep model infer-\\nence without GPUs for medical 3D segmentation using sparse and binary\\nconvolutions. arXiv preprint arXiv:180109449 2018;.\\nHeinrich MP, Oktay O. BRIEFnet: Deep pancreas segmentation using binary\\nsparse convolutions. In: MICCAI. Springer; 2017. p. 329â€“37.\\nHu J, Shen L, Sun G. Squeeze-and-excitation networks. arXiv preprint\\narXiv:170901507 2017;.\\nJetley S, Lord NA, Lee N, Torr P. Learn to pay attention. In: International\\nConference on Learning Representations. 2018. URL:https://openreview.\\nnet/forum?id=HyzbhfWRW.\\n20Kamnitsas K, Bai W, Ferrante E, McDonagh S, Sinclair M, Pawlowski N, Rajchl'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='Conference on Learning Representations. 2018. URL:https://openreview.\\nnet/forum?id=HyzbhfWRW.\\n20Kamnitsas K, Bai W, Ferrante E, McDonagh S, Sinclair M, Pawlowski N, Rajchl\\nM, Lee M, Kainz B, Rueckert D, Glocker B. Ensembles of multiple models\\nand architectures for robust brain tumour segmentation. In: Brainlesion:\\nGlioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries. Cham; 2018.\\np. 450â€“62.\\nKamnitsas K, Ledig C, Newcombe VF, Simpson JP, Kane AD, Menon DK,\\nRueckert D, Glocker B. Eï¬ƒcient multi-scale 3D CNN with fully connected\\nCRFforaccuratebrainlesionsegmentation. Medicalimageanalysis2017;36:61â€“\\n78.\\nKawahara J, Hamarneh G. Multi-resolution-tract cnn with hybrid pretrained and\\nskin-lesion trained layers. In: International Workshop on Machine Learning in\\nMedical Imaging. Springer; 2016. p. 164â€“71.\\nKhened M, Kollerathu VA, Krishnamurthi G. Fully convolutional multi-scale\\nresidual densenets for cardiac segmentation and automated cardiac diagnosis'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='Medical Imaging. Springer; 2016. p. 164â€“71.\\nKhened M, Kollerathu VA, Krishnamurthi G. Fully convolutional multi-scale\\nresidual densenets for cardiac segmentation and automated cardiac diagnosis\\nusing ensemble of classiï¬ers. arXiv preprint arXiv:180105173 2018;.\\nLee CY, Xie S, Gallagher P, Zhang Z, Tu Z. Deeply-supervised nets. In: Artiï¬cial\\nIntelligence and Statistics. 2015. p. 562â€“70.\\nLiao F, Liang M, Li Z, Hu X, Song S. Evaluate the malignancy of pulmonary nod-\\nules using the 3D deep leaky noisy-or network. arXiv preprint arXiv:171108324\\n2017;.\\nLitjens GJS, Kooi T, Bejnordi BE, Setio AAA, Ciompi F, Ghafoorian M, van der\\nLaak JAWM, van Ginneken B, SÃ¡nchez CI. A survey on deep learning in\\nmedical image analysis. CoRR 2017;abs/1702.05747. URL:http://arxiv.\\norg/abs/1702.05747. arXiv:1702.05747.\\nLiu J, Wang G, Hu P, Duan LY, Kot AC. Global context-aware attention lstm\\nnetworks for 3d action recognition. In: CVPR. 2017. .'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='org/abs/1702.05747. arXiv:1702.05747.\\nLiu J, Wang G, Hu P, Duan LY, Kot AC. Global context-aware attention lstm\\nnetworks for 3d action recognition. In: CVPR. 2017. .\\nLong J, Shelhamer E, Darrell T. Fully convolutional networks for semantic\\nsegmentation. In: Proceedings of the IEEE conference on computer vision\\nand pattern recognition. 2015. p. 3431â€“40.\\nLu J, Xiong C, Parikh D, Socher R. Knowing when to look: Adaptive attention\\nvia A visual sentinel for image captioning. CoRR 2016;abs/1612.01887. URL:\\nhttp://arxiv.org/abs/1612.01887. arXiv:1612.01887.\\nLuong MT, Pham H, Manning CD. Eï¬€ective approaches to attention-based\\nneural machine translation. arXiv preprint arXiv:150804025 2015;.\\nMadani A, Arnaout R, Mofrad M, Arnaout R. Fast and accurate view classiï¬ca-\\ntion of echocardiograms using deep learning. npj Digital Medicine 2018;1(1):6.\\nMilletari F, Navab N, Ahmadi SA. V-net: Fully convolutional neural networks\\nfor volumetric medical image segmentation. In: 3D Vision (3DV), 2016 Fourth'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='Milletari F, Navab N, Ahmadi SA. V-net: Fully convolutional neural networks\\nfor volumetric medical image segmentation. In: 3D Vision (3DV), 2016 Fourth\\nInternational Conference on. IEEE; 2016. p. 565â€“71.\\n21Mnih V, Heess N, Graves A, et al. Recurrent models of visual attention. In:\\nAdvances in neural information processing systems. 2014. p. 2204â€“12.\\nNam H, Ha J, Kim J. Dual attention networks for multimodal reasoning and\\nmatching. CoRR 2016;abs/1611.00471. URL:http://arxiv.org/abs/1611.\\n00471. arXiv:1611.00471.\\nNHS Screening Programmes . Fetal Anomaly Screen Programme Handbook.\\nNHS, 2015.\\nOda M, Shimizu N, Roth HR, Karasawa K, Kitasaka T, Misawa K, Fujiwara M,\\nRueckert D, Mori K. 3D FCN feature driven regression forest-based pancreas\\nlocalization and segmentation. In: Deep Learning in Medical Image Analysis\\nand Multimodal Learning for Clinical Decision Support. Springer; 2017. p.\\n222â€“30.\\nPaszke A, Gross S, Chintala S, Chanan G, Yang E, DeVito Z, Lin Z, Desmaison'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='and Multimodal Learning for Clinical Decision Support. Springer; 2017. p.\\n222â€“30.\\nPaszke A, Gross S, Chintala S, Chanan G, Yang E, DeVito Z, Lin Z, Desmaison\\nA, Antiga L, Lerer A. Automatic diï¬€erentiation in pytorch 2017;.\\nPayer C, Å tern D, Bischof H, Urschler M. Multi-label whole heart segmentation\\nusing CNNs and anatomical label conï¬gurations. In: STACOM. Springer;\\n2017. p. 190â€“8.\\nPei W, Baltrusaitis T, Tax DMJ, Morency L. Temporal attention-gated model\\nfor robust sequence classiï¬cation. CoRR 2016;abs/1612.00385. URL:http:\\n//arxiv.org/abs/1612.00385. arXiv:1612.00385.\\nPesce E, Ypsilantis PP, Withey S, Bakewell R, Goh V, Montana G. Learning\\nto detect chest radiographs containing lung nodules using visual attention\\nnetworks. arXiv preprint arXiv:171200996 2017;.\\nRen M, Zemel RS. End-to-end instance segmentation and counting with recurrent\\nattention. CoRR 2016;abs/1605.09410. URL:http://arxiv.org/abs/1605.\\n09410. arXiv:1605.09410.'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='Ren M, Zemel RS. End-to-end instance segmentation and counting with recurrent\\nattention. CoRR 2016;abs/1605.09410. URL:http://arxiv.org/abs/1605.\\n09410. arXiv:1605.09410.\\nRonneberger O, Fischer P, Brox T. U-net: Convolutional networks for biomedical\\nimage segmentation. In: International Conference on Medical image computing\\nand computer-assisted intervention. Springer; 2015. p. 234â€“41.\\nRoth H, Farag A, Turkbey EB, Lu L, Liu J, Summers RM. Data from Pancreas-\\nCT. The Cancer Imaging Archive. 2016. URL:http://doi.org/10.7937/\\nK9/TCIA.2016.tNB1kqBU.\\nRoth HR, Lu L, Lay N, Harrison AP, Farag A, Sohn A, Summers RM. Spatial\\naggregation of holistically-nested convolutional neural networks for automated\\npancreas localization and segmentation. Medical Image Analysis 2018;45:94 â€“\\n107.\\nRoth HR, Oda H, Hayashi Y, Oda M, Shimizu N, Fujiwara M, Misawa K, Mori\\nK. Hierarchical 3D fully convolutional networks for multi-organ segmentation.\\narXiv preprint arXiv:170406382 2017;.'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='107.\\nRoth HR, Oda H, Hayashi Y, Oda M, Shimizu N, Fujiwara M, Misawa K, Mori\\nK. Hierarchical 3D fully convolutional networks for multi-organ segmentation.\\narXiv preprint arXiv:170406382 2017;.\\n22Saito A, Nawano S, Shimizu A. Joint optimization of segmentation and shape\\nprior from level-set-based statistical shape model, and its application to\\nthe automated segmentation of abdominal organs. Medical image analysis\\n2016;28:46â€“65.\\nSarraf S, DeSouza DD, Anderson J, Toï¬ghi G. Deepad: Alzheimerâ€™s dis-\\nease classiï¬cation via deep convolutional neural networks using mri and\\nfmri. bioRxiv 2017;URL: https://www.biorxiv.org/content/early/2017/\\n01/14/070441. doi:10.1101/070441.\\nShen T, Zhou T, Long G, Jiang J, Pan S, Zhang C. Disan: Directional self-\\nattention network for rnn/cnn-free language understanding. arXiv preprint\\narXiv:170904696 2017;.\\nSimonyan K, Zisserman A. Very deep convolutional networks for large-scale\\nimage recognition. arXiv preprint arXiv:14091556 2014;.'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='arXiv:170904696 2017;.\\nSimonyan K, Zisserman A. Very deep convolutional networks for large-scale\\nimage recognition. arXiv preprint arXiv:14091556 2014;.\\nVaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Å,\\nPolosukhin I. Attention is all you need. In: Advances in Neural Information\\nProcessing Systems. 2017. p. 6000â€“10.\\nVeliÄkoviÄ‡ P, Cucurull G, Casanova A, Romero A, LiÃ² P, Bengio Y. Graph\\nattention networks. arXiv preprint arXiv:171010903 2017;.\\nWang F, Jiang M, Qian C, Yang S, Li C, Zhang H, Wang X, Tang X. Residual\\nattention network for image classiï¬cation. arXiv preprint arXiv:170406904\\n2017a;.\\nWang X, Girshick R, Gupta A, He K. Non-local neural networks. arXiv preprint\\narXiv:171107971 2017b;.\\nWang X, Peng Y, Lu L, Lu Z, Summers RM. Tienet: Text-image embedding\\nnetwork for common thorax disease classiï¬cation and reporting in chest x-\\nrays. CoRR 2018;abs/1801.04334. URL:http://arxiv.org/abs/1801.04334.\\narXiv:1801.04334.'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='network for common thorax disease classiï¬cation and reporting in chest x-\\nrays. CoRR 2018;abs/1801.04334. URL:http://arxiv.org/abs/1801.04334.\\narXiv:1801.04334.\\nWolz R, Chu C, Misawa K, Fujiwara M, Mori K, Rueckert D. Automated\\nabdominal multi-organ segmentation with subject-speciï¬c atlas generation.\\nIEEE transactions on medical imaging 2013;32(9):1723â€“30.\\nXie S, Tu Z. Holistically-nested edge detection. In: Proceedings of the IEEE\\ninternational conference on computer vision. 2015. p. 1395â€“403.\\nXu K, Ba J, Kiros R, Cho K, Courville A, Salakhudinov R, Zemel R, Bengio Y.\\nShow, attend and tell: Neural image caption generation with visual attention.\\nIn: International Conference on Machine Learning. 2015. p. 2048â€“57.\\nYang Z, He X, Gao J, Deng L, Smola AJ. Stacked attention networks for image\\nquestion answering. CoRR 2015;abs/1511.02274. URL:http://arxiv.org/\\nabs/1511.02274. arXiv:1511.02274.\\n23Yaqub M, Kelly B, Papageorghiou AT, Noble JA. Guided random forests for'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='question answering. CoRR 2015;abs/1511.02274. URL:http://arxiv.org/\\nabs/1511.02274. arXiv:1511.02274.\\n23Yaqub M, Kelly B, Papageorghiou AT, Noble JA. Guided random forests for\\nidentiï¬cationofkeyfetalanatomyandimagecategorizationinultrasoundscans.\\nIn: International Conference on Medical Image Computing and Computer-\\nAssisted Intervention. Springer; 2015. p. 687â€“94.\\nYpsilantis PP, Montana G. Learning what to look in chest X-rays with a recurrent\\nvisual attention model. arXiv preprint arXiv:170106452 2017;.\\nYu Q, Xie L, Wang Y, Zhou Y, Fishman EK, Yuille AL. Recurrent saliency\\ntransformation network: Incorporating multi-stage visual cues for small organ\\nsegmentation. arXiv preprint arXiv:170904518 2017;.\\nZaharchuk G, Gong E, Wintermark M, Rubin D, Langlotz C. Deep learning in\\nneuroradiology. American Journal of Neuroradiology 2018;.\\nZhang Z, Chen P, Sapkota M, Yang L. Tandemnet: Distilling knowledge from\\nmedical images using diagnostic reports as optional semantic references. In:'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='Zhang Z, Chen P, Sapkota M, Yang L. Tandemnet: Distilling knowledge from\\nmedical images using diagnostic reports as optional semantic references. In:\\nInternational Conference on Medical Image Computing and Computer-Assisted\\nIntervention. Springer; 2017a. p. 320â€“8.\\nZhang Z, Xie Y, Xing F, McGough M, Yang L. Mdnet: A se-\\nmantically and visually interpretable medical image diagnosis network.\\nCoRR 2017b;abs/1707.02485. URL: http://arxiv.org/abs/1707.02485.\\narXiv:1707.02485.\\nZhao B, Feng J, Wu X, Yan S. A survey on deep learning-based ï¬ne-grained\\nobject classiï¬cation and semantic segmentation. International Journal of\\nAutomation and Computing 2017;14(2):119â€“35.\\nZhou Y, Xie L, Shen W, Wang Y, Fishman EK, Yuille AL. A ï¬xed-point model\\nfor pancreas segmentation in abdominal ct scans. In: International Conference\\non Medical Image Computing and Computer-Assisted Intervention. Springer;\\n2017. p. 693â€“701.\\nZhu W, Liu C, Fan W, Xie X. Deeplung: Deep 3d dual path nets for automated'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='on Medical Image Computing and Computer-Assisted Intervention. Springer;\\n2017. p. 693â€“701.\\nZhu W, Liu C, Fan W, Xie X. Deeplung: Deep 3d dual path nets for automated\\npulmonary nodule detection and classiï¬cation. CoRR 2018;abs/1801.09555.\\nURL: http://arxiv.org/abs/1801.09555. arXiv:1801.09555.\\nZografos V, Valentinitsch A, Rempï¬‚er M, Tombari F, Menze B. Hierarchical\\nmulti-organ segmentation without registration in 3D abdominal CT images.\\nIn: International MICCAI Workshop on Medical Computer Vision. Springer;\\n2015. p. 37â€“46.\\n24Appendix - W eakly Supervised Object Localisation (WSL)\\nIn (Baumgartner et al., 2016), WSL was performed by exploiting the pixel-\\nlevel saliency map obtained by guided-backpropagation, followed by ad-hoc\\nprocedure to extract bounding boxes. The same heuristics can be applied for\\nthe given network, however, owing to the attention map, we can device a much\\neï¬ƒcient way of performing object localisation. In particular, we generate object'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='the given network, however, owing to the attention map, we can device a much\\neï¬ƒcient way of performing object localisation. In particular, we generate object\\nlocation by simply: (1) blur the attention maps, (2) threshold the low activations,\\n(3) perform connected-component analysis, (4) select a component that overlaps\\nat each scale and (5) apply bounding box around the selected components. In\\nthis heuristics, backpropagation is not required so it can be executed eï¬ƒciently.\\nWe note, however, attention map outlines salient region used by the network to\\nperform classiï¬cation; in particular, it does not necessarily agree with the object\\nof interest. This behaviour makes sense because some part of object will appear\\nboth in the class as well as background frame until the ideal plane is reached.\\nTherefore, the quantitative result is shown in 7, however, the result is biased.\\nWe however deï¬ne new metric calledRelative Correctness, which is deï¬ned as'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='Therefore, the quantitative result is shown in 7, however, the result is biased.\\nWe however deï¬ne new metric calledRelative Correctness, which is deï¬ned as\\n50% of maximum achievable IOU (due to bias). We see that in this metric, the\\nmethod achieves very high results, indicating that it can detect relevant features\\nof the object of interest in its proximity.\\nTable 7: WSL performance for the proposed strategy with AG-Sononet-16. Correctness (Cor.)\\nis deï¬ned asIOU > 0.5. Relative Correctness (Rel.) is deï¬ned asIOU > 0.5 Ã— max(IOU class).\\nIOU Mean (Std) Cor. (%) Rel. (%)\\nBrain (Cb.) 0.69 (0.11) 0.96 0.96\\nBrain (Tv.) 0.68 (0.12) 0.96 0.96\\nProï¬le 0.31 (0.08) 0.00 0.80\\nLips 0.42 (0.18) 0.36 0.60\\nAbdominal 0.71 (0.10) 0.96 0.96\\nKidneys 0.73 (0.13) 0.92 0.98\\nFemur 0.31 (0.11) 0.02 0.58\\nSpine (Cor.) 0.53 (0.13) 0.56 0.76\\nSpine (Sag.) 0.53 (0.11) 0.54 0.94\\n4CH 0.61 (0.14) 0.76 0.86\\n3VV 0.42 (0.14) 0.34 0.62\\nRVOT 0.56 (0.15) 0.70 0.76\\nLVOT 0.54 (0.15) 0.62 0.80\\nAcknowledgements'),\n",
       " Document(metadata={'arxiv_id': '1808.08114v2', 'title': 'Attention Gated Networks: Learning to Leverage Salient Regions in Medical Images', 'section': 'body', 'authors': 'Jo Schlemper, Ozan Oktay, Michiel Schaap'}, page_content='Spine (Cor.) 0.53 (0.13) 0.56 0.76\\nSpine (Sag.) 0.53 (0.11) 0.54 0.94\\n4CH 0.61 (0.14) 0.76 0.86\\n3VV 0.42 (0.14) 0.34 0.62\\nRVOT 0.56 (0.15) 0.70 0.76\\nLVOT 0.54 (0.15) 0.62 0.80\\nAcknowledgements\\nWe thank the volunteers, radiographers and experts for providing manually\\nannotated datasets, Wellcome Trust IEH Award [102431], NVIDIA for their\\nGPU donations, and Intel.\\n25'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'title_abstract', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content=\"Title: Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning\\n\\nAbstract: Aortic stenosis (AS) is a degenerative valve condition that causes substantial morbidity and mortality. This condition is under-diagnosed and under-treated. In clinical practice, AS is diagnosed with expert review of transthoracic echocardiography, which produces dozens of ultrasound images of the heart. Only some of these views show the aortic valve. To automate screening for AS, deep networks must learn to mimic a human expert's ability to identify views of the aortic valve then aggregate across these relevant images to produce a study-level diagnosis. We find previous approaches to AS detection yield insufficient accuracy due to relying on inflexible averages across images. We further find that off-the-shelf attention-based multiple instance learning (MIL) performs poorly. We contribute a new end-to-end MIL approach with two key methodological innovations. First, a supervised attention technique guides the learned attention mechanism to favor relevant views. Second, a novel self-supervised pretraining strategy applies contrastive learning on the representation of the whole study instead of individual images as commonly done in prior literature. Experiments on an open-access dataset and an external validation set show that our approach yields higher accuracy while reducing model size.\"),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='Proceedings of Machine Learning Research 219:1â€“32, 2023 Machine Learning for Healthcare\\nDetecting Heart Disease from Multi-View Ultrasound Images\\nvia Supervised Attention Multiple Instance Learning\\nZhe Huang1 zhe.huang@tufts.edu\\nBenjamin S. Wessler2 bwessler@tuftsmedicalcenter.org\\nMichael C. Hughes1 michael.hughes@tufts.edu\\n1 Dept. of Computer Science, Tufts University, Medford, MA, USA\\n2 Division of Cardiology, Tufts Medical Center, Boston, MA, USA\\nAbstract\\nAortic stenosis (AS) is a degenerative valve condition that causes substantial morbidity\\nand mortality. This condition is under-diagnosed and under-treated. In clinical practice,\\nAS is diagnosed with expert review of transthoracic echocardiography, which produces\\ndozens of ultrasound images of the heart. Only some of these views show the aortic valve.\\nTo automate screening for AS, deep networks must learn to mimic a human expertâ€™s\\nability to identify views of the aortic valve then aggregate across these relevant images'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='To automate screening for AS, deep networks must learn to mimic a human expertâ€™s\\nability to identify views of the aortic valve then aggregate across these relevant images\\nto produce a study-level diagnosis. We find previous approaches to AS detection yield\\ninsufficient accuracy due to relying on inflexible averages across images. We further find\\nthat off-the-shelf attention-based multiple instance learning (MIL) performs poorly. We\\ncontribute a new end-to-end MIL approach with two key methodological innovations. First,\\na supervised attention technique guides the learned attention mechanism to favor relevant\\nviews. Second, a novel self-supervised pretraining strategy applies contrastive learning on\\nthe representation of the whole study instead of individual images as commonly done in\\nprior literature. Experiments on an open-access dataset and a temporally-external heldout\\nset show that our approach yields higher accuracy while reducing model size.\\n1. Introduction'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='prior literature. Experiments on an open-access dataset and a temporally-external heldout\\nset show that our approach yields higher accuracy while reducing model size.\\n1. Introduction\\nAortic stenosis (AS) is a progressive degenerative valve condition that is the result of fibrotic\\nand calcific changes to the heart valve. These structural changes occur over years, eventually\\nleading to obstruction of blood flow and can be fatal if not treated. AS is common and\\naffects over 12.6 million adults and causes an estimated 102,700 deaths annually. AS can\\nbe effectively treated when it is identified in a timely manner, though diagnosis remains\\nchallenging (Yadgir et al., 2020). One promising route to improving AS detection is to consider\\nautomatic screening of patients at risk using cardiac ultrasound. Automatic screening could\\nprovide a systematic, reproducible process and augment current approaches that rely on'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='automatic screening of patients at risk using cardiac ultrasound. Automatic screening could\\nprovide a systematic, reproducible process and augment current approaches that rely on\\ncardiac auscultation and miss a significant number of cases (Gardezi et al., 2018).\\nThe challenge in developing an automated system for diagnosing AS is that each echocar-\\ndiogram study consists ofdozens of images or videos (typically 27-97 in our data) that show\\nthe heartâ€™s complex anatomy from different acquisition angles. As illustrated in Fig. 1(a),\\nclinical readers are trained to look across many images to identify those that show the aortic\\n. Open-source Code for our Supervised Attention MIL (SAMIL): https://github.com/tufts-ml/SAMIL\\nÂ© 2023 Z. Huang, B.S. Wessler & M.C. Hughes.\\narXiv:2306.00003v3  [eess.IV]  5 Apr 2024Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\n(a) Human expert approach (b) Filter then Average (Holste et al., 2022b)\\nâ€¦\\n            (1) \\nIdentify images of \\nPLAX and PSAX'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='(a) Human expert approach (b) Filter then Average (Holste et al., 2022b)\\nâ€¦\\n            (1) \\nIdentify images of \\nPLAX and PSAX \\nview, so aortic valve \\nis visible\\n(2) \\nAssess severity of \\naortic stenosis from \\nimages of relevant \\nviews from (1)\\nView \\n0.04 0.96\\nView\\n0.55 0.45\\nView \\n0.03 0.97\\nView \\n0.76 0.24\\nâ€¦\\naverage\\n0.17 0.21 0.62\\nDiagnosis\\n0.04 0.26 0.70\\n 0.30 0.16 0.54\\nDiagnosis \\nâŒ\\n âŒ\\nâœ…\\n âœ…\\n \\nView\\n0.04 0.96\\nView\\n0.55 0.45\\nView\\n0.03 0.97\\nView\\n0.76 0.24\\nâ€¦\\nweighted avg.\\n0.24 0.19 0.57\\nDiagnosis\\n0.04 0.26 0.70\\n 0.08 0.22 0.60\\nDiagnosis\\n0.30 0.16 0.54\\nDiagnosis \\n0.60 0.10 0.30\\nDiagnosis\\nf\\nf\\nâ€¦\\nf\\n f\\n0.04 0.26 0.70\\ng\\nðœŽ   attention pooling                \\n(c) Weighted Average by View Relevance\\n(Wessler et al., 2023; Huang et al., 2021)\\n(d) Attention-based MIL\\nFigure 1: Overview of methods for diagnosing aortic valve disease from multiple images\\nof the heart. In our chosen diagnostic problem, the input is multiple ultrasound images representing'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='Figure 1: Overview of methods for diagnosing aortic valve disease from multiple images\\nof the heart. In our chosen diagnostic problem, the input is multiple ultrasound images representing\\ndifferent canonical view types of the heartâ€™s complex anatomy (e.g. PLAX, PSAX, A2C, A4C, and\\nmore, see Mitchell et al. (2019) for a taxonomy). The output is a probabilistic prediction of the\\nseverity of Aortic Stenosis (AS), on a 3-level scale of no / early / significant disease. We wish to\\ndevelop deep learning methods that can solve this problem like expert cardiologists (panel a). Two\\nrecent efforts (panel b by others, panel c by our group) made progress using a separately-trained\\nview type classifier and per-image diagnosis classifier, but rely on combining diagnosis probabilities\\nacross images via average pooling that cannot learn how to distribute attention non-uniformly among\\nimages of relevant views. In this work, we develop flexible attention-based multiple instance learning'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='images of relevant views. In this work, we develop flexible attention-based multiple instance learning\\n(MIL, panel d), with crucial contributions of supervised attention (Sec. 4.3) and improved pretraining\\nstrategies (Sec. 4.4) that substantially improve performance at our task.\\nvalve at sufficient quality and then use these â€œrelevantâ€ images to assess the valveâ€™s health.\\nTraining an algorithm to mimic this expert diagnostic process is difficult. Standard deep\\nlearning classifiers are designed to consume only one image and produce one prediction.\\nAutomatic screening of echocardiograms requires the ability to make one coherent prediction\\nfrom many images representing diverse view types. To make matters more difficult, each\\nimageâ€™s view type is not typically recorded in digital health records during routine collection.\\nMultiple-instance learning (MIL) is a branch of weakly supervised learning in which\\nclassifiers can consume a variable-sized set of images to make one prediction. Recent'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='Multiple-instance learning (MIL) is a branch of weakly supervised learning in which\\nclassifiers can consume a variable-sized set of images to make one prediction. Recent\\nimpressive advances in deep attention-based MIL have been published (Ilse et al., 2018;\\nLee et al., 2019; Sharma et al., 2021; Shao et al., 2021). However, their success at medical\\ndiagnostic tasks, especially those with ultrasound images from many possible view types, has\\nnot been previously evaluated.\\n2Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\nContributions to Clinical Translation and MIL Methodology\\nThis studyâ€™s contribution to applied clinical research is the development and validation\\nof a new deep MIL approach for automatic diagnosis of heart valve disease from multiple\\nultrasound images produced by a routine trans-thoracic echocardiogram (TTE) study. Our\\nend-to-end approach can take as input any number of images from various view types,'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='ultrasound images produced by a routine trans-thoracic echocardiogram (TTE) study. Our\\nend-to-end approach can take as input any number of images from various view types,\\neliminating the need for a separately-trained filtering step (Fig. 1(b)) to select relevant\\nviews for diagnosis required by some prior AS screening methods (Holste et al., 2022b).\\nOur approach is also more flexible and data-driven than the weighted average (Fig. 1(c))\\nof our teamâ€™s previous efforts for AS screening (Huang et al., 2021; Wessler et al., 2023).\\nHead-to-head evaluation in Sec. 5 demonstrates that our approach can yield superior balanced\\naccuracy for assigning AS severity grades to new studies, while keeping model size over 4x\\nsmaller than previous efforts like (Holste et al., 2022b). Small model sizes enable faster\\npredictions and ease portability to new hospital systems.\\nOur approachâ€™s success is made possible by two methodological contributions. First,'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='predictions and ease portability to new hospital systems.\\nOur approachâ€™s success is made possible by two methodological contributions. First,\\nwe propose a supervised attention mechanism (Sec. 4.3) that steers focus toward images of\\nrelevant views, mimicking a human expert. On our AS diagnosis task, supervised attention\\nyields notable gains â€“ balanced accuracy jumps from 60% to over 70% â€“ over previous\\noff-the-shelf attention-based MIL (Ilse et al., 2018). Second, we introduce a self-supervised\\npretraining strategy (Sec. 4.4) that focuses contrastive learning on the embedding of an entire\\nstudy (a.k.a. the embedding of the â€œbagâ€, using MIL vocabulary). In contrast, most previous\\npretraining focuses on representations of individual images. Both innovations are broadly\\napplicable to other MIL problems involving imaging data of multiple view types.\\nGeneralizable Insights about Machine Learning in the Context of Healthcare'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='applicable to other MIL problems involving imaging data of multiple view types.\\nGeneralizable Insights about Machine Learning in the Context of Healthcare\\nThis study offers critical insight into how multiple-instance learning can be applied to routine\\nechocardiography studies. We show that recent MIL architectures are insufficient to achieve\\ncompetitive performance because they attend to irrelevant instances and thus lack the ability\\nto makeclinically plausibledecisions. Our two innovations â€“ supervised attention (Sec.\\n4.3) and bag-level self-supervised pretraining (Sec. 4.4) can be broadly applicable to many\\nclinical image analysis problems that require non-trivial aggregation over multiple images\\nfrom multiple acquisition angles (views) to make one diagnosis. Beyond echocardiography,\\nthese insights could be useful for lung ultrasound, fetal ultrasound, head CT, and more.\\n2. Related Work\\n2.1. Multiple-Instance Learning.'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='these insights could be useful for lung ultrasound, fetal ultrasound, head CT, and more.\\n2. Related Work\\n2.1. Multiple-Instance Learning.\\nMultiple-instance learning (Dietterich et al., 1997; Maron and Lozano-PÃ©rez, 1997) describes a\\ntype of supervised learning problem where an unordered bag of instances and a corresponding\\nbag label are provided as input for model training, and the goal is to predict the bag label\\nfor unseen bags. This type of problem appears in many medical applications, including\\nwhole-slide image (WSI) analysis in pathology (Cosatto et al., 2013; Shao et al., 2021; Li\\net al., 2021a), diabetic retinopathy screening (Quellec et al., 2012; Li et al., 2021c), and\\ncancer diagnosis (Ding et al., 2012; Campanella et al., 2019). See App. F for a broader\\n3Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\nsummary of classic MIL techniques and more medical applications. For extensive reviews of'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='3Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\nsummary of classic MIL techniques and more medical applications. For extensive reviews of\\nthe MIL literature, see Zhou (2004); Quellec et al. (2017); Carbonneau et al. (2018).\\nTwo primary ways for modeling multiple instance learning problems are the instance-based\\napproach and the embedding-based approach. In the instance-based approach, an instance\\nclassifier is used to score each instance, and a pooling operator is then used to aggregate\\nthe instance scores to produce a bag score. In the embedding-based approach, a feature\\nextractor generates an embedding for each instance, which is then aggregated into a bag-level\\nembedding. A bag-level model is subsequently employed to compute a bag score based on\\nthe embedding. The embedding-based approach is argued to deliver better performance than\\nthe instance-based approach (Wang et al., 2018), but at the same time harder to determine'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='the embedding. The embedding-based approach is argued to deliver better performance than\\nthe instance-based approach (Wang et al., 2018), but at the same time harder to determine\\nthe key instances that trigger the classifier (Liu et al., 2017).\\nWhen input is one image from each desired view type.Some recent medical imaging\\nwork assumes that instead of an unordered â€œbagâ€ of instances of arbitrary size, the provided\\ninput will contain exactly one image for each of a few known view types (usually 2 or 4).\\nExamples include work on 2-view chest x-rays (Rubin et al., 2018; Hashir et al., 2020) as well\\nas work on breast cancer screening using 2 views (Carneiro et al., 2015; van Tulder et al.,\\n2021) or 4 views (Wu et al., 2020; Nasir Khan et al., 2019). Methods differ in whether they\\nfuse view-specific branches early or late, with latest innovations transferring information\\nacross views via transformers (van Tulder et al., 2021). In contrast to such work, the MIL'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='fuse view-specific branches early or late, with latest innovations transferring information\\nacross views via transformers (van Tulder et al., 2021). In contrast to such work, the MIL\\nmethods we develop consume dozens of images for which a view type is not known in advance,\\nreflecting the lack of recorded view annotations in typical echocardiograms.\\nDeep attention-based MIL. Our proposed method builds upon recent works advancing\\nattention-based deep neural networks for MIL. ABMIL (Ilse et al., 2018) is an embedding\\napproach where a two-layer neural network computes attention weights for each instance,\\nwith the final representation formed by averaging over instance embeddings weighted by\\nattention. Set Transformer (Lee et al., 2019) proposed to model the interactions among\\ninstances by using self-attention with multi-head attention (Vaswani et al., 2017). TransMIL\\n(Shao et al., 2021) uses a Transformer-based architecture to capture correlations among'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='instances by using self-attention with multi-head attention (Vaswani et al., 2017). TransMIL\\n(Shao et al., 2021) uses a Transformer-based architecture to capture correlations among\\npatches for whole-slide image classification. C2C (Sharma et al., 2021) divides patches from\\na whole-slide image into clusters, and sample multiple patches from each cluster for training.\\nC2C then tries to guide attention weights to be similar to a predefined uniform distribution,\\naiming to minimize intra-cluster variance for patches from the same cluster. A recent method\\ncalled DSMIL (Li et al., 2021a) attempts to benefit from instance-based and embedding-based\\napproaches via a dual-stream architecture. That work pretrains aninstance-level feature\\nextractor using self-supervised contrastive learning.\\n2.2. Self-supervised Learning and Pretraining of MIL\\nSelf-supervised learning (SSL) has demonstrated success in learning visual representations'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='extractor using self-supervised contrastive learning.\\n2.2. Self-supervised Learning and Pretraining of MIL\\nSelf-supervised learning (SSL) has demonstrated success in learning visual representations\\n(Chen et al., 2020a; He et al., 2020; Chen et al., 2020b; Grill et al., 2020; Caron et al.,\\n2020; Chen and He, 2021; Huang et al., 2023a). SSL requires defining a pretext task such\\nas predicting the future in latent space (Oord et al., 2018), predicting the rotation of an\\nimage (Gidaris et al., 2018), or solving a jigsaw puzzle (Noroozi and Favaro, 2016). The\\nterm â€œpretextâ€ suggests that the task being solved is not of primary downstream interest,\\n4Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\nbut rather serves as a means to learn a better data representation. After selecting a pretext\\ntask, an appropriate loss function must also be selected. Here, we focus on the instance\\ndiscrimination task (Wu et al., 2018) and InfoNCE loss (Oord et al., 2018) following the'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='task, an appropriate loss function must also be selected. Here, we focus on the instance\\ndiscrimination task (Wu et al., 2018) and InfoNCE loss (Oord et al., 2018) following the\\nsuccess ofmomentum contrastive learning(MoCo) (He et al., 2020; Chen et al., 2020b).\\nRecently, self-supervision has been successfully applied to pretrain MIL models (Holste\\net al., 2022a,b; Liu et al., 2022; Lu et al., 2019; Li et al., 2021a; Saillard et al., 2021; Dehaene\\net al., 2020; Rymarczyk et al., 2023). However, these studies all apply self-supervised\\ncontrastive learning to representations of individual images. For example, Li et al. (2021e)\\nencouragetheembeddingsofdifferentviewsofthesamepatienttobesimilar, whileChengetal.\\n(2022) specifically develop contrastive learning strategies for images from echocardiograms\\nwhen view labels are known. In our experiments, we observe image-level pretraining is\\nnot beneficial and sometimesslightly harmfulfor our AS diagnosis task. This may be'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='when view labels are known. In our experiments, we observe image-level pretraining is\\nnot beneficial and sometimesslightly harmfulfor our AS diagnosis task. This may be\\nbecause the pretraining taskâ€™s objective (learning good image level representations) being too\\ndistant from (or even contradict) the downstream taskâ€™s objective (learning good bag-level\\nrepresentations for AS diagnosis). This could relate to an issue prior literature callsclass\\ncollision (Arora et al., 2019; Chuang et al., 2020; Khosla et al., 2020; Dwibedi et al., 2021;\\nZheng et al., 2021; Ash et al., 2021; Li et al., 2021b).\\n2.3. Automated Screening of Aortic Stenosis.\\nWork on automatic screening for aortic stenosis from echocardiograms has accelerated in the\\npast few years (Ginsberg et al., 2021; Dai et al., 2023; Holste et al., 2022b; Wessler et al.,\\n2023), including recent work contemporaneous with this paper (Vaseli et al., 2023). Very'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='past few years (Ginsberg et al., 2021; Dai et al., 2023; Holste et al., 2022b; Wessler et al.,\\n2023), including recent work contemporaneous with this paper (Vaseli et al., 2023). Very\\nrecent work by Krishna et al. (2023) demonstrated that a commercial deep learning system\\ncan closely emulate human performance on most of the elementary echocardiogram-derived\\nmeasures for AS assessment, such as aortic valve area, peak velocity of blood through the\\nvalve, and mean pressure gradients. However, the inability to assign a study-level AS severity\\nrating limits its usefulness as a screening tool.\\nAmong previous efforts that can assign study-level AS grades, there are key differences\\nin how they overcome the challenge of multi-view images available in each patient scan or\\nstudy. Some groups have taken theFilter then Averageapproach diagrammed in Fig. 1(b).\\nDai et al. (2023) used a single video of the PLAX view to screen for AS. Holste et al. (2022b)'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='study. Some groups have taken theFilter then Averageapproach diagrammed in Fig. 1(b).\\nDai et al. (2023) used a single video of the PLAX view to screen for AS. Holste et al. (2022b)\\nsimilarly filters to several PLAX videos, then uses a deep learning architecture specialized to\\nvideo. Our team has previously pursued theWeighted Avg. by View Relevancestrategy in\\nFig. 1(c), combining separately-trained image-level view classifiers and image-level diagnostic\\nclassifiers via weighted averaging (Huang et al., 2021). This weighted averaging method\\nwas later refined for a clinical audience with external validation in Wessler et al. (2023). A\\nlimitation of both filtering and weighting strategies is that by construction they treat images\\nof relevant views equally; they cannot attend to some relevant views more than others.\\nOther work has pursued automated AS screening beyond echo images. Some have created'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='of relevant views equally; they cannot attend to some relevant views more than others.\\nOther work has pursued automated AS screening beyond echo images. Some have created\\nclassifiers based on time-varying electrocardiogram signals (Cohen-Shelly et al., 2021; Elias\\net al., 2022). Others have used wearable sensors (Yang et al., 2020). We argue that 2D\\nechocardiograms remain the gold-standard information source for diagnosis.\\nThe use of video, rather than still frames, is an advantage of some prior work (Dai et al.,\\n2023; Holste et al., 2022b) over our approach. However, these video efforts evaluate on\\n5Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\nproprietary data, while our work emphasizes reproducibility by using still images from the\\nopen-access TMED dataset described below. The MIL architecture proposed here could be\\nextended to video by a straightforward adaptation of the instance representation layer.\\n3. Dataset'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='open-access TMED dataset described below. The MIL architecture proposed here could be\\nextended to video by a straightforward adaptation of the instance representation layer.\\n3. Dataset\\nIn this work, for model training and primary evaluation we use an open-access dataset\\nthat our team created. The Tufts Medical Echocardiogram Dataset (TMED) (Huang et al.,\\n2021), now in its latest version known as TMED-2 (Huang et al., 2022), is a collection of\\n2D echocardiogram images gathered during routine care at Tufts Medical Center in Boston,\\nMA, USA from 2016-2021. Our research study of thesefully deidentifiedimages has been\\napproved by the Tufts Medical Center institutional review board.\\nEach study in the dataset represents a routine transthoracic echocardiogram (TTE) scan\\nof one patient and includesall collected 2D ultrasound images of the heart, with a median\\nof 68 images per study (10-90th percentile range = 27-97). No filtering to specific views was'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='of one patient and includesall collected 2D ultrasound images of the heart, with a median\\nof 68 images per study (10-90th percentile range = 27-97). No filtering to specific views was\\napplied except removal of Doppler images via metadata inspection. Each studyâ€™s available\\nimages are exactly the 2D TTE images available to cardiologists in the health records system.\\nTMED-2 contains a labeled set of 599 studies. Every study in the labeled set has a\\nstandard 5-level rating of aortic stenosis (AS) severity assigned by a board-certified expert\\nduring routine reading. To focus on automated screening use cases, we followed our previous\\nclinical work (Wessler et al., 2023) and mapped each rating to one of 3 diagnostic classes:\\nâ€œno ASâ€, â€œearly ASâ€ (combining mild and mild-to-moderate), and â€œsignificant ASâ€ (combining\\nmoderate and severe). See App. B.1 for further details on this label mapping. Experts who\\nassign these labels have access to more information than our algorithms (see Sec. 6).'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='moderate and severe). See App. B.1 for further details on this label mapping. Experts who\\nassign these labels have access to more information than our algorithms (see Sec. 6).\\nSplits. To make the most of the available data, we follow Huang et al. (2022) and\\naverage over 3 predefined training/validation/test splits. Each split divides the labeled set\\ninto 360/119/120 studies, each with similar proportions of no, early, and signficant AS.\\nView labels for view classifiers.Roughly 40% of images in TMED-2â€™s labeled set are\\nlabeled withview type, using 5 possible view labels: PLAX, PSAX, A2C, A4C, or Other.\\nOnly PLAX and PSAX views show the aortic valve and thus are relevant for AS assessment.\\nAs per Mitchell et al. (2019), there are at least 9 canonical view types in routine TTEs, so\\nmany images in TMED-2 depict views that are â€œirrelevantâ€ for AS diagnosis. Our later MIL\\napproach does not need view labels at training or test time. It does rely on a view classifier'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='many images in TMED-2 depict views that are â€œirrelevantâ€ for AS diagnosis. Our later MIL\\napproach does not need view labels at training or test time. It does rely on a view classifier\\nduring training (Sec. 4.3), which we pretrain using view labels in TMED-2â€™s train set.\\nUnlabeled set for pretraining.TMED-2 additionally makes available a largeunlabeled\\nset of 5486 studies from distinct patients. Studies in the unlabeled set have no diagnosis\\nlabel nor view label. We use this unlabeled set for pretraining representations (Sec. 4.4), but\\ncannot use them for the supervised training of our MIL due to the lack of labels.\\n2022-Validation dataset. For further evaluation, we obtained with IRB approval\\nadditional deidentified images from routine TTEs of 323 patients at our institution, collected\\nduring 2022 and thus temporally-external to the TMED-2 data. Each study was again\\nassigned an AS severity grading by a clinical expert during routine care. We call this data'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='during 2022 and thus temporally-external to the TMED-2 data. Each study was again\\nassigned an AS severity grading by a clinical expert during routine care. We call this data\\n2022-Validation. It contains 225/48/50 examples of no/early/significant AS.\\n6Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\nFigure 2: Overview of proposed method: Supervised Attention Multiple Instance Learning\\n(SAMIL). Given a study or â€œbagâ€ with many images of diverse views of unknown type, a feature\\nextractor processes each image individually into an embedding vector. Two attention modules (one\\nsupervised by a view classifier and one without) produce attention weights for each instance. The final\\nstudy representation averages the image embeddings by combining the two attentions (Eq.(5)). A\\nfully-connected (FC) layer maps the study representation to a 3-class diagnosis (no/early/significant\\nAS). Pretraining: SAMIL can be pretrained using bag-level (recommended, Sec. 4.4) or image-level'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='fully-connected (FC) layer maps the study representation to a 3-class diagnosis (no/early/significant\\nAS). Pretraining: SAMIL can be pretrained using bag-level (recommended, Sec. 4.4) or image-level\\ncontrastive learning. In either case, a projection head maps representations to a latent space where the\\ncontrastive loss is applied (Chen et al., 2020a,b). The projection head is discarded after pretraining.\\n4. Methods\\nWe now introduce our formulation of AS diagnosis as an MIL problem in Sec 4.1 and discuss\\na general architecture for MIL (Sec. 4.2). We then present the two key innovations of\\nour proposed method, which we callSupervised Attention Multiple Instance Learningor\\nSAMIL. First, Sec. 4.3 presents our supervised attention module that improves the MIL\\npooling layer to better attend to clinically relevant views. Second, Sec. 4.4 presents our\\nstudy-level contrastive learning strategy to improve representation of entire studies (rather'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='pooling layer to better attend to clinically relevant views. Second, Sec. 4.4 presents our\\nstudy-level contrastive learning strategy to improve representation of entire studies (rather\\nthan individual images). Fig 2 gives an overview of SAMIL.\\n4.1. Problem Formulation\\nLet D = {(X1, Y1), . . . ,(XN , YN)} be a training dataset containingN TTE studies. Each\\nstudy, indexed byi, consists of a bag of imagesXi and an (optional) diagnostic labelYi.\\nPrediction task. Given a training set of sizeN, our goal is to build a classifier that\\ncan consume a new echo studyXâˆ— and assign the appropriate labelYâˆ—.\\nInput. Each â€œbagâ€Xi contains Ki distinct images: {xi1, xi2, . . . , xiKi}, which are all 2D\\nTTE images gathered during a routine echocardiogram. The number of imagesKi varies\\nacross studies (TMED-2â€™s typical range 27-97). Eachxik is a grayscale 112x112 pixel image.\\n7Detecting Heart Disease from Multi-View Ultrasound via SAMIL'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='across studies (TMED-2â€™s typical range 27-97). Eachxik is a grayscale 112x112 pixel image.\\n7Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\nOutput. Each studyâ€™s diagnostic labelYi âˆˆ { 0, 1, 2} indicates the assessed severity\\nlevel of aortic stenosis (0 = no AS, 1 = early AS, 2 = significant AS). These labels are\\nassigned by a cardiologist with specialty training in echocardiography during a routine clinical\\ninterpretation of the entire study. Diagnosis labels for individual images are unavailable.\\nImage preprocessing. We used the released dataset without additional preprocessing.\\nAs documented in Huang et al. (2022), the images are extracted from DICOM files in the\\nhealth record by taking the first frame of the corresponding cineloop, removing identifying\\ninformation, padding the shorter axis to a square aspect ratio, and resizing to 112x112.\\n4.2. General MIL architecture\\nFollowing past work on deep neural network approaches to MIL (Ilse et al., 2018; Li et al.,'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='4.2. General MIL architecture\\nFollowing past work on deep neural network approaches to MIL (Ilse et al., 2018; Li et al.,\\n2021a), a typical architecture has 3 components, as illustrated in Fig. 1(d). First, an instance\\nrepresentation layer f transforms each instance into a feature representation. Second, a\\npooling layer Ïƒ aggregates across instances to form a bag-level representation in permutation-\\ninvariant fashion. Finally, an output layerg maps the bag-level representation to a prediction.\\nWe now describe the forward prediction process for one study or â€œbagâ€X under a 3-\\ncomponent architecture specialized to our AS prediction problem. LetX = {x1, . . . , xK} be\\nthe input bag of K instances, with individual instances indexed by integerk. (We use X\\ninterchangably with Xi, dropping study-specific indexi to reduce notational clutter.)\\nInstance representation layerf. Let f be a row-wise feedforward layer that processes'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='interchangably with Xi, dropping study-specific indexi to reduce notational clutter.)\\nInstance representation layerf. Let f be a row-wise feedforward layer that processes\\neach instance xk âˆˆ X independently and identically, producing an instance-specific em-\\nbedding hk = f(xk), where hk âˆˆ RM. Following Ilse et al. (2018)â€™s ABMIL, we use a\\nstack of convolution layers and a MLP layer to extract and project each instanceâ€™s feature\\nrepresentation to low-dimensional embedding. More details in App. B.2.\\nPooling layer Ïƒ. Following ABMIL, our pooling layer produces a bag-level representation\\nz âˆˆ RM via an attention-weighted average of theK instance embeddings {h1, . . . hK}:\\nz =\\nKX\\nk=1\\nakhk, a k = exp(wâŠ¤ tanh(U hk))PK\\nj=1 exp(wâŠ¤ tanh(U hj))\\n, (1)\\nwhere vector w âˆˆ RL and matrix U âˆˆ RLÃ—M are trainable parameters of layerÏƒ. Gated\\nattention modules are also possible (Ilse et al., 2018), but we find accuracy gains are marginal.'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content=', (1)\\nwhere vector w âˆˆ RL and matrix U âˆˆ RLÃ—M are trainable parameters of layerÏƒ. Gated\\nattention modules are also possible (Ilse et al., 2018), but we find accuracy gains are marginal.\\nOutput layer g. Given a bag-level feature vectorz = Ïƒ(f(X)), the output layer performs\\nprobabilistic classification for the 3 levels of AS severity (0=none, 1=early, 2=significant)\\nvia a standard linear-softmax transformation ofz:\\np(Y = r|X) = g(z)r for r âˆˆ {0, 1, 2}, g (z) =\\n\\x14exp(Î·âŠ¤\\n0 z)\\nS(Î·, z) , exp(Î·âŠ¤\\n1 z)\\nS(Î·, z) , exp(Î·âŠ¤\\n2 z)\\nS(Î·, z)\\n\\x15\\n. (2)\\nHere, Î·0, Î·1, Î·2 represent weights for each of the 3 severity levels of AS, and denominator\\nS =P2\\nr=0 exp(Î·âŠ¤\\nr z) ensures the probabilities sum to one. We do include an intercept term\\nfor each class, but omit from notation for clarity.\\nTraining. This 3-component deep MIL architecture has parametersÎ· for the output layer\\nas well asÎ¸ for the pooling and representation layers (Î¸ includes w, U from Eq.(1)). We'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='Training. This 3-component deep MIL architecture has parametersÎ· for the output layer\\nas well asÎ¸ for the pooling and representation layers (Î¸ includes w, U from Eq.(1)). We\\n8Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\ntrain these parameters by minimizing the cross-entropy loss between each studyâ€™s observed\\nAS diagnosis Y and the MIL-predicted probabilities given each bag of imagesX\\nÎ¸âˆ—, Î·âˆ— = argmin\\nÎ¸,Î·\\nX\\nX,Y\\nLCE (Y, gÎ·(ÏƒÎ¸(fÎ¸(X))) (3)\\nIn practice, weight decay is often used to regularize the model and improve generalization.\\n4.3. Contribution 1: Attention supervised by a view classifier\\nWe find the attention-based architecture described above yields unsatisfactory performance in\\nour diagnostic task (see entry labeled ABMIL in Tab. 1). Furthermore, the learned attention\\nvalues used in Eq.(1) do not pass a clinical sanity check: attention should be paid only to\\nPLAX and PSAX AoV view types, as only these show the aortic valve (see Fig. 3). This last'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='values used in Eq.(1) do not pass a clinical sanity check: attention should be paid only to\\nPLAX and PSAX AoV view types, as only these show the aortic valve (see Fig. 3). This last\\nobservation suggests a path forward: supervising the attention mechanism. Suppose we have\\naccess to a trustworthyview-type-relevance classifier v : X â†’ [0.0, 1.0], which maps an image\\nto the probability that it shows a relevant view depicting the aortic valve (either a PLAX\\nor PSAX AoV view), rather than another view type (such as A2C, A4C, A5C, etc.). This\\nclassifier could be used to guide the attention to focus on relevant images. Classifying the\\nview-type of a 2D TTE image has been demonstrated with high accuracy by several groups\\n(Madani et al., 2018; Zhang et al., 2018; Long and Wessler, 2018; Huang et al., 2021).\\nSupervised attention. To implement this idea, we introduce a new loss term, which we\\ncall supervised attention (SA), that steers the attention weightsA = {a1, . . . aK} produced'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='Supervised attention. To implement this idea, we introduce a new loss term, which we\\ncall supervised attention (SA), that steers the attention weightsA = {a1, . . . aK} produced\\nby Eq. (1) to match relevance scoresR = {r1, . . . rK} from a view-relevance classifierv:\\nLSA(w, U) = KL(R||A) =\\nKX\\nk=1\\nrk log rk\\nak\\n, r k = exp(v(xk)/Ï„v)PK\\nk=1 exp(v(xk)/Ï„v)\\n(4)\\nHere, KL means the KL-divergence between two discrete distributions over the sameK\\nchoices, and R âˆˆ âˆ†K is a non-negative vector that sums to one obtained via a softmax\\ntransform of the view relevance probabilities with temperature scalingÏ„v > 0. We define\\nview relevance probability as the sum of probability that the image is PLAX or PSAX.\\nThis supervision ensures the MIL diagnostic model attends to instances that are clinically\\nplausible for the disease in question. That is, attention to PLAX or PSAX views that\\nshow the aortic valve is encouraged, and attention to irrelevant view types like A4C or'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='plausible for the disease in question. That is, attention to PLAX or PSAX views that\\nshow the aortic valve is encouraged, and attention to irrelevant view types like A4C or\\nA2C is discouraged. We emphasize that our approach is classifier-guided because reliable\\nhuman-annotated labels are not always available. Only 40% percent of images in TMED-2\\ntraining set have view labels. If expert-derived labels were more readily available, we could\\nhave supervised directly on those. Using classifier-provided probabilistic labelsR allows us\\nto train easily on â€œas-isâ€ data without expensive annotation effort.\\nOur supervised attention module can be seen as an example ofknowledge distillation(Hin-\\nton et al., 2015), because the MIL model is â€œtaughtâ€ to output attentions weight similar to\\nthe relevant view predictions from the pretrained view classifier. In a sense, the knowledge\\nfrom the view classifier is distilled directly into the MIL model.'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='the relevant view predictions from the pretrained view classifier. In a sense, the knowledge\\nfrom the view classifier is distilled directly into the MIL model.\\nView classifier. We trained the view classifierv on TMED-2â€™s labeledand unlabeled sets\\nvia a recently proposed semi-supervised learning method (Huang et al., 2023b) designed to\\nbe robust to realistic medical image datasets. The classifier is trained to recognize the view\\n9Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\ntype of an image, classifying it as either PLAX, PSAX or Other. To prevent data leakage,\\nseparate view classifiers are trained for each data split. See App E for details.\\nFlexible attention. A potential drawback of enforcing a strict alignment between attention\\nweights and predicted view relevance is reduced flexibility. Ideally, even after identifying\\nimages ofrelevant views, we would like freedom to focus on some images more than others. To'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='weights and predicted view relevance is reduced flexibility. Ideally, even after identifying\\nimages ofrelevant views, we would like freedom to focus on some images more than others. To\\nachieve this, we introduce another set of attention weightsB = {b1, . . . , bK}. Together, the\\nview-classifier-supervised attention A and the flexible attentionB are combined to produce\\nthe final study-level representionz âˆˆ RM by a simple construction,\\nz =\\nKX\\nk=1\\nckhk, c k(A, B) = akbk\\nPK\\nj=1 ajbj\\n, b k = exp(wâŠ¤\\nb tanh(Ubhk))PK\\nj=1 exp(wâŠ¤\\nb tanh(Ubhj))\\n. (5)\\nIn this way, the ultimate attentionck paid to an image can span the full range of 0.0 to 1.0 if\\nthat image is a relevant view, but is likely to be near 0.0 if the classifier deems that imageâ€™s\\nview irrelevant. The trainable parameters that determineB â€“wb âˆˆ RL and Ub âˆˆ RLÃ—M â€“\\nare not supervised by view-relevance, unlike their counterpartsw, U that determine A.\\n4.4. Contribution #2: Contrastive learning of entire study representations'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='are not supervised by view-relevance, unlike their counterpartsw, U that determine A.\\n4.4. Contribution #2: Contrastive learning of entire study representations\\nSelf-supervised learning (SSL) is an effective way to pre-train models that can be later\\nfine-tuned to downstream tasks. As reviewed earlier, most previous methods (Holste et al.,\\n2022a; Saillard et al., 2021; Dehaene et al., 2020) applying SSL to MIL tasks focus on\\npretraining the instance-level feature extractor f (or part of f) aiming to learn better\\ninstance-level representations. In contrast, we propose to pretrain the 2-component network\\nÏƒ â—¦ f, thus refining the study-level representation vectorz summarizing all K images in\\na routine echocardiogram. In the vocabulary of MIL, we call this pretraining the â€œbag-\\nlevelâ€ representation. Later results in Tab. 4 show that our study-level pretraining leads to\\nsubstantial accuracy gains at AS diagnosis compared to image-level pretraining.'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='levelâ€ representation. Later results in Tab. 4 show that our study-level pretraining leads to\\nsubstantial accuracy gains at AS diagnosis compared to image-level pretraining.\\nMoCo(v2) for representations of individual images.Our pretraining strategy builds\\nupon MoCo (He et al., 2020; Chen et al., 2020b), a recent method for self-supervised\\nimage-level contrastive learning (img-CL) that yields state-of-the-art representations via an\\ninstance discrimination task (Wu et al., 2018; Ye et al., 2019; Bachman et al., 2019). The\\nlearned embedding for a training image is encouraged to be similar to embeddings of slight\\ntransformations of itself, while being different from the embeddings of other images.\\nTo obtain embeddings that should be similar, each imagexj in training goes through\\ndifferent transformations (e.g., random augmentation) to yield two versions of itself,xâ€²\\nj and\\nx+\\nj , referred to as the â€œqueryâ€ and the â€œpositive keyâ€. These images are thenencoded into an'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='different transformations (e.g., random augmentation) to yield two versions of itself,xâ€²\\nj and\\nx+\\nj , referred to as the â€œqueryâ€ and the â€œpositive keyâ€. These images are thenencoded into an\\nL-dimensional feature space by composing a projection layerÏˆ (a feed-forward network with\\nl2 normalization) onto the output of the instance-level representation layerf.\\nTo obtain embeddings that should bedissimilar to a given query, MoCo retrievesP\\nprevious embeddings from a first-in-first-out queue data structure. For each new query, these\\nare treated asP â€œnegative keysâ€. In practice, this queue is updated throughout training at\\neach new batch: the oldest elements are dequeued and all key embeddings from the current\\nbatch are enqueued.P is usually set to the size of the queue (He et al., 2020).\\n10Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\nTo train image-level encoderÏ• = Ïˆ â—¦ f that composes projection headÏˆ with feature'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='10Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\nTo train image-level encoderÏ• = Ïˆ â—¦ f that composes projection headÏˆ with feature\\nlayer f given a training set ofJ images, we minimize InfoNCE loss (Oord et al., 2018):\\nLimg-CL(Ï•q) =\\nJX\\nj=1\\nâˆ’ log\\nexp(qâŠ¤\\nj k+\\nj /t)\\nexp(qâŠ¤\\nj k+\\nj /t) +PP\\np=0 exp(qâŠ¤\\nj kâˆ’\\njp/t)\\n, qj = Ï•q(xâ€²\\nj)\\nk+\\nj = Ï•k(x+\\nj ) (6)\\nHere, qj âˆˆ RL is an embedding of the â€œqueryâ€ image,k+\\nj âˆˆ RL is an embedding of the\\nâ€œpositive keyâ€, andkâˆ’\\nj1, . . . kâˆ’\\njP âˆˆ RL are P embeddings of â€œnegative keysâ€ retrieved from the\\nqueue. Scalar temperature t > 0 is a hyperparameter (He et al., 2020).\\nTo improve representation quality, in MoCo queries and keys are encoded by separate\\nnetworks: a query encoderÏ•q with parameters Î¸q and a key encoderÏ•k with parameters\\nÎ¸k. The query encoder Ï•q is trained via standard backpropagation to minimize the loss\\nabove. The key encoder Ï•k is only updated via momentum-based moving average of the'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='Î¸k. The query encoder Ï•q is trained via standard backpropagation to minimize the loss\\nabove. The key encoder Ï•k is only updated via momentum-based moving average of the\\nquery encoder: Î¸k = mÎ¸k + (1âˆ’ m)Î¸q. Momentum m âˆˆ [0, 1) is often set to a relatively large\\nvalue such as 0.999 to make the key embeddings more consistent over time:\\nAdapting MoCo to bag-level representations.Most prior studies in the MIL literature,\\nsuch as Li et al. (2021a), use an â€œoff-the-shelfâ€ version of image-level contrastive learning\\nalgorithm (e.g., SimCLR (Chen et al., 2020a) or MoCo (He et al., 2020; Chen et al., 2020b))\\nto pretrain feature extractorf as described above. However, we find that naively applying\\nMoCo in this way does not yield useful results for our AS diagnosis problem.\\nReasoning that what ultimately matters is the quality of the study-level representationz\\nproduced by our MIL architecture, we adapted MoCo to produce solid representations of'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='Reasoning that what ultimately matters is the quality of the study-level representationz\\nproduced by our MIL architecture, we adapted MoCo to produce solid representations of\\nentire echocardiogram studies. Correspondingly, we modified the InfoNCE loss to operate on\\nthe bag-level representationsz. Given a training set ofN bags X1, . . . XN, our approach to\\nâ€œbag-levelâ€ contrastive learning tries to pull together positive pairs ofstudies and push away\\n(make dissimilar) negative pairs of studies, via the bag-level contrastive-learning loss\\nLbag-CL(Ï•q) =\\nNX\\ni=1\\nâˆ’ log exp(ËœzâŠ¤\\ni Ëœz+\\ni /t)\\nexp(ËœzâŠ¤\\ni Ëœz+\\ni /t) +PP\\np=0 exp(ËœzâŠ¤\\ni Ëœzâˆ’\\nip)/t)\\n, Ëœzi = Ï•q(X â€²\\ni),\\nËœz+\\ni = Ï•k(X +\\ni ). (7)\\nHere, encoderÏ• = Ïˆ â—¦ Ïƒ â—¦ f now operates onall images in a study, composing the same feature\\nextractor f with pooling layer Ïƒ and projection head Ïˆ (note that pooling Ïƒ is not used\\nin Eq.(6)). Ëœz = Ïˆ(z) is the projection ofz, zi = Ïƒq(fq(X â€²\\ni)) is the bag-level representation\\nof the â€œqueryâ€ study, andz+\\ni = Ïƒk(fk(X +'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='in Eq.(6)). Ëœz = Ïˆ(z) is the projection ofz, zi = Ïƒq(fq(X â€²\\ni)) is the bag-level representation\\nof the â€œqueryâ€ study, andz+\\ni = Ïƒk(fk(X +\\ni )) is the bag-level representation of the â€œpositive\\nkeyâ€ study.X â€² and X + are obtained from the given studyX by applying different random\\naugmentation to each of its images.Ëœzâˆ’\\nip are again sampled from MoCoâ€™s queue. The enqueue\\nand dequeue mechanisms and update rules ofÏ•q and Ï•k are the same as the image level case.\\n4.5. SAMIL Pipeline\\nStage 1: Self-Supervised Pretraining. We pretrain our SAMIL network on TMED-2\\ndata utilizing our proposed bag-level pretraining strategy (Sec. 4.4). This method can learn\\nfrom all available studies, including both the labeled train set as well as the much larger\\nunlabeled set (over 350,000 images). After pretraining finishes, following convention (Chen\\net al., 2020b,a), the projection headÏˆq is discarded, and parameters ofÏƒq and fq are retained\\nto warm-start the supervised fine-tuning. More details in App D.'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='et al., 2020b,a), the projection headÏˆq is discarded, and parameters ofÏƒq and fq are retained\\nto warm-start the supervised fine-tuning. More details in App D.\\n11Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\nStage 2: Fine-Tuning to Diagnose Aortic Stenosis (AS).After initializing f and Ïƒ\\nvia stage 1, we fine-tunef, Ïƒ and g using complete studies (all available 2D images regardless\\nof view label availability) from TMED-2â€™s train set by minimizing the overall loss\\nL = LCE + Î»SALSA, (8)\\nHere, the primary supervision comes from each studyâ€™s diagnosis labelY (via cross entropy\\nloss LCE defined in Eq.(3)), while the predicted view relevance of each image provides\\nadditional supervision to the attention module (via supervised attention lossLSA in Eq.(4)).\\nHyperparameter Î»SA > 0 sets the relative weight of the SA loss term.\\n5. Results\\nPerformance metrics. We usebalanced accuracyas our primary performance metric. The'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='Hyperparameter Î»SA > 0 sets the relative weight of the SA loss term.\\n5. Results\\nPerformance metrics. We usebalanced accuracyas our primary performance metric. The\\nclass imbalance in TMED-2 means standard accuracy is less suitable (Huang et al., 2021).\\nGiven a dataset ofN true labels Y1:N and N predicted labels Ë†Y1:N, with each AS diagnosis\\nlabel in {0, 1, 2}, we compute balanced accuracy asP2\\nc=0\\nTPc(Y1:N , Ë†Y1:N )\\nNc(Y1:N ) , whereTPc(Â·) counts\\ntrue positivesfor class c and Nc(Â·) counts all examples with class labelc. Later evaluations of\\nscreening potential assess discrimination between two classes viaarea under the ROC curve.\\nComparisons. We compared our methods with a set of strong baseline including general-\\npurpose multiple-instance learning algorithms (Ilse et al., 2018; Lee et al., 2019; Li et al.,\\n2021a) and prior methods for Aortic Stenosis diagnosis using deep neural networks (Wessler'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='purpose multiple-instance learning algorithms (Ilse et al., 2018; Lee et al., 2019; Li et al.,\\n2021a) and prior methods for Aortic Stenosis diagnosis using deep neural networks (Wessler\\net al., 2023; Holste et al., 2022b,a). We also tried DeepSet (Zaheer et al., 2017), but omit\\nthose results as we could not get DeepSet to perform better than random chance on this\\nchallenging diagnostic task despite substantial hyperparameter tuning (details in App. B.4).\\n5.1. Accuracy vs. Model Size Evaluation on TMED-2\\nTable 1 compares methods on test-set balanced accuracy for 3 diagnostic classes of AS\\nacross the 3 splits of TMED-2. Our proposed method, SAMIL, scores 76%, significantly\\nbetter than 4 other state-of-the-art attention-based MIL architectures we tested (which span\\n60-67%). SAMIL consistently improves over its predecessor ABMIL by a remarkable 13-17%\\ngain across all 3 splits. SAMIL also outperforms more recent MIL architectures like Set'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='60-67%). SAMIL consistently improves over its predecessor ABMIL by a remarkable 13-17%\\ngain across all 3 splits. SAMIL also outperforms more recent MIL architectures like Set\\nTransformer, which employs self-attention for both feature extraction and pooling, and the\\nstate-of-the-art DSMIL (Li et al., 2021a), which leverages a two-stream architecture.\\nTable 1 also compares to two previous approaches dedicated to AS diagnosis:Filter then\\nAverage and Weighted Average by View Relevance. Results suggest that our SAMIL method\\nachieves better accuracy at substantiallysmaller model size. Moreover, once trained, SAMIL\\ncan process the entire TTE study (dozens of images of different views) without the need to\\ndeploy an additional view classifier to filter (Holste et al., 2022b,a) or downweight (Wessler\\net al., 2023) images. We thus find SAMIL to be an effective and portable alternative.\\nTo understand the source of SAMILâ€™s gains, in the appendix we provide confusion matrices'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='et al., 2023) images. We thus find SAMIL to be an effective and portable alternative.\\nTo understand the source of SAMILâ€™s gains, in the appendix we provide confusion matrices\\nin Fig. A.1. SAMIL outperforms W. Avg. by View Rel. in early AS recall, while maintaining\\nsimilar or slightly lower no AS and significant AS recall. Compared to DSMIL, SAMIL\\nimproves no AS and early AS recall, with similar significant AS recall. Compared to ABMIL,\\nSAMIL performs better in all three categories. Further results in Fig A.2 show ROC curves\\nindicating discriminative performance of three clinical use cases for binary screening (no vs\\n12Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\nTest Set Bal. Accuracy\\nMethod split 1 split 2 split 3 average # params view clf.?\\nFilter then Avg. [b] 62.06 65.12 70.35 65.90 11.18 M Yes\\nW. Avg. by View Rel. [c]âˆ— 74.46 72.61 76.24 74.43 5.93 M Yes\\nSAMIL (ours) 75.41 73.78 79.42 76.20 2.31 M No\\nABMIL [d] 58.51 60.39 61.61 60.17 2.25 M No'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='W. Avg. by View Rel. [c]âˆ— 74.46 72.61 76.24 74.43 5.93 M Yes\\nSAMIL (ours) 75.41 73.78 79.42 76.20 2.31 M No\\nABMIL [d] 58.51 60.39 61.61 60.17 2.25 M No\\nABMIL + Gate Attn. [d] 57.83 62.60 59.79 60.07 2.31 M No\\nSet Transformer [e] 60.95 62.61 62.64 62.06 1.98 M No\\nDSMIL [f] 60.10 67.59 73.11 66.93 2.02 M No\\n[b] Holste et al. (2022b), [c] Wessler et al. (2023) [d] Ilse et al. (2018) [e] Lee et al. (2019) [f] Li et al. (2021a)\\nTable 1: AS diagnosis results on TMED2. Showing balanced accuracy (percentage, higher is better)\\non the test set across three train/test splits. Methods b, c, d are diagrammed in corresponding panel\\nin Fig. 1. Methods above the line are approaches specialized to the AS task, others are generic MIL\\nmethods. Column â€œ# paramsâ€ shows number of trainable parameters. Column â€œview clf.?â€ shows\\nwhether an additional view classifier is needed at deployment.âˆ—: value from the cited paper.\\nsome AS, early vs significant, and significant AS vs not). SAMIL outperforms ABMIL and'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='whether an additional view classifier is needed at deployment.âˆ—: value from the cited paper.\\nsome AS, early vs significant, and significant AS vs not). SAMIL outperforms ABMIL and\\nDSMIL across all tasks. In comparison to W. Avg. by View Rel, SAMIL reaches similar\\nperformance in screening No AS vs. Some AS, while doing better in the other two tasks.\\n5.2. Screening Evaluation on 2022-Validation Dataset\\nWe further validate methods on the separate 2022-Validation dataset described earlier,\\nwhich contains 225/48/50 examples of no/early/significant AS. Results in Tab. 2 compare\\nSAMIL to the best-performing baselines from previous section. SAMIL achieved competitive\\nperformance on two critical screening tasks: It seems best on Significant-vs-Not and equivalent\\nto the best on No-vs-Some. On the more challenging Early-vs-Significant task, where both\\nclasses have 50 or fewer examples in this set, all methods have wide uncertainty intervals'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='to the best on No-vs-Some. On the more challenging Early-vs-Significant task, where both\\nclasses have 50 or fewer examples in this set, all methods have wide uncertainty intervals\\nfrom bootstrap resamples and SAMIL scores slightly below DSMIL.\\nAUROC for AS screening\\nMethod No vs Some Significant vs. Not Early vs Significant\\nW. Avg. by View Rel. 0.934 (0.904, 0.959) 0.881 (0.837, 0.921) 0.653 (0.539, 0.760)\\nDSMIL. 0.897 (0.862, 0.929) 0.902 (0.857, 0.941) 0.765 (0.664, 0.857)\\nSAMIL (ours) 0.923 (0.885, 0.955) 0.921 (0.886, 0.951) 0.717 (0.610, 0.813)\\nTable 2: AUROC for AS screening on temporarily distinct cohort. Values in parenthesis show 2.5th\\nand 97.5th percentiles of AUROC values computed from 5000 bootstrap resamples of 323 studies.\\n5.3. Attention Quality Assessment\\nOur supervised attention module is intended to ensure that the modelâ€™s decision-making\\nprocess is consistent with human expert intuition, by only using relevant views to make'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content=\"Our supervised attention module is intended to ensure that the modelâ€™s decision-making\\nprocess is consistent with human expert intuition, by only using relevant views to make\\ndiagnostic judgments. Here, we evaluate how well the attention mechanisms of various models\\nalign with this goal. Fig 3 compares the predicted view relevance of SAMILâ€™s and ABMILâ€™s\\nattended images, aggregating across all studies in the test set. For instance, the first panel\\n13Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\nSplit1 Split2 Split3\\n1 2 3 4 5 6 7 8 9 10\\ntop i'th attended image\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1.0average view relevance\\nSAMIL\\nABMIL\\n1 2 3 4 5 6 7 8 9 10\\ntop i'th attended image\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1.0average view relevance\\nSAMIL\\nABMIL\\n1 2 3 4 5 6 7 8 9 10\\ntop i'th attended image\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1.0average view relevance\\nSAMIL\\nABMIL\\nFigure 3: Predicted view relevance of top-ranked images by attention (higher is better). Supervised\"),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content=\"top i'th attended image\\n0.4\\n0.5\\n0.6\\n0.7\\n0.8\\n0.9\\n1.0average view relevance\\nSAMIL\\nABMIL\\nFigure 3: Predicted view relevance of top-ranked images by attention (higher is better). Supervised\\nattention (SAMIL, ours) outperforms off-the-shelf ABMIL by wide margin across all 3 splits. The\\nx-axis indicates a rank position of images within an echo study when sorted by attention (1 = largest\\nak, 2 = second largest, etc.). The y-axis indicates the average view relevance (across studies in test\\nset) assigned by view classifierv(x) to image x at rank k.\\nreveals that after ranking by attention, the 9th ranked image by ABMIL on average has\\nless than 0.5 view relevance. This means that for many studies, some images in the top 9\\n(as ranked by attention) are likely from irrelevant views. In contrast, SAMILâ€™s 9th ranked\\nimage has an average view relevance above 0.9. Overall, the figure demonstrates that SAMIL\\nbases decisions on clinically relevant views, while ABMIL fails this clinical sanity check. We\"),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='image has an average view relevance above 0.9. Overall, the figure demonstrates that SAMIL\\nbases decisions on clinically relevant views, while ABMIL fails this clinical sanity check. We\\nhope these evaluations reveal how our SAMILâ€™s improved attention module contributes to\\nhelping audit a modelâ€™s overall interpretability, which is key to gaining trust from clinicians\\nand successfully adopting an ML system in medical applications (Holzinger et al., 2017;\\nLundberg and Lee, 2017; Tonekaboni et al., 2019).\\nWe provide two additional sanity checks for our supervised attention module. First, Fig\\nA.3 illustrates the top 10 images ranked by attention from one study (the first in the test\\nset to avoid cherry-picking). Among the top 10 images attended by ABMIL, 5 are actually\\nirrelevant views. In contrast, the top 10 images attended by SAMIL are all from relevant\\nviews. Second, we assess the view classifierâ€™s performance on the view classification task in'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='irrelevant views. In contrast, the top 10 images attended by SAMIL are all from relevant\\nviews. Second, we assess the view classifierâ€™s performance on the view classification task in\\nApp E, supporting that its predicted view relevance serves as a reliable indicator for assessing\\nwhether an image comes from a relevant view or not.\\n5.4. Ablation Evaluations of Supervised Attention and Pretraining\\nTables 3 and 4 verify the impact of our attention (Sec. 4.3) and pretraining (Sec. 4.4) methods.\\nTest Set Bal. Accuracy\\nMethod 1 2 3 average\\nABMIL 58.5 60.4 61.6 60.2\\nABMIL Gate Attn.57.8 62.6 59.8 60.1\\nSAMIL no pretrain72.7 71.6 73.5 72.6\\nTable 3: Ablation ofattention strategies on\\nTMED2. Showing balanced accuracy for AS\\nseverity(higherisbetter)onthetestsetacross\\nsplits. Model sizes are matched to (roughly)\\n2.3M parameters.\\nTest set Bal. Accuracy\\nMethod 1 2 3 average\\nSAMIL no pretrain72.7 71.6 73.5 72.6\\nSAMIL w/ img-CL71.2 67.0 75.8 71.4\\nSAMIL 75.4 73.8 79.4 76.2'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='2.3M parameters.\\nTest set Bal. Accuracy\\nMethod 1 2 3 average\\nSAMIL no pretrain72.7 71.6 73.5 72.6\\nSAMIL w/ img-CL71.2 67.0 75.8 71.4\\nSAMIL 75.4 73.8 79.4 76.2\\nTable 4: Ablation ofpretraining strategies\\non TMED-2. Reporting balanced accuracy for\\nAS severity (higher is better) on the test set\\nacross splits. Model sizes are matched. Last\\nrow uses recommended â€œbag-CLâ€ pretraining.\\n14Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\nOur ablation of the attention module used within pooling layerÏƒ in Table 3 demonstrates\\nthe effectiveness of SAMILâ€™s supervised attention (Eq.(4)). SAMIL achieves an improvement\\nof over 12% compared to ABMIL, the model it builds upon, even without any pretraining.\\nTo understand what SAMILâ€™s built-in study-level (aka bag-level) pretraining adds, we\\ncompare image-level contrastive learning (â€œw/ img-CLâ€) and without pretraining at all.\\nTable 4 shows that image-level pretraining does not improve AS diagnosis performance, while'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='compare image-level contrastive learning (â€œw/ img-CLâ€) and without pretraining at all.\\nTable 4 shows that image-level pretraining does not improve AS diagnosis performance, while\\nour proposed study-level pretraining strategy in SAMIL delivers gains.\\n6. Discussion\\nWe have developed an approach to deep multiple instance learning for diagnosing a common\\nheart valve disease (aortic stenosis) from the dozens of images collected in a routine echocar-\\ndiogram. In our evaluations on the open-access TMED-2 dataset, we find our approach\\nreaches better classifier accuracy than several alternatives, including two recent methods\\ndedicated to AS screening. We suspect that gains come from two sources. First, SAMIL\\ncan use images of both PLAX and PSAX views, not just PLAX as in Holste et al. (2022b).\\nSecond, SAMILâ€™s flexible attention (Eq.(5)) does not weight each relevant view equally.\\nHolste et al.â€™sFilter-then-Average and Wessler et al.â€™sWeighted Average by View Relevance'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='Second, SAMILâ€™s flexible attention (Eq.(5)) does not weight each relevant view equally.\\nHolste et al.â€™sFilter-then-Average and Wessler et al.â€™sWeighted Average by View Relevance\\nessentially treat each high-confidence PLAX imageequally in diagnosis. Instead, we empha-\\nsize that our method can learn a study-specific subset of PLAX images to attend to, based\\non image quality, anatomic visibility, or other factors.\\nLimitations in diagnostic potential. Human experts assess AS using several additional\\nfactors not available to our method. These include patient demographics, clinical variables,\\nand (most importantly) other imaging technologies such as doppler echocardiography as\\nwell as high-resolution cineloop videos from 2D TTE (not just lower-resolution single frame\\nimages used here). Adapting SAMIL to these modalities could provide improved accuracy.\\nLimitations in evaluation. As of this writing, TMED-2 is the only open-access dataset of'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='images used here). Adapting SAMIL to these modalities could provide improved accuracy.\\nLimitations in evaluation. As of this writing, TMED-2 is the only open-access dataset of\\nechos known to us with diagnostic labels for AS or other valve disease. However, it is limited\\nin size and in covered demographics due to drawing from just one hospital site. Further\\nassessment is needed to understand how our proposed method generalizes, especially to\\npopulations underrepresented at the Boston-based hospital where this data was collected.\\nAdvantages. Our SAMIL approach is designed to perform automatic screening of an echo\\nstudy without requiring a first-stage manual or automatic prefiltering to relevant view types.\\nWe further leverage large unlabeled data collections for pretraining. Another direction to\\nutilize unlabeled data is semi-supervised learning (Zhu, 2005), which we leave for future\\nwork.\\nSAMIL could be applied to other structural heart diseases including cardiomyopathies'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='utilize unlabeled data is semi-supervised learning (Zhu, 2005), which we leave for future\\nwork.\\nSAMIL could be applied to other structural heart diseases including cardiomyopathies\\nand mitral and tricuspid disease if suitable labels were available for some studies. Similar\\nmulti-view image diagnostic problems occur in fetal ultrasound, lung ultrasound, and head\\nCT applications; we hope translation to these other domains could bear fruit. Both key\\ninnovations â€“ supervised attention to steer toward clinically-relevant views for the diagnostic\\ntask and study-level representation learning â€“ are applicable to other prediction tasks.\\nUltimately, we hope our study plays a part in transforming early screening for AS and other\\nburdensome diseases to be more reproducible, effective, portable, and actionable.\\n15Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\nAcknowledgments\\nWe acknowledge financial support from the Pilot Studies Program at the Tufts Clinical and'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='15Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\nAcknowledgments\\nWe acknowledge financial support from the Pilot Studies Program at the Tufts Clinical and\\nTranslational Science Institute (Tufts CTSI NIH CTSA UL1TR002544). We are also grateful\\nfor computing infrastructure support from the Tufts High-performance Computing cluster,\\npartially funded by the National Science Foundation under grant OAC CC* 2018149. Author\\nB. W. was supported in part by K23AG055667 (NIH-NIA).\\nReferences\\nStuart Andrews, Ioannis Tsochantaridis, and Thomas Hofmann. Support vector machines for\\nmultiple-instance learning.Advances in neural information processing systems, 15, 2002.\\nSanjeev Arora, Hrishikesh Khandeparkar, Mikhail Khodak, Orestis Plevrakis, and Nikunj Saun-\\nshi. A theoretical analysis of contrastive unsupervised representation learning.arXiv preprint\\narXiv:1902.09229, 2019.\\nJordan T Ash, Surbhi Goel, Akshay Krishnamurthy, and Dipendra Misra. Investigating the role of'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='arXiv:1902.09229, 2019.\\nJordan T Ash, Surbhi Goel, Akshay Krishnamurthy, and Dipendra Misra. Investigating the role of\\nnegatives in contrastive representation learning.arXiv preprint arXiv:2106.09943, 2021.\\nPhilip Bachman, R Devon Hjelm, and William Buchwalter. Learning representations by maximizing\\nmutual information across views.Advances in neural information processing systems, 32, 2019.\\nAdriana Borowa, Dawid Rymarczyk, Dorota OchoÅ„ska, Monika Brzychczy-WÅ‚och, and Bartosz\\nZieliÅ„ski. Classifying bacteria clones using attention-based deep multiple instance learning inter-\\npreted by persistence homology.arXiv preprint arXiv:2012.01189, 2020.\\nGabriele Campanella, Matthew G Hanna, Luke Geneslaw, Allen Miraflor, Vitor Werneck Krauss Silva,\\nKlaus J Busam, Edi Brogi, Victor E Reuter, David S Klimstra, and Thomas J Fuchs. Clinical-grade\\ncomputational pathology using weakly supervised deep learning on whole slide images.Nature\\nmedicine, 25(8):1301â€“1309, 2019.'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='computational pathology using weakly supervised deep learning on whole slide images.Nature\\nmedicine, 25(8):1301â€“1309, 2019.\\nMarc-AndrÃ© Carbonneau, Veronika Cheplygina, Eric Granger, and Ghyslain Gagnon. Multiple\\ninstance learning: A survey of problem characteristics and applications.Pattern Recognition, 77:\\n329â€“353, 2018.\\nGustavo Carneiro, Jacinto Nascimento, and Andrew P. Bradley. Unregistered Multiview Mammogram\\nAnalysis with Pre-trained Deep Learning Models. In Nassir Navab, Joachim Hornegger, William M.\\nWells, and Alejandro F. Frangi, editors,Medical Image Computing and Computer-Assisted In-\\ntervention â€“ MICCAI 2015, Lecture Notes in Computer Science, pages 652â€“660, Cham, 2015.\\nSpringer International Publishing.\\nMathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.\\nUnsupervised learning of visual features by contrasting cluster assignments.Advances in neural\\ninformation processing systems, 33:9912â€“9924, 2020.'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='Unsupervised learning of visual features by contrasting cluster assignments.Advances in neural\\ninformation processing systems, 33:9912â€“9924, 2020.\\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for\\ncontrastive learning of visual representations. InInternational conference on machine learning,\\npages 1597â€“1607. PMLR, 2020a.\\nXinlei Chen and Kaiming He. Exploring simple siamese representation learning. InProceedings of\\nthe IEEE/CVF conference on computer vision and pattern recognition, pages 15750â€“15758, 2021.\\n16Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\nXinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum\\ncontrastive learning.arXiv preprint arXiv:2003.04297, 2020b.\\nLi-Hsin Cheng, Xiaowu Sun, and Rob J. van der Geest. Contrastive learning for echocardiographic\\nview integration. InMedical Image Computing and Computer Assisted Intervention â€“ MICCAI'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='Li-Hsin Cheng, Xiaowu Sun, and Rob J. van der Geest. Contrastive learning for echocardiographic\\nview integration. InMedical Image Computing and Computer Assisted Intervention â€“ MICCAI\\n2022, 2022. URL https://conferences.miccai.org/2022/papers/111-Paper1698.html.\\nPhilip Chikontwe, Meejeong Kim, Soo Jeong Nam, Heounjeong Go, and Sang Hyun Park. Multiple\\ninstance learning with center embeddings for histopathology classification. InMedical Image\\nComputing and Computer Assisted Interventionâ€“MICCAI 2020: 23rd International Conference,\\nLima, Peru, October 4â€“8, 2020, Proceedings, Part V 23, pages 519â€“528. Springer, 2020.\\nChing-YaoChuang, JoshuaRobinson, Yen-ChenLin, AntonioTorralba, andStefanieJegelka. Debiased\\ncontrastive learning.Advances in neural information processing systems, 33:8765â€“8775, 2020.\\nMichal Cohen-Shelly, Zachi I Attia, Paul A Friedman, Saki Ito, Benjamin A Essayagh, Wei-Yin\\nKo, Dennis H Murphree, Hector I Michelena, Maurice Enriquez-Sarano, Rickey E Carter, et al.'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='Michal Cohen-Shelly, Zachi I Attia, Paul A Friedman, Saki Ito, Benjamin A Essayagh, Wei-Yin\\nKo, Dennis H Murphree, Hector I Michelena, Maurice Enriquez-Sarano, Rickey E Carter, et al.\\nElectrocardiogram screening for aortic valve stenosis using artificial intelligence.European heart\\njournal, 42(30), 2021.\\nEric Cosatto, Pierre-Francois Laquerre, Christopher Malon, Hans-Peter Graf, Akira Saito, Tomoharu\\nKiyuna, Atsushi Marugame, and Kenâ€™ichi Kamijo. Automated gastric cancer diagnosis on h&e-\\nstained sections; ltraining a classifier on a large scale with multiple instance machine learning. In\\nMedical Imaging 2013: Digital Pathology, volume 8676, pages 51â€“59. SPIE, 2013.\\nWangzhi Dai, Hamed Nazzari, Mayooran Namasivayam, Judy Hung, and Collin M Stultz. Identifying\\naortic stenosis with a single parasternal long-axis video using deep learning.Journal of the\\nAmerican Society of Echocardiography, 36(1), 2023.'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='aortic stenosis with a single parasternal long-axis video using deep learning.Journal of the\\nAmerican Society of Echocardiography, 36(1), 2023.\\nOlivier Dehaene, Axel Camara, Olivier Moindrot, Axel de Lavergne, and Pierre Courtiol. Self-\\nsupervision closes the gap between weak and strong supervision in histology.arXiv preprint\\narXiv:2012.03583, 2020.\\nThomas G Dietterich, Richard H Lathrop, and TomÃ¡s Lozano-PÃ©rez. Solving the multiple instance\\nproblem with axis-parallel rectangles.Artificial intelligence, 89(1-2):31â€“71, 1997.\\nJianrui Ding, Heng-Da Cheng, Jianhua Huang, Jiafeng Liu, and Yingtao Zhang. Breast ultrasound\\nimage classification based on multiple-instance learning.Journal of digital imaging, 25:620â€“627,\\n2012.\\nDebidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. With\\na little help from my friends: Nearest-neighbor contrastive learning of visual representations. In'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='2012.\\nDebidatta Dwibedi, Yusuf Aytar, Jonathan Tompson, Pierre Sermanet, and Andrew Zisserman. With\\na little help from my friends: Nearest-neighbor contrastive learning of visual representations. In\\nProceedings of the IEEE/CVF International Conference on Computer Vision, pages 9588â€“9597,\\n2021.\\nPierre Elias, Timothy J Poterucha, Vijay Rajaram, Luca Matos Moller, Victor Rodriguez, Shreyas\\nBhave, Rebecca T Hahn, Geoffrey Tison, Sean A Abreau, Joshua Barrios, et al. Deep learning\\nelectrocardiographic analysis for detection of left-sided valvular heart disease.Journal of the\\nAmerican College of Cardiology, 80(6):613â€“626, 2022.\\nSyed KM Gardezi, Saul G Myerson, John Chambers, Sean Coffey, Joanna dâ€™Arcy, FD Richard Hobbs,\\nJonathan Holt, Andrew Kennedy, Margaret Loudon, Anne Prendergast, et al. Cardiac auscultation\\npoorly predicts the presence of valvular heart disease in asymptomatic primary care patients.\\nHeart, 104(22):1832â€“1835, 2018.'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='poorly predicts the presence of valvular heart disease in asymptomatic primary care patients.\\nHeart, 104(22):1832â€“1835, 2018.\\n17Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\nSpyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by\\npredicting image rotations.arXiv preprint arXiv:1803.07728, 2018.\\nTom Ginsberg, Ro-ee Tal, Michael Tsang, Calum Macdonald, Fatemeh Taheri Dezaki, John van\\nder Kuur, Christina Luong, Purang Abolmaesumi, and Teresa Tsang. Deep Video Networks\\nfor Automatic Assessment of Aortic Stenosis in Echocardiography. In J. Alison Noble, Stephen\\nAylward, Alexander Grimwood, Zhe Min, Su-Lin Lee, and Yipeng Hu, editors,Simplifying Medical\\nUltrasound, Lecture Notes in Computer Science, pages 202â€“210, Cham, 2021. Springer International\\nPublishing.\\nPriya Goyal, Piotr DollÃ¡r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='Publishing.\\nPriya Goyal, Piotr DollÃ¡r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew\\nTulloch, Yangqing Jia, and Kaiming He. Accurate, large minibatch sgd: Training imagenet in 1\\nhour. arXiv preprint arXiv:1706.02677, 2017.\\nJean-Bastien Grill, Florian Strub, Florent AltchÃ©, Corentin Tallec, Pierre Richemond, Elena\\nBuchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar,\\net al. Bootstrap your own latent-a new approach to self-supervised learning.Advances in neural\\ninformation processing systems, 33:21271â€“21284, 2020.\\nMohammad Hashir, Hadrien Bertrand, and Joseph Paul Cohen. Quantifying the Value of Lateral\\nViews in Deep Learning for Chest X-rays. InProceedings of the Third Conference on Medical\\nImaging with Deep Learning (MIDL), 2020. URL http://arxiv.org/abs/2002.02582.\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='Imaging with Deep Learning (MIDL), 2020. URL http://arxiv.org/abs/2002.02582.\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\\nrecognition. In Proceedings of the IEEE conference on computer vision and pattern recognition,\\npages 770â€“778, 2016.\\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for\\nunsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on\\ncomputer vision and pattern recognition, pages 9729â€“9738, 2020.\\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network.arXiv\\npreprint arXiv:1503.02531, 2015.\\nGregory Holste, Evangelos K Oikonomou, Bobak Mortazavi, Zhangyang Wang, and Rohan Khera.\\nSelf-supervised learning of echocardiogram videos enables data-efficient clinical diagnosis.arXiv\\npreprint arXiv:2207.11581, 2022a.\\nGregory Holste, Evangelos K Oikonomou, Bobak J Mortazavi, Andreas Coppi, Kamil F Faridi,'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='preprint arXiv:2207.11581, 2022a.\\nGregory Holste, Evangelos K Oikonomou, Bobak J Mortazavi, Andreas Coppi, Kamil F Faridi,\\nEdward J Miller, John K Forrest, Robert L McNamara, Lucila Ohno-Machado, Neal Yuan, et al.\\nAutomated severe aortic stenosis detection on single-view echocardiography: A multi-center deep\\nlearning study.medRxiv, 2022b.\\nAndreas Holzinger, Chris Biemann, Constantinos S Pattichis, and Douglas B Kell. What do we need\\nto build explainable ai systems for the medical domain?arXiv preprint arXiv:1712.09923, 2017.\\nLe Hou, Dimitris Samaras, Tahsin M Kurc, Yi Gao, James E Davis, and Joel H Saltz. Patch-based\\nconvolutional neural network for whole slide tissue image classification. InProceedings of the IEEE\\nconference on computer vision and pattern recognition, pages 2424â€“2433, 2016.\\nZhe Huang, Gary Long, Benjamin Wessler, and Michael C Hughes. A new semi-supervised learning\\nbenchmark for classifying view and diagnosing aortic stenosis from echocardiograms. InMa-'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='Zhe Huang, Gary Long, Benjamin Wessler, and Michael C Hughes. A new semi-supervised learning\\nbenchmark for classifying view and diagnosing aortic stenosis from echocardiograms. InMa-\\nchine Learning for Healthcare Conference, 2021. URL https://proceedings.mlr.press/v149/\\nhuang21a.html.\\n18Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\nZhe Huang, Gary Long, Benjamin S Wessler, and Michael C Hughes. TMED 2: A dataset for semi-\\nsupervised classification of echocardiograms. InDataPerf: Benchmarking Data for Data-Centric\\nAI Workshop at ICML, 2022.\\nZhe Huang, Ruijie Jiang, Shuchin Aeron, and Michael C Hughes. Accuracy versus time frontiers of\\nsemi-supervised and self-supervised learning on medical images.arXiv preprint arXiv:2307.08919,\\n2023a.\\nZhe Huang, Mary-Joy Sidhom, Benjamin Wessler, and Michael C Hughes. Fix-a-step: Semi-supervised\\nlearning from uncurated unlabeled data. InInternational Conference on Artificial Intelligence and\\nStatistics, pages 8373â€“8394. PMLR, 2023b.'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='learning from uncurated unlabeled data. InInternational Conference on Artificial Intelligence and\\nStatistics, pages 8373â€“8394. PMLR, 2023b.\\nMaximilian Ilse, Jakub Tomczak, and Max Welling. Attention-based deep multiple instance learning.\\nIn International conference on machine learning, pages 2127â€“2136. PMLR, 2018.\\nMelih Kandemir and Fred A Hamprecht. Computer-aided diagnosis from weak supervision: A\\nbenchmarking study.Computerized medical imaging and graphics, 42:44â€“50, 2015.\\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron\\nMaschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning.Advances in neural\\ninformation processing systems, 33:18661â€“18673, 2020.\\nMinyoung Kim and Fernando De la Torre. Gaussian processes multiple instance learning. InICML,\\npages 535â€“542. Citeseer, 2010.\\nHema Krishna, Kevin Desai, Brody Slostad, Siddharth Bhayani, Joshua H Arnold, Wouter Ouwerkerk,'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='pages 535â€“542. Citeseer, 2010.\\nHema Krishna, Kevin Desai, Brody Slostad, Siddharth Bhayani, Joshua H Arnold, Wouter Ouwerkerk,\\nYoran Hummel, Carolyn SP Lam, Justin Ezekowitz, Matthew Frost, et al. Fully automated artificial\\nintelligence assessment of aortic stenosis by echocardiography.Journal of the American Society of\\nEchocardiography, 2023.\\nSamuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning.arXiv preprint\\narXiv:1610.02242, 2016.\\nJuho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye Teh. Set trans-\\nformer: A framework for attention-based permutation-invariant neural networks. InInternational\\nconference on machine learning, pages 3744â€“3753. PMLR, 2019.\\nBin Li, Yin Li, and Kevin W Eliceiri. Dual-stream multiple instance learning network for whole slide\\nimage classification with self-supervised contrastive learning. InProceedings of the IEEE/CVF\\nconference on computer vision and pattern recognition (CVPR), 2021a.'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='image classification with self-supervised contrastive learning. InProceedings of the IEEE/CVF\\nconference on computer vision and pattern recognition (CVPR), 2021a.\\nJunnan Li, Caiming Xiong, and Steven CH Hoi. Comatch: Semi-supervised learning with contrastive\\ngraph regularization. In Proceedings of the IEEE/CVF international conference on computer\\nvision, pages 9475â€“9484, 2021b.\\nXirong Li, Wencui Wan, Yang Zhou, Jianchun Zhao, Qijie Wei, Junbo Rong, Pengyi Zhou, Limin\\nXu, Lijuan Lang, Yuying Liu, et al. Deep multiple instance learning with spatial attention for\\nrop case classification, instance selection and abnormality localization. In2020 25th International\\nConference on Pattern Recognition (ICPR), pages 7293â€“7298. IEEE, 2021c.\\nXirong Li, Yang Zhou, Jie Wang, Hailan Lin, Jianchun Zhao, Dayong Ding, Weihong Yu, and Youxin\\nChen. Multi-modal multi-instance learning for retinal disease recognition. InProceedings of the'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='Xirong Li, Yang Zhou, Jie Wang, Hailan Lin, Jianchun Zhao, Dayong Ding, Weihong Yu, and Youxin\\nChen. Multi-modal multi-instance learning for retinal disease recognition. InProceedings of the\\n29th ACM International Conference on Multimedia, pages 2474â€“2482, 2021d.\\n19Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\nZheren Li, Zhiming Cui, Sheng Wang, Yuji Qi, Xi Ouyang, Qitian Chen, Yuezhi Yang, Zhong Xue,\\nDinggang Shen, and Jie-Zhi Cheng. Domain Generalization for Mammography Detection via\\nMulti-style and Multi-view Contrastive Learning. InMedical Image Computing and Computer\\nAssisted Intervention (MICCAI). arXiv, 2021e. URLhttp://arxiv.org/abs/2111.10827.\\nKangning Liu, Weicheng Zhu, Yiqiu Shen, Sheng Liu, Narges Razavian, Krzysztof J Geras, and\\nCarlos Fernandez-Granda. Multiple instance learning via iterative self-paced supervised contrastive\\nlearning. arXiv preprint arXiv:2210.09452, 2022.'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='Carlos Fernandez-Granda. Multiple instance learning via iterative self-paced supervised contrastive\\nlearning. arXiv preprint arXiv:2210.09452, 2022.\\nYun Liu, Krishna Gadepalli, Mohammad Norouzi, George E Dahl, Timo Kohlberger, Aleksey Boyko,\\nSubhashini Venugopalan, Aleksei Timofeev, Philip Q Nelson, Greg S Corrado, et al. Detecting\\ncancer metastases on gigapixel pathology images.arXiv preprint arXiv:1703.02442, 2017.\\nGary Long and Benjamin S Wessler. Identification of echocardiographic imaging view using deep\\nlearning. Circulation: Cardiovascular Quality and Outcomes, 11(suppl_1):A276â€“A276, 2018.\\nMing Y Lu, Richard J Chen, Jingwen Wang, Debora Dillon, and Faisal Mahmood. Semi-supervised\\nhistology classification using deep multiple instance learning and contrastive predictive coding.\\narXiv preprint arXiv:1910.10825, 2019.\\nScott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions.Advances in\\nneural information processing systems, 30, 2017.'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='arXiv preprint arXiv:1910.10825, 2019.\\nScott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions.Advances in\\nneural information processing systems, 30, 2017.\\nAli Madani, Ramy Arnaout, Mohammad Mofrad, and Rima Arnaout. Fast and accurate view\\nclassification of echocardiograms using deep learning.NPJ digital medicine, 1(1):6, 2018.\\nOded Maron and TomÃ¡s Lozano-PÃ©rez. A framework for multiple-instance learning.Advances in\\nneural information processing systems, 10, 1997.\\nCarol Mitchell, Peter S Rahko, Lori A Blauwet, Barry Canaday, Joshua A Finstuen, Michael C\\nFoster, Kenneth Horton, Kofo O Ogunyankin, Richard A Palma, and Eric J Velazquez. Guidelines\\nfor performing a comprehensive transthoracic echocardiographic examination in adults: recom-\\nmendations from the american society of echocardiography.Journal of the American Society of\\nEchocardiography, 32(1):1â€“64, 2019.\\nHasan Nasir Khan, Ahmad Raza Shahid, Basit Raza, Amir Hanif Dar, and Hani Alquhayz. Multi-'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='Echocardiography, 32(1):1â€“64, 2019.\\nHasan Nasir Khan, Ahmad Raza Shahid, Basit Raza, Amir Hanif Dar, and Hani Alquhayz. Multi-\\nView Feature Fusion Based Four Views Model for Mammogram Classification Using Convolutional\\nNeural Network.IEEE Access, 7:165724â€“165733, 2019.\\nMehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving\\njigsaw puzzles. InComputer Visionâ€“ECCV 2016: 14th European Conference, Amsterdam, The\\nNetherlands, October 11-14, 2016, Proceedings, Part VI, pages 69â€“84. Springer, 2016.\\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive\\ncoding. arXiv preprint arXiv:1807.03748, 2018.\\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor\\nKilleen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,\\nhigh-performance deep learning library.Advances in neural information processing systems, 32,\\n2019.'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,\\nhigh-performance deep learning library.Advances in neural information processing systems, 32,\\n2019.\\nGwÃ©nolÃ© Quellec, Mathieu Lamard, Michael D AbrÃ moff, Etienne DecenciÃ¨re, Bruno Lay, Ali Erginay,\\nBÃ©atrice Cochener, and Guy Cazuguel. A multiple-instance learning framework for diabetic\\nretinopathy screening.Medical image analysis, 16(6):1228â€“1240, 2012.\\n20Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\nGwenolÃ© Quellec, Guy Cazuguel, BÃ©atrice Cochener, and Mathieu Lamard. Multiple-instance learning\\nfor medical image and video analysis.IEEE reviews in biomedical engineering, 10:213â€“234, 2017.\\nHerbert Robbins and Sutton Monro. A stochastic approximation method.The Annals of Mathematical\\nStatistics, 22(3):400â€“407, 1951.\\nJonathan Rubin, Deepan Sanghavi, Claire Zhao, Kathy Lee, Ashequl Qadir, and Minnan Xu-Wilson.'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='Statistics, 22(3):400â€“407, 1951.\\nJonathan Rubin, Deepan Sanghavi, Claire Zhao, Kathy Lee, Ashequl Qadir, and Minnan Xu-Wilson.\\nLarge Scale Automated Reading of Frontal and Lateral Chest X-Rays using Dual Convolutional\\nNeural Networks, 2018.\\nDawid Rymarczyk, Adam Pardyl, JarosÅ‚aw Kraus, Aneta KaczyÅ„ska, Marek Skomorowski, and\\nBartosz ZieliÅ„ski. Protomil: Multiple instance learning with prototypical parts for whole-slide\\nimage classification. In Machine Learning and Knowledge Discovery in Databases: European\\nConference, ECML PKDD 2022, Grenoble, France, September 19â€“23, 2022, Proceedings, Part I,\\npages 421â€“436. Springer, 2023.\\nCharlie Saillard, Olivier Dehaene, Tanguy Marchand, Olivier Moindrot, AurÃ©lie Kamoun, Benoit\\nSchmauch, and Simon Jegou. Self supervised learning improves dmmr/msi detection from histology\\nslides across multiple cancers.arXiv preprint arXiv:2109.05819, 2021.\\nZhuchen Shao, Hao Bian, Yang Chen, Yifeng Wang, Jian Zhang, Xiangyang Ji, et al. Transmil:'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='slides across multiple cancers.arXiv preprint arXiv:2109.05819, 2021.\\nZhuchen Shao, Hao Bian, Yang Chen, Yifeng Wang, Jian Zhang, Xiangyang Ji, et al. Transmil:\\nTransformer based correlated multiple instance learning for whole slide image classification.\\nAdvances in neural information processing systems, 34:2136â€“2147, 2021.\\nYash Sharma, Aman Shrivastava, Lubaina Ehsan, Christopher A Moskaluk, Sana Syed, and Donald\\nBrown. Cluster-to-conquer: A framework for end-to-end multi-instance learning for whole slide\\nimage classification. InMedical Imaging with Deep Learning, pages 682â€“698. PMLR, 2021.\\nSana Tonekaboni, Shalmali Joshi, Melissa D McCradden, and Anna Goldenberg. What clinicians\\nwant: contextualizing explainable machine learning for clinical end use. InMachine learning for\\nhealthcare conference, pages 359â€“380. PMLR, 2019.\\nDu Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='healthcare conference, pages 359â€“380. PMLR, 2019.\\nDu Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer\\nlook at spatiotemporal convolutions for action recognition. InProceedings of the IEEE conference\\non Computer Vision and Pattern Recognition, pages 6450â€“6459, 2018.\\nGijs van Tulder, Yao Tong, and Elena Marchiori. Multi-view analysis of unregistered medical images\\nusing cross-view transformers. InMedical Image Computing and Computer Assisted Intervention\\n(MICCAI), 2021.\\nHooman Vaseli, Ang Nan Gu, S. Neda Ahmadi Amiri, Michael Y. Tsang, Andrea Fung, Nima\\nKondori, Armin Saadat, Purang Abolmaesumi, and Teresa S. M. Tsang. ProtoASNet: Dynamic\\nPrototypes for Inherently Interpretable and Uncertainty-Aware Aortic Stenosis Classification in\\nEchocardiography. In Medical Image Computing and Computer Assisted Intervention (MICCAI),\\n2023. URL http://arxiv.org/abs/2307.14433.'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='Echocardiography. In Medical Image Computing and Computer Assisted Intervention (MICCAI),\\n2023. URL http://arxiv.org/abs/2307.14433.\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Åukasz\\nKaiser, and Illia Polosukhin. Attention is all you need.Advances in neural information processing\\nsystems, 30, 2017.\\nJun Wang and Jean-Daniel Zucker. Solving multiple-instance problem: A lazy learning approach. In\\nInternational Conference on Machine Learnign (ICML), 2000.\\nXinggang Wang, Yongluan Yan, Peng Tang, Xiang Bai, and Wenyu Liu. Revisiting multiple instance\\nneural networks.Pattern Recognition, 74:15â€“24, 2018.\\n21Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\nBenjamin S Wessler, Zhe Huang, Gary M Long Jr, Stefano Pacifici, Nishant Prashar, Samuel Karmiy,\\nRoman A Sandler, Joseph Z Sokol, Daniel B Sokol, Monica M Dehn, et al. Automated detection of'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='Benjamin S Wessler, Zhe Huang, Gary M Long Jr, Stefano Pacifici, Nishant Prashar, Samuel Karmiy,\\nRoman A Sandler, Joseph Z Sokol, Daniel B Sokol, Monica M Dehn, et al. Automated detection of\\naortic stenosis using machine learning.Journal of the American Society of Echocardiography, 2023.\\nNan Wu, Jason Phang, Jungkyu Park, Yiqiu Shen, Zhe Huang, Masha Zorin, Stanislaw Jastrzebski,\\nThibault Fevry, Joe Katsnelson, Eric Kim, Stacey Wolfson, Ujas Parikh, Sushma Gaddam, Leng\\nLeng Young Lin, Kara Ho, Joshua D. Weinstein, Beatriu Reig, Yiming Gao, Hildegard Toth,\\nKristine Pysarenko, Alana Lewin, Jiyon Lee, Krystal Airola, Eralda Mema, Stephanie Chung,\\nEsther Hwang, Naziya Samreen, S. Gene Kim, Laura Heacock, Linda Moy, Kyunghyun Cho, and\\nKrzysztof J. Geras. Deep Neural Networks Improve Radiologistsâ€™ Performance in Breast Cancer\\nScreening. IEEE Transactions on Medical Imaging, 39(4), 2020.\\nZhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='Screening. IEEE Transactions on Medical Imaging, 39(4), 2020.\\nZhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via\\nnon-parametric instance discrimination. InProceedings of the IEEE conference on computer vision\\nand pattern recognition (CVPR), 2018.\\nYan Xu, Jun-Yan Zhu, I Eric, Chao Chang, Maode Lai, and Zhuowen Tu. Weakly supervised\\nhistopathology cancer image segmentation and classification. Medical image analysis, 18(3):\\n591â€“604, 2014.\\nSimon Yadgir, Catherine Owens Johnson, Victor Aboyans, Oladimeji M Adebayo, Rufus Adesoji\\nAdedoyin, Mohsen Afarideh, Fares Alahdab, Alaa Alashi, Vahid Alipour, Jalal Arabloo, et al.\\nGlobal, regional, and national burden of calcific aortic valve and degenerative mitral valve diseases,\\n1990â€“2017.Circulation, 141(21):1670â€“1680, 2020.\\nChenxi Yang, Banish D. Ojha, Nicole D. Aranoff, Philip Green, and Negar Tavassolian. Classification'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='1990â€“2017.Circulation, 141(21):1670â€“1680, 2020.\\nChenxi Yang, Banish D. Ojha, Nicole D. Aranoff, Philip Green, and Negar Tavassolian. Classification\\nof aortic stenosis using conventional machine learning and deep learning methods based on\\nmulti-dimensional cardio-mechanical signals.Scientific Reports, 10(1), 2020.\\nMang Ye, Xu Zhang, Pong C Yuen, and Shih-Fu Chang. Unsupervised embedding learning via\\ninvariant and spreading instance feature. InProceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition, pages 6210â€“6219, 2019.\\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks.arXiv preprint arXiv:1605.07146,\\n2016.\\nManzil Zaheer, Satwik Kottur, Siamak Ravanbakhsh, Barnabas Poczos, Russ R Salakhutdinov, and\\nAlexander J Smola. Deep sets.Advances in neural information processing systems, 30, 2017.\\nCha Zhang, John Platt, and Paul Viola. Multiple instance boosting for object detection.Advances\\nin neural information processing systems, 18, 2005.'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='Cha Zhang, John Platt, and Paul Viola. Multiple instance boosting for object detection.Advances\\nin neural information processing systems, 18, 2005.\\nJeffrey Zhang, Sravani Gajjala, Pulkit Agrawal, Geoffrey H Tison, Laura A Hallock, Lauren Beussink-\\nNelson, Mats H Lassen, Eugene Fan, Mandar A Aras, ChaRandle Jordan, et al. Fully automated\\nechocardiogram interpretation in clinical practice: feasibility and diagnostic accuracy.Circulation,\\n138(16):1623â€“1635, 2018.\\nQi Zhang and Sally Goldman. Em-dd: An improved multiple-instance learning technique.Advances\\nin neural information processing systems, 14, 2001.\\nZhendong Zhao, Gang Fu, Sheng Liu, Khaled M Elokely, Robert J Doerksen, Yixin Chen, and\\nDawn E Wilkins. Drug activity prediction using multiple-instance learning via joint instance\\nand feature selection. BMC bioinformatics, 14(S16), 2013. URL https://doi.org/10.1186/\\n1471-2105-14-S14-S16.\\n22Detecting Heart Disease from Multi-View Ultrasound via SAMIL'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='and feature selection. BMC bioinformatics, 14(S16), 2013. URL https://doi.org/10.1186/\\n1471-2105-14-S14-S16.\\n22Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\nMingkai Zheng, Fei Wang, Shan You, Chen Qian, Changshui Zhang, Xiaogang Wang, and Chang Xu.\\nWeakly supervised contrastive learning. InProceedings of the IEEE/CVF International Conference\\non Computer Vision, pages 10042â€“10051, 2021.\\nZhi-Hua Zhou. Multi-instance learning: A survey.Department of Computer Science & Technology,\\nNanjing University, Tech. Rep, 1, 2004.\\nZhi-Hua Zhou, Yu-Yin Sun, and Yu-Feng Li. Multi-instance learning by treating instances as non-iid\\nsamples. In Proceedings of the 26th annual international conference on machine learning, pages\\n1249â€“1256, 2009.\\nXiaojin Jerry Zhu. Semi-supervised learning literature survey. 2005.\\n23Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\nAppendix Contents\\nA Further Results 25'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='1249â€“1256, 2009.\\nXiaojin Jerry Zhu. Semi-supervised learning literature survey. 2005.\\n23Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\nAppendix Contents\\nA Further Results 25\\nA.1 Confusion matrix . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25\\nA.2 ROC for AS Screening Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\\nA.3 Attended Images by SAMIL and ABMIL . . . . . . . . . . . . . . . . . . . . . 27\\nB Methods Details 27\\nB.1 Mapping between the TMED-2 labels and finer-grained clinical scale . . . . . 27\\nB.2 MIL Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\\nB.3 Details on Filter then Avg. Approach . . . . . . . . . . . . . . . . . . . . . . . 29\\nB.4 Details on DeepSet . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\\nC MIL Training Details 29\\nD Self-supervised Pretraining Details 31\\nE View Classifier Details 31\\nF Additional Related Work 32'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='C MIL Training Details 29\\nD Self-supervised Pretraining Details 31\\nE View Classifier Details 31\\nF Additional Related Work 32\\n24Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\nAppendix A. Further Results\\nA.1. Confusion matrix\\nSplit 1 Split 2 Split 3\\nW. Avg. by View Rel.\\nno early significant\\nPredicted Label\\nnoearlysignificant\\nTrue Label\\n22 2 1\\n8 19 7\\n3 9\\n48\\nno early significant\\nPredicted Label\\nnoearlysignificant\\nTrue Label\\n24 0 0\\n9 14 11\\n3 11\\n46\\nno early significant\\nPredicted Label\\nnoearlysignificant\\nTrue Label\\n26 0 0\\n8 16 10\\n4 7\\n49ABMIL\\nno early significant\\nPredicted Label\\nnoearlysignificant\\nTrue Label\\n19 5 2\\n12 15 7\\n8 17 35\\nno early significant\\nPredicted Label\\nnoearlysignificant\\nTrue Label\\n17 6 3\\n9 15 10\\n5 12\\n43\\nno early significant\\nPredicted Label\\nnoearlysignificant\\nTrue Label\\n19 6 1\\n4 21 9\\n4 26 30DSMIL\\nno early significant\\nPredicted Label\\nnoearlysignificant\\nTrue Label\\n17 8 1\\n10 13 11\\n1 13\\n46\\nno early significant\\nPredicted Label\\nnoearlysignificant\\nTrue Label\\n14 10 2'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='19 6 1\\n4 21 9\\n4 26 30DSMIL\\nno early significant\\nPredicted Label\\nnoearlysignificant\\nTrue Label\\n17 8 1\\n10 13 11\\n1 13\\n46\\nno early significant\\nPredicted Label\\nnoearlysignificant\\nTrue Label\\n14 10 2\\n3 24 7\\n3 10 47\\nno early significant\\nPredicted Label\\nnoearlysignificant\\nTrue Label\\n23 3 0\\n3 19 12\\n4 11\\n45SAMIL (ours)\\nno early significant\\nPredicted Label\\nnoearlysignificant\\nTrue Label\\n20 5 1\\n6 23 5\\n0 11\\n49\\nno early significant\\nPredicted Label\\nnoearlysignificant\\nTrue Label\\n20 5 1\\n6 27 1\\n3 18 39\\nno early significant\\nPredicted Label\\nnoearlysignificant\\nTrue Label\\n23 3 0\\n3 26 5\\n3 13 44\\nFigure A.1: Confusion matrices for the patient-level AS diagnosis classification, across three predefined\\ntrain/test splits of TMED2.\\n25Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\nA.2. ROC for AS Screening Tasks\\nNo vs Some AS Early vs Significant AS NoSig vs Significant AS\\nSplit1\\n0.0 0.2 0.4 0.6 0.8 1.0\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nSAMIL: 0.924\\nDSMIL: 0.904\\nABMIL: 0.846\\nW. Avg. by View Rel: 0.931'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='No vs Some AS Early vs Significant AS NoSig vs Significant AS\\nSplit1\\n0.0 0.2 0.4 0.6 0.8 1.0\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nSAMIL: 0.924\\nDSMIL: 0.904\\nABMIL: 0.846\\nW. Avg. by View Rel: 0.931\\n0.0 0.2 0.4 0.6 0.8 1.0\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nSAMIL: 0.89\\nDSMIL: 0.781\\nABMIL: 0.765\\nW. Avg. by View Rel: 0.783\\n0.0 0.2 0.4 0.6 0.8 1.0\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nSAMIL: 0.924\\nDSMIL: 0.856\\nABMIL: 0.827\\nW. Avg. by View Rel: 0.891Split2\\n0.0 0.2 0.4 0.6 0.8 1.0\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nSAMIL: 0.928\\nDSMIL: 0.867\\nABMIL: 0.842\\nW. Avg. by View Rel: 0.97\\n0.0 0.2 0.4 0.6 0.8 1.0\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nSAMIL: 0.847\\nDSMIL: 0.818\\nABMIL: 0.785\\nW. Avg. by View Rel: 0.755\\n0.0 0.2 0.4 0.6 0.8 1.0\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nSAMIL: 0.881\\nDSMIL: 0.859\\nABMIL: 0.808\\nW. Avg. by View Rel: 0.865Split3\\n0.0 0.2 0.4 0.6 0.8 1.0\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nSAMIL: 0.969\\nDSMIL: 0.951\\nABMIL: 0.919\\nW. Avg. by View Rel: 0.969\\n0.0 0.2 0.4 0.6 0.8 1.0\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nSAMIL: 0.826\\nDSMIL: 0.796\\nABMIL: 0.675\\nW. Avg. by View Rel: 0.702'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nSAMIL: 0.969\\nDSMIL: 0.951\\nABMIL: 0.919\\nW. Avg. by View Rel: 0.969\\n0.0 0.2 0.4 0.6 0.8 1.0\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nSAMIL: 0.826\\nDSMIL: 0.796\\nABMIL: 0.675\\nW. Avg. by View Rel: 0.702\\n0.0 0.2 0.4 0.6 0.8 1.0\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\nSAMIL: 0.897\\nDSMIL: 0.872\\nABMIL: 0.795\\nW. Avg. by View Rel: 0.855\\nFigure A.2: Diagnosis classification receiver operator curves. Showing results across three predefined\\ntrain/test splits of TMED2 and three clinically relevant screening tasks.\\n26Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\nA.3. Attended Images by SAMIL and ABMIL\\nABMIL\\nABMIL\\nSAMIL\\nSAMIL\\nFigure A.3: Showing top attended images for the first study in the test set. The top 2 rows show the\\ntop 10 attended images by ABMIL, bottom 2 rows show the top 10 attended images by SAMIL. Red\\nbox indicates the image is not a clinically relevant view for AS diagnosis.\\nAppendix B. Methods Details\\nB.1. Mapping between the TMED-2 labels and finer-grained clinical scale'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='box indicates the image is not a clinically relevant view for AS diagnosis.\\nAppendix B. Methods Details\\nB.1. Mapping between the TMED-2 labels and finer-grained clinical scale\\nHere we show how the 3-level course diagnosis classes in TMED-2 (advocated by Wessler\\net al. (2023)) map to the common 5-level fine-grained clinical scale used by clinicians.\\nWe chose this mapping because our study was designed with three overarching clinical\\nconsiderations: (1) An AS screening framework should be designed primarily to be sensitive\\nfor identifying disease rather than for comprehensive phenotyping of AS given the complexity\\nof this clinical syndrome; (2) Given the challenges with contemporary diagnosis (and the\\nmany subtypes of severe AS) and the concerns that severe AS might masquerade as moderate\\nAS with certain low-flow subtypes, we designed our disease classifiers to identify â€˜significant\\nASâ€™, a category that includes moderate and severe AS. This was purposely done to maximize'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='AS with certain low-flow subtypes, we designed our disease classifiers to identify â€˜significant\\nASâ€™, a category that includes moderate and severe AS. This was purposely done to maximize\\n27Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\n5-Level Scale TMED2 Label\\nno AS no AS\\nmild AS early AS\\nmild-to-moderate AS early AS\\nmoderate AS significant AS\\nsevere AS significant AS\\nTable B.1: Mapping between the TMED-2 labels and finer-grained clinical scale\\nutility as a screening tool; and (3) The expected clinical application of our fully automated\\nMIL models is that it will trigger referral for comprehensive echocardiography and heart\\nteam evaluation.\\nB.2. MIL Architecture\\nBelow we report the architecture details for SAMIL. For feature extractorf, we use a simple\\nstack of convolution layers as done in ABMIL (Ilse et al., 2018). We used the same feature\\nFeature Extractorf\\nConv2d(3, 20, kernel=(5,5))\\nReLU()\\nMaxPool2d(2, stride=2)\\nConv2d(20, 50, kernel=(5,5))\\nReLU()'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='Feature Extractorf\\nConv2d(3, 20, kernel=(5,5))\\nReLU()\\nMaxPool2d(2, stride=2)\\nConv2d(20, 50, kernel=(5,5))\\nReLU()\\nMaxPool2d(2, stride=2)\\nConv2d(50, 100, kernel=(5,5))\\nReLU()\\nConv2d(100, 200, kernel=(5,5))\\nReLU()\\nMaxPool2d(2, stride=2)\\nTable B.2: Details of Feature Extractorf\\nextractor f shown in B.2 for SAMIL, ABMIL, Set Transformer and DSMIL.\\nThe feature extractorf maps each of the original images into 200 feature maps with\\nsmaller size. In practice, a MLP can be use (optional) to further process the flattened feature\\nmaps (also see Fig 2). We use the same MLP [Linear(32000, 500), ReLU(), Linear(500, 250),\\nReLU(), Linear(250, 500), ReLU()] for both SAMIL and ABMIL. For Set Transformer, we\\ndirectly flattened the extracted feature maps and feed them to the Set Transformerâ€™s ISAB\\nblocks. Please refer to original paper (Lee et al., 2019) for more details. For DSMIL, the\\nextracted feature maps are flattened and projected to vectors of dimension 500 by a linear'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='blocks. Please refer to original paper (Lee et al., 2019) for more details. For DSMIL, the\\nextracted feature maps are flattened and projected to vectors of dimension 500 by a linear\\nlayer followed by ReLU, and then feed to its two streams. Please refer to original paper (Li\\net al., 2021a) for more details.\\nFor the pooling layerÏƒ, we use the same MLP architectures (shown in B.3) for both the\\nsupervised attention branch and flexible attention branch in SAMIL. Note that this is also\\nthe same MLP architecture to learn attention weights in ABMIL.\\n28Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\nMLP learning attention weights\\nLinear(500, 128)\\nTanh()\\nLinear(128, 1)\\nTable B.3: Details of MLP used to learn attention weights for SAMIL and ABMIL\\nFor output layerg both SAMIL and ABMIL use a simple linear layer (with softmax). Our\\nexperiments for DSMIL, and Set Transformer are mainly based on the official open-source'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='For output layerg both SAMIL and ABMIL use a simple linear layer (with softmax). Our\\nexperiments for DSMIL, and Set Transformer are mainly based on the official open-source\\ncode from corresponding paper. Please refer to the original papers for more details on their\\nÏƒ and g.\\nB.3. Details on Filter then Avg. Approach\\nTo apply the Filter then Avg. approach proposed on TMED2, we follow closely the steps\\noutlined in the paper Holste et al. (2022b,a). We first use the same view classifiers that are\\nused for SAMIL to prefilter images in the dataset, keeping only images that are predicted\\nas PLAX. We then use a 2D ResNet18 (He et al., 2016) to train the diagnosis classifier to\\nclassify each retained PLAX image as no AS, early AS or significant AS. In aggregation step,\\nwe average the AS predictions of all PLAX images in a study to obtain the study-level AS\\nprediction. Note that author in (Holste et al., 2022a) uses a 3D ResNet18 (Tran et al., 2018)'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='we average the AS predictions of all PLAX images in a study to obtain the study-level AS\\nprediction. Note that author in (Holste et al., 2022a) uses a 3D ResNet18 (Tran et al., 2018)\\nsince their proprietary dataset consists of 3D videos while the open access TMED2 consists\\nof 2D images. For the same reason, we are not able to directly use their self-superivsed\\ntraining strategy that are proposed for 3D videos.\\nB.4. Details on DeepSet\\nDeepSet (Zaheer et al., 2017) process each instance in the bag independently, and aggregate\\nthe processed feature embedding using simple pooling (mean or max). Fully connected layers\\nare then used to map the aggregated feature embeddings into a bag prediction.\\nWe perform the same hyperparameter search for DeepSet as shown in App C. However,\\nwe wonâ€™t able to obtain any meaningful results, which suggest that problem of using multiple\\nultrasound images for AS diagnosis is too challenging for simple architecture like DeepSet.\\nAppendix C. MIL Training Details'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='ultrasound images for AS diagnosis is too challenging for simple architecture like DeepSet.\\nAppendix C. MIL Training Details\\nOur open source code (https://github.com/tufts-ml/SAMIL/) uses PyTorch (Paszke et al.,\\n2019). For all methods compared, we use SGD (Robbins and Monro, 1951) as optimizer.\\nEach method is set to train for 2000 epochs, and early stop if validation performance does\\nnot increase for 200 consecutive epochs. Each training run uses one NVIDIA A100 GPU.\\nWe perform a grid search for each algorithm and each data split. From our preliminary\\nexperiments, we found that learning rate around 0.0005 and weight decay around 0.0001 is a\\ngood starting point.\\nFor DSMIL, ABMIL, Set Transformer, DeepSet and Filter then Avg, we search learning\\nrate in [0.0003, 0.0005, 0.0008, 0.001, 0.003] and weight decay in [0.00001, 0.00003, 0.0001,\\n0.0003, 0.001]. SAMIL involves two additional hyperparameters, a temperature scaling\\n29Detecting Heart Disease from Multi-View Ultrasound via SAMIL'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='0.0003, 0.001]. SAMIL involves two additional hyperparameters, a temperature scaling\\n29Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\nterm Ï„v used in eq. 4, andÎ»SA in eq. 8 that balance the supervised attention loss and the\\ncross-entropy loss. For SAMIL, we search learning rate in [0.0005, 0.0008], weight decay\\nin [0.0001, 0.001],Ï„v in [0.1, 0.05, 0.03] andÎ»SA in [5, 15, 20]. Note that for ABMIL with\\ngated attention, we did not search hyperparameters again, but directly use the corresponding\\nbest hyperparameter from its general attention version. Note that we perform same set of\\nindependent hyperparameter search for experiments on SAMIL with bag-level pretraining,\\nimage-level pretraining and without pretraining.\\nFinal hyperparameter used are reported as follow:\\nSAMIL (with study-level SSL)\\nHyperparameter split1 split2 split3\\nLearning rate 0.0008 0.0005 0.0005\\nWeight decay 0.0001 0.0001 0.001\\nTemperature T 0.1 0.05 0.1\\nÎ»SA 15.0 20.0 20.0'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='SAMIL (with study-level SSL)\\nHyperparameter split1 split2 split3\\nLearning rate 0.0008 0.0005 0.0005\\nWeight decay 0.0001 0.0001 0.001\\nTemperature T 0.1 0.05 0.1\\nÎ»SA 15.0 20.0 20.0\\nLearning rate schedulecosine cosine cosine\\nTable C.1: Hyperparameter settings for SAMIL across different data splits.\\nDSMIL\\nHyperparameter split1 split2 split3\\nLearning rate 0.001 0.0008 0.0008\\nWeight decay 0.0001 0.00003 0.00001\\nLearning rate schedulecosine cosine cosine\\nTable C.2: Hyperparameter settings for DSMIL across different data splits.\\nABMIL\\nHyperparameter split1 split2 split3\\nLearning rate 0.0008 0.0005 0.0008\\nWeight decay 0.0001 0.00005 0.00005\\nLearning rate schedulecosine cosine cosine\\nTable C.3: Hyperparameter settings for ABMIL across different data splits.\\nSet Transformer\\nHyperparameter split1 split2 split3\\nLearning rate 0.0010 0.0008 0.0008\\nWeight decay 0.00003 0.0001 0.00001\\nLearning rate schedule cosine cosine cosine'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='Set Transformer\\nHyperparameter split1 split2 split3\\nLearning rate 0.0010 0.0008 0.0008\\nWeight decay 0.00003 0.0001 0.00001\\nLearning rate schedule cosine cosine cosine\\nTable C.4: Hyperparameter settings for Set Transformer across different data splits.\\n30Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\nFilter then Avg.\\nHyperparameter split1 split2 split3\\nLearning rate 0.003 0.001 0.003\\nWeight decay 0.00003 0.00001 0.00001\\nLearning rate schedule cosine cosine cosine\\nTable C.5: Hyperparameter settings for Filter then Avg. across different data splits.\\nAppendix D. Self-supervised Pretraining Details\\nOur implementation is based on the official code from MoCo (He et al., 2020; Chen et al.,\\n2020b). For image-level contrastive learning, we set learning rate to 0.06, weight decay to\\n0.0005, batch size to 512, size of queue to 4096, momentum m to 0.99, softmax temperature\\nto 0.1. For bag-level contrastive learning, we set learning rate to 0.00015 (following the'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='0.0005, batch size to 512, size of queue to 4096, momentum m to 0.99, softmax temperature\\nto 0.1. For bag-level contrastive learning, we set learning rate to 0.00015 (following the\\nlinear Scaling Relu (Goyal et al., 2017), which is also recommended by MoCoâ€™s author),\\nweight decay to 0.0005, batch size to 1, size of queue to 4096, momentum m to 0.99, softmax\\ntemperature to 0.1. Note that we did not tune hyperparameters for the self-supervised\\npretraining.\\nWe train the model using the train set as well as the unlabeled set for both image-level\\nand bag-level contrastive learning. The model is set to train for 200 epochs, with early\\nstopping monitored by knn protocol on the validation set. The early stopping patience is set\\nto 20.\\nprojection headÏˆ. The projection head is a two-layer MLP with the structure [Linear(500,\\n500), ReLU(), Linear(500, 128)]. The projection head is used to project the image or bag'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='to 20.\\nprojection headÏˆ. The projection head is a two-layer MLP with the structure [Linear(500,\\n500), ReLU(), Linear(500, 128)]. The projection head is used to project the image or bag\\nrepresentation to a latent space where the contrastive loss is applied. The projection head is\\ndiscarded after training following the convention from (Chen et al., 2020b,a).\\nAppendix E. View Classifier Details\\nWe train a view classifier for each of the three splits independently. We train the classifiers\\nusing a recently proposed semi-supervised learning method (?) with Pi-model (Laine and\\nAila, 2016). We used the view labeled images in each splitâ€™s train set (as the labeled data)\\nas well as the unlabeled set (as the unlabeled data).\\nThe view classifiers are trained to output probabilities of three category: PLAX, PSAX\\nand Other. The view classifiersâ€™ performance is shown in E.1\\nMethod split1 split2 split3\\nFix-A-Step + Pi97.20 98.14 98.00'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='and Other. The view classifiersâ€™ performance is shown in E.1\\nMethod split1 split2 split3\\nFix-A-Step + Pi97.20 98.14 98.00\\nTable E.1: Balanced accuracy on view classification. Showing view classification on TMED2 test\\nsetâ€™s view labeled images.\\nBackbone. The view classifiers use Wide ResNet (Zagoruyko and Komodakis, 2016) as\\nbackbone, specifically, the â€œWRN-28-2â€ that has a depth 28 and width 2.\\n31Detecting Heart Disease from Multi-View Ultrasound via SAMIL\\nTraining and Hyperparameters. We train the view classifiers using SGD (Robbins and\\nMonro, 1951) as optimizer. We train the classifiers for 500 epochs, and retain the checkpoint\\nthat has maximum validation accuracy on the validation set. Hyperparameters used are\\nreported below E.2\\nHyperparameter split1 split2 split3\\nLabeled batch size 64 64 64\\nUnlabeled batch size 64 64 64\\nLearning rate 0.0003 0.009 0.009\\nWeight decay 0.05 0.0005 0.0005\\nMax consistency coefficient 0.3 0.3 0.3\\nBeta shape Î± 0.5 0.5 0.5'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='Labeled batch size 64 64 64\\nUnlabeled batch size 64 64 64\\nLearning rate 0.0003 0.009 0.009\\nWeight decay 0.05 0.0005 0.0005\\nMax consistency coefficient 0.3 0.3 0.3\\nBeta shape Î± 0.5 0.5 0.5\\nUnlabeled loss warmup schedulelinear linear linear\\nLearning rate schedule cosine cosine cosine\\nTable E.2: Hyperparameters used for the view classifiers in each split.\\nAppendix F. Additional Related Work\\nClassic approaches. Examples of classic MIL methods includes iARP (Dietterich et al.,\\n1997), Diverse Density (Maron and Lozano-PÃ©rez, 1997), Citation-kNN (Wang and Zucker,\\n2000), MI-Kernels (Zhang and Goldman, 2001), MI/mi-SVM (Andrews et al., 2002), mi-\\nGraph (Zhou et al., 2009), MILBoost (Zhang et al., 2005), GPMIL (Kim and De la Torre,\\n2010), among others.\\nAdditional examples of medical applications of MIL.Other medical applications\\nof MIL include diabetic retinopathy screening (Quellec et al., 2012; Li et al., 2021c,d;'),\n",
       " Document(metadata={'arxiv_id': '2306.00003v3', 'title': 'Detecting Heart Disease from Multi-View Ultrasound Images via Supervised Attention Multiple Instance Learning', 'section': 'body', 'authors': 'Zhe Huang, Benjamin S. Wessler, Michael C. Hughes'}, page_content='2010), among others.\\nAdditional examples of medical applications of MIL.Other medical applications\\nof MIL include diabetic retinopathy screening (Quellec et al., 2012; Li et al., 2021c,d;\\nKandemir and Hamprecht, 2015), bacteria clone analysis (Borowa et al., 2020), drug activity\\nprediction (Dietterich et al., 1997; Zhao et al., 2013), and cancer diagnosis (Campanella\\net al., 2019; Chikontwe et al., 2020; Hou et al., 2016; Ding et al., 2012; Xu et al., 2014).\\n32'),\n",
       " Document(metadata={'arxiv_id': '2503.17057v1', 'title': 'Semi-supervised Cervical Segmentation on Ultrasound by A Dual Framework for Neural Networks', 'section': 'title_abstract', 'authors': 'Fangyijie Wang, Kathleen M. Curran, GuÃ©nolÃ© Silvestre'}, page_content='Title: Semi-supervised Cervical Segmentation on Ultrasound by A Dual Framework for Neural Networks\\n\\nAbstract: Accurate segmentation of ultrasound (US) images of the cervical muscles is crucial for precision healthcare. The demand for automatic computer-assisted methods is high. However, the scarcity of labeled data hinders the development of these methods. Advanced semi-supervised learning approaches have displayed promise in overcoming this challenge by utilizing labeled and unlabeled data. This study introduces a novel semi-supervised learning (SSL) framework that integrates dual neural networks. This SSL framework utilizes both networks to generate pseudo-labels and cross-supervise each other at the pixel level. Additionally, a self-supervised contrastive learning strategy is introduced, which employs a pair of deep representations to enhance feature learning capabilities, particularly on unlabeled data. Our framework demonstrates competitive performance in cervical segmentation tasks. Our codes are publicly available on https://github.com/13204942/SSL\\\\_Cervical\\\\_Segmentation.'),\n",
       " Document(metadata={'arxiv_id': '2503.17057v1', 'title': 'Semi-supervised Cervical Segmentation on Ultrasound by A Dual Framework for Neural Networks', 'section': 'body', 'authors': 'Fangyijie Wang, Kathleen M. Curran, GuÃ©nolÃ© Silvestre'}, page_content='SEMI-SUPERVISED CERVICAL SEGMENTA TION ON ULTRASOUND BY A DUAL\\nFRAMEWORK FOR NEURAL NETWORKS\\nFangyijie Wangâ‹†â€  Kathleen M. Curran â‹†â€  GuÂ´enolÂ´e Silvestreâ‹†Â§\\nâ‹† Taighde Â´Eireann â€“ Research Ireland Centre for Research Training in Machine Learning\\nÂ§ School of Computer Science, University College Dublin, Dublin, Ireland\\nâ€  School of Medicine, University College Dublin, Dublin, Ireland\\nABSTRACT\\nAccurate segmentation of ultrasound (US) images of the\\ncervical muscles is crucial for precision healthcare. The\\ndemand for automatic computer-assisted methods is high.\\nHowever, the scarcity of labeled data hinders the develop-\\nment of these methods. Advanced semi-supervised learning\\napproaches have displayed promise in overcoming this chal-\\nlenge by utilizing labeled and unlabeled data. This study\\nintroduces a novel semi-supervised learning (SSL) frame-\\nwork that integrates dual neural networks. This SSL frame-\\nwork utilizes both networks to generate pseudo-labels and'),\n",
       " Document(metadata={'arxiv_id': '2503.17057v1', 'title': 'Semi-supervised Cervical Segmentation on Ultrasound by A Dual Framework for Neural Networks', 'section': 'body', 'authors': 'Fangyijie Wang, Kathleen M. Curran, GuÃ©nolÃ© Silvestre'}, page_content='introduces a novel semi-supervised learning (SSL) frame-\\nwork that integrates dual neural networks. This SSL frame-\\nwork utilizes both networks to generate pseudo-labels and\\ncross-supervise each other at the pixel level. Additionally,\\na self-supervised contrastive learning strategy is introduced,\\nwhich employs a pair of deep representations to enhance fea-\\nture learning capabilities, particularly on unlabeled data. Our\\nframework demonstrates competitive performance in cervi-\\ncal segmentation tasks. Our codes are publicly available on\\nhttps://github.com/13204942/SSL Cervical Segmentation.\\nIndex Terms â€” Semi-supervised Learning, Ultrasound\\nImage, Cervical Segmentation, Contrastive Learning.\\n1. INTRODUCTION\\nTransvaginal ultrasound is the preferred method for visualiz-\\ning the cervix in most patients as it provides detailed insight\\ninto cervical anatomy and structure [1]. Accurate segmenta-\\ntion of ultrasound (US) images of the cervical muscles is cru-'),\n",
       " Document(metadata={'arxiv_id': '2503.17057v1', 'title': 'Semi-supervised Cervical Segmentation on Ultrasound by A Dual Framework for Neural Networks', 'section': 'body', 'authors': 'Fangyijie Wang, Kathleen M. Curran, GuÃ©nolÃ© Silvestre'}, page_content='ing the cervix in most patients as it provides detailed insight\\ninto cervical anatomy and structure [1]. Accurate segmenta-\\ntion of ultrasound (US) images of the cervical muscles is cru-\\ncial for analyzing the deep muscle structures, evaluating their\\nfunction, and monitoring customized treatment protocols for\\nindividual patients [2].\\nThe manual annotation of cervical structures in transvagi-\\nnal ultrasound images is a labor-intensive and time-consuming\\nprocess, which restricts the availability of extensive labeled\\ndatasets essential for building robust machine learning mod-\\nels. To address this challenge, semi-supervised learning\\n(SSL) techniques have shown potential by incorporating la-\\nbeled and unlabeled data, thus enhancing the extraction of\\nvaluable insights from unannotated data [3, 4].\\nThis study presents an SSL framework for cervical seg-\\nmentation on ultrasound images. First, the SSL framework is\\ndesigned for network training on many unlabeled data. Then,'),\n",
       " Document(metadata={'arxiv_id': '2503.17057v1', 'title': 'Semi-supervised Cervical Segmentation on Ultrasound by A Dual Framework for Neural Networks', 'section': 'body', 'authors': 'Fangyijie Wang, Kathleen M. Curran, GuÃ©nolÃ© Silvestre'}, page_content='This study presents an SSL framework for cervical seg-\\nmentation on ultrasound images. First, the SSL framework is\\ndesigned for network training on many unlabeled data. Then,\\npixel-level cross-supervised learning is introduced within the\\nframework. Therefore, the network is trained with the help of\\nthe other network via pseudo-labeling. A contrastive learning\\nstrategy is introduced within this framework, incorporating a\\npair of embedded features to maximize the feature learning\\ncapabilities using both labeled and unlabeled data. Moreover,\\nthe framework is validated using a dataset in a public chal-\\nlenge, demonstrating competitive performance.\\n2. METHODOLOGY\\nThe SSL framework of dual neural networks is illustrated\\nin Fig.1, where (XL, Ygt) âˆˆ L denotes the labeled train-\\ning dataset, while (XUL) âˆˆ U denotes the unlabeled train-\\ning dataset. XL, XUL âˆˆ R3Ã—nhÃ—nw represents a 2D ultra-\\nsound image of size nh Ã— nw with 3 channels. The dual\\nneural networks are denoted by f 1\\nÎ¸ (Â·) and f 2'),\n",
       " Document(metadata={'arxiv_id': '2503.17057v1', 'title': 'Semi-supervised Cervical Segmentation on Ultrasound by A Dual Framework for Neural Networks', 'section': 'body', 'authors': 'Fangyijie Wang, Kathleen M. Curran, GuÃ©nolÃ© Silvestre'}, page_content='ing dataset. XL, XUL âˆˆ R3Ã—nhÃ—nw represents a 2D ultra-\\nsound image of size nh Ã— nw with 3 channels. The dual\\nneural networks are denoted by f 1\\nÎ¸ (Â·) and f 2\\nÏ•(Â·), respectively.\\nYpseudo, Ygt âˆˆ R3Ã—nhÃ—nw represents a three-class labeled seg-\\nmentation masks with pixel values ranging from 0 to 2. The\\nsegmentation masks predicted by the segmentation networks\\nf 1\\nÎ¸ (Â·) and f 2\\nÏ•(Â·) are f 1\\nÎ¸ (X) and f 2\\nÏ•(X), respectively, with Î¸\\nand Ï• as their parameters. In our proposed method, the pre-\\ndiction of a network is considered as a pseudo label to ex-\\npand the unlabeled dataset to (XU, Ypseudo) to train the other\\nnetwork. We utilize each network to extract the deep repre-\\nsentation features of XL and XUL for contrastive learning.\\nTo select and save the best network from f 1\\nÎ¸ (Â·) and f 2\\nÏ•(Â·),\\nwe evaluate their segmentation performance by measuring the\\ndifference between (f 1\\nÎ¸ (X), Ygt) and (f 2\\nÏ•(X), Ygt) in our val-\\nidation set.\\nOur training objective is minimizing the total loss Ltotal'),\n",
       " Document(metadata={'arxiv_id': '2503.17057v1', 'title': 'Semi-supervised Cervical Segmentation on Ultrasound by A Dual Framework for Neural Networks', 'section': 'body', 'authors': 'Fangyijie Wang, Kathleen M. Curran, GuÃ©nolÃ© Silvestre'}, page_content='we evaluate their segmentation performance by measuring the\\ndifference between (f 1\\nÎ¸ (X), Ygt) and (f 2\\nÏ•(X), Ygt) in our val-\\nidation set.\\nOur training objective is minimizing the total loss Ltotal\\nby updating the network parameters Î¸ and Ï•. The total loss\\nLtotal consist of the supervision loss Lsup, semi-supervised\\nloss Lsemi, and self-supervised contrastive loss Lcontra. Math-\\nematically, it can be expressed as:\\nLtotal = (L1\\nsup + L2\\nsup ) + Î»(L1\\nsemi + L2\\nsemi ) + Lcontra (1)\\nwhere Î» represents the weighting factor for a ramp-up func-\\ntion used exclusively for the unlabeled training set [5]. This\\narXiv:2503.17057v1  [eess.IV]  21 Mar 2025Fig. 1. Framework for deep representations contrastive cross-supervised neural networks for semi-supervised ultrasound image\\nsegmentation.\\nfunction facilitates the gradual transition of f 1\\nÎ¸ (Â·) and f 2\\nÏ•(Â·)\\nfrom being initialized with the labeled training set to priori-\\ntizing learning from the unlabeled training set. In Fig.1, all'),\n",
       " Document(metadata={'arxiv_id': '2503.17057v1', 'title': 'Semi-supervised Cervical Segmentation on Ultrasound by A Dual Framework for Neural Networks', 'section': 'body', 'authors': 'Fangyijie Wang, Kathleen M. Curran, GuÃ©nolÃ© Silvestre'}, page_content='function facilitates the gradual transition of f 1\\nÎ¸ (Â·) and f 2\\nÏ•(Â·)\\nfrom being initialized with the labeled training set to priori-\\ntizing learning from the unlabeled training set. In Fig.1, all\\nloss functions are highlighted by a red dashed line. L1\\nsup and\\nL2\\nsup are the supervision losses for f 1\\nÎ¸ (Â·) and f 2\\nÏ•(Â·) based on\\nthe labeled training set. Lsup is designed with a combination\\nof the Dice Similarity Coefficient (DSC) and cross-entropy\\n(CE) losses, as follows:\\nL1\\nsup = CE\\n\\x00\\nf 1\\nÎ¸ (XL) , Ygt\\n\\x01\\n+ DSC\\n\\x00\\nf 1\\nÎ¸ (XL) , Ygt\\n\\x01\\nL2\\nsup = CE\\n\\x00\\nf 2\\nÏ• (XL) , Ygt\\n\\x01\\n+ DSC\\n\\x00\\nf 2\\nÏ• (XL) , Ygt\\n\\x01 (2)\\n2.1. CNN and Transformer Segmentation Network\\nThe UNet [6] presents a groundbreaking modification of the\\ntraditional encoder-decoder segmentation network, incorpo-\\nrating diverse network blocks tailored for medical image anal-\\nysis. We adopt UNet as the convolutional neural network\\n(CNN) f 1\\nÎ¸ (Â·) within our framework. In recent years, several'),\n",
       " Document(metadata={'arxiv_id': '2503.17057v1', 'title': 'Semi-supervised Cervical Segmentation on Ultrasound by A Dual Framework for Neural Networks', 'section': 'body', 'authors': 'Fangyijie Wang, Kathleen M. Curran, GuÃ©nolÃ© Silvestre'}, page_content='rating diverse network blocks tailored for medical image anal-\\nysis. We adopt UNet as the convolutional neural network\\n(CNN) f 1\\nÎ¸ (Â·) within our framework. In recent years, several\\nvariants of UNet have demonstrated superior segmentation\\nperformance in medical imaging analysis [7]. We leverage\\na Swin-UNet architecture [8] as f 2\\nÏ•(Â·) to implement our SSL\\nstrategy. The detailed architecture of Swin-Unet is elucidated\\nin the study by Cao et al. [8]. We initialize both networks\\nwith random weights.\\n2.2. Cross-Supervised Learning\\nInspired by consistency regularization and multi-view learn-\\ning principles, including cross-pseudo-supervision [9, 10],\\nthis study applies these concepts to construct dual architec-\\ntures that facilitate mutual learning. CNNs excel in learn-\\ning spatial hierarchies of features, while Transformer-based\\nnetworks excel in capturing broad, non-local interactions.\\nHence, we propose a simple yet powerful cross-supervised'),\n",
       " Document(metadata={'arxiv_id': '2503.17057v1', 'title': 'Semi-supervised Cervical Segmentation on Ultrasound by A Dual Framework for Neural Networks', 'section': 'body', 'authors': 'Fangyijie Wang, Kathleen M. Curran, GuÃ©nolÃ© Silvestre'}, page_content='ing spatial hierarchies of features, while Transformer-based\\nnetworks excel in capturing broad, non-local interactions.\\nHence, we propose a simple yet powerful cross-supervised\\nlearning approach that combines CNNs and Transformers to\\nassist each other mutually. The semi-supervised loss function,denoted as Lsemi, can be expressed as:\\nL1\\nsemi = CE\\n\\x00\\nf 1\\nÎ¸ (XUL) , Y 2\\npseudo\\n\\x01\\n+ DSC\\n\\x00\\nf 1\\nÎ¸ (XUL) , Y 2\\npseudo\\n\\x01\\nL2\\nsemi = CE\\n\\x00\\nf 2\\nÏ• (XUL) , Y 1\\npseudo\\n\\x01\\n+ DSC\\n\\x00\\nf 2\\nÏ• (XUL) , Y 1\\npseudo\\n\\x01\\n(3)\\nWe also investigated the approach of cross-teaching be-\\ntween two networks, including identical two CNNs or Trans-\\nformer networks. Nevertheless, our proposed method, which\\nintegrates CNN and Transformer-based models, surpasses the\\neffectiveness of this approach.\\n2.3. Contrastive Learning\\nContrastive learning is widely acknowledged as a power-\\nful paradigm for extracting robust and discriminative fea-\\ntures, marking a significant advancement in the field of'),\n",
       " Document(metadata={'arxiv_id': '2503.17057v1', 'title': 'Semi-supervised Cervical Segmentation on Ultrasound by A Dual Framework for Neural Networks', 'section': 'body', 'authors': 'Fangyijie Wang, Kathleen M. Curran, GuÃ©nolÃ© Silvestre'}, page_content='2.3. Contrastive Learning\\nContrastive learning is widely acknowledged as a power-\\nful paradigm for extracting robust and discriminative fea-\\ntures, marking a significant advancement in the field of\\nself-supervised learning [11]. The fundamental concept of\\ncontrastive learning is that both positive and negative samples\\nare discriminative. The utilization of contrastive learning in\\nthe field of medical image analysis enhances the capability\\nof feature extraction, ultimately leading to improved model\\nperformance [12, 13, 10].\\nIn the paper [14], contrastive learning can be considered\\nas a task to search a dictionary. Given an encoded query q, a\\nset of encoded keys {k1, k2, . . .} is retrieved from the mem-\\nory bank. Among these keys, a specific positive keyk+ aligns\\nwith the query q, while the remaining negative keys kâˆ’ rep-\\nresent different images. A contrastive loss function InfoNCE\\n[11] is utilized to bring q closer to positive key k+ and simul-'),\n",
       " Document(metadata={'arxiv_id': '2503.17057v1', 'title': 'Semi-supervised Cervical Segmentation on Ultrasound by A Dual Framework for Neural Networks', 'section': 'body', 'authors': 'Fangyijie Wang, Kathleen M. Curran, GuÃ©nolÃ© Silvestre'}, page_content='with the query q, while the remaining negative keys kâˆ’ rep-\\nresent different images. A contrastive loss function InfoNCE\\n[11] is utilized to bring q closer to positive key k+ and simul-\\ntaneously distance it from the negative keys kâˆ’:\\nLNCE\\nq = âˆ’ log exp (q Â· k+/Ï„ )\\nexp (q Â· k+/Ï„ ) +P\\nkâˆ’ exp (q Â· kâˆ’/Ï„ ) (4)\\nwhere Ï„ denotes a temperature hyper-parameter. In our SSL\\nframework, we refer to the original InfoNCE loss, which is\\nformalized as follows:\\nLcontra =\\nLX\\ni\\nULX\\nj\\nInfoNCE(Z i\\nL, Zj\\nUL) (5)\\nwhere InfoNCE represents the original InfoNCE loss func-\\ntion, Z i\\nL and Z i\\nL represent deep representations obtained by\\nmodels f 1\\nÎ¸ (Â·) and f 2\\nÏ•(Â·) for XL and XU L, respectively. Incor-\\nporating the InfoNCE loss aids in generating complex pixel\\nrepresentations by leveraging ample unlabeled data, thereby\\nenhancing the resilience and label efficiency of segmentation\\nmodels.\\n3. EXPERIMENTS AND RESULTS\\n3.1. Datasets\\nThe FUGC dataset: We utilize one transvaginal dataset'),\n",
       " Document(metadata={'arxiv_id': '2503.17057v1', 'title': 'Semi-supervised Cervical Segmentation on Ultrasound by A Dual Framework for Neural Networks', 'section': 'body', 'authors': 'Fangyijie Wang, Kathleen M. Curran, GuÃ©nolÃ© Silvestre'}, page_content='enhancing the resilience and label efficiency of segmentation\\nmodels.\\n3. EXPERIMENTS AND RESULTS\\n3.1. Datasets\\nThe FUGC dataset: We utilize one transvaginal dataset\\nin this study. It contains images that include anatomical\\nstructures of the cervix, namely the anterior and posterior\\nlips. They are captured using a curved transducer with a\\nfrequency range of 2 to 10 MHz, specifically a vaginal probe\\nutilized for cervical ultrasound screening in second-trimester\\npatients. Operators are directed to refrain from applying post-\\nprocessing techniques or introducing artifacts like smoothing,\\nnoise, pointers, or calipers whenever feasible. Other image\\nsettings, such as gain, frequency, and gain compensation, are\\nadjusted according to individual discretion. The training set\\ncomprises 500 images, the external validation set comprises\\n90 images, and the testing set comprises 300 images. Among\\nthese 500 images, only 50 images are annotated by experi-'),\n",
       " Document(metadata={'arxiv_id': '2503.17057v1', 'title': 'Semi-supervised Cervical Segmentation on Ultrasound by A Dual Framework for Neural Networks', 'section': 'body', 'authors': 'Fangyijie Wang, Kathleen M. Curran, GuÃ©nolÃ© Silvestre'}, page_content='comprises 500 images, the external validation set comprises\\n90 images, and the testing set comprises 300 images. Among\\nthese 500 images, only 50 images are annotated by experi-\\nenced operators. The resolutions for all images are336Ã—544.\\nDuring model training, 500 images are resized to the size of\\n224 Ã— 224. We use the 50 labeled images to evaluate the\\nsegmentation performance of the dual neural networks and\\nsave the best model among them.\\nData Augmentation: In this study, data augmentations\\nare implemented on the labeled dataset. These augmentation\\ntechniques include rotation within range (âˆ’20â—¦, 20â—¦), ran-\\ndom brightness contrast, random blur with probabilityP(Â·) =\\n0.3, and gaussian noise with probability P(Â·) = 0 .3.\\n3.2. Implementation Details\\nAll experiments were implemented in Pytorch and trained on\\na single RTX 3090 24G GPU. We use a batch size of 8 for\\ntraining, including two labeled samples and six unlabeled data\\nsamples. We adopted a stochastic gradient descent optimizer'),\n",
       " Document(metadata={'arxiv_id': '2503.17057v1', 'title': 'Semi-supervised Cervical Segmentation on Ultrasound by A Dual Framework for Neural Networks', 'section': 'body', 'authors': 'Fangyijie Wang, Kathleen M. Curran, GuÃ©nolÃ© Silvestre'}, page_content='a single RTX 3090 24G GPU. We use a batch size of 8 for\\ntraining, including two labeled samples and six unlabeled data\\nsamples. We adopted a stochastic gradient descent optimizer\\nwith a learning rate 0.001, momentum of 0.9, and weight de-\\ncay of 0.0001. Besides, we set a maximum of 30,000 iter-\\nations for training. All neural networks are initialized with\\nrandom parameters. The experiments used the same hyperpa-\\nrameter for a fair comparison. We also investigated various\\nhyperparameter settings, including a labeled batch size of 1,\\nan unlabeled batch size of 9, and an initial learning rate of\\n0.01. However, it was our adopted hyperparameter setting\\nthat ultimately delivered optimal performance, characterized\\nby fast convergence.\\nFig. 2 . Plots of the hyperparameter settings. From left to\\nright: (a) Learning rate. (b) Consistency weight Î» in Equation\\n1. (c) Training loss of model f 2\\nÏ•(Â·).\\n3.3. Evaluation Metrics\\nTo measure the performance of our framework, we employ'),\n",
       " Document(metadata={'arxiv_id': '2503.17057v1', 'title': 'Semi-supervised Cervical Segmentation on Ultrasound by A Dual Framework for Neural Networks', 'section': 'body', 'authors': 'Fangyijie Wang, Kathleen M. Curran, GuÃ©nolÃ© Silvestre'}, page_content='right: (a) Learning rate. (b) Consistency weight Î» in Equation\\n1. (c) Training loss of model f 2\\nÏ•(Â·).\\n3.3. Evaluation Metrics\\nTo measure the performance of our framework, we employ\\nthe area metric and the boundary metric: DSC and Haus-Fig. 3 . Example ultrasound images from our validation set, ground truth (GT), and corresponding segmentation results of\\nSwin-UNet, U-Net, Attention UNet, Efficient UNet, ResNet UNet, and MiT UNet.\\ndorff Distance (HD). The DSC measures the percentage of\\narea overlap between the predicted and ground truth segmen-\\ntation maps. HD measures the error between the predicted\\nand ground truth segmentation boundaries in pixels. Addi-\\ntionally, we incorporate the execution time to measure the in-\\nference speed.\\nTable 1. Segmentation performance of different neural net-\\nworks on the external validation set.\\nf 1\\nÎ¸ (Â·) f 2\\nÏ•(Â·) DSC â†‘ HD â†“ Time â†“\\nU-Net U-Net 0.76 63.99 5.65\\nU-Net Attention U-Net 0.78 78.68 8.89\\nU-Net Efficient-Unet 0.84 60.52 74.62'),\n",
       " Document(metadata={'arxiv_id': '2503.17057v1', 'title': 'Semi-supervised Cervical Segmentation on Ultrasound by A Dual Framework for Neural Networks', 'section': 'body', 'authors': 'Fangyijie Wang, Kathleen M. Curran, GuÃ©nolÃ© Silvestre'}, page_content='works on the external validation set.\\nf 1\\nÎ¸ (Â·) f 2\\nÏ•(Â·) DSC â†‘ HD â†“ Time â†“\\nU-Net U-Net 0.76 63.99 5.65\\nU-Net Attention U-Net 0.78 78.68 8.89\\nU-Net Efficient-Unet 0.84 60.52 74.62\\nU-Net ResNet-Unet 0.82 69.94 7.79\\nU-Net MiT-Unet 0.82 62.88 7.04\\nU-Net Swin-Unet 0.86 46.44 16.95\\n3.4. Validation\\nFig. 2 illustrates our optimized hyperparameter configuration.\\nThe learning rate decays to facilitate the convergence of mod-\\nels f 1\\nÎ¸ (Â·) and f 2\\nÏ•(Â·) at the completion of training. The con-\\nsistency weight Î» guides the models towards learning effec-\\ntively from the unlabeled training set. The training loss is\\nminimized without additional decays over the course of our\\nmaximum epochs.\\nTable 1 illustrates the quantitative results of our SSL\\nframework with several types of UNet, including U-Net [6],\\nAttention U-Net [15], Efficient-Unet [16], ResNet-Unet [17],\\nMiT-Unet [18] and Swin-Unet [8] when 50 cases in the\\ntraining set are labeled data for training. The Swin-UNet'),\n",
       " Document(metadata={'arxiv_id': '2503.17057v1', 'title': 'Semi-supervised Cervical Segmentation on Ultrasound by A Dual Framework for Neural Networks', 'section': 'body', 'authors': 'Fangyijie Wang, Kathleen M. Curran, GuÃ©nolÃ© Silvestre'}, page_content='Attention U-Net [15], Efficient-Unet [16], ResNet-Unet [17],\\nMiT-Unet [18] and Swin-Unet [8] when 50 cases in the\\ntraining set are labeled data for training. The Swin-UNet\\noutperforms other UNet variants, achieving a DSC of 0.86\\nand an HD of 46.44. Although the Swin-UNet has a longer\\ninference time than U-Net and Attention U-Net, the value of\\n16.95 remains within an acceptable range. We computed the\\naverage evaluation metrics for models on the validation set\\nto determine the optimal model for future testing. Then we\\nchoose the best one for the challenge competition.\\nIn Fig. 3, we visualize two randomly selected example ul-\\ntrasound images from our validation set (50 images). The\\nSwin-Unet model demonstrates highly accurate predictions\\nthat closely align with the ground truth.\\nTable 2 . Performance comparison between the baseline\\nmethod and our proposed framework on the testing dataset.\\nBaseline: A model provided by the challenge organizer.\\nRank Method DSC â†‘ HD â†“ Time â†“'),\n",
       " Document(metadata={'arxiv_id': '2503.17057v1', 'title': 'Semi-supervised Cervical Segmentation on Ultrasound by A Dual Framework for Neural Networks', 'section': 'body', 'authors': 'Fangyijie Wang, Kathleen M. Curran, GuÃ©nolÃ© Silvestre'}, page_content='Table 2 . Performance comparison between the baseline\\nmethod and our proposed framework on the testing dataset.\\nBaseline: A model provided by the challenge organizer.\\nRank Method DSC â†‘ HD â†“ Time â†“\\n1 N/A 0.90 44.94 365.91\\n2 N/A 0.90 40.20 340.24\\n3 N/A 0.87 45.70 518.38\\n11 Ours 0.78 66.74 43.51\\n14 Baseline 0.72 115.41 402.30\\n3.5. Comparison with Other Methods\\nIn Table 2, we compare our method to the top 3 methods and\\nthe baseline method on the testing dataset provided by the\\nchallenge organizer. Compared to the baseline method, our\\nSSL framework increases the DSC by 0.06 while reducing\\nthe inference time by 82%. Moreover, the HD significantly\\ndecreased from 115.41 to 66.74.\\n4. CONCLUSION\\nWe conducted a study to explore the implementation of UNet-\\nbased architectures within a semi-supervised approach for\\ncervical segmenting on ultrasound images. We introduced an\\ninnovative learning strategy that integrates cross-supervision\\nand contrastive learning techniques to optimize the perfor-'),\n",
       " Document(metadata={'arxiv_id': '2503.17057v1', 'title': 'Semi-supervised Cervical Segmentation on Ultrasound by A Dual Framework for Neural Networks', 'section': 'body', 'authors': 'Fangyijie Wang, Kathleen M. Curran, GuÃ©nolÃ© Silvestre'}, page_content='cervical segmenting on ultrasound images. We introduced an\\ninnovative learning strategy that integrates cross-supervision\\nand contrastive learning techniques to optimize the perfor-\\nmance of SSL. Through our experimental analyses, we high-\\nlighted the efficacy of our SSL framework. In future studies,\\nwe aim to enhance our research by refining our methodologies\\nin restricted supervised learning scenarios while persisting in\\nutilizing the distinctive features offered by UNet variants.5. ACKNOWLEDGMENT\\nThis publication has emanated from research conducted with\\nthe financial support of Taighde Â´Eireann â€“ Research Ireland\\nunder Grant number 18/CRT/6183. For the purpose of Open\\nAccess, the author has applied a CC BY public copyright\\nlicence to any Author Accepted Manuscript version arising\\nfrom this submission\\n6. REFERENCES\\n[1] Juan Luis Alc Â´azar, Sara Arribas, Jos Â´e Angel M Â´Ä±nguez,\\nand MatÂ´Ä±as Jurado, â€œThe role of ultrasound in the as-'),\n",
       " Document(metadata={'arxiv_id': '2503.17057v1', 'title': 'Semi-supervised Cervical Segmentation on Ultrasound by A Dual Framework for Neural Networks', 'section': 'body', 'authors': 'Fangyijie Wang, Kathleen M. Curran, GuÃ©nolÃ© Silvestre'}, page_content='from this submission\\n6. REFERENCES\\n[1] Juan Luis Alc Â´azar, Sara Arribas, Jos Â´e Angel M Â´Ä±nguez,\\nand MatÂ´Ä±as Jurado, â€œThe role of ultrasound in the as-\\nsessment of uterine cervical cancer,â€ The Journal of\\nObstetrics and Gynecology of India , vol. 64, no. 5, pp.\\n311â€“316, Oct. 2014.\\n[2] Eva Bone Ë‡s, Marco Gergolet, Ciril Bohak, Ë‡Ziga Lesar,\\nand Matija Marolt, â€œAutomatic segmentation and align-\\nment of uterine shapes from 3D ultrasound data,â€ Com-\\nputers in Biology and Medicine , vol. 178, pp. 108794,\\nAug. 2024.\\n[3] Jesper E van Engelen and Holger H Hoos, â€œA survey on\\nsemi-supervised learning,â€ Machine Learning, vol. 109,\\nno. 2, pp. 373â€“440, Feb. 2020.\\n[4] Xiangli Yang, Zixing Song, Irwin King, and Zenglin\\nXu, â€œA survey on deep semi-supervised learning,â€IEEE\\nTransactions on Knowledge and Data Engineering , vol.\\n35, no. 9, pp. 8934â€“8954, 2023.\\n[5] Samuli Laine and Timo Aila, â€œTemporal ensembling for\\nsemi-supervised learning,â€ in International Conference'),\n",
       " Document(metadata={'arxiv_id': '2503.17057v1', 'title': 'Semi-supervised Cervical Segmentation on Ultrasound by A Dual Framework for Neural Networks', 'section': 'body', 'authors': 'Fangyijie Wang, Kathleen M. Curran, GuÃ©nolÃ© Silvestre'}, page_content='Transactions on Knowledge and Data Engineering , vol.\\n35, no. 9, pp. 8934â€“8954, 2023.\\n[5] Samuli Laine and Timo Aila, â€œTemporal ensembling for\\nsemi-supervised learning,â€ in International Conference\\non Learning Representations (ICLR), 2017.\\n[6] Olaf Ronneberger, Philipp Fischer, and Thomas Brox,\\nâ€œU-Net: Convolutional networks for biomedical image\\nsegmentation,â€ in LNCS: Medical Image Computing\\nand Computer-Assisted Intervention (MICCAI) . 2015,\\nvol. 9351, pp. 234â€“241, Springer.\\n[7] M. Krithika alias AnbuDevi and K. Suganthi, â€œReview\\nof semantic segmentation of medical images using mod-\\nified architectures of unet,â€Diagnostics, vol. 12, no. 12,\\n2022.\\n[8] Hu Cao, Yueyue Wang, Joy Chen, Dongsheng Jiang, Xi-\\naopeng Zhang, Qi Tian, and Manning Wang, â€œSwin-\\nUnet: Unet-Like pure transformer for medical image\\nsegmentation,â€ in Computer Vision â€“ ECCV 2022 Work-\\nshops. 2023, pp. 205â€“218, Springer Nature Switzerland.\\n[9] Xiaokang Chen, Yuhui Yuan, Gang Zeng, and Jing-'),\n",
       " Document(metadata={'arxiv_id': '2503.17057v1', 'title': 'Semi-supervised Cervical Segmentation on Ultrasound by A Dual Framework for Neural Networks', 'section': 'body', 'authors': 'Fangyijie Wang, Kathleen M. Curran, GuÃ©nolÃ© Silvestre'}, page_content='segmentation,â€ in Computer Vision â€“ ECCV 2022 Work-\\nshops. 2023, pp. 205â€“218, Springer Nature Switzerland.\\n[9] Xiaokang Chen, Yuhui Yuan, Gang Zeng, and Jing-\\ndong Wang, â€œSemi-supervised semantic segmentation\\nwith cross pseudo supervision,â€ in Proceedings of the\\nIEEE/CVF conference on computer vision and pattern\\nrecognition, 2021, pp. 2613â€“2622.\\n[10] Chao Ma and Ziyang Wang, â€œSemi-mamba-unet: Pixel-\\nlevel contrastive and pixel-level cross-supervised visual\\nmamba-based unet for semi-supervised medical image\\nsegmentation,â€ 2024.\\n[11] Aaron van den Oord, Yazhe Li, and Oriol Vinyals, â€œRep-\\nresentation learning with contrastive predictive coding,â€\\n2019.\\n[12] Chenyu You, Yuan Zhou, Ruihan Zhao, Lawrence\\nStaib, and James S. Duncan, â€œSimcvd: Simple con-\\ntrastive voxel-wise representation distillation for semi-\\nsupervised medical image segmentation,â€ IEEE Trans-\\nactions on Medical Imaging , vol. 41, no. 9, pp. 2228â€“\\n2237, 2022.\\n[13] Ziyang Wang and Congying Ma, â€œDual-contrastive'),\n",
       " Document(metadata={'arxiv_id': '2503.17057v1', 'title': 'Semi-supervised Cervical Segmentation on Ultrasound by A Dual Framework for Neural Networks', 'section': 'body', 'authors': 'Fangyijie Wang, Kathleen M. Curran, GuÃ©nolÃ© Silvestre'}, page_content='supervised medical image segmentation,â€ IEEE Trans-\\nactions on Medical Imaging , vol. 41, no. 9, pp. 2228â€“\\n2237, 2022.\\n[13] Ziyang Wang and Congying Ma, â€œDual-contrastive\\ndual-consistency dual-transformer: A semi-supervised\\napproach to medical image segmentation,â€ in Proceed-\\nings of the IEEE/CVF international conference on com-\\nputer vision, 2023, pp. 870â€“879.\\n[14] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and\\nRoss Girshick, â€œMomentum contrast for unsupervised\\nvisual representation learning,â€ in Proceedings of the\\nIEEE/CVF conference on computer vision and pattern\\nrecognition, 2020, pp. 9729â€“9738.\\n[15] Ozan Oktay, Jo Schlemper, Loic Le Folgoc, Matthew\\nLee, Mattias Heinrich, Kazunari Misawa, Kensaku\\nMori, Steven McDonagh, Nils Y Hammerla, Bernhard\\nKainz, Ben Glocker, and Daniel Rueckert, â€œAttention u-\\nnet: Learning where to look for the pancreas,â€ in Medi-\\ncal Imaging with Deep Learning , 2018.\\n[16] Mingxing Tan and Quoc Le, â€œEfficientnetv2: Smaller'),\n",
       " Document(metadata={'arxiv_id': '2503.17057v1', 'title': 'Semi-supervised Cervical Segmentation on Ultrasound by A Dual Framework for Neural Networks', 'section': 'body', 'authors': 'Fangyijie Wang, Kathleen M. Curran, GuÃ©nolÃ© Silvestre'}, page_content='net: Learning where to look for the pancreas,â€ in Medi-\\ncal Imaging with Deep Learning , 2018.\\n[16] Mingxing Tan and Quoc Le, â€œEfficientnetv2: Smaller\\nmodels and faster training,â€ in Proceedings of the 38th\\nInternational Conference on Machine Learning, Marina\\nMeila and Tong Zhang, Eds. 18â€“24 Jul 2021, vol. 139 of\\nProceedings of Machine Learning Research, pp. 10096â€“\\n10106, PMLR.\\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\\nSun, â€œDeep residual learning for image recognition,â€ in\\n2016 IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR), 2016, pp. 770â€“778.\\n[18] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandku-\\nmar, Jose M Alvarez, and Ping Luo, â€œSegformer: Sim-\\nple and efficient design for semantic segmentation with\\ntransformers,â€ in Neural Information Processing Sys-\\ntems (NeurIPS), 2021.'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'title_abstract', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='Title: One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning\\n\\nAbstract: Objective: Ultrasound (US) examination has unique advantages in diagnosing carpal tunnel syndrome (CTS) while identifying the median nerve (MN) and diagnosing CTS depends heavily on the expertise of examiners. To alleviate this problem, we aimed to develop a one-stop automated CTS diagnosis system (OSA-CTSD) and evaluate its effectiveness as a computer-aided diagnostic tool. Methods: We combined real-time MN delineation, accurate biometric measurements, and explainable CTS diagnosis into a unified framework, called OSA-CTSD. We collected a total of 32,301 static images from US videos of 90 normal wrists and 40 CTS wrists for evaluation using a simplified scanning protocol. Results: The proposed model showed better segmentation and measurement performance than competing methods, reporting that HD95 score of 7.21px, ASSD score of 2.64px, Dice score of 85.78%, and IoU score of 76.00%, respectively. In the reader study, it demonstrated comparable performance with the average performance of the experienced in classifying the CTS, while outperformed that of the inexperienced radiologists in terms of classification metrics (e.g., accuracy score of 3.59% higher and F1 score of 5.85% higher). Conclusion: The OSA-CTSD demonstrated promising diagnostic performance with the advantages of real-time, automation, and clinical interpretability. The application of such a tool can not only reduce reliance on the expertise of examiners, but also can help to promote the future standardization of the CTS diagnosis process, benefiting both patients and radiologists.'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='1'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='One-stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning Jiayu Peng1,2#, Jiajun Zeng3,4,5#, Manlin Lai6#, Ruobing Huang3,4,5, Dong Ni3,4,5, Zhenzhou Li1,2* 1Department of Ultrasound, The Second Peopleâ€™s Hospital of Shenzhen, The First Affiliated Hospital of Shenzhen University, Shenzhen, China 2Shenzhen University Health Science Center, Shenzhen, 518000, P.R. China 3National-Regional Key Technology Engineering Laboratory for Medical Ultrasound, School of Biomedical Engineering, Health Science Center, Shenzhen University, Shenzhen, China 4Medical Ultrasound Image Computing (MUSIC) Lab, Shenzhen University, China 5Marshall Laboratory of Biomedical Engineering, Shenzhen University, China 6Department of Medical Imaging (DMI) - Ultrasound Division, The University of Hong Kong-Shenzhen Hospital, Shenzhen, China * Correspondence: Zhenzhou Li, Department of Ultrasound, The Second Peopleâ€™s Hospital of Shenzhen, The First Affiliated Hospital of'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='The University of Hong Kong-Shenzhen Hospital, Shenzhen, China * Correspondence: Zhenzhou Li, Department of Ultrasound, The Second Peopleâ€™s Hospital of Shenzhen, The First Affiliated Hospital of Shenzhen University, No. 3002, Sungang West Road, Futian District, Shenzhen, China, 518061, E-mail: lizhenzhou2004@126.com # Jiayu Peng, Jiajun Zeng and Manlin Lai contributed equally to this manuscript.  Abstract Objective: Ultrasound (US) examination has unique advantages in diagnosing carpal tunnel syndrome (CTS) while identifying the median nerve (MN) and diagnosing CTS depends heavily on the expertise of examiners. To alleviate this problem, we aimed to develop a one-stop automated CTS diagnosis system (OSA-CTSD) and evaluate its effectiveness as a computer-aided diagnostic tool. Methods: We combined real-time MN delineation, accurate biometric'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='2'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='measurements, and explainable CTS diagnosis into a unified framework, called OSA-CTSD. We collected a total of 32,301 static images from US videos of 90 normal wrists and 40 CTS wrists for evaluation using a simplified scanning protocol. Results: The proposed model showed better segmentation and measurement performance than competing methods, reporting that HD95 score of 7.21px, ASSD score of 2.64px, Dice score of 85.78%, and IoU score of 76.00%, respectively. In the reader study, it demonstrated comparable performance with the average performance of the experienced in classifying the CTS, while outperformed that of the inexperienced radiologists in terms of classification metrics (e.g., accuracy score of 3.59% higher and F1 score of 5.85% higher). Conclusion: The OSA-CTSD demonstrated promising diagnostic performance with the advantages of real-time, automation, and clinical interpretability. The application of such a tool can not only reduce reliance on the expertise of examiners,'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='diagnostic performance with the advantages of real-time, automation, and clinical interpretability. The application of such a tool can not only reduce reliance on the expertise of examiners, but also can help to promote the future standardization of the CTS diagnosis process, benefiting both patients and radiologists. Keywords Carpal tunnel syndrome, Ultrasound image, Deep learning, Median nerve, Automated diagnosis, Computer-aided diagnosis.  Introduction Carpal tunnel syndrome (CTS) is the most frequently encountered type of peripheral compression neuropathy characterized by median nerve (MN) entrapment at the wrist(1). The carpal tunnel is bounded by the transverse carpal ligament on the volar side and eight carpal bones on the dorsal side. Nine flexor digital tendons and the MN pass through the carpal tunnel in the wrist. In CTS, nerve compression causes local circulatory disorders, damage to the blood-nerve barrier, increased pressure of the nerve endoneurial fluid, edema and'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='through the carpal tunnel in the wrist. In CTS, nerve compression causes local circulatory disorders, damage to the blood-nerve barrier, increased pressure of the nerve endoneurial fluid, edema and thickening of blood vessel walls, non-inflammatory synovial fibrosis and vascular proliferation, fibrosis, and thinning of the nerve myelin sheath, resulting in a series of clinical symptoms such as pain, sensory disorders, and'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='3'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='motor disorders. Paraesthesias are initially nocturnal and subsequently diurnal(2, 3). In advanced phases, weakness and thenar atrophy occur. The prevalence of CTS in the general population is about 3.4% in women and 0.6% in men(4). In addition to the case history and clinical examination, electrodiagnostic testing (EDT) is currently considered the gold standard for confirmation of a clinical diagnosis of CTS(5). Nevertheless, EDT is an expensive, time-consuming, and invasive test that is not readily accessible. In recent years, with the development and innovation of modern ultrasound (US) technology and high-frequency transducer, as well as continuously improved quality of US images, US technology is widely used in the diagnosis of CTS due to free invasion, simple operation, and no radiation(6). US examination has unique advantages in the diagnosis of CTS, providing multiple reference indicators to improve diagnostic accuracy and reduce treatment risks(6). Lin et al.(7) demonstrates'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='US examination has unique advantages in the diagnosis of CTS, providing multiple reference indicators to improve diagnostic accuracy and reduce treatment risks(6). Lin et al.(7) demonstrates that US, particularly measurement of the median nerve cross-sectional area (CSA) at the carpal tunnel inlet, is a useful imaging modality for diagnosing CTS. Additional studies have shown that when the cross-sectional area is between 8.5 and 15.0 mm2, there is a significant difference in sensitivity (62.0% to 97.9%) and specificity (63% to 100%) for the diagnosis of CTS(6). Besides median nerve swelling, sonoelastography and Doppler US reveal increased stiffness and vascularity of the nerve in CTS patients, providing additional information on disease activity and severity. Some scholars believe that the swelling ratio (SR) of the MN can be an essential indicator for the US diagnosis of CTS. The results of Sugimoto et al. showed that the critical value of the SR of the MN for the diagnosis of CTS'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='the swelling ratio (SR) of the MN can be an essential indicator for the US diagnosis of CTS. The results of Sugimoto et al. showed that the critical value of the SR of the MN for the diagnosis of CTS is 1.55(8, 9). Wilson et al. believed that an SR of â‰¥1.3 had the highest sensitivity (72.5%) for the diagnosis of CTS. Buchberger et al. proposed the concept of the MN flattening ratio (FR), and their research results showed that the MN FR at the level of the hook of the hamate was significantly increased, with a critical diagnostic value of 4.2(10). Although many studies have reported the application of US in the diagnosis of CTS, there has always been controversy over the selection of different diagnostic parameters, which'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='4'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='may be influenced by different diagnostic criteria, operator errors, measurement methods, machine resolution, and subjective differences among observers in delineating the MN. The following disputes have always existed: which index has the highest specificity and sensitivity? In which section is the diagnostic standard established?  Recent studies have shown that US has similar or even higher sensitivity and specificity than EDT in diagnosing CTS, as well as excellent intra- and inter-observer reliability(11). The reliability between evaluators is more likely to change, and a highly reliable US image processing method needs to be sought. Furthermore, currently, the tracking of MN in consecutive US images still depends on manual recognition and delineation, which requires massive human labor and makes it hard to implement the analysis clinically. Moreover, images acquired in dynamic US as the nerve is moving within the screen and are usually noisier than static images because of the'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='and makes it hard to implement the analysis clinically. Moreover, images acquired in dynamic US as the nerve is moving within the screen and are usually noisier than static images because of the motion artifacts, which further increases the difficulty in manual tracking. To address these, an appealing solution is employing machine learning to segment the tracked nerve in the images automatically. In previous studies, deep learning has demonstrated satisfying performance in various applications(12-15). It has also been utilized to analyze the MN, while existing studies mostly focused on automatic tracking and segmentation of MN(16). Current methods predominantly adopt convolutional neural network (CNN) models(17), especially U-Net(18, 19), which counteracts the progressive loss of feature resolution with network depth through a symmetric U-shaped architecture combining low-level spatial information and high-level semantic features. Horng et al.(13) demonstrated the effectiveness of'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='resolution with network depth through a symmetric U-shaped architecture combining low-level spatial information and high-level semantic features. Horng et al.(13) demonstrated the effectiveness of DeepNerve, a CNN-based model, for MN segmentation. They combined a modified U-Net, convolutional long short-term memory network, and MaskTrack method. However, their method requires manual labeling of the region of interest (ROI), limiting its flexibility. Festen et al.(19) utilized U-Net for MN segmentation and measurement. Similarly, Yang et al.(20) employed a modified Deeplabv3+ to segment carpal tunnel and its contents. Huang et'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='5'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='al. and Shao et al.(21, 22)extended this work by incorporating attention mechanisms and modifying the encoder, respectively. However, Cosmo et al., Di Cosmo et al., and Smerilli et al. adopted a detection-based model, specifically Mask R-CNN, for MN segmentation(23-25). Recently, Yeh et al.(26) introduced Solov2-MN, a modified instance segmentation model, and reported modest improvements in segmentation accuracy. Notably, accurate segmentation in US images remains challenging. CNN-based methods often struggle to model relationships between distant elements due to the local inductive bias of convolutional operations. Although attention mechanisms and image pyramids have been attempted, they have not yielded significant improvements. In contrast, Transformers have emerged as an alternative architecture in computer vision, leveraging multi-head self-attention to effectively model long-range dependencies. The pioneering method, Vision Transformer (ViT)(27), exhibits high performance but'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='architecture in computer vision, leveraging multi-head self-attention to effectively model long-range dependencies. The pioneering method, Vision Transformer (ViT)(27), exhibits high performance but computational complexity. Despite their success, these methods could only provide the segmentation or bounding boxes of MN and failed to diagnosis the CTS directly. Recently, some attempts have been made to develop diagnostic tools for CTS. For example, Obuchowicz et al. utilized a feature-selection tool MaZda to find four key texture features to input into a Support Vector Machine (SVM) model(28). When conducted on 30 swollen MN and 30 normal people, the method achieved 79% accuracy on CTS diagnosis. Faeghi et al. developed a CAD system using radiomics features and SVM to diagnose CTS. The CAD system demonstrated higher performance than two radiologists, with an AUC of 0.926(29). For a more convenient diagnosis, Shinohara et al. used a deep learning algorithm to diagnose CTS directly from'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='system demonstrated higher performance than two radiologists, with an AUC of 0.926(29). For a more convenient diagnosis, Shinohara et al. used a deep learning algorithm to diagnose CTS directly from US images of the carpal tunnel inlet. They reported a high accuracy score, while this approach lacked clinical interpretability. However, an integrated diagnostic system that automatically calculates clinically quantifiable CTS diagnostic indicators to obtain more easily understandable classification results has not yet been developed. In response to these clinical requirements, we propose a one-stop automated CTS'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='6'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='diagnosis system (OSA-CTSD) as an effective computer-aided diagnosis (CAD) tool to assist radiologists in diagnosing CTS using US images. To the best of our knowledge, the OSA-CTSD is the first method that combines three processes into a unified framework, including real-time MN delineation, accurate biometric measurements, and explainable CTS diagnosis. To comprehensively assess the clinal value of the method, the OSA-CTSD was evaluated on a large-scale US imaging dataset containing a total of 32,301 static images from US videos. It demonstrated promising diagnostic performance based on clinical-interpretable parameters, achieving the accuracy score of 93.85% and F1 score of 89.47%. Also, it was fully automated with a simplified scanning protocol (i.e., a straight sweep on the wrist). The utilization of such a powerful tool can not only reduce reliance on the expertise of examiners, but also has the potential to promote the future standardization of the CTS diagnosis process,'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content=\"The utilization of such a powerful tool can not only reduce reliance on the expertise of examiners, but also has the potential to promote the future standardization of the CTS diagnosis process, benefiting both patients and radiologists. Methods Study Participants This study was approved by the ethics committee of The Second People's Hospital of Shenzhen, The First Affiliated Hospital of Shenzhen University. Informed consent was obtained from all participants involved. Between May 2022 and January 2023, a total of 138 wrists were invited to participate in the study: 43 wrists from 31 patients with clinically evident carpal tunnel syndrome and typical clinical history, symptoms, and EDT(30); and 95 wrists from 52 healthy volunteers who met the inclusion criteria for age (at least 18 years). Exclusion criteria included contraindications for participation, previous wrist injury or surgery, central nervous system disorders, endocrine, metabolic, neuromuscular, musculoskeletal disorders\"),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='Exclusion criteria included contraindications for participation, previous wrist injury or surgery, central nervous system disorders, endocrine, metabolic, neuromuscular, musculoskeletal disorders relevant to CTS development, bifid MN, or persistent median artery. Three wrists from two patients and five wrists from three healthy volunteers were excluded based on these criteria. The final sample consisted of two groups: a CTS group comprising 40 wrists in 29 patients (mean age =55.1 years; range=33-72 years), and a control group comprising'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='7  \\n90 wrists in 52 healthy volunteers (mean age=29.4 years; range=22-67 years). (Fig 1)'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='Figure 1. Selection of CTS (wrists from CTS patients) and Non-CTS (wrists from healthy volunteers). According to access criteria, 40 wrists from 29 CTS patients and 90 wrists in 52 healthy volunteers were included. In total, 130 videos were divided into 104, 13, and 13 for the training, validation, and internal test sets, respectively.  US Technique The study utilized a PHILIPS EPIQ 7C US platform with a non-linear transducer that had an acquisition frequency of 5-12 MHz to image the subjects. The subjects were seated facing the examiner, with their arms extended and wrists rested on a hard flat surface while their forearms were supinated. To obtain dysnamic video images of the MN axial B-mode, a straight sweep is performed from about 2.5cm proximal to the flexor support band as the starting point to 2.5cm distal as the endpoint, including the proximal end of the wrist canal, entrance, and exit. During the entire scanning process, only minimal deviation (<1cm) between the probe'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='starting point to 2.5cm distal as the endpoint, including the proximal end of the wrist canal, entrance, and exit. During the entire scanning process, only minimal deviation (<1cm) between the probe center and palmar midpoint is required by US radiologists (to ensure that MN is collected), without imposing special restrictions on imaging parameters (e.g., depth, etc.) for better reproducibility in the'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='8'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='future applications. This results in depths ranging from 3.0cm to 4.0cm, frame rates ranging from 45Hz to 51Hz. Each video has an average duration of 4.97 seconds, consisting of 248 frames. The study included patients exhibiting clinical symptoms of CTS, such as numbness and pain in fingers supplied by MN along with the decline in some active and passive motor functions as study subjects, while healthy individuals without any clinical symptoms served as the control group. A total of 130 dynamic videos were randomized for independent viewing and diagnosis by three experienced radiologists having more than ten years of experience in musculoskeletal US diagnostics along with three inexperienced radiologists who underwent a two-hour training session but had no prior exposure to musculoskeletal US diagnostics. All radiologists remained blinded to both clinical diagnosis results as well as diagnostic results provided by the other five radiologists throughout this process eliminating'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='US diagnostics. All radiologists remained blinded to both clinical diagnosis results as well as diagnostic results provided by the other five radiologists throughout this process eliminating potential subjective interference factors. Finally, two experienced musculoskeletal US diagnosticians used internal software to describe all dynamic video images obtained during this process. (Fig. 2)'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='Figure 2. (a) The probe was positioned just proximal to the carpal tunnel inlet, with their arms extended and wrists rested on a hard flat surface. (b) US images of CTS patients and healthy volunteers at the proximal, entrance, and exit locations of the carpal tunnel.  Dataset In total 81 participants were enrolled in this study (52 normal participants and 29 \\n \\n9'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='patients). 130 videos were obtained following the image collection protocol. A total of 32,301 2D images were extracted from the raw videos, without any down-sampling or interpolation. To the best of our knowledge, it is currently the largest MN US dataset. The whole dataset is randomly split at the patient level into 8:1:1, for training (25,326 images), validation (3,527 images), and testing (3,448 images), respectively. The only image pre-processing we carried out is anonymization to remove privacy information and ensure reliable performance in handling unseen test cases in real-world applications. All these US images were delineated by experienced radiologists using Pair software (http://www.aipair.com.cn/). The ground truth (GT) MN measurements of each patient were then calculated based on these delineations. The GT diagnostic labels were confirmed by an experienced radiologist (>8-year experience) based on the combination of EDT results and clinical symptoms. Model design To'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='based on these delineations. The GT diagnostic labels were confirmed by an experienced radiologist (>8-year experience) based on the combination of EDT results and clinical symptoms. Model design To facilitate a time-efficient, labor-saving diagnosis process, we propose a one-stop automated CTS diagnosis system (OSA-CTSD) that integrates real-time MN delineation, accurate biometric measurements, and explainable CTS diagnosis into a unified framework. It can process raw US images and does not require any human intervention. Figure 3 displays the overall schematic.'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='Figure 3. The proposed method (OSA-CTSD) used in the present study mainly consists of three processes: real-time MN delineation, accurate biometric measurements, and explainable CTS diagnosis. \\nReal-time Measurement\\n(1) Perimeter(2) Cross-sectional area (CSA)\\n(3) Anteroposterior diameter\\nd Classifier\\nDiagnostic Parameters\\nTree #1\\nâ€¦\\nV otingTree #2Tree #n\\nDiagnosis Result\\nSegformer\\nDiagnostic ParametersIntelligent Diagnosis\\n(4) Flattening ratio\\nInput window\\n(1) Perimeterratio(2) Swellingratio\\n(4) Anteroposterior diameter ratio\\n(3)  Max flattening ratio\\n(5) Max CSA\\nRandom Forest\\nCTS \\nNormal\\n(a) (b) (c) \\n10'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='Segmentation model Segmentation is a vital technique that involves locating and delineating MN for analysis or visualization. Accurate and consistent MN delineation is the fundamental step in the diagnosis of CTS, as it provides morphological characteristics of MN and helps the interpretation of the severity of the condition. Compared to the CNN-based models and ViT, SegFormer(31) introduced a more simple and more efficient Transformer-based architecture. As shown in Figure 3, it consists of two primary components: a positional-encoding-free, hierarchically structured Transformer encoder and a lightweight All-MLP decoder. An input image of size HÃ—WÃ—3 is divided into patches of size 4Ã—4, which differs from the coarse-grained patch approach used in ViT. The patches are then fed to the hierarchically structured Transformer encoder, which produces multi-level features with different resolutions. The resulting features are directed to the All-MLP decoder, which predicts the segmentation'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='structured Transformer encoder, which produces multi-level features with different resolutions. The resulting features are directed to the All-MLP decoder, which predicts the segmentation mask. The SegFormer model has six variants ranging from SegFormer-B0 to SegFormer-B5, each with varying hyperparameters of the encoder (e.g., Ki: patch size, Si: stride, Pi: padding size, Ci: channel number, Li: number of encoder layers of Stage i, etc.), while maintaining the same model design. For example, the B0 is the most compact one, while B5 has the largest modeling capacity. In this study, we opted for the SegFormer-B2 variant to achieve a balanced trade-off between model performance and computational cost. For more information on the specific hyperparameters used, please refer to the original paper(31). Automated measurement In the automated measurement of MN masks, segmentation helps to precisely identify and measure the nerve structure from complex medical images. To translate the'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='paper(31). Automated measurement In the automated measurement of MN masks, segmentation helps to precisely identify and measure the nerve structure from complex medical images. To translate the segmentation results into diagnosis-related descriptors, biometric measurements are commonly used to monitor the changes in size, shape, and location of the MN. It is helpful to clarify the diagnosis and to provide guidance for the choice of subsequent treatment. Existing approaches either require manual selection of ROI or are restricted by unstable scanning location or inconsecutive scanning window,'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='11'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='while the proposed OSA-CTSD system is intervention-free and utilizes a unified scanning protocol. In specific, we first measure the morphological parameters of the MN in each 2D image of the same patient. Fig. 3a depicts the automated measurement process, where morphological parameters acquired from MN in each image are perimeter (Fig. 3a (1)), cross-sectional area (CSA) (Fig. 3a (2)), anteroposterior diameter (AD) (Fig. 3a (3)), flattening ratio (FR) (Fig. 3a (4)), where FR is defined as the ratio of horizontal to vertical diameter of the nerve.  As the level of swelling and area variation are strong indicators of CTS, we calculate a set of diagnostic measurements to better summarizes the overall MN status of a patient based on all obtained 2D parameters. This set consists of the perimeter ratio (PR) (Fig. 3b (1)), swelling ratio (SR) (Fig. 3b (2)), maximum flattening ratio (Max FR) (Fig. 3b (3)), anteroposterior diameter ratio (ADR) (Fig. 3b (4)), and maximum cross-sectional area'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='ratio (PR) (Fig. 3b (1)), swelling ratio (SR) (Fig. 3b (2)), maximum flattening ratio (Max FR) (Fig. 3b (3)), anteroposterior diameter ratio (ADR) (Fig. 3b (4)), and maximum cross-sectional area (Max CSA) (Fig. 3b (5)), which form integral components of our approach. The definition of PR and ADR (Fig. 3b (1), b (3)) is the ratio of maximum and minimum values of respective parameters acquired from Fig. 3a. And SR (Fig. 3b (2)) is defined as the ratio of CSA at MN compression to CSA at swelling. Explainable diagnosis EDT and physical examination are common diagnostic tests for CTS, while they could introduce patient discomfort, inconclusive results, and false-positive diagnoses. US is immune from these limitations, and its diagnostic results correlate well with EDT and clinical examination results. However, performing accurate diagnoses based on US relies on experience and skills. The CAD system can assist this effectively. To fully exploit the MN descriptors defined in the previous'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='results. However, performing accurate diagnoses based on US relies on experience and skills. The CAD system can assist this effectively. To fully exploit the MN descriptors defined in the previous section, we propose to use random forest due to its additional insights into feature importance and interactions, facilitating model interpretation and feature selection. Our random forest model consisted of 135 trees, with a maximum depth per tree set to 6. To prevent overfitting, we included a minimum of 12 samples required for splitting internal nodes and a minimum of 9 samples to form a new leaf node. We also selected a maximum of log2 features to consider at each split to increase the diversity among trees.'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='12'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content=\"The model's performance was measured using the Gini index, and the random state was set to 90 for reproducibility. Experimental setup Experiments To fully evaluate the proposed method, we conducted comparative experiments in three key stages, namely segmentation, measurement, and diagnosis. All the experiments were evaluated on our in-house MN dataset. 1). We implemented several state-of-the-art (SOTA) methods in segmenting the MN. (i.e., U-Net-MN(19), DeepLabV3+-MN(20), Mask R-CNN-MN(25), and Solov2-MN(26)). In order to explore the potential of these models, we selected their best performance variants in the comparison (i.e., DeepLabV3+-MN, Mask R-CNN-MN, and Solov2-MN with the backbone of ResNet-101)(26, 32). Also, for simplicity and fairness, these models were trained suitably according to their best hyper-parameter selection. 2). To ensure the automated segmentation results could generate biometric measurements that were consistent with the manual results, we calculated both\"),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='to their best hyper-parameter selection. 2). To ensure the automated segmentation results could generate biometric measurements that were consistent with the manual results, we calculated both frame-level measurements (i.e., perimeter, CSA, ADR, and FR), and video-level measurements (PR, SR, Max CSA, ADR, and Max CSA) from the automated delineations. This extension not only further examined the segmentation capability of the models but also provided a solid foundation for subsequent diagnosis. 3). Finally, we investigated whether OSA-CTSD could benefit the diagnosis process by conducting the following reader study. In specific, we invited two groups of radiologists with different levels of experience to classify the MN US videos. Each of the groups contained 3 radiologists to provide statistically consistent results. To further examine the robustness of the model, we also implemented OSA-CTSD variants with Logistic regression and SVM classifiers, respectively. Note that the rest of'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='consistent results. To further examine the robustness of the model, we also implemented OSA-CTSD variants with Logistic regression and SVM classifiers, respectively. Note that the rest of the framework maintained the same for a fair comparison. Evaluation Metrics The segmentation performance was evaluated based on Intersection over Union (IoU), Dice coefficient (Dice), HD95 (95th percentile of Hausdorff Distance), and ASSD (Average Symmetric Surface Distance). IoU and Dice measure the overlap between the prediction and the GT.'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='13'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='HD95 measures the distance between two sets of points (typically, the points on the boundaries of two segmented regions). Suppose that ð´ and ðµ represent two sets of points, where ð´ is the GT and ðµ is the segmentation result. Then the HD95 score is calculated as follows: ð»ð·95(ð´,ðµ)=ð‘šð‘Žð‘¥.â„Ž!\"(ð´,ðµ),â„Ž!\"(ðµ,ð´)0, where â„Ž!\"(ð´,ðµ) is the 95th percentile of the Hausdorff distance between points in set ð´ and points in set ðµ, and\\tâ„Ž!\"(ðµ,ð´) is the 95th percentile of the Hausdorff distance between points in set ðµ and points in set ð´. The ASSD score measures the average distance between the surfaces of two segmented regions, and a smaller value of ASSD indicates a better segmentation performance. The ASSD score is calculated as follows: ð´ð‘†ð‘†ð·(ð´,ðµ)=#|%|&|\\'|(âˆ‘ð‘‘(ð‘Ž,ðµ)(âˆˆ%+âˆ‘ð‘‘(ð‘,ð´)*âˆˆ\\'), where |ð´| and |ðµ| are the numbers of points in set ð´ and set ðµ, respectively, and ð‘‘(ð‘Ž,ðµ) and ð‘‘(ð‘,ð´) are the shortest distances from point ð‘Ž in set ð´ to set ðµ and from point ð‘ in set ðµ to set ð´, respectively. Based on segmentation'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='of points in set ð´ and set ðµ, respectively, and ð‘‘(ð‘Ž,ðµ) and ð‘‘(ð‘,ð´) are the shortest distances from point ð‘Ž in set ð´ to set ðµ and from point ð‘ in set ðµ to set ð´, respectively. Based on segmentation results, we evaluated the measurement performances of different segmentation models by mean absolute error (MAE) in both levels. Qualitative analyses were also conducted to evaluate segmentation and measurement results visually. Meanwhile, the classification performance was evaluated using accuracy (ACC), sensitivity (SEN), specificity (SPE), F1 score (F1), receiver operating characteristic curve (ROC), area under the curve (AUC), false negative rate (FNR), and false positive rate (FPR). Statistical Analysis We also conducted statistical analysis to fully demonstrate the superiority of the proposed method. In specific, the differences in segmentation metrics (Dice, IoU, HD95, ASSD) among models were compared using Wilcoxon signed-rank test. Furthermore, measurement (e.g., perimeter, CSA, PR,'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='method. In specific, the differences in segmentation metrics (Dice, IoU, HD95, ASSD) among models were compared using Wilcoxon signed-rank test. Furthermore, measurement (e.g., perimeter, CSA, PR, SR, etc.) evaluation metric (Absolute Error) was also analyzed by Wilcoxon signed-rank test on two levels (frame-level and video-level). For the reader study, chi-square test was used to identify the'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='14'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='statistical differences in diagnostic performance among two groups of radiologists and that of the proposed system. Also, the statistical significances of the variants of proposed system were examined by Delongâ€™s test. For each statistical test, p<0.05 was considered to indicate significance. The analysis was performed using software package SciPy statistical toolkit in Python version = 3.8.  Implementation details Our proposed method was trained and evaluated on an Nvidia GeForce RTX 3090 GPU. The method was performed using the Pytorch, OpenCV , and Scikit-learn packages. Our segmentation neural network used is SegFormer-B2, a variant with encoder parameters number of 24.2M that balances segmentation performance and inference speed. We conducted data augmentation via random horizontal flipping, random resize with a ratio of 0.5-2.0, and random cropping to dimensions of 512Ã—512. Our training procedure involves using the AdamW optimizer for 160K iterations on the MN dataset, and the'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='flipping, random resize with a ratio of 0.5-2.0, and random cropping to dimensions of 512Ã—512. Our training procedure involves using the AdamW optimizer for 160K iterations on the MN dataset, and the batch size was 16. We set the learning rate to an initial value of 1e-6 and then apply a \"poly\" learning rate schedule with factor 1.0 by default. We implemented all models and conducted hyper-parameters selection for all experiments suitably. Meanwhile, the classifiers were trained on diagnostic parameters calculated from radiologistsâ€™ delineations and tested on diagnostic parameters calculated from our segmentation. Results MN segmentation results We compared some popular CNN-based semantic segmentation models (U-Net-MN, DeepLabV3+-MN), instance segmentation models (Mask R-CNN-MN, Solov2-MN), and our model to delineate the MN region. Quantitative segmentation evaluation results were displayed in Table 1. Among these models, ours exhibited the highest Dice and IoU values, indicating that'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='and our model to delineate the MN region. Quantitative segmentation evaluation results were displayed in Table 1. Among these models, ours exhibited the highest Dice and IoU values, indicating that its predictions were more similar to the GT (p<0.05). Additionally, ours demonstrated the lowest HD95 and ASSD values, indicating that its boundaries correlated better with the manual results (p<0.05). In contrast, U-Net-MN showed the lowest Dice and IoU values and the highest HD95 and ASSD values,'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='15'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='suggesting that its segmentation performance was relatively poor (with statistically significant difference, p<0.05). Additionally, because U-Net-MN used the original U-Shape architecture, its capability of feature extraction was not comparable to the others and performed worst. The heavy encoders used by DeepLabV3+-MN, Mask R-CNN-MN, and Solov2-MN led to good results, even DeepLabV3+-MN achieved comparable results to ours (with no statistically significant differences in ASSD, Dice, IoU; p-values of 0.13, 0.54, 0.54 respectively). However, they also brought a larger number of parameters and slower inference speeds. It is worth mentioning that the proposed OSA-CTSD used only 1/3 the number of parameters of that used by DeepLabV3+-MN, Mask R-CNN-MN, and Solov2-MN, while achieving superior performance (see Table 1).  Table 1. Segmentation performances of U-Net-MN, DeepLabV3+-MN, Mask R-CNN-MN, Solov2-MN, and ours. Parameters number of whole model and test speed are also present in the'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='performance (see Table 1).  Table 1. Segmentation performances of U-Net-MN, DeepLabV3+-MN, Mask R-CNN-MN, Solov2-MN, and ours. Parameters number of whole model and test speed are also present in the table. (\"â†“\" indicates that the smaller the value, the better the performance. \"â†‘\" indicates the larger the value, the better the performance.) Method HD95 (px)â†“ p  ASSD (px)â†“ p Params (M)â†“ U-Net-MN 8.44Â±6.38 <0.05 3.08Â±2.38 <0.05 29.06 DeepLabV3+-MN 7.51Â±4.87 <0.05 2.69Â±1.79 0.13 62.57 Mask R-CNN-MN 8.52Â±6.47 <0.05 2.90Â±2.03 <0.05 62.74 Solov2-MN 7.64Â±5.33 <0.05 2.79Â±2.02 <0.05 65.22 Ours 7.21Â±4.74 - 2.64Â±1.52 - 24.73 Method Dice (%)â†‘ p IoU (%)â†‘ p FPS (img/s)â†‘ U-Net-MN 84.54Â±10.85 <0.05 74.50Â±13.84 <0.05 35.81 DeepLabV3+-MN 85.76Â±8.59 0.54 75.93Â±11.75 0.54 28.72 Mask R-CNN-MN 84.80Â±9.69 <0.05 74.74Â±12.69 <0.05 27.44 Solov2-MN 85.20Â±9.85 <0.05 75.27Â±12.53 <0.05 28.75 Ours 85.78Â±8.71 - 76.00Â±11.98 - 47.28 HD95 Hausdorff distance 95th percentile; ASSD Average symmetric surface distance; Dice'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='74.74Â±12.69 <0.05 27.44 Solov2-MN 85.20Â±9.85 <0.05 75.27Â±12.53 <0.05 28.75 Ours 85.78Â±8.71 - 76.00Â±11.98 - 47.28 HD95 Hausdorff distance 95th percentile; ASSD Average symmetric surface distance; Dice Dice'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='16'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='coefficient; IoU Intersection over union; Params Parameters; FPS Frames per second; px pixels; img/s Images per second; M Million. The qualitative segmentation results of our method and competing methods were also present in the Figure 4. In general, ours as well as DeepLabV3+-MN performed well, and U-Net-MN, Mask R-CNN-MN, and Solov2-MN obtained bad performances occasionally. For example, although there was no significant variance among these models in frame 185, the segmentation performance degradation could be obviously observed in frame 219, where U-Net-MN failed to recognize MN at all. Also note that only ours and DeepLabV3+-MN delineated the left and right boundaries of MN in frame 219 and achieved high segmentation accuracy. Meanwhile, the segmentation results of Mask R-CNN-MN and Solov2-MN both deviated from the GTs in the upper regions. And the left and right MN boundaries obtained from instance segmentation models (i.e., Mask R-CNN-MN, Solov2-MN) displayed discrepancies'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='and Solov2-MN both deviated from the GTs in the upper regions. And the left and right MN boundaries obtained from instance segmentation models (i.e., Mask R-CNN-MN, Solov2-MN) displayed discrepancies compared to the GTs in frames 219 and 254. Also, the segmentation area of Solov2-MN was smaller than the GT in frame 254. Frame 307 represented the end of the US scanning, which also meant that the MN had reached the deepest point in the arm. Thus, all the models were not performing well: the delineation of U-Net-MN outlined in the lower left of the MN was too thick, but it performed better at the right nerve boundary. DeepLabV3+-MN performed poorly at the right boundary, which was far from the accurate morphology of MN. While the performance of Mask R-CNN-MN was not as poor as the two models, its segmented morphology still considerably differed from the GT. Solov2-MN yielded the poorest performance in frame 307, as it erroneously identified the MN as a stripe-like object.'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='17'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='Figure 4. Visual samples of the segmentation result delineated by different segmentation models. â€œGTâ€ and â€œOursâ€ refer to the GT and SegFormer-B2.  MN automated measurement results To evaluate segmentation performance more comprehensively, we extended the experiment to frame-level and video-level measurements of MN. MAEs of MN morphological measurements (frame-level) and diagnostic parameters (video-level) were shown in Table 2 and Table 3, respectively. As we can observe in Table 2, the proposed produced the smallest MAE values among all competing methods. Our approach outperformed its counterparts in all of the parameters measurement tasks (with statistically significant differences, p<0.05). As the frame-level morphological parameters were directly acquired from the corresponding segmentation predictions, their measurement performance of each model aligned well with that shown in Table 1. For example, U-Net-MN achieved relatively larger measurement errors (e.g., 1.67Â±0.60 vs'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='segmentation predictions, their measurement performance of each model aligned well with that shown in Table 1. For example, U-Net-MN achieved relatively larger measurement errors (e.g., 1.67Â±0.60 vs 1.41Â±0.44 and 2.30Â±1.21 vs 1.71Â±0.80, p<0.05) with the lowest Dice and IoU scores, and DeepLabV3+-MN performed relatively better in measuring CSA and AD. However, there were a few exceptions as could be observed in Table 2. For example, the MAEs of the perimeter, CSA, and AD acquired by DeepLabV3+-MN were slightly higher than those of Solov2-MN, while its FR MAE score was lower than the latter. Additionally, Mask R-CNN-MN exhibited another exception where their perimeter and FR MAE scores were higher despite having a better Dice score. An explanation is that measurement results are not decided directly by segmentation'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='18  \\nmetrics, but these metrics are correlated. Also note that the MAE values of different methods were relatively close, as they exhibited the averaged deviation of perimeter, CSA, AD, and FR across all frames of all test videos.  Table 2. MN morphological parameters measurement performances of U-Net-MN, DeepLabV3+-MN, Mask R-CNN-MN, Solov2-MN, and ours evaluated by MAE. (\"â†“\" indicates the smaller the value, the better the measurement performance.)'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='Method Mean Absolute Error (MAE)â†“ Perimeter (mm) CSA (mm2) AD (mm) FR (%) U-Net-MN 1.67Â±0.60 2.30Â±1.21 0.28Â±0.09 35.51Â±14.47 DeepLabV3+-MN 1.44Â±0.45 1.80Â±0.80 0.21Â±0.09 30.94Â±7.73 Mask R-CNN-MN 1.69Â±0.85 1.77Â±0.93 0.22Â±0.11 39.32Â±14.80 Solov2-MN 1.43Â±0.38 1.69Â±0.76 0.20Â±0.08 34.40Â±14.10 Ours 1.41Â±0.44 1.71Â±0.80 0.20Â±0.09 30.65Â±9.61 CSA Cross-sectional area; AD Anteroposterior diameter; FR Flattening ratio. On the contrary, the video-level measurements are often defined by the maximum difference of the MN across frames in a single video. Therefore, a few outliers in frame-level measurements could lead to large deviations in video-level ones. Table 3 displayed the video-level results. For example, U-Net-MN showed 262.95% deviation in PR and 207.44% deviation in ADR (see row 1, Table 3). After examining its predictions, we found that this method failed to identify the MN or delineate the correct shape of the MN in some frames, which led to considerably higher PR and ADR MAE values'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='After examining its predictions, we found that this method failed to identify the MN or delineate the correct shape of the MN in some frames, which led to considerably higher PR and ADR MAE values (p<0.05). They adopted a symmetric structure that may cause information bottleneck problems, which cannot make full use of contextual information and local detail information. Note that the differences among competing methods reported in Table 3 are much higher than that in Table 2. As stated earlier, the video-level measurement values may deteriorate due to the increase of outliers, while the frame-level measurements may smooth out these exceptions.'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='19  \\n Table 3. MN automated diagnostic parameters measurement performances of U-Net-MN, DeepLabV3+-MN, Mask R-CNN-MN, Solov2-MN, and ours evaluated by MAE. (\"â†“\" indicates the smaller the value, the better the measurement performance.) \\nMethod Mean Absolute Error (MAE)â†“ \\nPR (%) SR (%) ADR (%) Max FR (%) Max CSA (mm2) U-Net-MN 262.95 Â±434.85 59.38 Â±63.04 207.44 Â±367.47 44.89 Â±35.44 2.61Â±1.70 \\nDeepLabV3+-MN 27.13 Â±33.36 58.72 Â±118.58 14.26 Â±11.66 33.40 Â±29.34 1.84Â±1.33 \\nMask R-CNN-MN 23.32 Â±28.34 54.56 Â±64.02 15.78 Â±15.98 37.13 Â±27.98 1.80Â±1.35 \\nSolov2-MN 31.06 Â±49.94 47.95 Â±57.53 14.78 Â±14.16 51.95 Â±38.22 2.89Â±3.29 \\nOurs 19.57 Â±28.44 56.01 Â±112.15 14.48 Â±15.22 36.08 Â±36.43 1.82Â±2.27'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='Solov2-MN 31.06 Â±49.94 47.95 Â±57.53 14.78 Â±14.16 51.95 Â±38.22 2.89Â±3.29 \\nOurs 19.57 Â±28.44 56.01 Â±112.15 14.48 Â±15.22 36.08 Â±36.43 1.82Â±2.27 \\nPR Perimeter ratio; SR Swelling ratio; ADR Anteroposterior diameter ratio; Max FR Maximum flattening ratio; Max CSA Maximum cross-sectional area.   Carpal tunnel syndrome diagnosis results Table 4 reported the quantitative results of our reader study that evaluated the diagnostic accuracy for CTS among radiologists with varying levels of experience, including three inexperienced radiologists (L, P, Z) and three experienced radiologists (C, G, W). The experienced radiologists outperformed the inexperienced radiologists in terms of all metrics (with the higher ACC, SEN, SPE, F1, and the lower FNR, FPR) with large margins and statistically significant differences (p<0.05). They scored an  \\n20'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='average ACC of 98.21% - around 8% higher than that of the inexperienced group. Furthermore, although the average SPE is only about 4% higher than that of less experienced radiologists, the SEN is as much as 16% higher. This not only results in a high F1 score (97.08%) but also in very low FNR and FPR (3.33% and 1.11%, respectively). Meanwhile, among the group of inexperienced radiologists, there was a difference of up to 10% in SEN between different individuals. It was also noted that the average SEN score of inexperienced radiologists was about 10% lower than their SPE score (80.83% vs 94.44%).   Table 4. Diagnosis performances of radiologists with varying levels of experience and different variants of our proposed method (OSA-CTSD). â€œOurs+LRâ€ and â€œOurs+SVMâ€ refer to OSA-CTSD variants using and Logistic regression and SVM classifier, respectively. (\"â†“\" indicates that the smaller the value, the better the performance. \"â†‘\" indicates the larger the value, the better the performance.)'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='Radiologists ACC (%)â†‘ SEN (%)â†‘ SPE (%)â†‘ F1(%)â†‘ FNR (%)â†“ FPR (%)â†“ \\nInexperienced L 93.08 85.00 96.67 88.31 15.00 3.33 P 90.77 82.50 94.44 84.62 17.50 5.56 Z 86.92 75.00 92.22 77.92 25.00 7.78 \\nAvg 90.26 Â±3.11 80.83 Â±5.20 94.44 Â±2.22 83.62 Â±5.27 19.17 Â±5.20 5.56 Â±2.23 \\nExperienced C 98.46 97.50 98.89 97.50 2.50 1.11 G 98.46 95.00 100.00 97.44 5.00 0.00 W 97.69 97.50 97.78 96.30 2.50 2.22 \\nAvg 98.21 Â±0.44 96.67 Â±1.44 98.89 Â±1.11 97.08 Â±0.68 3.33 Â±1.44 1.11 Â±1.11  \\n21'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='Model variants ACC (%)â†‘ SEN (%)â†‘ SPE (%)â†‘ F1â†‘(%) FNR (%)â†‘ FPR (%)â†‘ Ours+LR 92.31 80.00 97.78 86.49 20.00 2.22 Ours+SVM 93.08 82.50 97.78 88.00 17.50 2.22 Ours 93.85 85.00 97.78 89.47 15.00 2.22 ACC Accuracy; SEN Sensitivity; SPE Specificity; F1 F1 score; FNR False negative rate; FPR False positive rate.  In addition, we implemented several variants of our proposed models by replacing different classifiers to evaluate the robustness of our models. Results showed that these variants scored slightly lower ACC, and F1 scores than OSA-CTSD, while still outperforming than those of the inexperienced radiologists (p<0.05). There were no significant differences among these variants (p>0.05), which demonstrated that our proposed framework was general and compatible with various classifiers. Moreover, to further investigate the influence of individual diagnostic parameters, we implemented 5 LR models using only PR, SR, ADR, Max FR, and Max CSA, respectively. Their diagnostic performances were'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='to further investigate the influence of individual diagnostic parameters, we implemented 5 LR models using only PR, SR, ADR, Max FR, and Max CSA, respectively. Their diagnostic performances were reported in Table 5. The corresponding ROC curves were plotted in Fig. 5, which is a plot of the true-positive rate and false-positive rate on the coordinate axis. Our approach presented a robust classification of CTS, with sensitivity of 85.00%, specificity of 97.78%, accuracy of 93.85%, F1 score of 89.47%, FNR of 15.00%, FPR of 2.22%, and AUC of 0.98.'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='22'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='Figure 5. Receiver operating characteristic curve of OSA-CTSD and LR models using only PR, SR, ADR, Max FR, and Max CSA, respectively. PR Perimeter ratio; SR Swelling ratio; ADR Anteroposterior diameter ratio; Max FR Maximum flattening ratio; Max CSA Maximum cross-sectional area; LR Logistic regression; SVM Support vector machine.  Discussion The quantitative segmentation results of the same MN dataset shows that ours achieved the most balanced and powerful performance among these models. While Dice and IoU are important metrics, the ASSD and HD95 are more sensitive to the following biometric measurements. As we can see in Table 3, Mask R-CNN-MN and Solov2-MN produce relatively poorer MAE values, while their Dice and IoU scores are still acceptable (Table 1). It may stem from that their HD95 and ASSD are considerably larger and may lead to inaccurate measurement results. In Figure 6, there were two segmentation results from different models. It could be observed that the higher HD95'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='HD95 and ASSD are considerably larger and may lead to inaccurate measurement results. In Figure 6, there were two segmentation results from different models. It could be observed that the higher HD95 scores could result in worse measurement results when their Dice and IoU scores are'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='23'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='close. In our study, we found that HD95 and ASSD results are more closely related to measurement results (such as PR, CSA, AD, and FR) than other metrics. It is because these metrics characterize the boundary of the MN, whereas Dice and IoU scores focus more on the degree of overlap between the predictions and the GTs. We conjecture that the relatively poorer performance of Mask R-CNN-MN and Solov2-MN in measuring PR and SR may be explained by that they both utilized instance segmentation models. These approaches are known to be effective in detecting a large number or occlusion of objects in an image, while may not be well-suited for the task of contouring the MN in ultrasonography due to the lack of multi-scale skip connections and detailed boundary information. Also, the misaligned and low-resolution mask head of Mask R-CNN-MN could lead to inaccurate and blurry edge segmentation. Meanwhile, Solov2-MN also used the dynamic convolution kernel, which is highly sensitive to the'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='and low-resolution mask head of Mask R-CNN-MN could lead to inaccurate and blurry edge segmentation. Meanwhile, Solov2-MN also used the dynamic convolution kernel, which is highly sensitive to the quality and resolution of the input features. As a result, their approaches resulted in higher HD95 scores and performed poorly in video-level measurements when applied to our dataset.'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='24  \\n Figure 6. Visual samples of the correlation between the different segmentation scores and measurement errors. Red regions refer to the segmentation result, green regions refer to the GT. Dice Dice coefficient; IoU Intersection over union; HD95 Hausdorff distance 95th percentile; ASSD Average symmetric surface distance; CSA Cross-sectional area; AD Anteroposterior diameter; FR Flattening ratio.  In addition, OSA-CTSD is not only reliable in segmentation but also is computation efficient. The inference speed (the number of inferred frames per second) of it can reach 47.2 FPS, which is the fastest of these models (Table 1). It may be explained by that our model utilized a lightweight All-MLP decoder that could capture powerful representations with a limited number of parameters. Also due to the adoption \\n \\n25'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='of the hierarchically structured Transformer encoder for ours, this allows the models to better capture context information, which is essential for accurate segmentation. It is worth pointing out that in the last column in Fig. 4 that all methods have difficulty in identifying the MN boundaries (especially the right). As scanning goes further and MNâ€™s position gets deeper, the boundary of the MN becomes unclear, bringing additional challenges in capturing the target. Note that unstable performance in such a scenario could lead to inaccurate frame-level measurements and ultimately hamper video-level measurement accuracy. Therefore, accurate hard-case segmentation such as those generated by OSA-CTSD builds a firm foundation for subsequent measurement and diagnosis. As reported in Table 4, the experienced group have showed performance to inexperienced one in diagnostic evaluation metrics (e.g., ACC, SEN, SPE, etc.), especially higher F1 score. These results indicate that experienced'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='4, the experienced group have showed performance to inexperienced one in diagnostic evaluation metrics (e.g., ACC, SEN, SPE, etc.), especially higher F1 score. These results indicate that experienced radiologists are not only capable of accurately diagnosing CTS, but also are better at balancing the FNR and FPR, thus reducing missed and erroneous diagnoses. However, due to the shortage of experienced radiologists, timely and accurate diagnosis of CTS may not always be possible, leading to delayed or inappropriate treatment for patients. Meanwhile, the lower SEN score and higher SPE score of inexperienced radiologists suggest a potential conservative bias in their diagnostic tendencies, i.e., inclining to identify Non-CTS people. This could lead to a higher FNR and more missed diagnoses.  Table 5. Diagnosis performances of Logistic regression (LR) models using only PR, SR, ADR, Max FR, and Max CSA, respectively.'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='Method ACC (%)â†‘ SEN (%)â†‘ SPE (%)â†‘ F1â†‘(%) FNR (%)â†“ FPR (%)â†“ PR 80.00 37.50 98.89 53.57 62.50 1.11 SR 92.31 80.00 97.78 86.49 20.00 2.22 ADR 80.77 40.00 98.89 56.14 60.00 1.11  \\n26'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='Max FR 69.23 0.00 100.00 0.00 100.00 0.00 Max CSA 80.77 50.00 94.44 61.54 50.00 5.56 Ours 93.85 85.00 97.78 89.47 15.00 2.22 PR Perimeter ratio; SR Swelling ratio; ADR Anteroposterior diameter ratio; Max FR Maximum flattening ratio; Max CSA Maximum cross-sectional area. To avoid missed diagnoses as well as enhance diagnostic efficiency, there is a need for a convenient, efficient, quantitative, and interpretable automated diagnostic tool. The proposed fully automated OSA-CTSD offers such a solution. Notably, OSA-CTSD could delineate the MN in real-time (Table 1), calculate diagnostic parameters for CTS, and rapidly generate accurate diagnosis results without any manual intervention. In specific, the SEN and F1 scores of OSA-CTSD were significantly higher than the average levels of the inexperienced group with statistically significant difference (p<0.05). It can be seen that it reported better performance than the inexperienced group in all metrics. In terms of single factor analysis'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='group with statistically significant difference (p<0.05). It can be seen that it reported better performance than the inexperienced group in all metrics. In terms of single factor analysis (Table 5), the superiority of the OSA-CTSD may be explained by that it fully incorporates comprehensive MN structural information regarding the proximal entrance, carpal tunnel segment, and distal exit segments to analyze the MN. If only measuring maximum CSA alone, some patients may be missed due to less severe swelling near proximal or distal ends despite clear compression at the carpal tunnel site; however, using SR as an auxiliary diagnosis may yield better results. This suggests that the OSA-CTSD may help to reduce the number of missed diagnoses. On the other hand, OSA-CTSD scored similar SPE and FPR score with that of the experienced radiologists and even reached the level of one of the experienced radiologists (W). It was not regarded as statistically significant difference in the diagnostic'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='score with that of the experienced radiologists and even reached the level of one of the experienced radiologists (W). It was not regarded as statistically significant difference in the diagnostic efficacy between the experienced group and OSA-CTSD (p=0.08). These results suggest that OSA-CTSD demonstrates significantly superior diagnostic performance compared to inexperienced group and is also more closely aligned with that of experienced radiologists. For inexperienced radiologists, OSA-CTSD can not only provide a comprehensive measurement report of the MN, but also diagnostic'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='27  \\nsuggestions to better assist their diagnostic workflow. Meanwhile, OSA-CTSD may also help the experienced radiologists by automating the measuring process of the MN that can accelerate the evaluation.'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='Figure 7. Feature importances of diagnostic parameters in OSA-CTSD.  Furthermore, we analyzed the contribution ratios for each indicator in the OSA-CTSD method (Fig. 7). As shown in Figure 7, it showed a strong correlation between the feature contribution of a single indicator and its diagnostic accuracy when used alone (Table 5). And the contribution value for SR was much higher than that of other indicators with a percentage as high as 42.9%. This also confirms that CTS pathogenesis involves compression on the MN within the carpal tunnel leading to secondary edema near proximal or/ and distal ends. However, FR performed poorly, possibly because when the MN is significantly compressed in a narrow segment, both its transverse diameter and longitudinal axis are often simultaneously compressed. According to literature reports, SR had high specificity for diagnosing CTS(8, 9, 33). \\n \\n28'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='However, due to significantly increased workload and low reproducibility among US radiologists, this method has not been used frequently. On the contrary, the proposed OSA-CTSD can automatically generate accurate and reproducible SR measurements without manual interventions. Furthermore, this study is currently unique among AI-assisted US diagnoses for CTS by providing comprehensive analysis across the proximal entrance, carpal tunnel segment, and distal exit segments, which can better fit the CTS pathogenesis mechanism while reducing the misdiagnosis rate for US diagnosis of CTS.     Conclusions In this study, we proposed a one-stop automated CTS diagnosis system (OSA-CTSD) as an effective CAD tool for diagnosing CTS based on US. The OSA-CTSD combined three processes into a unified framework, including real-time MN delineation, accurate biometric measurements, and explainable CTS diagnosis. The proposed tool is evaluated on a large-scale dataset including 32,301 images from 90 normal'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='including real-time MN delineation, accurate biometric measurements, and explainable CTS diagnosis. The proposed tool is evaluated on a large-scale dataset including 32,301 images from 90 normal wrists and 40 CTS wrists, and by multiple metrics. It demonstrated promising diagnostic performance based on clinical-interpretable parameters. Besides, it is fully automated with a simplified scanning protocol. The application of such a tool could not only reduce reliance on the expertise of examiners, but also could help to promote the standardization of the CTS diagnosis process in the future.  Acknowledgmentsâ€”This work was supported by the National Natural Science Foundation of China (No. 62101342, and No. 62171290); Guangdong Basic and Applied Basic Research Foundation (No.2023A1515012960); Shenzhen-Hong Kong Joint Research Program (No. SGDX20201103095613036) and Shenzhen Key Medical Discipline Construction Fund (SZXK052). Authors Jiayu Peng, Jiajun Zeng, and Manlin Lai contributed'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='Shenzhen-Hong Kong Joint Research Program (No. SGDX20201103095613036) and Shenzhen Key Medical Discipline Construction Fund (SZXK052). Authors Jiayu Peng, Jiajun Zeng, and Manlin Lai contributed equally to this work.  Conflict of interestâ€”None of the authors has a financial conflict related to the'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='29  \\ncontent of this work.    \\n30'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='References 1. Alfonso C, Jann S, Massa R, Torreggiani A. Diagnosis, treatment and follow-up of the carpal tunnel syndrome: a review. Neurol Sci. 2010;31(3):243-52. 2. Yoshii Y , Villarraga HR, Henderson J, Zhao C, An KN, Amadio PC. Ultrasound assessment of the displacement and deformation of the median nerve in the human carpal tunnel with active finger motion. J Bone Joint Surg Am. 2009;91(12):2922-30. 3. Keir PJ, Rempel DM. Pathomechanics of peripheral nerve loading. Evidence in carpal tunnel syndrome. J Hand Ther. 2005;18(2):259-69. 4. de Krom MC, Knipschild PG, Kester AD, Thijs CT, Boekkooi PF, Spaans F. Carpal tunnel syndrome: prevalence in the general population. J Clin Epidemiol. 1992;45(4):373-6. 5. Jablecki CK, Andary MT, So YT, Wilkins DE, Williams FH. Literature review of the usefulness of nerve conduction studies and electromyography for the evaluation of patients with carpal tunnel syndrome. AAEM Quality Assurance Committee. Muscle Nerve. 1993;16(12):1392-414. 6. McDonagh'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='of nerve conduction studies and electromyography for the evaluation of patients with carpal tunnel syndrome. AAEM Quality Assurance Committee. Muscle Nerve. 1993;16(12):1392-414. 6. McDonagh C, Alexander M, Kane D. The role of ultrasound in the diagnosis and management of carpal tunnel syndrome: a new paradigm. Rheumatology (Oxford). 2015;54(1):9-19. 7. Lin TY , Chang KV , Wu WT, Ã–zÃ§akar L. Ultrasonography for the diagnosis of carpal tunnel syndrome: an umbrella review. J Neurol. 2022;269(9):4663-75. 8. Sugimoto T, Ochi K, Hosomi N, Mukai T, Ueno H, Takahashi T, et al. Ultrasonographic reference sizes of the median and ulnar nerves and the cervical nerve roots in healthy Japanese adults. Ultrasound Med Biol. 2013;39(9):1560-70. 9. Wilson D. Ultrasound assessment of carpal tunnel syndrome. Clin Radiol. 2004;59(10):909. 10. Buchberger W, Judmaier W, Birbamer G, Lener M, Schmidauer C. Carpal tunnel syndrome: diagnosis with high-resolution sonography. AJR Am J Roentgenol.'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='tunnel syndrome. Clin Radiol. 2004;59(10):909. 10. Buchberger W, Judmaier W, Birbamer G, Lener M, Schmidauer C. Carpal tunnel syndrome: diagnosis with high-resolution sonography. AJR Am J Roentgenol. 1992;159(4):793-8. 11. Kluge S, Kreutziger J, Hennecke B, V ogelin E. Inter- and intraobserver reliability of predefined diagnostic levels in high-resolution sonography of the carpal tunnel syndrome - a validation study on healthy volunteers. Ultraschall Med. 2010;31(1):43-7. 12. Chen X, Xie C, Chen Z, Li Q. Automatic Tracking of Muscle Cross-Sectional Area Using Convolutional Neural Networks with Ultrasound. J Ultrasound Med. 2019;38(11):2901-8. 13. Horng MH, Yang CW, Sun YN, Yang TH. DeepNerve: A New Convolutional Neural Network for the Localization and Segmentation of the Median Nerve in Ultrasound Image Sequences. Ultrasound Med Biol. 2020;46(9):2439-52. 14. Huang C, Zhou Y , Tan W, Qiu Z, Zhou H, Song Y , et al. Applying deep learning in recognizing the femoral nerve block region on'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='Image Sequences. Ultrasound Med Biol. 2020;46(9):2439-52. 14. Huang C, Zhou Y , Tan W, Qiu Z, Zhou H, Song Y , et al. Applying deep learning in recognizing the femoral nerve block region on ultrasound images. Ann Transl Med.'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='31'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='2019;7(18):453. 15. Zhou S, Zhou F, Sun Y , Chen X, Diao Y , Zhao Y , et al. The application of artificial intelligence in spine surgery. Front Surg. 2022;9:885599. 16. Wang JC, Shu YC, Lin CY , Wu WT, Chen LR, Lo YC, et al. Application of deep learning algorithms in automatic sonographic localization and segmentation of the median nerve: A systematic review and meta-analysis. Artif Intell Med. 2023;137:102496. 17. Xu Y , He X, Xu G, Qi G, Yu K, Yin L, et al. A medical image segmentation method based on multi-dimensional statistical features. Front Neurosci. 2022;16:1009581. 18. Tian D, Zhu B, Wang J, Kong L, Gao B, Wang Y , et al. Brachial Plexus Nerve Trunk Recognition From Ultrasound Images: A Comparative Study of Deep Learning Models. IEEE Access. 2022;10:82003-14. 19. Festen RT, Schrier V , Amadio PC. Automated Segmentation of the Median Nerve in the Carpal Tunnel using U-Net. Ultrasound Med Biol. 2021;47(7):1964-9. 20. Yang T-H, Yang C-W, Sun Y-N, Horng M-H. A Fully-Automatic'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='V , Amadio PC. Automated Segmentation of the Median Nerve in the Carpal Tunnel using U-Net. Ultrasound Med Biol. 2021;47(7):1964-9. 20. Yang T-H, Yang C-W, Sun Y-N, Horng M-H. A Fully-Automatic Segmentation of the Carpal Tunnel from Magnetic Resonance Images Based on the Convolutional Neural Network-Based Approach. Journal of Medical and Biological Engineering. 2021. 21. Huang A, Jiang L, Zhang J, Wang Q. Attention-VGG16-UNet: a novel deep learning approach for automatic segmentation of the median nerve in ultrasound images. Quant Imaging Med Surg. 2022;12(6):3138-50. 22. Shao J, Zhou K, Cai YH, Geng DY . Application of an Improved U2-Net Model in Ultrasound Median Neural Image Segmentation. Ultrasound Med Biol. 2022;48(12):2512-20. 23. Cosmo MD, Chiara Fiorentino M, Villani FP, Sartini G, Smerilli G, Filippucci E, et al. Learning-Based Median Nerve Segmentation From Ultrasound Images For Carpal Tunnel Syndrome Evaluation. Annu Int Conf IEEE Eng Med Biol Soc. 2021;2021:3025-8. 24. Di'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='Smerilli G, Filippucci E, et al. Learning-Based Median Nerve Segmentation From Ultrasound Images For Carpal Tunnel Syndrome Evaluation. Annu Int Conf IEEE Eng Med Biol Soc. 2021;2021:3025-8. 24. Di Cosmo M, Fiorentino MC, Villani FP, Frontoni E, Smerilli G, Filippucci E, Moccia S. A deep learning approach to median nerve evaluation in ultrasound images of carpal tunnel inlet. Med Biol Eng Comput. 2022;60(11):3255-64. 25. Smerilli G, Cipolletta E, Sartini G, Moscioni E, Di Cosmo M, Fiorentino MC, et al. Development of a convolutional neural network for the identification and the measurement of the median nerve on ultrasound images acquired at carpal tunnel level. Arthritis Res Ther. 2022;24(1):38. 26. Yeh CL, Wu CH, Hsiao MY , Kuo PL. Real-Time Automated Segmentation of Median Nerve in Dynamic Ultrasonography Using Deep Learning. Ultrasound Med Biol. 2023;49(5):1129-36. 27. Dosovitskiy A BL, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T, Dehghani M, Minderer M, Heigold G, Gelly S,'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='Ultrasonography Using Deep Learning. Ultrasound Med Biol. 2023;49(5):1129-36. 27. Dosovitskiy A BL, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T, Dehghani M, Minderer M, Heigold G, Gelly S, Uszkoreit J, Houlsby N. AN IMAGE IS WORTH 16X16 WORDS:TRANSFORMERS FOR IMAGE RECOGNITION AT'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='32'),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content=\"SCALE. arXiv - CS - Artificial Intelligence. 2021. 28. Obuchowicz R, KruszyÅ„ska J, Strzelecki M. Classifying median nerves in carpal tunnel syndrome: Ultrasound image analysis. Biocybernetics and Biomedical Engineering. 2021;41(2):335-51. 29. Faeghi F, Ardakani AA, Acharya UR, Mirza-Aghazadeh-Attari M, Abolghasemi J, Ejtehadifar S, Mohammadi A. Accurate automated diagnosis of carpal tunnel syndrome using radiomics features with ultrasound images: A comparison with radiologists' assessment. Eur J Radiol. 2021;136:109518. 30. Jablecki CK, Andary MT, Floeter MK, Miller RG, Quartly CA, Vennix MJ, Wilson JR. Practice parameter: Electrodiagnostic studies in carpal tunnel syndrome. Report of the American Association of Electrodiagnostic Medicine, American Academy of Neurology, and the American Academy of Physical Medicine and Rehabilitation. Neurology. 2002;58(11):1589-92. 31. Xie E WW, Yu Z, Anandkumar A, Alvarez JM, Luo P. SegFormer: Simple and Efï¬cient Design for Semantic Segmentation\"),\n",
       " Document(metadata={'arxiv_id': '2402.05554v1', 'title': 'One-Stop Automated Diagnostic System for Carpal Tunnel Syndrome in Ultrasound Images Using Deep Learning', 'section': 'body', 'authors': 'Jiayu Peng, Jiajun Zeng, Manlin Lai'}, page_content='Academy of Physical Medicine and Rehabilitation. Neurology. 2002;58(11):1589-92. 31. Xie E WW, Yu Z, Anandkumar A, Alvarez JM, Luo P. SegFormer: Simple and Efï¬cient Design for Semantic Segmentation with Transformers. arXiv - CS - Machine Learning. 2021. 32. Wu CH, Syu WT, Lin MT, Yeh CL, Boudier-Reveret M, Hsiao MY , Kuo PL. Automated Segmentation of Median Nerve in Dynamic Sonography Using Deep Learning: Evaluation of Model Performance. Diagnostics (Basel). 2021;11(10). 33. Ulasli AM, Duymus M, Nacir B, Rana Erdem H, Kosar U. Reasons for using swelling ratio in sonographic diagnosis of carpal tunnel syndrome and a reliable method for its calculation. Muscle Nerve. 2013;47(3):396-402.'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'title_abstract', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='Title: Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images\\n\\nAbstract: Incorporating human domain knowledge for breast tumor diagnosis is challenging, since shape, boundary, curvature, intensity, or other common medical priors vary significantly across patients and cannot be employed. This work proposes a new approach for integrating visual saliency into a deep learning model for breast tumor segmentation in ultrasound images. Visual saliency refers to image maps containing regions that are more likely to attract radiologists visual attention. The proposed approach introduces attention blocks into a U-Net architecture, and learns feature representations that prioritize spatial regions with high saliency levels. The validation results demonstrate increased accuracy for tumor segmentation relative to models without salient attention layers. The approach achieved a Dice similarity coefficient of 90.5 percent on a dataset of 510 images. The salient attention model has potential to enhance accuracy and robustness in processing medical images of other organs, by providing a means to incorporate task-specific knowledge into deep learning architectures.'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='Attention-Enriched Deep Learning Model for Breast Tumor Segmentation \\nin Ultrasound Images \\nAleksandar Vakanskia*, Min Xiana, and Phoebe E. Freerb \\na Department of Computer Science, University of Idaho, Idaho Falls, USA \\nb University of Utah School of Medicine, Salt Lake City, USA \\n*Corresponding Author: Aleksandar Vakanski, 1776 Science Center Drive, Idaho Falls, ID 83402; \\nEmail, vakanski@uidaho.edu; Phone: (+1)208-757-5422 \\n \\nAbstract \\nIncorporating human domain knowledge for breast tumor diagnosis is cha llenging, since shape, boundary, curvature, \\nintensity, or other common medical priors vary significantly across patients and cannot be employed. This work \\nproposes a new approach for integrating visual saliency into a deep learning model for breast tumor segmentation in \\nultrasound images. Visual saliency refers to image maps containing regions that are more likely to attract radiologistsâ€™'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='ultrasound images. Visual saliency refers to image maps containing regions that are more likely to attract radiologistsâ€™ \\nvisual attention. The proposed approach introduces attention blocks into a U-Net architecture, and learns feature \\nrepresentations that prioritize spatial regions with high saliency levels. The validation results demonstrate increased \\naccuracy for tumor segmentation relative to models without salient attention layers. The approach  achieved a Dice \\nsimilarity coefficient of 90.5% on a dataset of 510 images. The salient attention model has potential to enhance \\naccuracy and robustness in processing medical images of other organs, by providing a means to incorporate task -\\nspecific knowledge into deep learning architectures.  \\n \\nKeywords: Breast ultrasound; Medical image segmentation; Visual saliency; Domain knowledge-enriched learning \\n \\nIntroduction'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='specific knowledge into deep learning architectures.  \\n \\nKeywords: Breast ultrasound; Medical image segmentation; Visual saliency; Domain knowledge-enriched learning \\n \\nIntroduction \\nComputer-aided image analysis can assist radiologistsâ€™ interpretation and diagnosis, and reduce error rates, as well as \\nthe level of stress regarding erroneous diagnosis (Cheng et al. 2010; Inoue et al. 2017; Jalalian et al. 2017; Moon et \\nal. 2011; Wu et al. 2019). For instance, 3 to 6% of all radiologistsâ€™ image interpretations contain clinically important \\nerrors, and also, significant variability in the inter- and intra-observer image interpretation is often reported (Elmore \\net al. 1994; Langlotz et al. 2019; Waite et al. 2016). \\nThe emphasis in this work is on automated computer-aided diagnosis of tumors in breast ultrasound (BUS) images \\n(Xian et al. 2018b) . A large body of research work employed conventional and deep le arning approaches to address'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='(Xian et al. 2018b) . A large body of research work employed conventional and deep le arning approaches to address \\ntasks related to automated lesion localization, segmentation, and classification (Inoue et al. 2017; Jalalian et al. 2017; \\nLitjens et al. 2017; Moon et al. 2011; Wu et al. 2019) . In spite of this progress, existing methods lack robustness and \\nconsistency when processing images taken with different imaging equipment, where the variations in image intensity, \\ncontrast, and density often result in a degraded performance of models that otherwise perform well on custom -built \\ndatasets. \\nAn important way to improve the performance of data-driven models is by incorporating prior domain-specific \\nknowledge (Nosrati and Hamarneh 2016). On the other hand, incorporating prior knowledge in deep models for breast \\ncancer detection is challenging, because unlike other medical organsâ€”such as the kidney or the heart, whose features'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='cancer detection is challenging, because unlike other medical organsâ€”such as the kidney or the heart, whose features \\nnaturally lend themselves to the application of shape or boundary priors â€”breast tumors have a large variability in \\nshape and boundaries from case to case. Extracting other priors in the form of curvature, texture, intensity, or number \\nof regions for breast tumors is also not an option.  \\nOur proposed approach incorporates topological and anatomical prior information into a deep learning model for \\nimage segmentation . More specifically, m aps of visual saliency are employed for integrating i mage topology \\nknowledge (Xu et al. 2016; Xu et al. 2018) . The model for visual saliency estimation is formulated as a quadratic \\noptimization problem, and it is based on calculations of neutro-connectedness between regions in the image (Xian et \\nal. 2016; Xian 2017) . Anatomical prior knowledge is integrated by decomposing the tissue layers into skin, fat,'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='al. 2016; Xian 2017) . Anatomical prior knowledge is integrated by decomposing the tissue layers into skin, fat, \\nmammary, and muscle layers (Xu et al. 2019), and applying higher weights to the salient regions in images belonging \\nto the mammary layer. \\nIn this paper, we propose a novel approach to integrate domain knowledge into a deep neural network model by \\nusing the attention mechanism (Simonyan et al. 2013). A U-Net architecture (Ronneberger et al. 2015) is selected for \\nincorporating the prior knowledge in the form of a pyramid of visual sa liency maps. Attention blocks are integrated 2 \\n \\nwith the layers of the encoder to force the network to learn feature representations that place spatial attention to target \\nregions with high saliency values.  Unlike similar deep learning models that introduce attention blocks by merging \\ninternal feature representations from different layers  (Chen et al. 2016; Jetley et al. 2018; Oktay et al. 2018b) , the'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='internal feature representations from different layers  (Chen et al. 2016; Jetley et al. 2018; Oktay et al. 2018b) , the \\nproposed approach employs external auxiliary inputs in the form of visual saliency maps for training the mo del \\nparameters.  \\nThe main contrib utions of this paper are: (1) attention enriched deep learning model for integrating prior \\nknowledge of tumor saliency; and (2) confidence level calculation for visual saliency maps.  \\nThe paper is organized as follows. The next section overview s related works in the literature. The Materials \\nsection describes the used image  dataset. The Methods section  covers the proposed network architecture, attention \\nblocks, and visual saliency maps. Experimental validation is provided in the Results section. The Discussion section \\npresents the findings of the experiments, and the Conclusion section summarizes the work. \\n \\nRelated Works'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='presents the findings of the experiments, and the Conclusion section summarizes the work. \\n \\nRelated Works \\nComputer-aided segmentation in medical imaging has been an important research topic for several decades, and it \\nencompasses a vast body of work in the published literature. Recent advances in deep learning models (Goodfellow \\net al. 2016; LeCun et al. 2015) demonstrated great improvements in semantic image segmentation (Badrinarayanan et \\nal. 2017; Chen et al. 2018a; Chen et al. 2018b; He et al. 2015; Lin et al. 2017; Long et al. 2015; Ronneberger et al. \\n2015; Zhao et al. 2017) . Consequently, significant efforts have been devoted toward the implementation and design \\nof deep neural networks for a wide range of medical applications, including segmentation of tumors and lesions (e.g., \\nbrain tumor (Kamnitsas et al. 2017) , skin lesions (GonzÃ¡lez-DÃ­az 2017), histopathology images (Chen et al. 2017;'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='brain tumor (Kamnitsas et al. 2017) , skin lesions (GonzÃ¡lez-DÃ­az 2017), histopathology images (Chen et al. 2017; \\nGraham et al. 2018; Kumar et al. 2017; Lin et al. 2018; Naylor et al. 2019)), and segmentation of organs (e.g., pancreas \\n(Oktay et al. 2018b), lung (Hu et al. 2019), heart (Oktay et al. 2018a), or head and neck anatomy (Zhu et al. 2019)). \\nLikewise, the implementation of deep models for breast tumor segmentation has spurred interest in the research \\ncommunity in recent years (Xian et al. 2018b) . Whereas the most popular image modality for this task have been \\nultrasound images (Abraham and Khan 201 9; Chiang et al. 2019; Huang et al. 2018; Yap et al. 2018)  and digital \\nmammography images (Akselrod-Ballin et al. 2017; Dhungel et al. 2015; Jung et al. 2018 ; Kooi et al. 2017; Moor et \\nal. 2018; Ribli et al. 2017), a body of literature used MRI (Jaeger et al. 2018),  and histology images (Lin et al. 2018).'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='al. 2018; Ribli et al. 2017), a body of literature used MRI (Jaeger et al. 2018),  and histology images (Lin et al. 2018). \\nU-Net (Ronneberger et al. 2015)  and its numerous variants and modifications have been the most commo nly used \\narchitecture for this problem to date. In spite of this progress, breast tumor segmentation is still an open research topic, \\ndue to challenges related to the inherent presence of noise and low contrast of images, sensitivity of current methods \\nto the used image-acquisition method, equipment, and settings, and the lack of large open datasets of annotated images \\nfor training purposes.  \\nPriors in medical image segmentation.  Incorporating prior task -specific knowledge for  medical image \\nsegmentation is important for improved model performance  (Nosrati and Hamarneh 20 16), and it can be crucial in \\ntasks with small datasets of annotated medical images (i.e., most medical tasks at the present time) . Prior knowledge'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='tasks with small datasets of annotated medical images (i.e., most medical tasks at the present time) . Prior knowledge \\ncan generally be in the form of shape, boundary, curvature, appearance (e.g., intensity, texture), topology ( e.g., \\nconnectivity), anatomical information/atlas (structure of tissues or organs), user information (seed points or bounding \\nboxes), moments (size, area, volume), distance (between organs and structures), and other forms . Although recent \\ndeep learning-based models have caused a leap of performance in image segmentation over conventional methods \\nbased on thresholding, region -growing, graph -based approaches, and deformable models (Cai and Wang 2013; \\nGÃ³mez-Flores and Ruiz -Ortega 2016; Huang et al. 2017; Liu et al. 2010; Rodrigues et al. 2015; Xiao et al. 2002) , \\nincorporating prior knowledge in deep neural networks has proven to  be a difficult task, and consequently, has not'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='incorporating prior knowledge in deep neural networks has proven to  be a difficult task, and consequently, has not \\nbeen widely investigated. Namely, semantic image segmentation using deep networks typically relies on loss functions \\nthat optimize the model predictions at a pixel level, without taking into consideration inter-pixel interactions and \\nsemantic correlations  among regions at the image level. To integrate prior knowledge into segmentation models, \\nseveral works have proposed custom loss functions that enforce learning feature representations compatible with the \\npriors. For instance, a loss function that penalizes both geometric priors (boundary smoothness) and topological priors \\n(containment or exclusion of lumen in epithelium and stroma) was devised for histology gland segmentation \\n(BenTaieb and Hamarneh 2016). Likewise, loss functions in fully convolutional networks (FCNs) that encode a shape'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='(BenTaieb and Hamarneh 2016). Likewise, loss functions in fully convolutional networks (FCNs) that encode a shape \\nprior were proposed for kidney segmentation (Ravishankar et al. 2017) , cardiac segmentation (Oktay et al. 2018a) , \\nand segmentation of star shapes in skin lesions (Mirikharaji and Hamarneh 2018). The disadvantage of this approach \\nis that the related models are task-specific and cannot be repurposed for segmentation of other objects  of interest in \\nmedical images. Another line of research introduces a post-processing step with Conditional Random Fields where 3 \\n \\nthe segmentation predictions by a deep learning network are improved through assigning class labels to regions with \\nsimilar topological properties (Chen et al. 2018a; Havaei et al. 2017; Huang et al. 2018) . However, these methods \\nincrease the processing complexity  and computational expense , and have been mostly replaced in recent years with'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='increase the processing complexity  and computational expense , and have been mostly replaced in recent years with \\nend-to-end training models. Furthermore, a body of work proposed to incorporate shape priors by redesigning the \\nnetwork architecture. For example, Li et al. (Li et al. 2016)  employed an FCN with a VGG-16 base model where \\nshape priors are learned by a consecutive concatenation of the original images with the obtained segmen tation maps \\nduring several iterations of the procedure. Gonzalez-Diaz (GonzÃ¡lez-DÃ­az 2017) created probability maps based on \\nthe knowledge of the patterns of skin lesions (e.g., dots, globules, streaks, or vascular structures) and merged them \\nwith extracted feature maps in a ResNet-based architecture. Furthermore, a boundary prior was incorporated in a deep \\nlearning model called deep contour-aware network (DCAN) that has two subnetworks for learning concurrently shapes'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='learning model called deep contour-aware network (DCAN) that has two subnetworks for learning concurrently shapes \\nand contour boundaries in histology images (Chen et al. 2017). Yet another class of methods utilizes generative models \\nfor introducing prior knowledge. E.g., in several early pre-FCN image segmentation models, Boltzmann machines \\nnetworks were employed for learning shape priors (Chen et al. 2013; Eslami et al. 2014). A more recent research uses \\nvariational Bayes autoencoders for incorporating prior anatomical knowledge of the brain geometry in segmentation \\nof MRI images (Dalca et al. 2018).  \\nDespite the potential demonstrated by the above-described research work, to the best of our knowledge, there are \\nno previous studies on the incorporation of prior knowledge in deep models for breast cancer detection. The challenge \\nstems from the fact that unlike other medical organs  (e.g., kidney, heart) where shape  or boundary  priors can be'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='stems from the fact that unlike other medical organs  (e.g., kidney, heart) where shape  or boundary  priors can be \\napplied, such constraints ar e not applicable to breast cancer detection, due to the wide difference in the geometry of \\nbreast tumors.  Analogously, it is difficult to extract generalized prior knowledge regarding curvature, moments, \\nappearance, intensity, or number of regions for breast tumors. In this work, we introduce prior topology information \\nin a deep learning segmentation model in the form of region connectivity and visual saliency. Such prior information \\nis combined with anatomical prior knowledge of the tissue layers in breast images, as explained in the subsequent \\nsections.  \\nAttention mechanism in deep learning. Attention mechanism is an approach in deep networks layer design where \\nthe goal is to recognize discriminative features in the inner activation maps  and to utilize this knowledge toward'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='the goal is to recognize discriminative features in the inner activation maps  and to utilize this knowledge toward \\nenhanced task-specific data representation and improved model performance (Simonyan et al. 2013). This mechanism \\ncontributes to suppressing less relevant features and emphasizing more important features for a considered task; e.g., \\nin image classification, important features lie in salient spatial locations in the images.  \\nAttention mechanism has been integrated into various deep learning models designed for image captioning (Li et \\nal. 2018; Xu et al. 2015) , language translation  (Bahdanau et al. 2015) , and image classification (Jetley et al. 2018; \\nWang et al. 2017) . In general, attention in deep neural networks is traditionally impleme nted in two main forms , \\nknown as hard and soft attention. The implementation of hard (or stochastic) attention is non -differentiable, the'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='known as hard and soft attention. The implementation of hard (or stochastic) attention is non -differentiable, the \\ntraining procedure is based on a sampling technique, and as a consequence, the models are difficult to optimize (Cao \\net al. 2015; Mnih et al. 2014; Stollenga et al. 2014) . Soft (or deterministic) attention models are differentiable and \\ntrained with backpropagation; due to these properties, they have been the preferred form of implementation  (Chen et \\nal. 2016; Jaderberg et al. 2015; Wang et al. 2017) . In image proc essing, the attention mechan ism produces a \\nprobabilistic map of spatial locations  in images, where the parameters of the attention map are learned in end -to-end \\ntraining. Furthermore, the introduced architecture designs in image processing typically comprise of multiple attention \\nmaps with different resolutions, thereby capturing salient features across multiple levels of feature abstraction. For'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='maps with different resolutions, thereby capturing salient features across multiple levels of feature abstraction. For \\ninstance, Jetley et al. (Jetley et al. 2018)  introduced attention gates at three i ntermediate layers in a VGG  network, \\nand a weighted combination of the attention maps is used in the last layer for image classification. Chen et al. (Chen \\net al. 2018a) introduce attention blocks in the initial DeepLab model for image segmentation, where attention weights \\nare learned at different scales of a pyramidal feature representation.   \\nSimilar, attention  gates were introduced in a U -Net architecture  (Oktay et al. 2018b)  and were employed in \\nmedical image processing for segmentation of the pancreas (Oktay et al. 2018b), and for breast tumor and skin lesion \\nsegmentation (Abraham and Khan 2019). This type of models uses the extracted features maps in the encoder path of'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='segmentation (Abraham and Khan 2019). This type of models uses the extracted features maps in the encoder path of \\nthe network for calculation of the attention maps, which are afterward merged with the up-sampled features maps in \\nthe decoder network, typically via element-wise multiplication. Such design forces the model to encode the locations \\nand shapes of salient regions in extracted representations that are relevant for segmentation  of the objects of interest. \\nIn the work by Tomita et al. (Tomita et al. 2018) an attention module was implemented in a 3D residual convolutional \\nneural network to dynamically identify regions of interest (ROI) for processing high-resolution microscopy images, \\nthus replacing the common ly used  approach of sliding window ROI selection, and alleviating the computational 4 \\n \\nburden in processing microscopy images. In a related work to the proposed approach, AttentionNet is designed on top'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='burden in processing microscopy images. In a related work to the proposed approach, AttentionNet is designed on top \\nof a ResNeXt encoder-decoder architecture and applies both spatial and channel attention blocks for segmentation of \\nthe anatomical tissue layers in BUS images (Li et al. 2019). Conversely to our method, the authors in (Li et al. 2019) \\ndid not apply AttenionNet for breast tumor detection, as well as they used activations maps of the intermediate layers \\nof the network in the attention blocks. \\n \\nMaterials and Methods \\nThe proposed approach is validated on a data set of 510 breast ultrasound images (Xian et al. 2018a) . The dataset is \\ncollected from three hospitals:  the Second Affiliated Hospital of Harbin Medical University, the Affiliated Hospital \\nof Qingdao University, and the Second Hospital of Hebei Medical University. All images in the dataset  are de -'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='of Qingdao University, and the Second Hospital of Hebei Medical University. All images in the dataset  are de -\\nidentified, and informed consent to the protocol was obtained from all involved patients. Different types of imaging \\nultrasound devices were employed for acquiring the images, including GE VIVID 7  (General Electric Healthcare, \\nChicago, IL, USA ), GE LOGIQ E9 (General Electric Healthcare, Chicago, IL, USA ), Hitachi EUB -6500 (Hitachi \\nMedical Systems, Chiyoda, Japan ), Philips iU22  (Philips Healthcare, Amsterdam, Netherlands ), and Siemens \\nACUSON S2000 (Siemens Healthineers Global, Munich, Germany). GE VIVID 7 and Hitachi EUB-6500 were used \\nfor collecting ultrasound images at Harbin Medical University, GE LOGIQ E9 and Philips iU22 were used at Qingdao \\nUniversity, and Siemens ACUSON S2000 was used at Hebei Medical University. Image annotation related to the'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='University, and Siemens ACUSON S2000 was used at Hebei Medical University. Image annotation related to the \\nsegmentation and delineation of tumors in images was initially performed by three experienced radiologists, followed \\nby voting and creating a single segmentation mask per image on which all three medical professional s agreed. \\nAfterward, the annotations were reviewed by a senior radi ologist expert, who either approved, or if needed, applied \\ncorrections and amendments to the segmentation boundaries (Xian et al. 2018a).    \\n \\nNetwork Architecture \\nThe proposed network is based on the well-known U-Net architecture (Ronneberger et al. 2015) , which consists of \\nfully convolutional encoder and decoder sub-networks with skip connections . The layers in the encoder employ a \\ncascade of convolutional and max-pooling layers, which reduce the resolution of input images and extract increasingly'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='cascade of convolutional and max-pooling layers, which reduce the resolution of input images and extract increasingly \\nabstract features. The decoder comprises convolutional and up -sampling layers that provide  an expanding path for \\nrecovering the spatial resolution of the extracted feature maps to the initial level of the input imag es. A unique \\ncharacteristic of the U -Net architecture is the presence of skip connections from the feature maps in encoderâ€™s \\ncontracting path to the corresponding layers in the decoder. The features from the respective encoderâ€™s and decoderâ€™s \\nlayers are merged via concatenation that allows to recover the spatial accuracy of the objects in images and improves \\nthe resulting segmentation masks.  Namely, although the central layer of the network offers high-level features with \\nsemantic rich data representation  and a large receptive field , it also has low level of spatial context detail due to the'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='semantic rich data representation  and a large receptive field , it also has low level of spatial context detail due to the \\ndown-sampling max-pooling layers along the contracting path, and impacts the localization accuracy around the object \\nboundaries in the predictions . The skip connections provide a means to transmit low-level feature information from \\nthe initial high-resolution layers in the encoder to the reconstructing layers in the decoder, thereby restoring the local \\nspatial information  in predicted segmentations . Despite the introduction of deeper and more powerful models for \\nimage segmentation in recen t years, the U -Net architecture has remained popular especially in medical image \\nsegmentation, where datasets have small size and large models can overfit on the available sets.  \\nA graphical representation of the proposed model is presented in Figure 1. Besides the main input consisting of'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='A graphical representation of the proposed model is presented in Figure 1. Besides the main input consisting of \\nBUS images, t he network has an auxiliary input consisting of the corresponding salient maps. Attention blocks \\nintroduce salient maps with reduced scale in all layers on the contracting path of the encoder in the form of an image \\npyramid. This enforces the network to focus the attention onto regions in the saliency maps with high intensity values. \\nMore specifically, the introduced attention block s put more weights on areas in the extracted feature maps at each \\nlayer that have higher levels of saliency  in the salient maps. Thus, the topology of the salient maps influences the \\nlearned feature representations.  \\nThe images and saliency maps are grey-scale 8-bit data resampled into floating point with normalization. Resized \\nimages and saliency maps to 256 Ã— 256 pixels are used as input s to the model. The number of convolutional filters'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='images and saliency maps to 256 Ã— 256 pixels are used as input s to the model. The number of convolutional filters \\nper layer in the network is (32, 32, 64, 64, 128), which is reduced in comparison to the original U -Net, to account for \\nthe relatively small dataset. The output segmentation probability maps have the same spatial dimension as the inputs. \\nThe proposed network is trained in an end -to-end fashion; however, the saliency maps are precomputed and used at \\nboth training and inference. 5 \\n \\n \\nFigure 1: Architecture of the proposed U -Net model with salient attention. The model uses BUS images and saliency maps as \\ninputs, and produces segmentation probability maps as outputs. \\n \\nAttention Blocks \\nA block diagram of Attention Block n is depicted in Figure 2. The input feature maps to the attention block are denoted \\nð¹ð‘› =  {ð‘“1, ð‘“2, â€¦ , ð‘“ð‘˜ð‘› } , where each feature map has horizontal and vertical spatial dimensions of 256 2(ð‘›âˆ’1)â„ Ã—'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='ð¹ð‘› =  {ð‘“1, ð‘“2, â€¦ , ð‘“ð‘˜ð‘› } , where each feature map has horizontal and vertical spatial dimensions of 256 2(ð‘›âˆ’1)â„ Ã—\\n256 2(ð‘›âˆ’1)â„  pixels for the block in the layer level  ð‘› âˆˆ  {1, 2, 3, 4}. The symbol ð‘˜ð‘› is the channel  dimension of the \\nfeature maps  in block n, i.e., ð‘˜ð‘›  âˆˆ  {32, 32, 64, 64}. For example, the input Feature Maps in Fig. 2 related to the \\noutput activations of the convolutional â€˜Con v 64â€™ layer entering Attention B lock 4 in Fig. 1 have dimensions of \\n32 Ã— 32 Ã— 64 (i.e., for ð‘› = 4, the size of the feature maps ð¹4 is 256 23â„ Ã— 256 23 Ã— ð‘˜4 = 32 Ã— 32 Ã— 64â„ ). \\nThe input Salient Map in Fig. 2 is denoted ð‘† and it is down-sampled through a max-pooling layer, resulting in ð‘†ð‘›, \\nwhich matches the spatial dimension of the input feature maps ð¹ð‘› in Attention Block n.  N ext, 1 Ã— 1 convolutions \\nfollowed by  rectified linear unit  (ReLU) activation functions are used to increase the number of channels of the'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='followed by  rectified linear unit  (ReLU) activation functions are used to increase the number of channels of the \\nsaliency map ð‘†ð‘› to 128. An element-wise sum block performs addition of ð¹ð‘› and ð‘†ð‘› producing intermediate maps ð¼ð‘› \\nof size 256 2ð‘›â„ Ã— 256 2ð‘› Ã— 128.â„  The intermediate maps ð¼ð‘› are further refined through a series of linear 128 Ã— 3 Ã— 3 \\nand 1 Ã— 1 Ã— 1  convolutions, followed by nonlinear ReLU activations. A sigmoid activation function normalizes the \\nvalues of the activation maps into the [0, 1] range.  The produced output is the attention map ð´ = (ð›¼ð‘–) with a spatial \\nsize of 256 2ð‘›â„ Ã— 256 2ð‘› Ã— 1â„ , where the attention coefficients ð›¼ð‘– have scalar values for each pixel i. Next, soft \\nattention is applied via element-wise multiplication of the attention map ð´ with the max-pooled features ð‘ƒð‘›, i.e., ð‘‚ð‘› =\\n ð´ âˆ— ð‘ƒð‘›. The activation maps ð‘‚ð‘› with size 256 2ð‘›â„ Ã— 256 2ð‘› Ã—  ð‘˜ð‘›â„  are the Output of Attention Block n, and they are \\nfurther propagated to the next layer, as depicted in Fig. 1.'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='ð´ âˆ— ð‘ƒð‘›. The activation maps ð‘‚ð‘› with size 256 2ð‘›â„ Ã— 256 2ð‘› Ã—  ð‘˜ð‘›â„  are the Output of Attention Block n, and they are \\nfurther propagated to the next layer, as depicted in Fig. 1. \\nThe design of the attention block was inspired by the attention gate s in (Oktay et al. 2018b)  and (Jetley et al . \\n2018). Differently from these two works, where the attention blocks employ activation maps from the intermediate \\nlayers in the model as saliency maps for enhancing the discriminative characteristics of extracted intermediary \\nfeatures, the proposed attention block in this work utilizes precomputed saliency maps that point out to target spatial \\nregions. If the attention block in this work applies directly the self-attention blocks described in (Abraham and Khan \\n2019; Jetley et al. 2018; Oktay et al. 2018b) , the segmentation performance of the model would not improve. The \\nreason for that lies in the distribution of salient regions in the used maps, since in many images background non-tumor'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='reason for that lies in the distribution of salient regions in the used maps, since in many images background non-tumor \\nareas have certain level of saliency in the salient maps; consequently, placing equal attention weights on all salient \\nregions leads to higher level of false positive errors and degraded performance. The introduction of additional 3 Ã— 3  \\nand 1 Ã— 1   convolutional layers for feature refinement in the proposed salient attention block was conducive toward \\nimproved segmentation outputs, which was confirmed via empirical validation of the proposed layers design.  \\n \\nSaliency Maps \\nVisual saliency estimation is an important paradigm for automatic tumor diagnosis in BUS images, where the aim is \\nto model the level of saliency of image regions in correspondence to the capacity to attract radiologistsâ€™ visual attention \\n(Shao et al. 2015; Xie et al. 2017). For an input image, the output of such models is a visual saliency map with assigned \\n6'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='(Shao et al. 2015; Xie et al. 2017). For an input image, the output of such models is a visual saliency map with assigned \\n6 \\n \\nsaliency values in the [0, 1] range to every image pixel. High saliency value indicates a high probability that the pixel \\nbelongs to a tumor.     \\n \\n \\nFigure 2:  Attention block n, for  ð‘› âˆˆ  {1, 2, 3, 4}. Inputs to the block are feature maps from layer n with spatial dimension \\n256 2(ð‘›âˆ’1)â„ Ã— 256 2(ð‘›âˆ’1)â„  with kn number of channels, and a salient map, and the output are down -sampled weighted maps with \\nspatial dimension 256 2ð‘›â„ Ã— 256 2ð‘›â„  and kn number of channels.  \\n \\nThe adopted approach for generating saliency maps of BUS images is based on our previous work (Xian et al. \\n2016; Xian 2017; Xu et al. 2016; Xu et al. 2018; Xu et al. 2019) . In particular, the task of visual saliency estimation \\nis formulated as a quadratic programming optimization that integrates high -level image information and low -level'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='is formulated as a quadratic programming optimization that integrates high -level image information and low -level \\nsaliency assumptions. The model assigns a saliency value ð‘ ð‘– to each superpixel region  ð‘– in an image. The objective \\nfunction of the model optimizes several terms, as follows. First, one term is a function of a foreground map that \\ncalculates the probability that the ð‘–th image region belongs to a tumor, and the distance between the ð‘–th region and the \\ncenter of the foreground map of the image. Second, another term defines the cost of assigning zero saliency to an \\nimage region, and it employs the connectedness to the boundary regions to calculate the probability of the ð‘–th region \\nbelonging t o a non -tumor image background. A third term applies a penalty if similar regions in the image have \\ndifferent saliency values. The formulation of the above functions is based on our Neutro -Connectedness (NC)'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='different saliency values. The formulation of the above functions is based on our Neutro -Connectedness (NC) \\napproach (Xian, 2017; Xian et al., 2016) that exploits the information of the degree of connectedness and confidence \\nof connectedness between the image regions. The complete set of formulas for derivation of the optimization model \\ncan be found in (Xu et al., 2019) and (Xu et al., 2018). \\nOur most recent work on this topic (Xu et al., 2019) introduces additional constraints in the model related to the \\nbreast anatomy by decomposing the images into four anatomical layers: skin, fat, mammary, and muscle layers. The \\nfour layers have different appearan ces in BUS images, and the fact that tumors are present predominantly in the \\nmammary layer is used in our framework as an anatomical prior for saliency estimation. Two low -level saliency \\nassumptions are utilized in the framework as well: 1) adaptive -center bias assumption forces the regions nearer the'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='assumptions are utilized in the framework as well: 1) adaptive -center bias assumption forces the regions nearer the \\nadaptive center to have higher saliency values; 2) the region-correlation assumption forces the similar regions to have \\nsimilar saliency values.  The extensive experiments in (Xu et al. 2019)  showed the new model wit h anatomical \\nknowledge generated improved performance than other models in related works on the dataset (Xian, 2018b). Another \\nadvantage of the approach proposed in (Xu et al. 2019) is the capability to interpret images without tumors, whereas \\nmany related approaches assume the presence of tumors in each image. Full implementation details can be found in \\nthe respective publications.  \\nExamples of breast images and corresponding saliency maps are presented in Figure 3. The top row in the figure \\nshows five BUS images, and the middle row displays the ground truth segmentation masks provided by radiologists.'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='shows five BUS images, and the middle row displays the ground truth segmentation masks provided by radiologists. \\nThe bottom row displays the saliency maps for the images. One can note that the saliency maps assign a value to every \\n7 \\n \\npixel regarding the probability of belonging to a tumor, and differently from the ground truth  masks, saliency values \\nare assigned to background regions in images as well. Furthermore, the saliency maps are generated in an unsupervised \\nmanner, i.e., the information of the ground truth is not used by the saliency estimation model.  \\nThe incorporation of saliency maps into a  deep learning model as complementary prior information is based on \\nan assumption that the areas in images with high saliency values correspond to a high probability of tumor presence. \\nTherefore, it is important that the saliency maps are of adequate quali ty and provide reliable information regarding'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='Therefore, it is important that the saliency maps are of adequate quali ty and provide reliable information regarding \\nthe tumor locations. Otherwise, poor quality saliency maps can degrade the model performance.  \\nThe selected five examples of saliency maps depicted in Figure 3 have different levels of quality. More \\nspecifically, a map is considered of satisfactory quality when the location and intensity of the tumor region are clearly \\ndiscernable in the saliency map. The example in the middle column in Figure 3 with moderate quality indicates the \\ntumor location correctly, but the tumor shape and boundary do not match very well the ground truth, which may cause \\nerrors in the edge segmentation when applied to a deep network. For the case with low quality in Figure 3 there are \\nseveral regions with similar area and saliency values, and it is not clear which of these regions may be tumors. Lastly,'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='several regions with similar area and saliency values, and it is not clear which of these regions may be tumors. Lastly, \\nthe saliency map with poor quality in Figure 3 assigns zero saliency values to the tumor region and completely misses \\nthe tumor.  \\n \\nFigure 3: Examples of saliency maps with varying level of quality. Top row: original BUS image; Middle row: ground truth mask; \\nBottom row: saliency map. \\n \\nIn order to account for the cases with lower quality of saliency maps, we devised an algorithm that calculates the \\nlevel of confidence in the saliency maps, and subsequently, eliminates the maps with low confidence. The approach \\nis based on the following parameters: contour area ð´ð‘ =  âˆ‘ ð‘ð‘—ð‘—  is the number of pixels of a contour ð‘ in an image with \\na saliency value per pixel ð‘ð‘— greater than a threshold value; cumulative intensity ð¼ð‘ =  âˆ‘ ð‘†(ð‘ð‘—)ð‘—  calculates the sum of'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='a saliency value per pixel ð‘ð‘— greater than a threshold value; cumulative intensity ð¼ð‘ =  âˆ‘ ð‘†(ð‘ð‘—)ð‘—  calculates the sum of \\nthe saliency values for the pixels in contour ð‘; and, mean intensity ð‘€ð‘ =  âˆ‘ ð‘†(ð‘ð‘—)ð‘— ð´ð‘â„  of a contour ð‘ is calculated as \\nthe ratio of the cumulative intensity and the area. The first rule in Algorithm 1 states that if the contour with the largest \\ncumulative intensity ð¼ð‘Žð‘Ÿð‘”ð‘šð‘Žð‘¥(ð¼1:ð‘) has similar cumulative intensity to the second-largest contour and its mean intensity \\nð‘€ð‘Žð‘Ÿð‘”ð‘šð‘Žð‘¥(ð¼1:ð‘) is not the highest of all contours (see Figure 4, left column), then eliminate the saliency map from the \\nset. The second rule is similar to the first rule, and  takes into account cases with larger ambiguities in the cumulative \\nintensity and mean intensity of contours (Figure 4, middle column). The third rule considers the cases when a contour \\nhas high mean saliency intensity but smaller cumulative intensity than other contours in the image (see Figure 4, right'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='has high mean saliency intensity but smaller cumulative intensity than other contours in the image (see Figure 4, right \\ncolumn). The parameters in the algorithm are empirically set to: ð‘Ž1 = 2, ð‘Ž2 = 3, ð‘Ž3 = 0.2, and ð‘Ž4 = 0.55. In total 52 \\nsaliency maps satisfied the given conditions and were removed from the original set of 562 i mages, resulting in a \\nreduced set of 510 images. That is, approximately 91% of the saliency maps are with high level of confidence. Having \\na low level of confidence for a saliency map does not necessarily mean that the saliency map is not correct: e.g., on e \\n8 \\n \\ncan argue that the saliency for the example in the middle column in Figure 4 is correct. Rather, the proposed algorithm \\nis designed to identify saliency maps with ambiguities regarding the spatial regions for tumor existence. The algorithm \\ntakes as inputs only the saliency maps, and it does not use the knowledge of the ground truth in estimating the level \\nof confidence.'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='takes as inputs only the saliency maps, and it does not use the knowledge of the ground truth in estimating the level \\nof confidence. \\n \\n \\n \\n \\nFigure 4: Examples of eliminated saliency maps from the original dataset. Top row: original BUS image; Middle row: ground \\ntruth mask; Bottom row: saliency map.  \\n \\nEvaluation Metrics \\nWe used Dice similarity coefficient (DSC), Jaccard index (JI) , true positives ratio (TPR), false positives ratio (FPR), \\nand global accuracy (ACC) to evaluate the model performance: \\nð·ð‘†ð¶ =\\n2|ð´ð‘”âˆ©ð´ð‘|\\n|ð´ð‘”|+|ð´ð‘|                                                                      (1) \\nð½ð¼ =\\n|ð´ð‘”âˆ©ð´ð‘|\\n|ð´ð‘”âˆªð´ð‘|                                                                          (2) \\nð‘‡ð‘ƒð‘… =\\n|ð´ð‘”âˆ©ð´ð‘|\\n|ð´ð‘”|                                                                        (3) \\nð¹ð‘ƒð‘… =\\n|ð´ð‘”âˆªð´ð‘âˆ’ð´ð‘”|\\n|ð´ð‘”|                                                                     (4) \\nð´ð¶ð¶ =\\n|ð´ð‘”âˆ©ð´ð‘|+|ð´ð‘”Ì…Ì…Ì…Ì…âˆ’ð´ð‘”âˆªð´ð‘|\\n|ð´ð‘”|+|ð´ð‘”Ì…Ì…Ì…Ì…|                                                    (5)'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='ð¹ð‘ƒð‘… =\\n|ð´ð‘”âˆªð´ð‘âˆ’ð´ð‘”|\\n|ð´ð‘”|                                                                     (4) \\nð´ð¶ð¶ =\\n|ð´ð‘”âˆ©ð´ð‘|+|ð´ð‘”Ì…Ì…Ì…Ì…âˆ’ð´ð‘”âˆªð´ð‘|\\n|ð´ð‘”|+|ð´ð‘”Ì…Ì…Ì…Ì…|                                                    (5) \\nAlgorithm 1: Confidence level calculation for saliency maps \\nFor saliency map ð‘– = 1: ð‘ \\nFind all fully connected contours with threshold > 0.3 \\nFor contour ð‘ = 1: ð¶ \\nif ð¼ð‘Žð‘Ÿð‘”ð‘šð‘Žð‘¥(ð¼1:ð‘) < ð‘Ž1ð¼ð‘Žð‘Ÿð‘”ð‘šð‘Žð‘¥(ð¼1:ð‘)âˆ’1  and  ð‘€ð‘Žð‘Ÿð‘”ð‘šð‘Žð‘¥(ð¼1:ð‘) < ð‘€ð‘Žð‘Ÿð‘”ð‘šð‘Žð‘¥(ð‘€1:ð‘) \\nor if ð¼ð‘Žð‘Ÿð‘”ð‘šð‘Žð‘¥(ð¼1:ð‘) < ð‘Ž2ð¼ð‘Žð‘Ÿð‘”ð‘šð‘Žð‘¥(ð¼1:ð‘)âˆ’1  and  ð‘€ð‘Žð‘Ÿð‘”ð‘šð‘Žð‘¥(ð¼1:ð‘) + ð‘Ž3 < ð‘€ð‘Žð‘Ÿð‘”ð‘šð‘Žð‘¥(ð‘€1:ð‘) \\nor if ð‘€ð‘Žð‘Ÿð‘”ð‘šð‘Žð‘¥(ð‘€1:ð‘) > ð‘Ž4  and  ð‘Žð‘Ÿð‘”ð‘šð‘Žð‘¥(ð‘€1:ð‘) â‰  ð‘Žð‘Ÿð‘”ð‘šð‘Žð‘¥(ð¼1:ð‘) \\nRemove saliency map ð‘– from the set 9 \\n \\nIn the above equations, ð´ð‘” is the set of pixels that belong to a tumor region in the ground truth segmented images, ð´ð‘”Ì…Ì…Ì…Ì… \\nis the set of pixels that belong to the background region without tumors in the ground truth segmented images, and ð´ð‘ \\nis the corresponding set of pixel s that are predicted to belong to a tumor region by the segmentation method. It is'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='is the corresponding set of pixel s that are predicted to belong to a tumor region by the segmentation method. It is \\nimportant to note that FPR is calculated as the ratio divided by the number of positives (i.e., pixels in tumor regions \\nin the ground truth masks), as opposed to a ratio divi ded by the number of negatives (i.e., pixels in the background \\nregions in the ground truth masks) as it is often defined in related tasks. Since the positive regions are smaller in BUS \\ntumor segmentation, the selected formulation for FPR is more descriptiv e for this task. Additional metrics that we \\nused for performance evaluation are the area under the curve of receiver operating characteristic  score (AUC-ROC), \\nHausdorff distance (HD), and mean distance (MD).  For most of the above metrics, the values are in  the [0, 1] range, \\nwhere higher values indicate improved performance (except for FPR, HD, and MD, where low values are preferred).'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='where higher values indicate improved performance (except for FPR, HD, and MD, where low values are preferred).  \\nThe differences in the values of the metrics obtained by different models are evaluated with a paired-comparison \\nstatistical hypothesis testing. A null hypothesis assumes that the metrics values are drawn from the same distribution \\nand have a median value equal to zero.  \\n \\nImplementation Details \\nThe proposed approach is validated on the described dataset of BUS images. We used five-fold cross-validation, where \\nfour folds (80% of images) are used for training, and one fold (20% of images) is used for testing. Validation during \\ntraining is performed on 20% of the training set of images. All images in the dataset are first resized to a 256 Ã— 256 \\npixels resolution. Since we focus on understanding the impact of the introduced salient attention  on the model \\nperformance, we did not apply image augmentation.'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='pixels resolution. Since we focus on understanding the impact of the introduced salient attention  on the model \\nperformance, we did not apply image augmentation.  \\nThe proposed model is trained with randomly initialized weights using Xavier normal initialization  (Glorot and \\nBengio 2010). Dice loss function was used for training, defined as  \\nâ„’ = 1 âˆ’ ð·ð‘†ð¶ = 1 âˆ’\\n2|ð´ð‘”âˆ©ð´ð‘|\\n|ð´ð‘”|+|ð´ð‘|                                                            (6) \\nwhere the same notation is preserved, i.e., ð´ð‘” and ð´ð‘ denote the ground truth and predicted masks, respectively. \\nThe models were implemented using TensorFlow (Google, Menlo Park, CA, USA) and Keras (Francois Chollet, \\nMenlo Park, CA, USA)  libraries on the Google Colaboratory  cloud computing services, which employ Tesla K80 \\nGPUs. The network was trained by using adaptive moment estimation optimizer (Adam) with a learning rate of 10-4,'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='GPUs. The network was trained by using adaptive moment estimation optimizer (Adam) with a learning rate of 10-4, \\nand a batch size of 4 images. The training was stopped when the loss of the validation set did not improve for 20 \\nepochs.  \\n \\nResults \\nEvaluation and Comparative Analysis \\nThe experimental validation of the proposed approach is based on a comparative analysis of the following three \\nmodels:  \\n(1) U-Net;  \\n(2) U-Net-SA. It applied the proposed salient attention approach; and  \\n(3) U-Net-SA-C. It is a model with salient attention applied to a modified version where only one contour with \\nthe highest saliency is extracted in each salient map.  \\nExamples of input BUS images, ground truth masks, saliency maps, and output segmentation maps by the models \\nare presented in Figure 5. The values of the performance metrics are provided in Table 1. For the BUS images \\ndisplayed in Figure 5, the segmentation outputs by the U-Net model are inferior in comparison to the predicted masks'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='displayed in Figure 5, the segmentation outputs by the U-Net model are inferior in comparison to the predicted masks \\nproduced by the models with salient attention U -Net-SA and U -Net-SA-C. One particular aspect of improved \\nperformance entails the false positive predictions by U-Net (see rows A-G in Figure 5). In these cases, U-Net produces \\npositive predictions of tumor presence for image regions that donâ€™t belong to a tumor. The attention models U -Net-\\nSA and U-Net-SA-C benefited from the information in the salient maps, which led to a reduced rate of false positive \\npredictions in A-G. This is especially noticeable in rows B, E, and G that have high quality salient maps, resulting in \\ngreat improvement over the predictions by the basic U-Net model.  \\nFurthermore, improved performance with respect to the true positive predictions by  U-Net is displayed for rows \\nH and I in Figure 5. The provision of salient maps for these two cases helps the model to focus on target regions with'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='H and I in Figure 5. The provision of salient maps for these two cases helps the model to focus on target regions with \\nhigh saliency, leading to higher true positives rate of the segmentation masks by U-Net-SA over the basic U -Net \\nmodel. In addition, rows J and K provide examples where the geometry of the salient regions in the saliency maps 10 \\n \\ncontributes to more accurate predictions of the proposed models in comparison to U-Net. Cases C and I are instances \\nof BUS images with small size tumors, where the salient attention models successfully located the tumor regions. As \\nexplained earlier, the U-Net-SA-C model employs salient maps with one contour with the highest saliency intensity, \\nand in many images it further improves the segmentation outputs. This is noticeable in row A in Figure 5, where the \\nfalse positives in the segmentation are reduced in comparison to U-Net-SA. However, U-Net-SA-C model is based on'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='false positives in the segmentation are reduced in comparison to U-Net-SA. However, U-Net-SA-C model is based on \\nan assumption that there is only one tumor in the images, which may not always be the case.  \\nThe results in Table 1 show the average and standard deviation (in parenthesis) per fold  in the five-fold cross-\\nvalidation procedure for the three deep models. The obtained values indicate that the models with salient attention U-\\nNet-SA and U-Net-SA-C outperform the basic U -Net network without attention blocks for all performance metrics. \\nThe model U -Net-SA-C trained on the dataset with a single contour in the salient maps produced improved \\nsegmentation performance in comparison to U-Net-SA. The average training time per fold for the basic U-Net model \\nwas 7.58 minutes, where as the corresponding times for training the salient attention models U -Net-SA and U -Net-'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='was 7.58 minutes, where as the corresponding times for training the salient attention models U -Net-SA and U -Net-\\nSA-C were 8.54 and 8.08 minutes, respectively. Segmentation of the test ing set of images with a trained model took \\n1.09, 1.26, and 1.37  seconds per fold (i.e., 102 images)  for U-Net, U-Net-SA, and U -Net-SA-C, respectively. This \\ntranslates to processing times of 12 milliseconds per image for U-Net-SA and 13 milliseconds per image for U-Net-\\nSA-C.  \\n \\nTable 1: Performance evaluation metrics for models without and with salient attention. The shown values correspond \\nto the average and standard deviation (in parenthesis) per fold in five-fold cross-validation. \\nModel DSC JI (IOU) TPR FPR ACC AUC- \\nROC HD MD \\nU-Net 0.894 \\n(Â±0.013) \\n 0.821  \\n(Â±0.017) \\n0.903 \\n(Â±0.011) \\n0.107  \\n(Â±0.019) \\n0.978  \\n(Â±0.002) \\n0.951  \\n(Â±0.006) \\n4.346 \\n(Â±1.377) \\n0.224 \\n(Â±0.240) \\nU-Net-SA 0.901 \\n(Â±0.013) \\n0.832 \\n(Â±0.014) \\n0.904 \\n(Â±0.016) \\n0.092 \\n(Â±0.008) \\n0.979 \\n(Â±0.001) \\n0.955 \\n(Â±0.002) \\n4.326'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='(Â±0.019) \\n0.978  \\n(Â±0.002) \\n0.951  \\n(Â±0.006) \\n4.346 \\n(Â±1.377) \\n0.224 \\n(Â±0.240) \\nU-Net-SA 0.901 \\n(Â±0.013) \\n0.832 \\n(Â±0.014) \\n0.904 \\n(Â±0.016) \\n0.092 \\n(Â±0.008) \\n0.979 \\n(Â±0.001) \\n0.955 \\n(Â±0.002) \\n4.326 \\n(Â±1.360) \\n0.209 \\n(Â±0.234) \\nU-Net-SA-C 0.905 \\n(Â±0.013) \\n0.838 \\n(Â±0.014) \\n0.910 \\n(Â±0.011) \\n0.089  \\n(Â±0.012) \\n0.980  \\n(Â±0.001) \\n0.957  \\n(Â±0.004) \\n4.271 \\n(Â±1.326) \\n0.201 \\n(Â±0.218) \\n \\nA Wilcoxon signed rank test was adopted for statistical analysis, based on the distribution of the metrics values. \\nThe hypothesis testing results are presented in Table 2. The cells with asterisk indicate rejection of the null hypothesis \\nwith P-value < 0.05. ACC and AUC-ROC metrics are not included in the test since their values are calculated per a \\nfold of 20% of the images, and not per individual images. Accordingly, for almost all metrics there is a statistically \\nsignificant difference in the median values by the proposed models in comparison to U -Net. The exception s are the'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='significant difference in the median values by the proposed models in comparison to U -Net. The exception s are the \\nTPR and HD values between U-Net and U-Net-SA, for which there isnâ€™t a statistically significant difference.   \\n \\nTable 2: Wilcoxon signed rank test of the performance metrics per image. * Statistically significant difference, P-\\nvalue < 0.05. \\nModel DSC JI (IOU) TPR FPR HD MD \\nU-Net and U-Net-SA P = 0.0011* P < 0.0001* P = 0.5822 P < 0.001* P = 0.2592 P < 0.001* \\nU-Net and U-Net-SA-C P < 0.0001* P < 0.0001* P = 0.0052* P = 0.0098* P = 0.0345* P < 0.001* \\n \\nNext, a comparison of our salient attention model for tumor segmentation U-Net-SA and three respective deep \\nmodels for image segmentation is provided in Table 3. The dataset with 510 images is used for training the models. \\nFor a fair comparison, all models are trained in the same manner as th e proposed architecture, i.e., five -fold cross-'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='For a fair comparison, all models are trained in the same manner as th e proposed architecture, i.e., five -fold cross-\\nvalidation, batch size of 4, Xavier normal weights initialization, dice loss, Adam optimizer, and a stopping criterion \\nof 20 epochs of non -improved validation loss. Due to the relatively small size of the dataset, for the comparison we \\nselected smaller versions of the models. For instance, DenseNet is based on a network with 26 layers, and for PSPNet \\n(that requires a base model) the small residual model ResNet18 is employed. The learning rate is fine -tuned for the \\ndifferent models, where an initial learning rate is selected,  and when the validation loss does not improve for 10 \\nepochs, the learning rate is reduced by a certain step size. The procedure is repeated until a preset value for the learning \\nrate is reached. T he details regarding the used learning rates for the different models are provided in Table 3. Our'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='rate is reached. T he details regarding the used learning rates for the different models are provided in Table 3. Our \\nproposed U-Net-SA model listed last in the table  outperformed the other deep learning networks for image \\nsegmentation on most of the employed performance metrics. 11 \\n \\n \\nFigure 5: Segmentation results. First column: original BUS image; Second column: ground truth mask; Third column: saliency \\nmap; Fourth column: segmentation mask produced by U -Net; Fifth column: segmentation mask produced by U -Net-SA; Sixth \\ncolumn: segmentation mask produced by U-Net-SA-C. \\n12 \\n \\nTable 3: Values of the performance metrics for tumor segmentation by different models. The shown values correspond \\nto the average and standard deviation (in parenthesis) per fold in five -fold cross-validation. LR represents t he used \\nlearning rates for training the models. \\nModel Training Setting DSC JI (IOU) TPR FPR ACC AUC- \\nROC \\nSeg-Net LR=8Â·10-4, decreased by 0.5 \\nafter 10 epochs until 1Â·10-4'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='learning rates for training the models. \\nModel Training Setting DSC JI (IOU) TPR FPR ACC AUC- \\nROC \\nSeg-Net LR=8Â·10-4, decreased by 0.5 \\nafter 10 epochs until 1Â·10-4 \\n0.889 \\n(Â±0.011) \\n0.811 \\n(Â±0.015) \\n0.877 \\n(Â±0.019) \\n0.088 \\n(Â±0.014)  \\n0.977 \\n(Â±0.002)  \\n0.957 \\n(Â±0.004) \\nDenseNet-26 LR=1Â·10-3, decreased by 0.1 \\nafter 10 epochs until 1Â·10-4 \\n0.888 \\n(Â±0.016) \\n0.818 \\n(Â±0.017) \\n0.886 \\n(Â±0.019) \\n0.093 \\n(Â±0.025) \\n0.978 \\n(Â±0.002) \\n0.958 \\n(Â±0.005) \\nPSPNet-ResNet18 \\nLR=1Â·10-4, decreased by 0.5 \\nafter 10 epochs until 5 Â·10-5, \\nimages size of 384x384 pix. \\n0.886 \\n(Â±0.008) \\n0.808 \\n(Â±0.008) \\n0.884 \\n(Â±0.014) \\n0.107 \\n(Â±0.016) \\n0.976 \\n(Â±0.002) \\n0.953 \\n(Â±0.005) \\nOurs: U-Net-SA LR = 1Â·10-4 0.901 \\n(Â±0.013) \\n0.832 \\n(Â±0.014) \\n0.904 \\n(Â±0.016) \\n0.092 \\n(Â±0.008) \\n0.979 \\n(Â±0.001) \\n0.955 \\n(Â±0.002) \\n \\nTable 4 provides the values of the performance metrics for the models on the original dataset of 562 images. In'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='0.832 \\n(Â±0.014) \\n0.904 \\n(Â±0.016) \\n0.092 \\n(Â±0.008) \\n0.979 \\n(Â±0.001) \\n0.955 \\n(Â±0.002) \\n \\nTable 4 provides the values of the performance metrics for the models on the original dataset of 562 images. In \\ncomparison to the values presented in Table 1 on the reduced dataset of 510 images, the results  in Table 4 indicate \\nthat the performance s of the proposed attention enriched models U -Net-SA and U -NET-SA-C are reduced on the \\noriginal dataset. Moreover, the basic U -Net model without salient attention ha s also reduced performance on the \\ndataset of 562 images, which implies that the subset of 52 images that were removed from the original dataset contains \\nbreast tumors that are more challenging for segmentation in general. In conclusion, the algorithm for determining the \\nlevel of confidence of the saliency maps contributed to improved performance on the reduced dataset of 510 images, \\nby ensuring that the model predictions are not inhibited by poor data.'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='level of confidence of the saliency maps contributed to improved performance on the reduced dataset of 510 images, \\nby ensuring that the model predictions are not inhibited by poor data.  \\n \\nTable 4: Performance evaluation metrics for the models on the original dataset of 562 images. The shown values \\ncorrespond to the average and standard deviation (in parenthesis) per fold in five -fold cross-validation. The values in \\nbold font indicate the best performance for each metric. \\nModel DSC JI (IOU) TPR FPR ACC AUC- \\nROC \\nU-Net 0.891 \\n(Â±0.005) \\n0.817 \\n(Â±0.008) \\n0.900 \\n(Â±0.009) \\n0.120 \\n(Â±0.027) \\n0.977 \\n(Â±0.002) \\n0.950 \\n(Â±0.006) \\nU-Net-SA 0.894 \\n(Â±0.006) \\n0.824 \\n(Â±0.008) \\n0.901 \\n(Â±0.017) \\n0.111 \\n(Â±0.032) \\n0.978 \\n(Â±0.002) \\n0.952 \\n(Â±0.012) \\nU-Net-SA-C 0.896 \\n(Â±0.007) \\n0.825 \\n(Â±0.010) \\n0.899 \\n(Â±0.020) \\n0.106 \\n(Â±0.025) \\n0.978 \\n(Â±0.002) \\n0.955 \\n(Â±0.010) \\n \\nDiscussion \\nBased on the evaluation results presented in Table 1, the models with attention blocks outperformed the basic U-Net'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='0.899 \\n(Â±0.020) \\n0.106 \\n(Â±0.025) \\n0.978 \\n(Â±0.002) \\n0.955 \\n(Â±0.010) \\n \\nDiscussion \\nBased on the evaluation results presented in Table 1, the models with attention blocks outperformed the basic U-Net \\nmodel. In addition, if only one contour with the highest saliency is extracted in the saliency maps  (the U-Net-SA-C \\nmodel), the performance improves further. This can be explained by the increasing spatial attention to a single salient \\nregion in the map s, resulting in reduced false positives in the outputs.  As we mentioned earlier, this is based on an \\nassumption that there is only one tumor in the images, which may not always be the case.  \\nThe design of the  attention blocks has an impact on the segmentation output; therefore, we investigated several \\nalternatives for the block layers and their parameters. Compared to similar attention blocks in deep models  (Chen et'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='alternatives for the block layers and their parameters. Compared to similar attention blocks in deep models  (Chen et \\nal. 2016; Jetley et al. 2018; Oktay et al. 2018b) , the used block in this work requires additional feature refinement by \\nusing convolutional 3Ã—3 and 1Ã—1 layers. T he refinement layers balance the impact of inaccurate boundaries of the \\nregions in salient maps on the learned features. In other words, the saliency maps do not provide accurate local \\ninformation of the edges and boundaries of tumors in images, but rather, they provide global information of the spatial \\nprobability regarding the presence of tumors. Larger values of the attention coefficients put more emphasis on the \\nedges and boundaries in salient maps and can reduce the segmentation outputs. The use of additional refinement layers \\nlessens the values of the attention coefficients and results in improved tumor segmentation. 13'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='lessens the values of the attention coefficients and results in improved tumor segmentation. 13 \\n \\nThe fact that the ultrasound images for validation of the approach were collected with various imaging systems \\nis a strength of the paper, as it makes the dataset suitable for training data-driven models with enhanced robustness to \\nvariations across images from different sources. \\nOne limitation of the presented approach is that it relies on the quality of saliency maps. Using low quality maps \\ncan at best not improve the results, or result in degraded performance. To deal with this shortcoming, we proposed an \\nalgorithm that calculates a confidence score and eliminates the saliency maps with low confidence  in their level of \\nquality. Whereas visual saliency estimation is not the focus  of this work, improvements in the models for visual \\nsaliency estimation can lead to improved segmentation by the proposed approach.'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='quality. Whereas visual saliency estimation is not the focus  of this work, improvements in the models for visual \\nsaliency estimation can lead to improved segmentation by the proposed approach. \\nAvenues for futur e work include investigation of custom loss functions in deep learning models for encoding \\nprior information, and working with medical partners to obtain annotated images with breast tissue layers and \\nafterward integrating such anatomical prior with salient maps in a unified segmentation model.  \\n \\nConclusion \\nThis paper proposes a novel deep learning architecture that incorporates radiologistsâ€™ visual attention for breast tumor \\nsegmentation. The proposed architecture consists of a variant of the basic U-Net model with attention blocks integrated \\nalong the contracting path in the layers of the encoder. The proposed attention blocks allow the deep learning model'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='along the contracting path in the layers of the encoder. The proposed attention blocks allow the deep learning model \\nto suppress spatial regions with low saliency values, and respectively, to focus on regions with high saliency values.  \\nThe attention blocks use multi -scaled versions of the saliency maps . The approach is validat ed on a dataset of 510 \\nimages, and the results demonstrate improved segmentation performance. The importance of this work stems from the \\ndifficulties in incorporating priors in to deep learning models for medical image processing, and in particular  for \\nsegmentation of breast ultrasound images, where most of the traditionally used prior forms cannot be applied.  \\n \\nAcknowledgments \\nThis work was supported by the Center for Modeling Complex Interactions (CMCI) at the University of Idaho through \\nNIH Award #P20GM104420. We would like to thank Fei Xu for providing the visual saliency maps from her latest'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='NIH Award #P20GM104420. We would like to thank Fei Xu for providing the visual saliency maps from her latest \\nresearch and for her constructive feedback and review of the paper. \\n \\nReferences \\nAbraham N, Kha n NM. A Novel Focal Tversky Loss Function With Improved Attention U -Net for Lesion Segmentation. 2019 IEEE 16th \\nInternational Symposium on Biomedical Imaging (ISBI 2019) 2019. pp. 683â€“687.  \\nAkselrod-Ballin A, Karlinsky L, Hazan A, Bakalo R, Horesh AB, Shos han Y, Barkan E. Deep Learning for Automatic Detection of Abnormal \\nFindings in Breast Mammography. Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support - Third \\nInternational Workshop, DLMIA 2017, and 7th International Workshop, ML-CDS 2017, Held in Conjunction with MICCAI 2017, QuÃ©bec \\nCity, QC, Canada, September 14, 2017, Proceedings 2017. pp. 321â€“329.'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='International Workshop, DLMIA 2017, and 7th International Workshop, ML-CDS 2017, Held in Conjunction with MICCAI 2017, QuÃ©bec \\nCity, QC, Canada, September 14, 2017, Proceedings 2017. pp. 321â€“329.  \\nBadrinarayanan V, Kendall A, Cipolla R. SegNet: A Deep Convolutional Encoder -Decoder Architecture for Image Segment ation. IEEE Trans \\nPattern Anal Mach Intell 2017;39:2481â€“2495.  \\nBahdanau D, Cho K, Bengio Y. Neural Machine Translation by Jointly Learning to Align and Translate. In: Bengio Y, LeCun Y, ed s. 3rd \\nInternational Conference on Learning Representations, ICLR 20 15, San Diego, CA, USA, May 7 -9, 2015, Conference Track Proceedings \\n2015.  \\nBenTaieb A, Hamarneh G. Topology Aware Fully Convolutional Networks for Histology Gland Segmentation. In: Ourselin S, Joskowi cz L, \\nSabuncu MR, Ãœnal GB, Wells W, eds. Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016 - 19th International'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='Sabuncu MR, Ãœnal GB, Wells W, eds. Medical Image Computing and Computer-Assisted Intervention - MICCAI 2016 - 19th International \\nConference, Athens, Greece, October 17-21, 2016, Proceedings, Part II 2016. pp. 460â€“468.  \\nCai L, Wang Y. A phase -based active contour model for segmentation of breast ultrasound  images. 6th International Conference on Biomedical \\nEngineering and Informatics, BMEI 2013, Hangzhou, China, December 16-18, 2013 2013. pp. 91â€“95.  \\nCao C, Liu X, Yang Y, Yu Y, Wang J, Wang Z, Huang Y, Wang L, Huang C, Xu W, Ramanan D, Huang TS. Look and Th ink Twice: Capturing \\nTop-Down Visual Attention with Feedback Convolutional Neural Networks. 2015 IEEE International Conference on Computer Vision, ICCV \\n2015, Santiago, Chile, December 7-13, 2015 IEEE Computer Society, 2015. pp. 2956â€“2964.  \\nChen F, Yu H, Hu  R, Zeng X. Deep Learning Shape Priors for Object Segmentation. 2013 IEEE Conference on Computer Vision and Pattern'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='Chen F, Yu H, Hu  R, Zeng X. Deep Learning Shape Priors for Object Segmentation. 2013 IEEE Conference on Computer Vision and Pattern \\nRecognition, Portland, OR, USA, June 23-28, 2013 IEEE Computer Society, 2013. pp. 1870â€“1877.  \\nChen H, Qi X, Yu L, Dou Q, Qin J, Heng P -A. DCAN: Deep contour-aware networks for object instance segmentation from histology images. \\nMedical Image Analysis 2017;36:135â€“146.  \\nChen L-C, Papandreou G, Kokkinos I, Murphy K, Yuille AL. DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atr ous \\nConvolution, and Fully Connected CRFs. IEEE Trans Pattern Anal Mach Intell 2018a;40:834â€“848.  \\nChen L-C, Yang Y, Wang J, Xu W, Yuille AL. Attention to Scale: Scale-Aware Semantic Image Segmentation. 2016 IEEE Conference on Computer \\nVision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016 IEEE Computer Society, 2016. pp. 3640â€“3649.'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016 IEEE Computer Society, 2016. pp. 3640â€“3649.  \\nChen L-C, Zhu Y, Papandreou G, Schroff F, Adam H. Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation. In: \\nFerrari V, Hebe rt M, Sminchisescu C, Weiss Y, eds. Computer Vision - ECCV 2018 - 15th European Conference, Munich, Germany, \\nSeptember 8-14, 2018, Proceedings, Part VII Springer, 2018b. pp. 833â€“851.  \\nCheng H-D, Shan J, Ju W, Guo Y, Zhang L. Automated breast cancer detecti on and classification using ultrasound images: A survey. Pattern \\nRecognition 2010;43:299â€“317.  14 \\n \\nChiang T-C, Huang Y-S, Chen R-T, Huang C-S, Chang R-F. Tumor Detection in Automated Breast Ultrasound Using 3 -D CNN and Prioritized \\nCandidate Aggregation. IEEE Trans Med Imaging 2019;38:240â€“249.  \\nDalca AV, Guttag JV, Sabuncu MR. Anatomical Priors in Convol utional Networks for Unsupervised Biomedical Segmentation. 2018 IEEE'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='Candidate Aggregation. IEEE Trans Med Imaging 2019;38:240â€“249.  \\nDalca AV, Guttag JV, Sabuncu MR. Anatomical Priors in Convol utional Networks for Unsupervised Biomedical Segmentation. 2018 IEEE \\nConference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018 IEEE Computer Society, \\n2018. pp. 9290â€“9299. \\nDhungel N, Carneiro G, Bradley AP. Deep Learning and Structured Prediction for the Segmentation of Mass in Mammograms. In: Navab N, \\nHornegger J, III WMW, Frangi AF, eds. Medical Image Computing and Computer-Assisted Intervention - MICCAI 2015 - 18th International \\nConference Munich, Germany, October 5-9, 2015, Proceedings, Part I Springer, 2015. pp. 605â€“612.  \\nElmore JG, Wells CK, Lee CH, Howard DH, Feinstein AR. Variability in Radiologistsâ€™ Interpretations of Mammograms. New England Journal of \\nMedicine 1994;331:1493â€“1499.'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='Elmore JG, Wells CK, Lee CH, Howard DH, Feinstein AR. Variability in Radiologistsâ€™ Interpretations of Mammograms. New England Journal of \\nMedicine 1994;331:1493â€“1499.  \\nEslami SMA, Heess N , Williams CKI, Winn JM. The Shape Boltzmann Machine: A Strong Model of Object Shape. International Journal of \\nComputer Vision 2014;107:155â€“176.  \\nGlorot X, Bengio Y. Understanding the difficulty of training deep feedforward neural networks. Proceedings of the Thirteenth International \\nConference on Artificial Intelligence and Statistics, AISTATS 2010, Chia Laguna Resort, Sardinia, Italy, May 13-15, 2010. pp. 249â€“256.  \\nGÃ³mez-Flores W, Ruiz-Ortega BA. New Fully Automated Method for Segmentation of Breast Lesions on Ultrasound Based on Texture Analysis. \\nUltrasound in Medicine & Biology 2016;42:1637â€“1650.  \\nGonzÃ¡lez-DÃ­az I. Incorporating the Knowledge of Dermatologists to Convolutional Neural Networks for the Diagn osis of Skin Lesions. CoRR \\n2017;abs/1703.01976.'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='GonzÃ¡lez-DÃ­az I. Incorporating the Knowledge of Dermatologists to Convolutional Neural Networks for the Diagn osis of Skin Lesions. CoRR \\n2017;abs/1703.01976.  \\nGoodfellow I, Bengio Y, Courville A. Deep Learning. The MIT Press, 2016.  \\nGraham S, Vu QD, Raza S e A, Kwak JT, Rajpoot NM. XY Network for Nuclear Segmentation in Multi -Tissue Histology Images. CoRR \\n2018;abs/1812.06499.  \\nHavaei M, Davy A, Warde -Farley D, Biard A, Courville AC, Bengio Y, Pal C, Jodoin P -M, Larochelle H. Brain tumor segmentation with Deep \\nNeural Networks. Medical Image Analysis 2017;35:18â€“31.  \\nHe K, Zhang X, Ren S, Sun J. Deep Residual Learning for Image Recognition. arXiv:151203385 [cs] 2015. \\nHu S, Worrall DE, Knegt S, Veeling BS, Huisman H, Welling M. Supervised Uncertainty Quantification for Segmentation with Mult iple \\nAnnotations. CoRR 2019;abs/1907.01949. \\nHuang K, Cheng H -D, Zhang Y, Zhang B, Xing P, Ning C. Medical Knowledge Constrained Se mantic Breast Ultrasound Image Segmentation.'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='Annotations. CoRR 2019;abs/1907.01949. \\nHuang K, Cheng H -D, Zhang Y, Zhang B, Xing P, Ning C. Medical Knowledge Constrained Se mantic Breast Ultrasound Image Segmentation. \\n24th International Conference on Pattern Recognition, ICPR 2018, Beijing, China, August 20 -24, 2018 IEEE Computer Society, 2018. pp. \\n1193â€“1198.  \\nHuang Q, Luo Y, Zhang Q. Breast ultrasound image segmentation: a survey. Int J Comput Assist Radiol Surg 2017;12:493â€“507.  \\nInoue K, Yamanaka C, Kawasaki A, Koshimizu K, Sasaki T, Doi T. Computer Aided Detection of Breast Cancer on Ultrasound Imagin g Using \\nDeep Learning. Ultrasound in Medicine and Biology 2017;43:S19.  \\nJaderberg M, Simonyan K, Zisserman A, Kavukcuoglu K. Spatial Transformer Networks. In: Cortes C, Lawrence ND, Lee DD, Sugiyama M, \\nGarnett R, eds. Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems 2015, \\nDecember 7-12, 2015, Montreal, Quebec, Canada 2015. pp. 2017â€“2025.'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='December 7-12, 2015, Montreal, Quebec, Canada 2015. pp. 2017â€“2025.  \\nJaeger PF, Kohl SAA, Bickelhaupt S, Isensee F, Kuder TA, Schlemmer H-P, Maier-Hein KH. Retina U-Net: Embarrassingly Simple Exploitation \\nof Segmentation Supervision for Medical Object Detection. CoRR 2018;abs/1811.08661.  \\nJalalian A, Mashohor S, Mahmud R, Karasfi B, Saripan MIB, Ramli ARB. Foundation and methodologies in computer -aided diagnosis systems \\nfor breast cancer detection. EXCLI J 2017;16:113â€“137.  \\nJetley S, Lord NA, Lee N, Torr PHS. Learn to Pay Attention. 6th International Conference on Learning Representations, ICLR 2018, Vancouver, \\nBC, Canada, April 30 - May 3, 2018, Conference Track Proceedings OpenReview.net, 2018.  \\nJung H, Kim B, Lee I, Yoo M, Lee J, Ham S, Woo O, Kang J. Detection of masses in mammograms usi ng a one-stage object detector based on a \\ndeep convolutional neural network. PLOS ONE 2018;13:e0203355.'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='deep convolutional neural network. PLOS ONE 2018;13:e0203355.  \\nKamnitsas K, Bai W, Ferrante E, McDonagh SG, Sinclair M, Pawlowski N, Rajchl M, Lee M, Kainz B, Rueckert D, Glocker B. Ensemb les of \\nMultiple Models and  Architectures for Robust Brain Tumour Segmentation. In: Crimi A, Bakas S, Kuijf HJ, Menze BH, Reyes M, eds. \\nBrainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries - Third International Workshop, BrainLes 2017, Held in \\nConjunction with MICCAI 2017, Quebec City, QC, Canada, September 14, 2017, Revised Selected Papers Springer, 2017. pp. 450â€“462.  \\nKooi T, Litjens GJS, Ginneken B van, Gubern -MÃ©rida A, SÃ¡nchez CI, Mann R, Heeten A den, Karssemeijer N. Large scale deep learning for \\ncomputer aided detection of mammographic lesions. Medical Image Analysis 2017;35:303â€“312.  \\nKumar N, Verma R, Sharma S, Bhargava S, Vahadane A, Sethi A. A Dataset and a Technique for Generalized Nuclear Segmentation f or'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='Kumar N, Verma R, Sharma S, Bhargava S, Vahadane A, Sethi A. A Dataset and a Technique for Generalized Nuclear Segmentation f or \\nComputational Pathology. IEEE Transactions on Medical Imaging 2017;36:1550â€“1560.  \\nLanglotz CP, Allen B, Erickson BJ, Kalpathy -Cramer J, Bigelow K, Cook TS, Flanders AE, Lungren MP, Mendelson DS, Rudie JD, Wang G, \\nKandarpa K. A Roadmap for Foundational Research on Artificial Intelligence in  Medical I maging: From the 2018 NIH/RSNA/ACR/The \\nAcademy Workshop. Radiology 2019;291:781â€“791.  \\nLeCun Y, Bengio Y, Hinton G. Deep learning. Nature 2015;521:436â€“444.  \\nLi H, Cheng J-Z, Chou Y-H, Qin J, Huang S, Lei B. AttentionNet: Learning Where to Focus via Attention Mechanism for Anatomical Segmentation \\nof Whole Breast Ultrasound Images. 16th IEEE International Symposium on Biomedical Imaging, ISBI 2019, Venice, Italy, April 8-11, 2019 \\nIEEE, 2019. pp. 1078â€“1081.'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='of Whole Breast Ultrasound Images. 16th IEEE International Symposium on Biomedical Imaging, ISBI 2019, Venice, Italy, April 8-11, 2019 \\nIEEE, 2019. pp. 1078â€“1081.  \\nLi K, Hariharan B, Malik J. Iterative Instance Segmen tation. 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, \\nLas Vegas, NV, USA, June 27-30, 2016 IEEE Computer Society, 2016. pp. 3659â€“3667.  \\nLi L, Tang S, Zhang Y, Deng L, Tian Q. GLA: Global â€“Local Attention for Image Description. IEEE Transactions on Multimedia 2018;20:726 â€“\\n737.  \\nLin G, Milan A, Shen C, Reid ID. RefineNet: Multi -path Refinement Networks for High -Resolution Semantic Segmentation . 2017 IEEE \\nConference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017 IEEE Computer Society, 2017. \\npp. 5168â€“5177.  15 \\n \\nLin H, Chen H, Dou Q, Wang L, Qin J, Heng P-A. ScanNet: A Fast and Dense Scanning Framework for Metastastic Breast Cancer Detection from'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='pp. 5168â€“5177.  15 \\n \\nLin H, Chen H, Dou Q, Wang L, Qin J, Heng P-A. ScanNet: A Fast and Dense Scanning Framework for Metastastic Breast Cancer Detection from \\nWhole-Slide Image. 2018 IEEE Winter Conference on Applications of Computer Vision, WACV 2018, Lake Tahoe, NV, USA, March 12 -\\n15, 2018 IEEE Computer Society, 2018. pp. 539â€“546.  \\nLitjens GJS, Kooi T, Bejnordi BE, Setio AAA, Ciompi F, Ghafoorian M, Laak JAWM van der, Ginneken B van, SÃ¡nchez CI. A survey on deep \\nlearning in medical image analysis. Medical Image Analysis 2017;42:60â€“88.  \\nLiu B, Cheng H-D, Huang J, Tian J, Tang X, Liu J. Fully automatic and segmentation-robust classification of breast tumors based on local texture \\nanalysis of ultrasound images. Pattern Recognition 2010;43:280â€“298.  \\nLong J, Shelhamer E, Darrell T. Fully convolutional networks for semantic segmentation. IEEE Conference on Computer Vision an d Pattern'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='Long J, Shelhamer E, Darrell T. Fully convolutional networks for semantic segmentation. IEEE Conference on Computer Vision an d Pattern \\nRecognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015 IEEE Computer Society, 2015. pp. 3431â€“3440.  \\nMirikharaji Z, Hamarneh G. Star Shape Prior in Fully Convolutional Networks for Skin Lesion Segmentation. In: Frangi AF, Schn abel JA, \\nDavatzikos C, Alberola-LÃ³pez C, Fichtinger G, eds. Medical Image Computing and Computer Assisted Intervention - MICCAI 2018 - 21st \\nInternational Conference, Granada, Spain, September 16-20, 2018, Proceedings, Part IV Springer, 2018. pp. 737â€“745.  \\nMnih V, Heess N, Graves A, Kavukcuoglu K. Recurrent Models of Visual Attention. In: Ghahramani Z, Welling M, Cortes C, Lawrence ND, \\nWeinberger KQ, eds. Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems \\n2014, December 8-13 2014, Montreal, Quebec, Canada 2014. pp. 2204â€“2212.'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='2014, December 8-13 2014, Montreal, Quebec, Canada 2014. pp. 2204â€“2212.  \\nMoon WK, Shen Y-W, Huang C-S, Chiang L-R, Chang R-F. Computer-aided diagnosis for the classification of breast masses in automated whole \\nbreast ultrasound images. Ultrasound Med Biol 2011;37:539â€“548.  \\nMoor T de, RodrÃ­guez-Ruiz A, Mann RM, Teuwen J. Automated soft tissue lesion detection and segmentation in digital mammography using a u-\\nnet deep learning network. ArXiv 2018;abs/1802.06865.  \\nNaylor P, Lae M, Reyal F, Walter T. Segmentation of Nuclei in Histopathology Images by Deep Regression of the Distance Map. IEEE Trans Med \\nImaging 2019;38:448â€“459.  \\nNosrati MS, Hamarneh G. Incorporating prior knowledge in medical image segmentation: a survey. CoRR 2016;abs/1607.01092.  \\nOktay O, Ferrante E, Kamnitsas  K, Heinrich MP, Bai W, Caballero J, Cook SA, Marvao A de, Dawes T, Oâ€™Regan DP, Kainz B, Glocker B,'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='Oktay O, Ferrante E, Kamnitsas  K, Heinrich MP, Bai W, Caballero J, Cook SA, Marvao A de, Dawes T, Oâ€™Regan DP, Kainz B, Glocker B, \\nRueckert D. Anatomically Constrained Neural Networks (ACNNs): Application to Cardiac Image Enhancement and Segmentation. IEEE  \\nTrans Med Imaging 2018a;37:384â€“395.  \\nOktay O, Schlemper J, Folgoc LL, Lee M, Heinrich M, Misawa K, Mori K, McDonagh S, Hammerla NY, Kainz B, Glocker B, Rueckert D . \\nAttention U-Net: Learning Where to Look for the Pancreas. 1st Conference on Medical Imaging with Deep Learning (MIDL), Ams terdam, \\nThe Netherlands 2018b. pp. 1â€“10.  \\nRavishankar H, Venkataramani R, Thiruvenkadam S, Sudhakar P, Vaidya V. Learning and Incorporating Shape Models for Semantic Segmentation. \\nIn: Descoteaux M, Maier -Hein L, Franz AM, Jannin P, Collins DL, Duchesne S, eds. Medical Image Computing and Computer Assisted'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='In: Descoteaux M, Maier -Hein L, Franz AM, Jannin P, Collins DL, Duchesne S, eds. Medical Image Computing and Computer Assisted \\nIntervention - MICCAI 2017 - 20th International Conference, Quebec City, QC, Canada, September 11-13, 2017, Proceedings, Part I Springer, \\n2017. pp. 203â€“211.  \\nRibli D, HorvÃ¡th A, Unger Z, Pollner P, Csabai I. Detecting and classifying lesions in mammograms with Deep Learning. CoRR \\n2017;abs/1707.08401.  \\nRodrigues R, Braz R, Pereira M, Moutinho J, Pinheiro AMG. A Two -Step Segmentation Method for Breast Ultrasound Masses Based on Multi -\\nresolution Analysis. Ultrasound in Medicine & Biology 2015;41:1737â€“1748.  \\nRonneberger O, Fischer P, Brox T. U -Net: Convolutional Networks for Biomedical Image Segmentation. In: Navab N, Hornegger J, III WMW, \\nFrangi AF, eds. Medical Image Computing and Computer -Assisted Intervention - MICCAI 2015 - 18th International Conference Munich, \\nGermany, October 5 - 9, 2015, Proceedings, Part III Springer, 2015. pp. 234â€“241.'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='Germany, October 5 - 9, 2015, Proceedings, Part III Springer, 2015. pp. 234â€“241.  \\nShao H, Zhang Y, Xian M, Cheng H -D, Xu F, Ding J. A saliency model for automated tumor detection in breast ultrasound images. 2015 IEEE \\nInternational Conference on Image Processing, ICIP 2015, Quebec City, QC, Canada, September 27-30, 2015. pp. 1424â€“1428.  \\nSimonyan K, Vedaldi A, Zisserman A. Deep Inside Convolutional Networks: Visualising Image Classification Models and S aliency Maps. \\narXiv:13126034 [cs] 2013. \\nStollenga MF, Masci J, Gomez FJ, Schmidhuber J. Deep Networks with Internal Selective Attention through Feedback Connections.  In: \\nGhahramani Z, Welling M, Cortes C, Lawrence ND, Weinberger KQ, eds. Advances in Neural Information Processing Systems 27: Annual \\nConference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada 2014. pp. 3545â€“3553.'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada 2014. pp. 3545â€“3553.  \\nTomita N, Abdollahi B, Wei J, Ren B, Suriawinata AA, Hassanpour S. Finding a Needle in the Haystack: Attention-Based Classification of High \\nResolution Microscopy Images. CoRR 2018;abs/1811.08513.  \\nWaite S, Scott J, Gale B, Fuchs T, Kolla S, Reede D. Interpretive Error in Radiology. American Journal of Roentgenology 2016;208:739â€“749.  \\nWang F, Jiang M, Qian C, Yang S, Li C, Zhang H, Wang X, Tang X. Residual Attention Network for Image Classification. 2017 IEEE Conference \\non Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017 IEEE Computer Society, 2017. pp. 6450â€“\\n6458.  \\nWu N, Phang J, Park J, Shen Y, Huang Z, Zorin M, Jastrzebski S, FÃ©vry T, Katsnelson J, Kim E, Wolfson S, Parikh U, Gaddam S, Lin LLY, Ho'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='6458.  \\nWu N, Phang J, Park J, Shen Y, Huang Z, Zorin M, Jastrzebski S, FÃ©vry T, Katsnelson J, Kim E, Wolfson S, Parikh U, Gaddam S, Lin LLY, Ho \\nK, Weinstein JD, Reig B, Gao Y, Toth H, Pysarenko K, Lewin A, Lee J, Airola K, Mema E, Chung S, Hwang E, Samr een N, Kim SG, \\nHeacock L, Moy L, Cho K, Geras KJ. Deep Neural Networks Improve Radiologistsâ€™ Performance in Breast Cancer Screening. CoRR \\n2019;abs/1903.08297.  \\nXian M. Neutro-Connectedness Theory, Algorithms and Applications. PhD Thesis 2017;101.  \\nXian M, Zhang Y, Cheng H, Xu F, Ding J. Neutro-Connectedness Cut. IEEE Transactions on Image Processing 2016;25:4691â€“4703.  \\nXian M, Zhang Y, Cheng H -D, Xu F, Huang K, Zhang B, Ding J, Ning C, Wang Y. A Benchmark for Breast Ultrasound Image Segmentation \\n(BUSIS). CoRR 2018a;abs/1801.03182.  \\nXian M, Zhang Y, Cheng H -D, Xu F, Zhang B, Ding J. Automatic breast ultrasound image segmentation: A survey. Pattern Recognition \\n2018b;79:340â€“355.'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='(BUSIS). CoRR 2018a;abs/1801.03182.  \\nXian M, Zhang Y, Cheng H -D, Xu F, Zhang B, Ding J. Automatic breast ultrasound image segmentation: A survey. Pattern Recognition \\n2018b;79:340â€“355.  \\nXiao G, Brady M, Noble JA, Zhang Y. Segmentation of Ultrasound B -mode Images with Intensity Inhomogeneity Correction. IEEE Trans Med \\nImaging 2002;21:48â€“57.  \\nXie Y, Chen K, Lin J. An Automatic Localization Algorithm for Ultrasound Breast Tumors Based on Hum an Visual Mechanism. Sensors \\n2017;17:1101.  16 \\n \\nXu K, Ba JL, Kiros R, Cho K, Courville A, Salakhutdinov R, Zemel RS, Bengio Y. Show, Attend and Tell: Neural Image Caption Generation with \\nVisual Attention. Proceedings of the 32Nd International Conference on Int ernational Conference on Machine Learning - Volume 37 \\nJMLR.org, 2015. pp. 2048â€“2057.  \\nXu F, Xian M, Cheng H -D, Ding J, Zhang Y. Unsupervised saliency estimation based on robust hypotheses. 2016 IEEE Winter Conference on'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='JMLR.org, 2015. pp. 2048â€“2057.  \\nXu F, Xian M, Cheng H -D, Ding J, Zhang Y. Unsupervised saliency estimation based on robust hypotheses. 2016 IEEE Winter Conference on \\nApplications of Computer Vision, WACV 2016, Lake Placid, NY, USA, March 7-10, 2016 2016. pp. 1â€“6.  \\nXu F, Xian M, Zhang Y, Huang K, Cheng H -D, Zhang B, Ding J, Ning C, Wang Y. A Hybrid Framework for Tumor Saliency Estimation. 24th \\nInternational Conference on Pattern Recognition, ICPR 2018, Beijing, China, August 20-24, 2018 2018. pp. 3935â€“3940.  \\nXu F, Zhang Y, Xian M, Cheng H -D, Zhang B, Ding J, Ning C, Wang Y. Tumor Saliency Estimation for Breast Ultrasound Images via Breast \\nAnatomy Modeling. CoRR 2019;abs/1906.07760.  \\nYap MH, Pons G, MartÃ­ J , Ganau S, SentÃ­s M, Zwiggelaar R, Davison AK, Marti R. Automated Breast Ultrasound Lesions Detection Using \\nConvolutional Neural Networks. IEEE J Biomedical and Health Informatics 2018;22:1218â€“1226.'),\n",
       " Document(metadata={'arxiv_id': '1910.08978v2', 'title': 'Attention Enriched Deep Learning Model for Breast Tumor Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Aleksandar Vakanski, Min Xian, Phoebe Freer'}, page_content='Convolutional Neural Networks. IEEE J Biomedical and Health Informatics 2018;22:1218â€“1226.  \\nZhao H, Shi J, Qi X, Wang X, Jia J. Pyramid Scene Parsing Network. 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR \\n2017, Honolulu, HI, USA, July 21-26, 2017 IEEE Computer Society, 2017. pp. 6230â€“6239.  \\nZhu W, Huang Y, Zeng L, Chen X, Liu Y, Qian Z, Du N, Fan W, Xie X. AnatomyNet: Deep learning for fast and fully automated whole-volume \\nsegmentation of head and neck anatomy. Medical physics 2019;46:576â€“589.'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'title_abstract', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='Title: Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images\\n\\nAbstract: Automatic breast lesion segmentation in ultrasound helps to diagnose breast cancer, which is one of the dreadful diseases that affect women globally. Segmenting breast regions accurately from ultrasound image is a challenging task due to the inherent speckle artifacts, blurry breast lesion boundaries, and inhomogeneous intensity distributions inside the breast lesion regions. Recently, convolutional neural networks (CNNs) have demonstrated remarkable results in medical image segmentation tasks. However, the convolutional operations in a CNN often focus on local regions, which suffer from limited capabilities in capturing long-range dependencies of the input ultrasound image, resulting in degraded breast lesion segmentation accuracy. In this paper, we develop a deep convolutional neural network equipped with a global guidance block (GGB) and breast lesion boundary detection (BD) modules for boosting the breast ultrasound lesion segmentation. The GGB utilizes the multi-layer integrated feature map as a guidance information to learn the long-range non-local dependencies from both spatial and channel domains. The BD modules learn additional breast lesion boundary map to enhance the boundary quality of a segmentation result refinement. Experimental results on a public dataset and a collected dataset show that our network outperforms other medical image segmentation methods and the recent semantic segmentation methods on breast ultrasound lesion segmentation. Moreover, we also show the application of our network on the ultrasound prostate segmentation, in which our method better identifies prostate regions than state-of-the-art networks.'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images\\nCheng Xuea, Lei Zhub,âˆ—, Huazhu Fuc, Xiaowei Hua, Xiaomeng Lid, Hai Zhange, Pheng-Ann Henga,f\\naDepartment of Computer Science and Engineering, The Chinese University of Hong Kong\\nbDepartment of Applied Mathematics and Theoretical Physics, University of Cambridge\\ncInception Institute of Artiï¬cial Intelligence, Abu Dhabi, UAE\\ndDepartment of Electronic and Computer Engineering, The Hong Kong University of Science and Technology\\neShenzhen Peopleâ€™s Hospital, The Second Clinical College of Jinan University, The First Afï¬liated Hospital of Southern University of Science and Technology\\nfShenzhen Key Laboratory of Virtual Reality and Human Interaction Technology,Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences,\\nChina\\nAbstract\\nAutomatic breast lesion segmentation in ultrasound helps to diagnose breast cancer, which is one of the dreadful diseases that'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='China\\nAbstract\\nAutomatic breast lesion segmentation in ultrasound helps to diagnose breast cancer, which is one of the dreadful diseases that\\naffect women globally. Segmenting breast regions accurately from ultrasound image is a challenging task due to the inherent\\nspeckle artifacts, blurry breast lesion boundaries, and inhomogeneous intensity distributions inside the breast lesion regions.\\nRecently, convolutional neural networks (CNNs) have demonstrated remarkable results in medical image segmentation tasks.\\nHowever, the convolutional operations in a CNN often focus on local regions, which suffer from limited capabilities in capturing\\nlong-range dependencies of the input ultrasound image, resulting in degraded breast lesion segmentation accuracy. In this pa-\\nper, we develop a deep convolutional neural network equipped with a global guidance block (GGB) and breast lesion boundary'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='per, we develop a deep convolutional neural network equipped with a global guidance block (GGB) and breast lesion boundary\\ndetection (BD) modules for boosting the breast ultrasound lesion segmentation. The GGB utilizes the multi-layer integrated\\nfeature map as a guidance information to learn the long-range non-local dependencies from both spatial and channel domains.\\nThe BD modules learn additional breast lesion boundary map to enhance the boundary quality of a segmentation result reï¬ne-\\nment. Experimental results on a public dataset and a collected dataset show that our network outperforms other medical image\\nsegmentation methods and the recent semantic segmentation methods on breast ultrasound lesion segmentation. Moreover, we\\nalso show the application of our network on the ultrasound prostate segmentation, in which our method better identiï¬es prostate\\nregions than state-of-the-art networks.\\nKeywords: Non-local features, breast lesion segmentation, deep neural network'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='regions than state-of-the-art networks.\\nKeywords: Non-local features, breast lesion segmentation, deep neural network\\n1. Introduction\\nBreast cancer is one of the dreadful diseases that affect\\nwomen globally. According to the statistic information re-\\nported in (American Cancer Society, 2019), an estimated\\n42,260 breast cancer deaths would occur in 2019. An ac-\\ncurate breast lesion segmentation from the ultrasound images\\nhelps the early diagnosis of breast cancer. However, the auto-\\nmatic breast lesion segmentation in a 2D ultrasound image is a\\nchallenging task, since there are the speckle noise, and strong\\nshadows in the ultrasound, inhomogeneous distributions in the\\nbreast lesion regions, and ambiguous boundaries between the\\nbreast lesion and non-lesion regions, as well as the irregular\\nâˆ—Lei Zhu (lz437@cam.ac.uk) and Hai Zhang are the co-corresponding au-\\nthor of this work.\\nbreast lesion shapes; see Fig. 1 for the examples.\\nSegmenting breast lesion in ultrasound images has been'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='âˆ—Lei Zhu (lz437@cam.ac.uk) and Hai Zhang are the co-corresponding au-\\nthor of this work.\\nbreast lesion shapes; see Fig. 1 for the examples.\\nSegmenting breast lesion in ultrasound images has been\\nwidely studied in the research community. Early attempts,\\ne.g., (Shan et al., 2012; Madabhushi and Metaxas, 2002; Shan\\net al., 2008; Kwak et al., 2005; Madabhushi and Metaxas, 2003;\\nYezzi et al., 1997; Chen et al., 2002; Xian et al., 2015; Ashton\\nand Parker, 1995; Boukerroui et al., 1998; Xiao et al., 2002) de-\\ntected the breast lesion boundaries mainly based on the hand-\\ncrafted features. These features, however, have the limited fea-\\nture representation ability, leading to misrecognize the breast\\nlesions in a complex environment. Recently, the convolutional\\nneural networks (CNNs) have achieved impressive progress on\\nbreast ultrasound segmentation task. For examples, Yap et al.,\\nadopted U-Net, FCN-AlexNet, and patch-based LeNet for 2D'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='neural networks (CNNs) have achieved impressive progress on\\nbreast ultrasound segmentation task. For examples, Yap et al.,\\nadopted U-Net, FCN-AlexNet, and patch-based LeNet for 2D\\nultrasound image breast lesion detection (Yap et al., 2017). Lei\\net al., employed a deep neural network with the supervision\\narXiv:2104.01896v1  [eess.IV]  5 Apr 2021ï¼ˆaï¼‰\\nï¼ˆbï¼‰\\nï¼ˆcï¼‰\\nFigure 1: Examples of challenging cases in breast ultrasound lesion segmen-\\ntation. The green contour denotes the breast lesion boundary. Left: the input\\nultrasound images. Right: the lesion region. (a) Inhomogeneous distributions\\ninside the breast lesion region. (b) Ambiguous boundary due to similar appear-\\nance between lesion regions and non-lesion backgrounds. (c) Irregular breast\\nlesion shapes.\\nsignals on the boundary to address the whole breast ultrasound\\nimage (Lei et al., 2018). Xu et al., adopted an eight-layer CNN\\nto segment 3D breast in the ultrasound data (Xu et al., 2019).'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='signals on the boundary to address the whole breast ultrasound\\nimage (Lei et al., 2018). Xu et al., adopted an eight-layer CNN\\nto segment 3D breast in the ultrasound data (Xu et al., 2019).\\nThe ultrasound image has many distant pixels, which have\\nthe similar appearance as the breast lesions. Incorporating\\nthese pixels could provide long-term non-local features to\\nlearning discriminative features for the ultrasound breast lesion\\nsegmentation. Capturing the global contextual information for\\nultrasound image segmentation is a long-standing topic in the\\nmedical image community. Previous studies proposed to en-\\nlarge the receptive ï¬eld with dilated convolutions, pooling op-\\nerations (Chen et al., 2018, 2017b, 2014); or fuse the middle\\nlevel and high level features with more task-related semantic\\nfeatures (Ronneberger et al., 2015; Lin et al., 2017). However,\\nthese methods fail to capture the contextual information in a\\nglobal view and only consider the inter-dependencies among'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='features (Ronneberger et al., 2015; Lin et al., 2017). However,\\nthese methods fail to capture the contextual information in a\\nglobal view and only consider the inter-dependencies among\\nspatial domains. In medical image analysis community, most\\nprevious approaches rely on local region operation for segmen-\\ntation task (Ronneberger et al., 2015; Dou et al., 2016; Lin\\net al., 2017). However, capturing the long-range dependen-\\ncies information holds promising potentials but has not been\\nwell explored yet. Traditional non-local blocks in these net-\\nworks (Qi et al., 2019; Dou et al., 2018) are only embedded\\ninto the deep CNN layers to learn long-range dependencies for\\nnetwork predictions. However, due to the relatively larger re-\\nceptive ï¬elds than shallow CNN layers, the deep layers of a\\nsegmentation network are responsible for capturing cues of the\\nwhole breast lesions and somehow lack parts of breast lesion\\nregions, degrading the segmentation performance.'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='segmentation network are responsible for capturing cues of the\\nwhole breast lesions and somehow lack parts of breast lesion\\nregions, degrading the segmentation performance.\\nIn this work, we develop a convolutional neural network\\n(CNN) to integrate features at all CNN layers (including deep\\nand shallow CNN layers) to produce multi-level integrated fea-\\ntures (MLIF) as a guidance information of the non-local blocks\\nin spatial and channel manners to complement more breast le-\\nsion boundary details, which are usually neglected by deep\\nCNN layers. Moreover, we propose to predict additional breast\\nlesion boundary map such that the predicted boundary map is\\nregularized to be as similar as the underlying ground truth. By\\ndoing so, our network can produce a segmentation result with\\nmore accurate breast lesion boundaries. In summary, our con-\\ntributions are four-fold:\\nâ€¢ First, we present a CNN (denoted as GG-Net) with a\\nglobal guidance block (GGB) to aggregate non-local fea-'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='more accurate breast lesion boundaries. In summary, our con-\\ntributions are four-fold:\\nâ€¢ First, we present a CNN (denoted as GG-Net) with a\\nglobal guidance block (GGB) to aggregate non-local fea-\\ntures in both spatial and channel domains under the guid-\\nance of multi-layer integrated features for learning a pow-\\nerful non-local contextual information.\\nâ€¢ Second, we develop a breast lesion boundary detection\\n(BD) module in shallow CNN layers to embed additional\\nboundary maps of breast lesions for obtaining the segmen-\\ntation result with high-quality boundaries.\\nâ€¢ Third, the experimental results on two ultrasound breast\\nlesion datasets show that our network outperforms the\\nstate-of-the-art medical image segmentation methods on\\nbreast lesion segmentation.\\nâ€¢ Moreover, we also show the application of our network on\\nthe ultrasound prostate segmentation, where our network\\nobtains satisfactory performance.\\n2. Related works\\nBreast lesion segmentation from ultrasound images is very'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='the ultrasound prostate segmentation, where our network\\nobtains satisfactory performance.\\n2. Related works\\nBreast lesion segmentation from ultrasound images is very\\nchallenging due to the speckle artifacts, low contrast, shadows,\\nblurry boundaries, and the variance in lesion shapes (Kirberger,\\n1995). A variety of breast lesion segmentation algorithms have\\nbeen proposed and these methods can be broadly classiï¬ed into\\nfour categories, including region based approach (Shan et al.,\\n2012; Madabhushi and Metaxas, 2002; Shan et al., 2008; Kwak\\n2ASPPConcat&UpsampleUpsample\\nConcat\\nSupervision\\nSupervisionSupervisionSupervision\\nSupervision\\n1Ã—1ConvandDownsample/Upsample\\nBDBDBDBD GlobalGuidanceBlockSpatial-wise  Global GuidanceBlockChannel-wise Global Guidance Block\\nGuidanceGuidance\\nFigure 2: The schematic illustration of the proposed breast lesion segmentation network (GG-Net) in this work. (i) We ï¬rst use a convolutional neural network'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='GuidanceGuidance\\nFigure 2: The schematic illustration of the proposed breast lesion segmentation network (GG-Net) in this work. (i) We ï¬rst use a convolutional neural network\\n(CNN) to produce a set of feature maps with different scales, followed by a ASPP module to enlarge the receptive ï¬eld. (ii) In each CNN layer, we pass its\\nfeature map to a breast lesion boundary detection (BD) module (see Section III. B) to detect breast lesion boundaries. (iii) We concatenate features at all CNN\\nlayers and use it as the guidance to the developed global guidance block (GGB), which includes a spatial-wise global guidance block and a channel-wise global\\nguidance block, to learn long-range dependencies for each pair of positions on the feature maps over spatial and channel domains. (iv) We use the output feature\\nmap of the GGB to predict the segmentation result of our network.\\net al., 2005), deformable models (Madabhushi and Metaxas,\\n2003; Yezzi et al., 1997; Chen et al., 2002), graph-based ap-'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='map of the GGB to predict the segmentation result of our network.\\net al., 2005), deformable models (Madabhushi and Metaxas,\\n2003; Yezzi et al., 1997; Chen et al., 2002), graph-based ap-\\nproaches (Xian et al., 2015; Ashton and Parker, 1995; Bouk-\\nerroui et al., 1998; Xiao et al., 2002) and learning based ap-\\nproaches (Liu et al., 2010; Huang et al., 2008; Lo et al., 2014;\\nMoon et al., 2014; Othman and Tizhoosh, 2011). These ap-\\nproaches usually employed texture features to represent the lo-\\ncal variation of pixel intensities and then detect abnormal re-\\ngions in the ultrasound image. However, these methods rely on\\nhand-crafted features and have limited representation capacity.\\nConvolutional neural networks (CNNs) have shown remark-\\nable performance in many medical image analysis tasks, in-\\ncluding image classiï¬cation (Yu et al., 2018, 2017), semantic\\nsegmentation (Ronneberger et al., 2015; Dou et al., 2016; Yu\\net al., 2016; Li et al., 2018). These methods utilized the su-'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='cluding image classiï¬cation (Yu et al., 2018, 2017), semantic\\nsegmentation (Ronneberger et al., 2015; Dou et al., 2016; Yu\\net al., 2016; Li et al., 2018). These methods utilized the su-\\nperior learning capability of neural network and outperformed\\nother traditional segmentation methods. For breast image anal-\\nysis, recent works have featured CNN based methods (Yap\\net al., 2017; Lei et al., 2018; Xu et al., 2019; Dhungel et al.,\\n2017; Mordang et al., 2016a; Ahn et al., 2017; Mordang et al.,\\n2016b; Hu et al., 2019; Mishra et al., 2018). Yapet al. adopted\\npacth-based LeNet, U-Net, and FCN-AlexNet for breast lesion\\ndetection Yap et al. (2017). Lei et al. proposed a ConvEDNet\\nfor whole breast ultrasound image segmentation with the deep\\nboundary supervision and adaptive domain transfer knowl-\\nedge (Lei et al., 2018). Some works adopted CNNs with dif-\\nferent layers to detect mass, estimate the breast density, and\\nsegment breast ultrasound images (Dhungel et al., 2017; Ahn'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='edge (Lei et al., 2018). Some works adopted CNNs with dif-\\nferent layers to detect mass, estimate the breast density, and\\nsegment breast ultrasound images (Dhungel et al., 2017; Ahn\\net al., 2017; Xu et al., 2019). Mordang et al. , adopted Ox-\\nfordNet for mammography microcalciï¬cation detection (Mor-\\ndang et al., 2016b). Hu et al., proposed a dilated fully convolu-\\ntional network for breast tumor segmentation (Hu et al., 2019).\\nMishra et al., developed a fully convolutional neural network\\nwith deep supervision for lumen segmentation and liver lesion\\nsegmentation (Mishra et al., 2018).\\nTo improve the pixel-wise prediction accuracy, many re-\\nsearchers considered incorporating the long-range dependen-\\ncies and contextual information in the network, thus enhanc-\\ning the feature representation for pixel-wise prediction. For\\nexample, atrous spatial pyramid pooling (ASPP) was de-\\nsigned to embed the global contextual information, and it\\nwas widely adopted in DeepLabv2 (Chen et al., 2014) and'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='example, atrous spatial pyramid pooling (ASPP) was de-\\nsigned to embed the global contextual information, and it\\nwas widely adopted in DeepLabv2 (Chen et al., 2014) and\\nDeepLabv3 (Chen et al., 2018). Similarly, Zhao et al. , de-\\nsigned a pyramid pooling module to collect the effective con-\\ntextual prior with different scales (Zhao et al., 2017). Besides,\\nan EncNet was introduced a channel attention mechanism to\\ncapture the global context (Zhang et al., 2018). Peng et al., ar-\\ngued that large kernel plays an important role in semantic seg-\\nmentation tasks, and a global convolutional network was pro-\\nposed to learn the context information (Peng et al., 2017). In\\nmedical image analysis ï¬eld, there are some recently work that\\nalso considered the context information, such as the encoder-\\n3(a) inputs (b) 1st layer (c) 2nd layer (d) 3-th ayer (e) 4-th layer (f) ground truth'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='also considered the context information, such as the encoder-\\n3(a) inputs (b) 1st layer (c) 2nd layer (d) 3-th ayer (e) 4-th layer (f) ground truth\\nFigure 3: Two examples are shown to illustrate the learned breast lesion feature on different layers. (a) Input images. (b)-(e) Segmentation maps predicted from\\nthe feature map from the 1-st layer to the 4-th layer. (f) Ground truths. The shallow layers (b), (c) and (d) contains more detail features compared to (e).\\nSoftmax\\nSoftmax\\nSoftmax\\nSG\\nhwÃ—hw\\nSX\\nSM\\n: Matrix multiplication: Element-wise Addition\\n: Dot product\\nhÃ—wÃ—c\\nG\\nhÃ—wÃ—c\\nX\\nWÎ·\\nWÏ\\nWÎ¸\\nWÎ¦\\nWÎ¼\\nhÃ—wÃ—c\\n: Reshape to hwÃ—c\\nhwÃ—hw\\nhwÃ—hw\\nhÃ—wÃ—c\\nReshape YÊ¼\\nY\\nhÃ—wÃ—c\\nhÃ—wÃ—c\\nhÃ—wÃ—c\\nhÃ—wÃ—c\\nhÃ—wÃ—c\\nReshape\\nReshape\\nReshape\\nReshape\\nFigure 4: The schematic illustration of the details of spatial-wise GGB, where\\nG is the guidance map, and X is the input feature map.\\ndecoder structures (Ronneberger et al., 2015) fused the mid-\\nlevel and high-level features to obtain different scale context.'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='G is the guidance map, and X is the input feature map.\\ndecoder structures (Ronneberger et al., 2015) fused the mid-\\nlevel and high-level features to obtain different scale context.\\nIn OBELISK-Net (Heinrich et al., 2019), sparse deformable\\nconvolutions were formulated to learn large context informa-\\ntion. However, these methods mostly stacked a series of con-\\nvolutional layers to capture the context information. Several\\nworks have been proposed to alleviate this issue by implic-\\nitly utilizing attention mechanisms or non-local operations to\\nincrease the receptive ï¬elds and capture contextual informa-\\nReshape cÃ—c\\nhÃ—wÃ—c\\nhÃ—wÃ—c\\nChannel \\nweight\\ncÃ—c\\nSoftmax\\nSoftmax\\nSoftmax\\nhÃ—wÃ—c\\nG SÄœ\\nSZY\\nÄœ\\nhwÃ—c\\nReshape\\nhwÃ—c SQ\\nhÃ—wÃ—c\\nReshape\\nZ\\nZÊ¼\\n: Matrix multiplication: Element-wise Addition\\n: Dot product\\ncÃ—c\\nFigure 5: The schematic illustration of the channel-wise GGB, where G is the\\nguidance map and Y is the input feature map.\\ntion (Wang et al., 2018a; Vaswani et al., 2017; Schlemper et al.,'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='cÃ—c\\nFigure 5: The schematic illustration of the channel-wise GGB, where G is the\\nguidance map and Y is the input feature map.\\ntion (Wang et al., 2018a; Vaswani et al., 2017; Schlemper et al.,\\n2019; Zhang et al., 2017; Roy et al., 2018; Joutard et al., 2019).\\nHowever, the meticulous features in the multi-layer features\\nand the long range dependencies between feature channels are\\nignored. In this regard, we introduce a network that gracefully\\nuniï¬es the approaches mentioned above, which not only con-\\nsider the long-range dependencies spatial-wisely and channel-\\nwisely, but also embed contextual information from different\\nlayers.\\n4(a) (b) (c) (e)\\n(d)\\nFigure 6: An analysis of segmentation improvement based on detected bound-\\naries. (a) Input images. (b) Detected boundary map at the BD module of\\nthe fourth CNN layer. (c) Ground truths of breast lesion segmentation. (d)\\nSegmentation results of our method. (e) Our results without the BD module.'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='the fourth CNN layer. (c) Ground truths of breast lesion segmentation. (d)\\nSegmentation results of our method. (e) Our results without the BD module.\\nApparently, learning additional boundary maps of breast lesion incurs a better\\nsegmentation result.\\n3. Methodology\\nFig. 2 illustrates the architecture of the developed network\\n(denoted as GG-Net). Our network takes a breast ultrasound\\nimage as the input and produces a segmented mask in an end-\\nto-end manner. Speciï¬cally, our GG-Net starts by using a CNN\\nto generate multi-level feature maps with different spatial res-\\nolutions and adopting the ASPP (Chen et al., 2018) to enhance\\nthe receptive ï¬eld of features. In order to utilize the comple-\\nmentary information among different CNN layers, the GGB is\\nintroduced to reï¬ne the features by learning long-range feature\\ndependencies under the guidance of an integrated feature map\\nfrom the shallow CNN layers. Moreover, the BD module is em-'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='introduced to reï¬ne the features by learning long-range feature\\ndependencies under the guidance of an integrated feature map\\nfrom the shallow CNN layers. Moreover, the BD module is em-\\nbedded in the shallow CNN layers to capture the breast lesion\\ncontour and provide a strong cue for better segmenting breast\\nlesions and reï¬ning lesion boundaries. Finally, the prediction\\nmap is produced as the segmentation result of our network. In\\nthe following subsections, we will introduce details of the de-\\nveloped GGB and BD in our method.\\n3.1. Global Guidance Block\\nConvolutional and recurrent operations of CNNs only cap-\\nture the spatial dependencies within a local neighborhood.\\nAlthough stacking convolutional layers can learn the long\\nrange dependencies, such repeating local convolutions is time-\\nconsuming and leads to the optimization difï¬culties that need\\nto be carefully addressed (Wang et al., 2018a). Moreover,\\nbreast ultrasound images usually contain speckles and shadows'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='consuming and leads to the optimization difï¬culties that need\\nto be carefully addressed (Wang et al., 2018a). Moreover,\\nbreast ultrasound images usually contain speckles and shadows\\nthat tend to be recognized as breast lesion due to the limited re-\\nceptive ï¬elds of local convolutions. In this regard, we develop a\\nglobal guidance block (GGB), which leverages a guidance fea-\\nture map to learn the long range dependencies by considering\\nspatial and channel information.\\n  \\nprediction maxpooling\\n: Element-wise Addition\\n: Element-wise Substraction\\nFÎ¦(i)i)F(i)i)\\nFigure 7: The schematic illustration of the breast lesion boundary detection\\n(BD) module. F(i) is the feature map at the i-th CNN layer.\\n3.1.1. Spatial-wise Global Guidance Block\\nThe feature maps from the shallow CNN layers provide de-\\ntailed information but contain more non-lesion regions, while\\nthe deep CNN layers with larger reception ï¬elds eliminate the\\nnon-lesion regions, but tend to lose the local details. In this re-'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='tailed information but contain more non-lesion regions, while\\nthe deep CNN layers with larger reception ï¬elds eliminate the\\nnon-lesion regions, but tend to lose the local details. In this re-\\ngard, we argue that feature maps at different CNN layers con-\\ntain the complementary information, as shown in Fig. 3. In our\\nmethod, we ï¬rst resize the feature maps of the ï¬rst four CNN\\nlayers to the size of feature map from the second CNN layer,\\nand then concatenate them to one multi-layer integrated feature\\n(MLIF) map. After that, a spatial-wise global guidance block\\n(spatial-wise GGB) is proposed to learn the long-range position\\ndependencies by taking MLIF as a guidance map.\\nFig. 4 shows the schematic illustration of our spatial-wise\\nGGB. Speciï¬cally, letX (xâˆˆ RhÃ—wÃ—c) denote the output feature\\nmap of the ASPP module (see Fig. 2), and G (g âˆˆ RhÃ—wÃ—c)\\ndenotes the guidance map. The spatial-wise GGB ï¬rst feeds X\\ninto three 1Ã— 1 convolution layers with different parameters,'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='map of the ASPP module (see Fig. 2), and G (g âˆˆ RhÃ—wÃ—c)\\ndenotes the guidance map. The spatial-wise GGB ï¬rst feeds X\\ninto three 1Ã— 1 convolution layers with different parameters,\\nWÎ¸(x), WÏ†(x), and WÂµ(x)), to generate three feature maps, Î¸(x),\\nÏ†(x), and Âµ(x), respectively. After that, we reshape Î¸(x),Ï†(x),\\nand Âµ(x) as RhwÃ—c matrices, multiply the reshaped Ï†(x) with\\nthe transpose of the reshaped Î¸(x), and apply a softmax layer\\non the multiplication result to compute ahwÃ—hw spatial-wisely\\nposition similarity map S x:\\nS x = Softmax(XT W T\\nÎ¸(x)WÏ†(x)X), (1)\\nwhere so f tmax follows the traditional sigmoid function and it\\nis applied on each element of the hwÃ— hw X T W T\\nÎ¸(x)WÏ†(x)X. On\\nthe other side, two 1Ã— 1 convolution layers with parameters,\\nWÎ·(g), and WÏ(g)), are applied on guidance map G to obtain two\\nfeature maps,Î·(x) andÏ(x), reshapeÎ·(x) andÏ(x), multiply the\\nreshapedÎ·(x) to the transpose of the reshapedÏ(x), and apply a\\nsoftmax layer for producing anotherhwÃ—hw position similarity'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='feature maps,Î·(x) andÏ(x), reshapeÎ·(x) andÏ(x), multiply the\\nreshapedÎ·(x) to the transpose of the reshapedÏ(x), and apply a\\nsoftmax layer for producing anotherhwÃ—hw position similarity\\nmatrix (denoted as S g) from the guidance map G:\\nS g = Softmax(GT W T\\nÏ(g)WÎ·(g)G)). (2)\\n5Once obtaining two similarity matrices S X and S G, we use a\\nsoftmax layer on the element-wise multiplication result of S X\\nand S G to generate a guided similarity matrix S M. Then, we\\nmultiply S M with the featuresÂµ(x) to obtain a new feature map\\nY\\nâ€²\\n, which is then added with the input features X to generate\\nthe output feature map Y:\\nY =Âµ(x) Softmax(S xÂ· S g) + X. (3)\\n3.1.2. Channel-wise Global Guidance Block\\n3.2. Loss Function\\nOur spatial-wise GGB treats each feature channel equally\\nwhen learning the long range dependencies, resulting in ne-\\nglecting the correlations among different feature channels. Re-\\ncently, allowing varied contributions from different feature'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='when learning the long range dependencies, resulting in ne-\\nglecting the correlations among different feature channels. Re-\\ncently, allowing varied contributions from different feature\\nchannels has achieved superior performance in many computer\\nvision tasks (Hou et al., 2019; Chen et al., 2017a; Hu et al.,\\n2018). Motivated by these, we develop a channel-wise global\\nguidance block (channel-wise GGB) to further learn the long\\nrange inter-dependencies between different feature channels.\\nFig. 5 illustrates the schematic details of the proposed channel-\\nwise GGB, which takes a feature map Y and a guidance map G\\nas two inputs and generates a reï¬ned feature map Z. Speciï¬-\\ncally, we reshape Y to RcÃ—hw, multiply the reshaped Y and the\\ntranspose of the reshaped Y, and use a softmax layer to obtain\\na channel-wise similarity map S Zâˆˆ RcÃ—c. Regarding the input\\nguidance feature map G, we ï¬rst use squeeze-and-excitation\\nblock to emphasis informative feature channels of G and sup-'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='a channel-wise similarity map S Zâˆˆ RcÃ—c. Regarding the input\\nguidance feature map G, we ï¬rst use squeeze-and-excitation\\nblock to emphasis informative feature channels of G and sup-\\npress less useful ones. To achieve this, we use a global average\\npooling to generate the channel-wise statistics Î², and the k-th\\nelement of the descriptor (Î²) is given by\\nÎ²k = 1\\nhÃ— w\\nhâˆ‘\\ni=1\\nwâˆ‘\\nj=1\\nG(i, j, k), (4)\\nwhere G(i, j, k) denotes the element at the position (i, j, k) of\\nguidance map G. After that, we use two fully connected (fc)\\nlayers and a sigmoid activation function on the channel-wise\\nstatisticsÎ² to generate a coefï¬cient vectorVÎ»:\\nVÎ» = Î¦(W2â„¦(W1Î²)), (5)\\nwhere W1 and W2 denote the parameters of the two fully con-\\nnected layers, â„¦ and Î¦ are the ReLU and the sigmoid activation\\nfunction, respectively. Then, we multiply VÎ» with G to assign\\ndifferent weights on channels of G and obtain a reï¬ned feature\\nmap (denoted as Ë†G). Once obtaining Ë†G, we reshape it to RcÃ—hw,'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='function, respectively. Then, we multiply VÎ» with G to assign\\ndifferent weights on channels of G and obtain a reï¬ned feature\\nmap (denoted as Ë†G). Once obtaining Ë†G, we reshape it to RcÃ—hw,\\nmultiple the reshaped Ë†G and the transpose of the reshaped Ë†G,\\nand use a softmax layer to generate a cÃ— c similarity map S Ë†G.\\nLater, a softmax layer is applied on the multiplication ofS Z and\\nS Ë†G to obtain a guided similarity map S Q. Finally, we multiply\\nS Q with the input Y to produce a new feature map Z\\nâ€²\\n, which is\\nthen added to the input features Y to obtain the output feature\\nmap Z of our channel-wise GGB.\\n3.3. Breast Lesion Boundary Detection Module\\nAlthough GGB generates a breast lesion segmentation result,\\nwe ï¬nd that there are many failed segmented regions in the re-\\nsults, as shown in Fig. 6(e), which have inaccurate boundary\\nmaps of the breast lesion. To alleviate this, we develop a breast\\nlesion boundary detection (BD) module to identify multi-level'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='sults, as shown in Fig. 6(e), which have inaccurate boundary\\nmaps of the breast lesion. To alleviate this, we develop a breast\\nlesion boundary detection (BD) module to identify multi-level\\nboundary maps of the breast lesions and enhance the segmen-\\ntation result with an additional boundary prediction loss. Fig. 7\\nshows the schematic illustration of the developed BD module at\\nthe i-th CNN layer to detect breast lesion boundaries. It takes\\nthe feature map of i-th CNN layer as the input and outputs a\\nboundary map of the breast lesion and a breast lesion segmenta-\\ntion result. Speciï¬cally, we ï¬rst use a1Ã—1 convolutional layer\\non the input features F(i) to obtain a new feature map FÏ†(i)\\nwith one channel. Then, we shift FÏ†(i) with one pixel via a\\nmaxpooling operation (stride = 1, padding = 1, kernel size =\\n3Ã—3; see (Feng et al., 2019) for details) and subtract the shifted\\nresult from FÏ†(i) to obtain a boundary map E of the breast le-'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='maxpooling operation (stride = 1, padding = 1, kernel size =\\n3Ã—3; see (Feng et al., 2019) for details) and subtract the shifted\\nresult from FÏ†(i) to obtain a boundary map E of the breast le-\\nsions. After that, we add FÏ†(i) with E to obtain a breast lesion\\nsegmentation map.\\nAs shown in Fig. 2, we add a BD module for the shallow\\nCNN layer to jointly locate breast lesions and detect a boundary\\nmap from feature map at the CNN layer. Hence, our network\\ngenerates four boundary maps and four breast lesion segmenta-\\ntion results at four CNN layers. Moreover, our network gener-\\nates a ï¬nal segmentation result of breast lesions from the GGB.\\nWith an annotated breast lesion mask, we apply a canny oper-\\nator (Canny, 1986) to obtain the boundary mask as the ground\\ntruth of the boundary prediction. Finally, we compute the total\\nloss of our network as:\\nLtotal =\\nNlayerâˆ‘\\ni=1\\n(Î»1Â· Li\\nseg +Î»2Â· Li\\nboundary) + L f\\nseg, (6)\\nwhere Nlayer is the number of CNN layers, and we empirically'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='loss of our network as:\\nLtotal =\\nNlayerâˆ‘\\ni=1\\n(Î»1Â· Li\\nseg +Î»2Â· Li\\nboundary) + L f\\nseg, (6)\\nwhere Nlayer is the number of CNN layers, and we empirically\\nset Nlayer as four in our implementation. Li seg and Liboundary de-\\nnote the segmentation loss and the boundary loss in the BLBD\\nmodule of i-th CNN layer, respectively. L f seg is the loss func-\\ntion of the ï¬nal segmentation result. The weights Î»1 and Î»2\\nare to balance Li seg, Liboundary, and L f seg, and their values are\\nempirically set asÎ»1 = 1 andÎ»2 = 10.\\nLetPi denote the predicted breast lesion segmentation result\\nat i-th CNN layer and G is the ground truth of the annotated\\nbreast lesion mask. Li seg combines a dice coefï¬cient loss and\\n6a binary cross-entropy loss to compute the difference between\\nPi andG:\\nLi\\nseg = 1âˆ’\\n2 âˆ‘Np\\nj=1(Pi) jÃ— (G) j\\nâˆ‘Np\\nj=1(Pi)2\\nj + âˆ‘Np\\nj=1(G)2\\nj\\nâˆ’ 1\\nNp\\nNpâˆ‘\\nj=1\\n(Pi) jlog(G) j, (7)\\nwhere Np is the number of pixels in thePi;\\nLiboundary is computed as the mean square error (MSE) be-'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='seg = 1âˆ’\\n2 âˆ‘Np\\nj=1(Pi) jÃ— (G) j\\nâˆ‘Np\\nj=1(Pi)2\\nj + âˆ‘Np\\nj=1(G)2\\nj\\nâˆ’ 1\\nNp\\nNpâˆ‘\\nj=1\\n(Pi) jlog(G) j, (7)\\nwhere Np is the number of pixels in thePi;\\nLiboundary is computed as the mean square error (MSE) be-\\ntween the predicted breast lesion boundary map (denoted as\\nDi) and ground truth of the boundary map (denoted asBG):\\nLi\\nboundary =\\nNpâˆ‘\\nj=1\\n{(Di) jâˆ’ (BG) j}2, (8)\\nwhere Np is the number of pixel inDi; (Di) j is the j-th pixel at\\nDi; and (BG) j is the j-th pixel atBG.\\nMoreover, following Li seg, L f seg also combines the dice co-\\nefï¬cient loss and binary cross-entropy loss to compute the dif-\\nference between the predicted segmentation map (denoted as\\nF ) andG (see Eqn. 7):\\nL f\\nseg = 1âˆ’\\n2 âˆ‘Np\\nj=1(F ) jÃ— (G) j\\nâˆ‘Np\\nj=1(F )2\\nj + âˆ‘Np\\nj=1(G)2\\nj\\nâˆ’ 1\\nNp\\nNpâˆ‘\\nj=1\\n(F ) jlog(G) j. (9)\\n3.4. Implementation\\n3.4.1. Training Parameters\\nTo accelerate the training process, we initialize the pa-\\nrameters of the feature extract network using the pre-trained'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='Np\\nNpâˆ‘\\nj=1\\n(F ) jlog(G) j. (9)\\n3.4. Implementation\\n3.4.1. Training Parameters\\nTo accelerate the training process, we initialize the pa-\\nrameters of the feature extract network using the pre-trained\\nResNext on ImageNet while other parameters are initialized\\nby random noise. The SGD algorithm is used to optimize the\\nwhole network with a momentum of 0.9, a weight decay of\\n0.0001, a mini-batch size of 4, and 100 epochs. We set the ini-\\ntial learning rate as 0.001 and reduce it by multiplying 0.1 af-\\nter ï¬nishing every 50 epochs. Random rotation and horizontal\\nï¬‚ip operations are adopted for performing the data augmenta-\\ntion on the training set. We implement the whole network us-\\ning PyTorch library and train our network on a single NVIDIA\\nTITIAN Xp GPU.\\n3.4.2. Inference\\nIn the testing stage, we take the segmentation result pre-\\ndicted from the reï¬ned features of the dual guided non-local\\nblock as the output of our segmentation network, and then pass'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='3.4.2. Inference\\nIn the testing stage, we take the segmentation result pre-\\ndicted from the reï¬ned features of the dual guided non-local\\nblock as the output of our segmentation network, and then pass\\nthe result to the conditional random ï¬elds (CRF) (KrÂ¨ahenbÂ¨uhl\\nand Koltun, 2011) for obtaining the ï¬nal segmentation result.\\nThe network has 55M trainable parameters. The inference time\\nwas 0.039 seconds per image.\\n4. Experiments\\nWe ï¬rst introduce two datasets on breast ultrasound lesion\\nsegmentation and evaluation metrics, then conduct ablation\\nstudies to verify the major components of our network, as well\\nas quantitatively and qualitatively compare our method against\\nthe state-of-the-art segmentation methods.\\n4.1. Datasets\\nWe evaluate our segmentation network on two datasets\\nincluding a public benchmark dataset ( i.e., BUSI in (Al-\\nDhabyani et al., 2020)) and our collected dataset. BUSI col-\\nlected 780 images from 600 female patients, with 437 benign'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='including a public benchmark dataset ( i.e., BUSI in (Al-\\nDhabyani et al., 2020)) and our collected dataset. BUSI col-\\nlected 780 images from 600 female patients, with 437 benign\\ncases, 210 benign masses, and 133 normal cases. Note that the\\nmain purpose of breast lesion segmentation in the clinical us-\\nage is for the lesion assessment, tracking the lesion change, and\\nidentifying distribution and seriousness of lesions. As a result,\\nclinicians usually screen the input ultrasound sample ï¬rstly to\\nidentify the lesion, and then conduct the breast lesion segmen-\\ntation for clinical measurement. As a result, we remove the\\nnormal cases without breast lesion masks to form the bench-\\nmark dataset, and adopt the three-fold cross-validation to test\\neach segmentation method.\\nOur collected dataset has 632 clinical breast ultrasound im-\\nages in total from 200 patients. The images are captured by\\ndifferent ultrasound imaging systems from Shenzhen Peoples'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='Our collected dataset has 632 clinical breast ultrasound im-\\nages in total from 200 patients. The images are captured by\\ndifferent ultrasound imaging systems from Shenzhen Peoples\\nHospital and the Second Afï¬liated Hospital of Jinan University.\\nWe follow the widely-used annotation procedure of the med-\\nical image segmentation for annotating breast lesions. Firstly,\\nthree experienced radiologists are invited to annotate the breast\\nlesion regions of each ultrasound image using a software in-\\nterface developed via Matlab. Each radiologist used about two\\nweeks to delineate all the breast lesion regions, and the segmen-\\ntation ground truths of each image were then obtained based\\non inner- and intra-observer agreement of the three radiolo-\\ngists. Then, the ï¬nal ground-truths were further reï¬ned by a\\nsenior radiologist with more than 10-year experience for qual-\\nity control. To make the comparisons fair, we adopt the seven-\\nfold cross-validation to test each segmentation method on this'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='senior radiologist with more than 10-year experience for qual-\\nity control. To make the comparisons fair, we adopt the seven-\\nfold cross-validation to test each segmentation method on this\\ndataset.\\n4.2. Evaluation Metrics\\nWe adopt seven commonly used metrics to quantitatively\\ncompare different methods on the breast lesion segmentation.\\nThey are Dice coefï¬cient (denoted as Dice), Jaccard index, Re-\\ncall, Precision, Accuracy, Hausdorff distance (denoted as HD)\\nand average boundary distance (denoted as ABD).\\n7input images ground truths basic basic+GGB our method\\nFigure 8: Visual results of ablation study. (a) Input images; (b) Ground truths; (c)-(e) are the segmentation produced by basic, â€œbasic+GGBâ€, and our method\\n(i.e., â€œbasic+GGB+BDâ€) respectively.\\nTable 1: Quantitative results on our collected dataset and the number of parameters for all networks constructed in ablation study on our collected dataset..'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='(i.e., â€œbasic+GGB+BDâ€) respectively.\\nTable 1: Quantitative results on our collected dataset and the number of parameters for all networks constructed in ablation study on our collected dataset..\\nThe ï¬rst row is Deeplabv3+ with ResNeXt as the feature extraction backbone. â€œGuidanceâ€ denotes guidance information. â€œSNLBâ€ denotes the traditional\\nspatial-wisely non-local block while â€œSNLB+Guidanceâ€ is our spatial-wise GGB (see Fig. 4). â€œCNLBâ€ is the traditional channel-wisely non-local block while\\nâ€œCNLB+Guidanceâ€ is our channel-wise GGB (see Fig. 5).\\nSNLB CNLB Guidance BD Parameters Jaccard % Dice % Accuracy % Recall % Precision %\\n53.4 M 73.4Â± 2.5 81.5 Â± 2.6 97.0 Â± 0.3 78.9 Â± 2.4 88.7 Â± 3.0\\nâœ“ 53.5 M 77.6Â± 1.3 84.5 Â± 1.2 97.2 Â± 0.3 83.1 Â± 1.6 90.7 Â± 1.7\\nâœ“ 53.5 M 77.5Â± 1.6 84.3 Â± 1.3 97.2 Â± 0.4 83.5 Â± 1.7 90.8 Â± 1.6\\nâœ“ âœ“ 53.9 M 78.1Â± 1.4 85.0 Â± 1.3 97.3 Â± 0.4 84.1 Â± 0.2 91.0 Â± 1.5\\nâœ“ âœ“ 53.9 M 78.2Â± 1.4 85.2 Â± 1.2 97.3 Â± 0.4 84.5 Â± 1.2 90.9 Â± 1.5'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='âœ“ 53.5 M 77.5Â± 1.6 84.3 Â± 1.3 97.2 Â± 0.4 83.5 Â± 1.7 90.8 Â± 1.6\\nâœ“ âœ“ 53.9 M 78.1Â± 1.4 85.0 Â± 1.3 97.3 Â± 0.4 84.1 Â± 0.2 91.0 Â± 1.5\\nâœ“ âœ“ 53.9 M 78.2Â± 1.4 85.2 Â± 1.2 97.3 Â± 0.4 84.5 Â± 1.2 90.9 Â± 1.5\\nâœ“ âœ“ 55.2 M 78.4Â± 1.6 85.4 Â± 1.4 97.2 Â± 0.4 84.9 Â± 1.8 90.9 Â± 1.7\\nâœ“ âœ“ âœ“ 55.4 M 78.8Â± 1.7 86.7 Â± 1.2 97.3 Â± 0.3 86.1 Â± 1.7 91.2 Â± 1.2\\nâœ“ âœ“ âœ“ âœ“ 55.4 M 79.1Â± 1.6 87.1 Â± 1.4 97.4 Â± 0.3 86.6 Â± 1.7 91.3 Â± 1.0\\nTable 2: Quantitative results on our method and that with the BD module on the network output branch on our collected dataset.\\nDice % Jaccard % Accuracy % Recall % Precision %\\nOur method 87.1Â± 1.4 79.1Â± 1.6 97.4 Â± 0.3 86.6 Â± 1.7 91.3 Â± 1.0\\nOurs-BD 87.0Â± 1.3 79.1Â± 1.5 97.3Â± 0.4 86.4 Â± 1.0 91.0 Â± 1.5\\n4.3. Ablation Analysis of our GG-Net\\nIn this section, we show the effectiveness of the principal\\ncomponents of our network, i.e., sptial-wise GGB, channel-\\nwise GGB, and BD module in our network. And the abla-\\ntion study experiments are mainly conducted on our collected'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='components of our network, i.e., sptial-wise GGB, channel-\\nwise GGB, and BD module in our network. And the abla-\\ntion study experiments are mainly conducted on our collected\\ndataset. The baseline (i.e., ï¬rst row of Table 1) is constructed\\nby removing both GGB and the BD module from our network.\\nIt is the original DeeplabV3+ network with ResNeXt as the\\nbackbone.\\nTable 1 shows the comparison results of our method with dif-\\nferent components. By comparing â€˜SNLBâ€™, â€˜CNLBâ€™ and base-\\nline (ï¬rst row of Table 1), we can see that learning the long-\\nrange dependencies has a superior performance in segmenting\\nthe breast lesion regions from the ultrasound images. Then,\\n8Table 3: Quantitative comparisons of our network with and without an alternative deep supervision in BDBL.\\nJaccard % Dice % Accuracy % Recall % Precision % HD ABD\\nOurs-ADS 78.5 Â± 1.7 86.6 Â± 1.5 97.3 Â± 0.3 86.3 Â± 1.2 86.1 Â± 1.5 16.4 Â± 2.3 5.5 Â±0.8'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='Jaccard % Dice % Accuracy % Recall % Precision % HD ABD\\nOurs-ADS 78.5 Â± 1.7 86.6 Â± 1.5 97.3 Â± 0.3 86.3 Â± 1.2 86.1 Â± 1.5 16.4 Â± 2.3 5.5 Â±0.8\\nGG-Net (our method) 79.1Â± 1.6 87.1 Â± 1.2 97.4 Â± 0.3 86.6 Â± 1.7 91.3 Â± 1.0 16.2 Â± 2.4 5.3 Â± 0.7\\nTable 4: Comparing our method (GG-Net) with the state-of-the-art methods for beast lesion segmentation on our collected dataset.\\nJaccard% Dice% Accuracy% Recall% Precision% HD ABD\\nU-Net (Ronneberger et al., 2015) 69.3 Â± 2.4 78.0 Â± 2.4 96.5 Â± 0.3 76.9 Â± 0.3 85.6 Â± 2.4 25.1 Â± 2.4 8.1 Â± 0.9\\nU-Net++ (Zhou et al., 2018) 73.3 Â± 2.1 82.1 Â± 2.2 96.6 Â± 0.4 81.1 Â± 1.7 87.9 Â± 2.6 25.6 Â± 4.0 8.4 Â± 1.2\\nTernausNet (Iglovikov and Shvets, 2018) 73.7Â± 1.5 82.2 Â± 1.5 96.8 Â± 0.3 82.1 Â± 1.2 86.9 Â± 0.2 21.6 Â± 2.6 7.5 Â± 0.9 5\\nFPN (Lin et al., 2017) 77.2 Â± 1.9 85.4 Â± 1.7 97.1 Â± 0.4 85.6 Â± 1.8 89.1 Â± 2.4 18.1 Â± 2.7 6.1 Â± 1.0\\nDeepLabv3+ (Chen et al., 2018) 73.4 Â± 2.5 81.5 Â± 2.6 97.0 Â± 0.3 78.9 Â± 2.4 88.7 Â± 3.0 22.3 Â± 4.1 7.9 Â± 1.3'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='DeepLabv3+ (Chen et al., 2018) 73.4 Â± 2.5 81.5 Â± 2.6 97.0 Â± 0.3 78.9 Â± 2.4 88.7 Â± 3.0 22.3 Â± 4.1 7.9 Â± 1.3\\nAG-Unet (Schlemper et al., 2019) 74.1 Â± 1.9 82.8 Â± 1.9 96.6 Â± 0.4 82.5 Â± 2.3 87.3 Â± 1.9 24.1 Â± 3.0 7.8 Â± 1.0\\nDAF (Wang et al., 2018b) 75.4 Â± 1.9 83.6 Â± 2.1 97.1 Â± 0.4 84.5 Â± 2.3 86.6 Â± 2.4 17.1 Â± 2.3 5.8 Â± 0.9\\nGG-Net (our method) 79.1Â± 1.6 87.1 Â± 1.2 97.4 Â± 0.3 86.6 Â± 1.7 91.3 Â± 1.0 16.2 Â± 2.4 5.3 Â± 0.7\\nâ€˜SNLB + Guidance â€™ (i.e., spatial-wise GGB) and â€˜CNLB +\\nGuidanceâ€™ (i.e., channel-wise GGB) have better results than\\nâ€˜SNLBâ€™ and â€˜CNLBâ€™, showing that adding our MLIF guid-\\nance information into the spatial non-local and the channel non-\\nlocal can help to capture the long-range position dependencies\\nfor the breast lesion segmentation. Moreover, the combination\\nof the spatial-wise GGB and the channel-wise GGB has su-\\nperior segmentation results over only using spatial-wise GGB\\nor channel-wise GGB, demonstrating that combining the spa-'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='of the spatial-wise GGB and the channel-wise GGB has su-\\nperior segmentation results over only using spatial-wise GGB\\nor channel-wise GGB, demonstrating that combining the spa-\\ntial and channel information into learning guided non-local fea-\\ntures can enhance the breast lesion segmentation performance.\\nFinally, our method with full components has the best segmen-\\ntation accuracy, which means that the detected breast lesion\\nboundaries in the BD module of our network also contribute to\\nthe superior breast lesion segmentation performance.\\nFig. 8 visually compares the segmentation results produced\\nby the baseline, â€œbasic+GGBâ€ and our method. From the vi-\\nsual results, we can easily ï¬nd that â€œbasic+GGBâ€ has a higher\\nsegmentation accuracy than â€œbasicâ€, showing that the devel-\\noped GGB can learn the long-range position dependencies to\\nboost the breast lesion segmentation performance. Moreover,\\nas shown in Fig. 8 (e) and Fig. 8 (d), our method (i.e., â€œba-'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='oped GGB can learn the long-range position dependencies to\\nboost the breast lesion segmentation performance. Moreover,\\nas shown in Fig. 8 (e) and Fig. 8 (d), our method (i.e., â€œba-\\nsic+GGB+BDâ€) can more accurately detect breast lesion re-\\ngions than â€œbasic+GGBâ€. It means that adding the BD module\\ninto our method can further improve the segmentation accuracy\\nby generating reï¬ned boundaries.\\nBD on the network output branch. Our network applies\\nthe BD module on different CNN layers; see Fig. 2. Here, we\\nmodify our network by applying the BD module on the output\\nbranch for detecting breast lesions and the modiï¬ed network is\\ndenoted as â€œOurs-BDâ€. Table 2 lists different metric values of\\nour method and â€œOurs-BDâ€, showing that our method has only\\nslightly better metric results than â€œOurs-BDâ€. It means that\\nadding the BD module on the network output branch reaches\\na similar segmentation accuracy as our network.\\nAlternative deep supervision in BD modules. Note that the'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='adding the BD module on the network output branch reaches\\na similar segmentation accuracy as our network.\\nAlternative deep supervision in BD modules. Note that the\\nBD module of our network imposes the deep supervision on\\ntwo predictions, i.e., the breast lesion segmentation and the\\nbreast lesion boundary detection. To really verify the contri-\\nbution of the BD module, we conduct an experiment by con-\\nstructing a network (denoted as â€˜Ours-ADSâ€™) by using alterna-\\ntive deep supervision methods in the BD module, which means\\nthat we only impose the deep supervisions on the breast le-\\nsion segmentation and remove the supervisions on breast le-\\nsion boundary predictions in each BD module. Table 3 summa-\\nrizes the quantitative results of our method and â€˜Ours-ADSâ€™ on\\nour collected dataset. From the results, we can easily conclude\\nthat our method has achieved superior quantitative results than\\nâ€˜Ours-ADSâ€™ on all the seven evaluation metrics, demonstrat-'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='our collected dataset. From the results, we can easily conclude\\nthat our method has achieved superior quantitative results than\\nâ€˜Ours-ADSâ€™ on all the seven evaluation metrics, demonstrat-\\ning that utilizing an alternative deep supervision method (i.e.,\\nremoving breast lesion boundary detection supervision) in the\\nBD module reduces the breast lesion segmentation accuracy of\\nour network.\\n4.4. Comparison with the State-of-the-arts\\nCompared methods. We compare our network against several\\ndeep-learning-based segmentation methods, including context-\\nbased methods : feature pyramid network (FPN) (Lin et al.,\\n2017), U-Net (Ronneberger et al., 2015), U-Net++ (Zhou et al.,\\n2018), pre-trained TernausNet (Iglovikov and Shvets, 2018),\\n9Table 5: Comparing our method (GG-Net) with the state-of-the-art methods for beast lesion segmentation on the BUSI dataset. Best results are marked with\\nbold texts.\\nJaccard % Dice % Accuracy% Recall % Precision% HD ABD'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='bold texts.\\nJaccard % Dice % Accuracy% Recall % Precision% HD ABD\\nU-Net (Ronneberger et al., 2015) 64.1 Â± 1.8 73.3 Â± 1.7 95.9 Â± 0.6 70.4 Â± 1.9 83.3 Â± 1.3 65.2 Â± 4.7 24.4 Â± 2.3\\nU-Net++ (Zhou et al., 2018) 56.2 Â± 1.7 66.0 Â± 1.4 95.4 Â± 0.4 62.8 Â± 1.5 78.2 Â± 1.2 78.6 Â± 6.1 31.8 Â± 4.0\\nFPN (Lin et al., 2017) 72.2 Â± 1.6 80.4 Â± 1.6 95.9 Â± 0.6 79.3 Â± 1.3 85.1 Â± 1.5 47.6 Â± 5.8 18.9 Â± 2.6\\nDeepLabv3+ (Chen et al., 2018) 68.2 Â± 1.8 77.2 Â± 1.6 96.3 Â± 0.6 74.4 Â± 2.5 84.8 Â± 1.8 54.4 Â± 5.9 22.4 Â± 2.9\\nSK-U-Net (Byra et al., 2020) - 70.9 95.6 - - - -\\nDAF (Wang et al., 2018b) 68.4 Â± 3.1 77.1 Â± 3.1 96.4 Â± 0.6 76.7 Â± 3.8 82.2 Â± 3.1 46.9 Â± 8.1 17.9 Â± 4.7\\nGG-Net (our method) 73.8Â± 1.1 82.1 Â± 1.1 96.9 Â± 0.5 81.2 Â± 1.6 86.5 Â± 0.5 43.9 Â± 4.8 16.4 Â± 2.2\\nTable 6: Comparing our method (GG-Net) with the state-of-the-art methods for beast lesion segmentation on the BUSI dataset. (include normal data). Best\\nresults are marked with bold texts.\\nJaccard% Dice % Accuracy% Recall % Precision% HD ABD'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='results are marked with bold texts.\\nJaccard% Dice % Accuracy% Recall % Precision% HD ABD\\nU-Net (Ronneberger et al., 2015) 51.2Â± 1.9 58.8 Â± 1.5 96.3 Â± 0.7 56.1 Â± 2.3 68.1 Â± 1.7 67.1 Â± 6.1 24.7 Â± 3.1\\nU-Net++ (Zhou et al., 2018) 44.5 Â± 3.5 52.1 Â± 3.7 95.9 Â± 0.2 48.8 Â± 4.7 63.6 Â± 2.4 73.5 Â± 5.0 27.8 Â± 2.0\\nFPN (Lin et al., 2017) 55.4 Â± 2.1 63.0 Â± 2.3 96.2 Â± 0.4 62.1 Â± 3.4 68.3 Â± 1.9 56.8 Â± 8.9 21.2 Â± 4.6\\nDeepLabv3+ (Chen et al., 2018) 54.3 Â± 2.1 62.1 Â± 2.5 96.4 Â± 0.5 59.2 Â± 2.4 63.6 Â± 2.5 55.5 Â± 10.7 21.3 Â± 5.5\\nDAF (Wang et al., 2018b) 55.8 Â± 1.5 62.8 Â± 1.8 96.6 Â± 0.6 62.8 Â± 2.3 66.5 Â± 1.1 52.8 Â± 4.2 20.3 Â± 2.3\\nGG-Net (our method) 56.6Â± 1.9 64.1 Â± 2.1 96.6 Â± 0.3 63.3 Â± 3.6 69.7 Â± 0.4 48.6 Â± 7.2 18.8 Â± 3.3\\nSK-U-Net (Byra et al., 2020), DeeplabV3+ (Chen et al., 2018);\\nas well as attention-based methods : AG-Unet (Schlemper\\net al., 2019), and DAF (Wang et al., 2018b). To provide fair\\ncomparisons, we obtain the segmentation results of compared'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='as well as attention-based methods : AG-Unet (Schlemper\\net al., 2019), and DAF (Wang et al., 2018b). To provide fair\\ncomparisons, we obtain the segmentation results of compared\\nmethods by downloading their public implementations and re-\\ntraining their networks on our dataset. Similarly, we also use\\nthe CRF (Kr Â¨ahenbÂ¨uhl and Koltun, 2011) to post-process the\\npredicted segmentation maps of compared methods.\\nQuantitative comparisons. Table 4 reports the mean and\\nstandard deviation values of the seven metrics among our\\nmethod and all the competitors on our collected dataset, while\\nTable 5 summarizes the mean and standard deviation scores of\\nseven metrics on BUSI. Compared to other segmentation meth-\\nods, our method has larger Jaccard, Dice, Accuracy, recall, and\\nprediction scores, as well as smaller HD and ABD values. It\\nindicates that our GG-Net can more accurately identify breast\\nlesions from ultrasound images than all the competitors.'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='prediction scores, as well as smaller HD and ABD values. It\\nindicates that our GG-Net can more accurately identify breast\\nlesions from ultrasound images than all the competitors.\\nOn the other hand, when further looking into the metric re-\\nsults in Table 4 and Table 5, we can ï¬nd that the segmentation\\nperformance on our collected dataset (see Table 4) is better than\\nthe results on the public BUSI dataset (seeTable 5) with respect\\nto all seven evaluation metrics. The reason behind is that the\\nultrasound image quality in our dataset is better than that in\\nBUSI, thereby making the better segmentation performance.\\nUtilizing BUSIâ€™s normal cases. The general purpose of breast\\nlesion segmentation in the clinical usage is mainly for the le-\\nsion assessment, tracking the lesion change, and identifying\\ndistribution and seriousness of lesions. As a result, people usu-\\nally assume that the input ultrasound samples possess one or\\nmore lesions, and then conduct the breast lesion segmentation'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='distribution and seriousness of lesions. As a result, people usu-\\nally assume that the input ultrasound samples possess one or\\nmore lesions, and then conduct the breast lesion segmentation\\nfor clinical analysis. Here, we conduct another experiment by\\nincluding the normal cases of BUSI into the training data and\\nre-training all the compared methods and our network to obtain\\ntheir new results. Table 5 and Table 6 report the results of each\\nmethod with and without the BUSIâ€™s normal cases. According\\nto the results, we can easily ï¬nd that the quantitative results\\nof all the competitors and our network tend to be worse when\\nconsidering normal cases in the network training. Among all\\nthe segmentation methods, our network still achieves the best\\nperformance of all seven metrics even though the normal cases\\nare added into the training set and the testing set.\\nVisual comparisons. We also visually compare the breast le-\\nsion segmentation results produced by our network and com-'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='are added into the training set and the testing set.\\nVisual comparisons. We also visually compare the breast le-\\nsion segmentation results produced by our network and com-\\npared methods; see Fig. 9 for examples. U-Net, U-Net++,\\nFPN, and DeeplabV3+ tend to neglect breast lesion details or\\nwrongly classify non-lesion regions as breast lesions into their\\npredicted segmentation maps, while our method produces more\\naccurate segmentation results on breast lesion regions. Fur-\\nthermore, our results are most consistent with ground truths\\n(see Fig. 9 (b)) among all segmentation results. This proves\\nthe effectiveness of long-range dependencies and breast lesion\\n10(a) inputs (b) ground truths (e) U-Net [29] (f) U-Net++ [46] (g) FPN [47](d) DeepLabv3+[42]\\n(c) Ours\\nFigure 9: Visual comparison of the breast lesion segmentation maps produced by different methods. (a) input breast ultrasound images; (b) ground truths; (c)-(g)'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='(c) Ours\\nFigure 9: Visual comparison of the breast lesion segmentation maps produced by different methods. (a) input breast ultrasound images; (b) ground truths; (c)-(g)\\nare segmentation results produced by our method, DeeplabV3+ (Chen et al., 2018), U-Net (Ronneberger et al., 2015), U-Net++ (Zhou et al., 2018), and FPN (Lin\\net al., 2017).\\nboundaries in our method.\\n5. Application\\nNote that our network can be retrained for other ultrasound\\nimage segmentation tasks. Hence, we further evaluate the\\neffectiveness of our network by testing it on the ultrasound\\nprostate segmentation task. To conduct fair comparisons, we\\nfollow the same experimental setting of a recent prostate seg-\\nmentation work, i.e., DAF (Wang et al., 2018b), to obtain\\nthe prostate segmentation results of our network. We use the\\nDAFâ€™s training set to train our network, test our method on\\nthe DAFâ€™s testing set, and report the results of same four met-\\nric (i.e., Jaccard, Dice, Recall and Precision; see (Wang et al.,'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='DAFâ€™s training set to train our network, test our method on\\nthe DAFâ€™s testing set, and report the results of same four met-\\nric (i.e., Jaccard, Dice, Recall and Precision; see (Wang et al.,\\n2018b) for their deï¬nitions) for comparisons. Table 7 sum-\\nmarizes the comparison results on four metrics between our\\nmethod and state-of-the-art networks, including U-Net (Ron-\\nneberger et al., 2015), FCN (Lin et al., 2017), BCRNN (Yang\\net al., 2017), and DAF (Wang et al., 2018b); see (Wang et al.,\\n2018b) for details of these compared methods. Apparently,\\nour method outperforms all the competitors on almost all the\\nfour metrics, demonstrating that our method can also identify\\nprostate regions better from ultrasound images. It further veri-\\nï¬es the effectiveness of the developed segmentation network in\\nour work.\\n11(a) Input ultrasound images (b) Ground truths (c) Our results'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='ï¬es the effectiveness of the developed segmentation network in\\nour work.\\n11(a) Input ultrasound images (b) Ground truths (c) Our results\\nFigure 10: Failure cases. (a) Input ultrasound images. (b) Ground truths of the breast lesion segmentation. (c) Segmentation results produced by our network.\\nTable 7: Metric results of different methods on ultrasound prostate segmenta-\\ntion.\\nJaccard% Dice% Recall% Precision%\\nFCN (Lin et al., 2017) 85.1 91.9 90.8 93.3\\nBCRNN (Yang et al., 2017) 86.0 92.4 90.5 94.5\\nU-Net (Ronneberger et al., 2015) 87.1 93.0 96.8 89.9\\nDAF (Wang et al., 2018b) 91.0 95.3 97.0 93.7\\nGG-Net (ours) 91.2 95.4 95.7 95.1\\n6. Discussions\\nBreast cancer is the most frequently diagnosed cancer and\\nthe leading cause of cancer-related death among women world-\\nwide. The automatic breast lesion segmentation from ultra-\\nsound images assists the doctors in ï¬nding early signals of\\nbreast cancer, which is of great importance in clinical practice.'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='wide. The automatic breast lesion segmentation from ultra-\\nsound images assists the doctors in ï¬nding early signals of\\nbreast cancer, which is of great importance in clinical practice.\\nTraditional CNN-based methods (Chen et al., 2018, 2017b,\\n2014; Ronneberger et al., 2015; Lin et al., 2017) conducted\\nconvolutional operations in local regions to learn deep discrim-\\ninative features for medical image analysis and thus suffered\\nfrom unsatisfactory segmentation accuracy due to the limited\\nreceptive ï¬elds of their local convolutions.\\nRecently, capturing non-local long-range pixel dependencies\\nhas achieved superior prediction performance in many medical\\nimaging community (Qi et al., 2019; Dou et al., 2018) by de-\\nvising non-local blocks. However, these non-local blocks are\\nonly embedded into the deep CNN layers for network predic-\\ntions. However, the deep layers of a segmentation network are\\nresponsible for capturing cues of the whole breast lesions and'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='only embedded into the deep CNN layers for network predic-\\ntions. However, the deep layers of a segmentation network are\\nresponsible for capturing cues of the whole breast lesions and\\nsomehow lack parts of breast lesion regions due to the rela-\\ntively larger receptive ï¬elds than shallow CNN layers. In this\\nregard, we integrate all CNN layers to produce multi-level in-\\ntegrated features (MLIF) as a guidance information of the non-\\nlocal blocks to complement more breast lesion boundary details\\n(neglected by deep CNN layers).\\nThis project presented a global guidance network (denoted\\nas â€œGG-Netâ€) with a spatial guidance block and a channel\\nguidance block to leverage guidance information for improving\\nlong-range dependency feature learning in spatial and channel\\nmanners. Moreover, a breast lesion boundary detection mod-\\nule is devised to learn boundary details for futher reï¬ning the\\nbreast lesion segmentation performance. Compared with state-'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='manners. Moreover, a breast lesion boundary detection mod-\\nule is devised to learn boundary details for futher reï¬ning the\\nbreast lesion segmentation performance. Compared with state-\\nof-the-art methods, our network achieves a signiï¬cant (p-value\\n<0.05, see Table 8) improvement on two datasets, which proves\\nthe effectiveness of the developed spatial and channel guidance\\n12Table 8: P-values between our method and other compared methods on different evaluation metrics.\\nMetrics U-Net vs Ours U-Net++ vs OursTernausNet vs OursFPN vs Ours AG-Net vs OursDAF vs Ours DeepLabv3+ vs Ours\\nJaccard 1.62Ã—10âˆ’7 1.64Ã—10âˆ’5 9.41Ã—10âˆ’5 4.60Ã—10âˆ’2 3.45Ã—10âˆ’4 4.20Ã—10âˆ’3 3.00Ã—10âˆ’6\\nDice 3.94Ã—10âˆ’8 2.81Ã—10âˆ’5 5.17Ã—10âˆ’5 4.00Ã—10âˆ’2 4.19Ã—10âˆ’4 1.10Ã—10âˆ’3 7.86Ã—10âˆ’6\\nAccuracy 1.74Ã—10âˆ’3 3.28Ã—10âˆ’3 3.48Ã—10âˆ’2 3.70Ã—10âˆ’2 1.33Ã—10âˆ’3 4.80Ã—10âˆ’2 3.57Ã—10âˆ’2\\nRecall 9.80Ã—10âˆ’11 2.31Ã—10âˆ’4 3.02Ã—10âˆ’3 3.50Ã—10âˆ’2 4.55Ã—10âˆ’3 1.20Ã—10âˆ’3 3.97Ã—10âˆ’9\\nPrecision 6.70Ã—10âˆ’3 2.36Ã—10âˆ’3 3.24Ã—10âˆ’4 6.60Ã—10âˆ’3 2.06Ã—10âˆ’3 8.90Ã—10âˆ’3 2.70Ã—10âˆ’3'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='Recall 9.80Ã—10âˆ’11 2.31Ã—10âˆ’4 3.02Ã—10âˆ’3 3.50Ã—10âˆ’2 4.55Ã—10âˆ’3 1.20Ã—10âˆ’3 3.97Ã—10âˆ’9\\nPrecision 6.70Ã—10âˆ’3 2.36Ã—10âˆ’3 3.24Ã—10âˆ’4 6.60Ã—10âˆ’3 2.06Ã—10âˆ’3 8.90Ã—10âˆ’3 2.70Ã—10âˆ’3\\nHD 1.64Ã—10âˆ’6 6.10Ã—10âˆ’6 7.35Ã—10âˆ’4 4.00Ã—10âˆ’4 2.89Ã—10âˆ’6 8.40Ã—10âˆ’3 4.00Ã—10âˆ’4\\nABD 1.01Ã—10âˆ’6 1.73Ã—10âˆ’7 8.07Ã—10âˆ’5 2.00Ã—10âˆ’3 6.03Ã—10âˆ’5 5.50Ã—10âˆ’3 7.40Ã—10âˆ’3\\nblock as well as boundary detection block. Moreover, com-\\npared with other segmentation networks, our method has bet-\\nter performance on relatively less obvious lesion segmentation.\\nThis is crucial in the clinical practice, especially for breast ul-\\ntrasound, where most of the lesions have low contrast, shad-\\nows, and blurry boundaries.\\nNote that the elastography image encodes the density of the\\ntissue in the screen. In our future work, we will leverage elas-\\ntography images for further boosting breast lesion segmenta-\\ntion results in ultrasound.\\nFailure cases. Like other breast lesion segmentation methods,\\nour network tends to fail in fully detecting breast lesion regions'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='tion results in ultrasound.\\nFailure cases. Like other breast lesion segmentation methods,\\nour network tends to fail in fully detecting breast lesion regions\\nwhen the target breast lesion has a very large size and a com-\\nplicated intensity distribution inside it, or unclear boundaries.\\nFig. 10 shows two examples, where our results in (c) wrongly\\nidentify non-lesion regions as lesion ones, or neglect a part of\\nbreast lesion regions of the input ultrasound image when com-\\nparing to the ground truths (see (b)).\\nStatistical test. To investigate the statistical signiï¬cance of the\\nproposed network over compared methods on different quanti-\\ntative metrics, we conduct a statistical analysis of p-values and\\nshow the p-values of our network against compared methods\\nin terms of different metrics in Table 8. As shown in Table 8,\\nwe can ï¬nd that the p-values of all the seven paired methods\\nare almost smaller than 0.05 for all the seven metrics, demon-'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='in terms of different metrics in Table 8. As shown in Table 8,\\nwe can ï¬nd that the p-values of all the seven paired methods\\nare almost smaller than 0.05 for all the seven metrics, demon-\\nstrating that our method can be regarded as reaching a signif-\\nicant improvement over the other six compared methods on\\nthese evaluation metrics. Note that the Accuracy p-values of\\nour method over TernausNet, FPN, DAF, and DeepLabv3+ are\\n3.48Ã—10âˆ’2, 3.70Ã—10âˆ’2, 4.80Ã—10âˆ’2, and 3.57Ã—10âˆ’2, which are\\ncloser to 0.05. It indicates that our method has a similar Accu-\\nracy performance to TernausNet, FPN, DAF, and DeepLabv3+.\\nGenerally, the superior metric performance of our method in\\nTables 4, 5, and 6 shows that our network can better segment\\nbreast lesions from ultrasound than other compared segmenta-\\ntion methods.\\n7. Conclusion\\nThis paper presents a global guidance network (GG-Net)\\nequipped with a global guidance block and a breast lesion\\nboundary detection module for breast lesion segmentation in'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='7. Conclusion\\nThis paper presents a global guidance network (GG-Net)\\nequipped with a global guidance block and a breast lesion\\nboundary detection module for breast lesion segmentation in\\nultrasound images. The global guidance block aims to combine\\nthe multi-layer context information as guidance information to\\nlearn the long-term non-local features in spatial and channel\\nmanners. The breast lesion boundary detection predicts addi-\\ntional breast lesion boundary map to assist in improving the\\nsegmentation performance. We evaluate our network on a pub-\\nlic dataset and our collected dataset of breast lesion segmenta-\\ntion in ultrasound images by comparing it against state-of-the-\\nart methods, and the experimental results show that our net-\\nwork can more accurately segment the breast lesions than all\\nthe competitors. We also show the application of our network\\non the ultrasound prostate segmentation task and our network\\nalso has a higher segmentation accuracy than state-of-the-art\\nmethods.'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='the competitors. We also show the application of our network\\non the ultrasound prostate segmentation task and our network\\nalso has a higher segmentation accuracy than state-of-the-art\\nmethods.\\nAcknowledgement.\\nThe work described in this paper was supported by Key-\\nArea Research and Development Program of Guangdong\\nProvince, China under Project No. 2020B010165004, Hong\\nKong Innovation and Technology Fund under Project No.\\nITS/311/18FP, National Natural Science Foundation of China\\n(Grant No. 61902275), National Natural Science Foundation\\nof China under Project No. U1813204, Natural Science Foun-\\ndation of SHENZHEN City NO: JCYJ20190806150001764,\\nNatural Science Foundation of GUANGDONG Province\\nNo:2020A1515010978, and The Sanming Project of Medicine\\nin Shenzhen training project No: SYJY201802.\\nReferences\\nAhn, C.K., Heo, C., Jin, H., Kim, J.H., 2017. A novel deep\\nlearning-based approach to high accuracy breast density esti-\\nmation in digital mammography, in: Medical Imaging 2017:'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='References\\nAhn, C.K., Heo, C., Jin, H., Kim, J.H., 2017. A novel deep\\nlearning-based approach to high accuracy breast density esti-\\nmation in digital mammography, in: Medical Imaging 2017:\\n13Computer-Aided Diagnosis, International Society for Optics\\nand Photonics. p. 101342O.\\nAl-Dhabyani, W., Gomaa, M., Khaled, H., FahmyaH, A., 2020.\\nDataset of breast ultrasound images. Data in Brief 28.\\nAmerican Cancer Society, 2019. Cancer facts & ï¬gures 2019.\\nAshton, E.A., Parker, K.J., 1995. Multiple resolution bayesian\\nsegmentation of ultrasound images. Ultrasonic imaging 17,\\n291â€“304.\\nBoukerroui, D., Basset, O., Guerin, N., Baskurt, A., 1998. Mul-\\ntiresolution texture based adaptive clustering algorithm for\\nbreast lesion segmentation. European Journal of Ultrasound\\n8, 135â€“144.\\nByra, M., Jarosik, P., Szubert, A., Galperin, M., Ojeda-\\nFournier, H., Olson, L., Oâ€™Boyle, M., Comstock, C., Andre,\\nM., 2020. Breast mass segmentation in ultrasound with se-'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='8, 135â€“144.\\nByra, M., Jarosik, P., Szubert, A., Galperin, M., Ojeda-\\nFournier, H., Olson, L., Oâ€™Boyle, M., Comstock, C., Andre,\\nM., 2020. Breast mass segmentation in ultrasound with se-\\nlective kernel u-net convolutional neural network. Biomedi-\\ncal Signal Processing and Control 61, 102027.\\nCanny, J., 1986. A computational approach to edge detection.\\nIEEE Transactions on pattern analysis and machine intelli-\\ngence , 679â€“698.\\nChen, C.M., Lu, H.H.S., Huang, Y .S., 2002. Cell-based dual\\nsnake model: a new approach to extracting highly winding\\nboundaries in the ultrasound images. Ultrasound in medicine\\n& biology 28, 1061â€“1073.\\nChen, L., Zhang, H., Xiao, J., Nie, L., Shao, J., Liu, W., Chua,\\nT.S., 2017a. SCA-CNN: Spatial and channel-wise attention\\nin convolutional networks for image captioning, in: Proceed-\\nings of the IEEE conference on computer vision and pattern\\nrecognition, pp. 5659â€“5667.\\nChen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille,'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='ings of the IEEE conference on computer vision and pattern\\nrecognition, pp. 5659â€“5667.\\nChen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille,\\nA.L., 2014. Semantic image segmentation with deep con-\\nvolutional nets and fully connected crfs. arXiv preprint\\narXiv:1412.7062 .\\nChen, L.C., Papandreou, G., Kokkinos, I., Murphy, K., Yuille,\\nA.L., 2017b. Deeplab: Semantic image segmentation with\\ndeep convolutional nets, atrous convolution, and fully con-\\nnected crfs. IEEE transactions on pattern analysis and ma-\\nchine intelligence 40, 834â€“848.\\nChen, L.C., Zhu, Y ., Papandreou, G., Schroff, F., Adam, H.,\\n2018. Encoder-decoder with atrous separable convolution\\nfor semantic image segmentation, in: Proceedings of the\\nEuropean conference on computer vision (ECCV), pp. 801â€“\\n818.\\nDhungel, N., Carneiro, G., Bradley, A.P., 2017. A deep learn-\\ning approach for the analysis of masses in mammograms\\nwith minimal user intervention. Medical Image Analysis 37,\\n114â€“128.'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='818.\\nDhungel, N., Carneiro, G., Bradley, A.P., 2017. A deep learn-\\ning approach for the analysis of masses in mammograms\\nwith minimal user intervention. Medical Image Analysis 37,\\n114â€“128.\\nDou, Q., Chen, H., Jin, Y ., Yu, L., Qin, J., Heng, P.A., 2016.\\n3D deeply supervised network for automatic liver segmenta-\\ntion from ct volumes, in: International Conference on Med-\\nical Image Computing and Computer-Assisted Intervention\\n(MICCAI), Springer. pp. 149â€“157.\\nDou, T., Zhang, L., Zheng, H., Zhou, W., 2018. Local and\\nnon-local deep feature fusion for malignancy characteriza-\\ntion of hepatocellular carcinoma, in: International Confer-\\nence on Medical Image Computing and Computer-Assisted\\nIntervention (MICCAI), Springer. pp. 472â€“479.\\nFeng, M., Lu, H., Ding, E., 2019. Attentive feedback network\\nfor boundary-aware salient object detection, in: Proceedings\\nof the IEEE Conference on Computer Vision and Pattern\\nRecognition, pp. 1623â€“1632.'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='for boundary-aware salient object detection, in: Proceedings\\nof the IEEE Conference on Computer Vision and Pattern\\nRecognition, pp. 1623â€“1632.\\nHeinrich, M.P., Oktay, O., Bouteldja, N., 2019. Obelisk-Net:\\nFewer layers to solve 3D multi-organ segmentation with\\nsparse deformable convolutions. Medical image analysis 54,\\n1â€“9.\\nHou, R., Ma, B., Chang, H., Gu, X., Shan, S., Chen, X.,\\n2019. Interaction-and-aggregation network for person re-\\nidentiï¬cation, in: Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition, pp. 9317â€“9326.\\nHu, J., Shen, L., Sun, G., 2018. Squeeze-and-excitation net-\\nworks, in: Proceedings of the IEEE conference on computer\\nvision and pattern recognition, pp. 7132â€“7141.\\nHu, Y ., Guo, Y ., Wang, Y ., Yu, J., Li, J., Zhou, S., Chang, C.,\\n2019. Automatic tumor segmentation in breast ultrasound\\nimages using a dilated fully convolutional network combined\\nwith an active contour model. Medical physics 46, 215â€“228.'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='2019. Automatic tumor segmentation in breast ultrasound\\nimages using a dilated fully convolutional network combined\\nwith an active contour model. Medical physics 46, 215â€“228.\\nHuang, S.F., Chen, Y .C., Moon, W.K., 2008. Neural net-\\nwork analysis applied to tumor segmentation on 3D breast\\nultrasound images, in: IEEE International Symposium on\\nBiomedical Imaging: From Nano to Macro, pp. 1303â€“1306.\\nIglovikov, V ., Shvets, A., 2018. Ternausnet: U-Net with vgg11\\nencoder pre-trained on imagenet for image segmentation.\\narXiv preprint arXiv:1801.05746 .\\nJoutard, S., Dorent, R., Isaac, A., Ourselin, S., Vercauteren, T.,\\nModat, M., 2019. Permutohedral attention module for ef-\\nï¬cient non-local neural networks, in: International Confer-\\n14ence on Medical Image Computing and Computer-Assisted\\nIntervention, Springer. pp. 393â€“401.\\nKirberger, R.M., 1995. Imaging artifacts in diagnostic ultra-\\nsound - a review. Veterinary Radiology & Ultrasound 36,\\n297â€“306.'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='Intervention, Springer. pp. 393â€“401.\\nKirberger, R.M., 1995. Imaging artifacts in diagnostic ultra-\\nsound - a review. Veterinary Radiology & Ultrasound 36,\\n297â€“306.\\nKrÂ¨ahenbÂ¨uhl, P., Koltun, V ., 2011. Efï¬cient inference in fully\\nconnected crfs with gaussian edge potentials, in: Advances\\nin neural information processing systems, pp. 109â€“117.\\nKwak, J.I., Kim, S.H., Kim, N.C., 2005. RD-based seeded re-\\ngion growing for extraction of breast tumor in an ultrasound\\nvolume, in: International Conference on Computational and\\nInformation Science, Springer. pp. 799â€“808.\\nLei, B., Huang, S., Li, R., Bian, C., Li, H., Chou, Y .H., Cheng,\\nJ.Z., 2018. Segmentation of breast anatomy for automated\\nwhole breast ultrasound images with boundary regularized\\nconvolutional encoderâ€“decoder network. Neurocomputing\\n321, 178â€“186.\\nLi, H., He, X., Zhou, F., Yu, Z., Ni, D., Chen, S., Wang, T.,\\nLei, B., 2018. Dense deconvolutional network for skin le-\\nsion segmentation. IEEE Journal of Biomedical and Health'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='321, 178â€“186.\\nLi, H., He, X., Zhou, F., Yu, Z., Ni, D., Chen, S., Wang, T.,\\nLei, B., 2018. Dense deconvolutional network for skin le-\\nsion segmentation. IEEE Journal of Biomedical and Health\\nInformatics 23, 527â€“537.\\nLin, T.Y ., DollÂ´ar, P., Girshick, R., He, K., Hariharan, B., Be-\\nlongie, S., 2017. Feature pyramid networks for object detec-\\ntion, in: Proceedings of the IEEE conference on computer\\nvision and pattern recognition, pp. 2117â€“2125.\\nLiu, B., Cheng, H.D., Huang, J., Tian, J., Tang, X., Liu, J.,\\n2010. Fully automatic and segmentation-robust classiï¬ca-\\ntion of breast tumors based on local texture analysis of ultra-\\nsound images. Pattern Recognition 43, 280â€“298.\\nLo, C., Shen, Y .W., Huang, C.S., Chang, R.F., 2014. Computer-\\naided multiview tumor detection for automated whole breast\\nultrasound. Ultrasonic imaging 36, 3â€“17.\\nMadabhushi, A., Metaxas, D., 2002. Automatic boundary ex-\\ntraction of ultrasonic breast lesions, in: Proceedings IEEE'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='ultrasound. Ultrasonic imaging 36, 3â€“17.\\nMadabhushi, A., Metaxas, D., 2002. Automatic boundary ex-\\ntraction of ultrasonic breast lesions, in: Proceedings IEEE\\nInternational Symposium on Biomedical Imaging, IEEE. pp.\\n601â€“604.\\nMadabhushi, A., Metaxas, D.N., 2003. Combining low-, high-\\nlevel and empirical domain knowledge for automated seg-\\nmentation of ultrasonic breast lesions. IEEE Transactions\\non Medical Imaging 22, 155â€“169.\\nMishra, D., Chaudhury, S., Sarkar, M., Soin, A.S., 2018. Ul-\\ntrasound image segmentation: A deeply supervised network\\nwith attention to boundaries. IEEE Transactions on Biomed-\\nical Engineering 66, 1637â€“1648.\\nMoon, W.K., Lo, C.M., Chen, R.T., Shen, Y .W., Chang, J.M.,\\nHuang, C.S., Chen, J.H., Hsu, W.W., Chang, R.F., 2014. Tu-\\nmor detection in automated breast ultrasound images using\\nquantitative tissue clustering. Medical physics 41, 042901.\\nMordang, J.J., Gubern-MÂ´erida, A., den Heeten, G., Karssemei-\\njer, N., 2016a. Reducing false positives of microcalciï¬cation'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='quantitative tissue clustering. Medical physics 41, 042901.\\nMordang, J.J., Gubern-MÂ´erida, A., den Heeten, G., Karssemei-\\njer, N., 2016a. Reducing false positives of microcalciï¬cation\\ndetection systems by removal of breast arterial calciï¬cations.\\nMedical physics 43, 1676â€“1687.\\nMordang, J.J., Janssen, T., Bria, A., Kooi, T., Gubern-M Â´erida,\\nA., Karssemeijer, N., 2016b. Automatic microcalciï¬ca-\\ntion detection in multi-vendor mammography using con-\\nvolutional neural networks, in: International Workshop on\\nBreast Imaging, Springer. pp. 35â€“42.\\nOthman, A.A., Tizhoosh, H.R., 2011. Segmentation of breast\\nultrasound images using neural networks, in: Engineering\\nApplications of Neural Networks. Springer, pp. 260â€“269.\\nPeng, C., Zhang, X., Yu, G., Luo, G., Sun, J., 2017. Large ker-\\nnel mattersâ€“improve semantic segmentation by global con-\\nvolutional network, in: Proceedings of the IEEE conference\\non computer vision and pattern recognition, pp. 4353â€“4361.'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='nel mattersâ€“improve semantic segmentation by global con-\\nvolutional network, in: Proceedings of the IEEE conference\\non computer vision and pattern recognition, pp. 4353â€“4361.\\nQi, K., Yang, H., Li, C., Liu, Z., Wang, M., Liu, Q., Wang,\\nS., 2019. X-Net: Brain stroke lesion segmentation based on\\ndepthwise separable convolution and long-range dependen-\\ncies. arXiv preprint arXiv:1907.07000 .\\nRonneberger, O., Fischer, P., Brox, T., 2015. U-Net: Con-\\nvolutional networks for biomedical image segmentation, in:\\nInternational Conference on Medical Image Computing and\\nComputer-Assisted Intervention (MICCAI), Springer. pp.\\n234â€“241.\\nRoy, A.G., Navab, N., Wachinger, C., 2018. Recalibrating fully\\nconvolutional networks with spatial and channel â€œsqueeze\\nand excitationâ€ blocks. IEEE transactions on medical imag-\\ning 38, 540â€“549.\\nSchlemper, J., Oktay, O., Schaap, M., Heinrich, M., Kainz, B.,\\nGlocker, B., Rueckert, D., 2019. Attention gated networks:'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='and excitationâ€ blocks. IEEE transactions on medical imag-\\ning 38, 540â€“549.\\nSchlemper, J., Oktay, O., Schaap, M., Heinrich, M., Kainz, B.,\\nGlocker, B., Rueckert, D., 2019. Attention gated networks:\\nLearning to leverage salient regions in medical images. Med-\\nical image analysis 53, 197â€“207.\\nShan, J., Cheng, H., Wang, Y ., 2012. Completely automated\\nsegmentation approach for breast ultrasound images using\\nmultiple-domain features. Ultrasound in medicine & biology\\n38, 262â€“275.\\n15Shan, J., Cheng, H.D., Wang, Y ., 2008. A novel automatic seed\\npoint selection algorithm for breast ultrasound images, in:\\n2008 19th International Conference on Pattern Recognition,\\nIEEE. pp. 1â€“4.\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L.,\\nGomez, A.N., Kaiser, Å., Polosukhin, I., 2017. Attention is\\nall you need, in: Advances in neural information processing\\nsystems, pp. 5998â€“6008.\\nWang, X., Girshick, R., Gupta, A., He, K., 2018a. Non-local'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='all you need, in: Advances in neural information processing\\nsystems, pp. 5998â€“6008.\\nWang, X., Girshick, R., Gupta, A., He, K., 2018a. Non-local\\nneural networks, in: Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition, pp. 7794â€“7803.\\nWang, Y ., Deng, Z., Hu, X., Zhu, L., Yang, X., Xu, X., Heng,\\nP.A., Ni, D., 2018b. Deep attentional features for prostate\\nsegmentation in ultrasound, in: International Conference on\\nMedical Image Computing and Computer-Assisted Interven-\\ntion (MICCAI), Springer. pp. 523â€“530.\\nXian, M., Zhang, Y ., Cheng, H.D., 2015. Fully automatic seg-\\nmentation of breast ultrasound images based on breast char-\\nacteristics in space and frequency domains. Pattern Recog-\\nnition 48, 485â€“497.\\nXiao, G., Brady, M., Noble, J.A., Zhang, Y ., 2002. Segmen-\\ntation of ultrasound b-mode images with intensity inhomo-\\ngeneity correction. IEEE Transactions on Medical Imaging\\n21, 48â€“57.\\nXu, Y ., Wang, Y ., Yuan, J., Cheng, Q., Wang, X., Carson, P.L.,'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='tation of ultrasound b-mode images with intensity inhomo-\\ngeneity correction. IEEE Transactions on Medical Imaging\\n21, 48â€“57.\\nXu, Y ., Wang, Y ., Yuan, J., Cheng, Q., Wang, X., Carson, P.L.,\\n2019. Medical breast ultrasound image segmentation by ma-\\nchine learning. Ultrasonics 91, 1â€“9.\\nYang, X., Yu, L., Wu, L., Wang, Y ., Ni, D., Qin, J., Heng, P.A.,\\n2017. Fine-grained recurrent neural networks for automatic\\nprostate segmentation in ultrasound images, in: Thirty-First\\nAAAI Conference on Artiï¬cial Intelligence.\\nYap, M.H., Pons, G., MartÂ´Ä±, J., Ganau, S., SentÂ´Ä±s, M., Zwigge-\\nlaar, R., Davison, A.K., Mart Â´Ä±, R., 2017. Automated breast\\nultrasound lesions detection using convolutional neural net-\\nworks. IEEE Journal of Biomedical and Health Informatics\\n22, 1218â€“1226.\\nYezzi, A., Kichenassamy, S., Kumar, A., Olver, P., Tannen-\\nbaum, A., 1997. A geometric snake model for segmentation\\nof medical imagery. IEEE Transactions on Medical Imaging\\n16, 199â€“209.'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='Yezzi, A., Kichenassamy, S., Kumar, A., Olver, P., Tannen-\\nbaum, A., 1997. A geometric snake model for segmentation\\nof medical imagery. IEEE Transactions on Medical Imaging\\n16, 199â€“209.\\nYu, L., Chen, H., Dou, Q., Qin, J., Heng, P.A., 2016. Au-\\ntomated melanoma recognition in dermoscopy images via\\nvery deep residual networks. IEEE Transactions on Medical\\nImaging 36, 994â€“1004.\\nYu, Z., Jiang, X., Zhou, F., Qin, J., Ni, D., Chen, S., Lei,\\nB., Wang, T., 2018. Melanoma recognition in dermoscopy\\nimages via aggregated deep convolutional features. IEEE\\nTransactions on Biomedical Engineering 66, 1006â€“1016.\\nYu, Z., Tan, E.L., Ni, D., Qin, J., Chen, S., Li, S., Lei, B.,\\nWang, T., 2017. A deep convolutional neural network-based\\nframework for automatic fetal facial standard plane recog-\\nnition. IEEE Journal of Biomedical and Health Informatics\\n22, 874â€“885.\\nZhang, H., Dana, K., Shi, J., Zhang, Z., Wang, X., Tyagi, A.,\\nAgrawal, A., 2018. Context encoding for semantic segmen-'),\n",
       " Document(metadata={'arxiv_id': '2104.01896v1', 'title': 'Global Guidance Network for Breast Lesion Segmentation in Ultrasound Images', 'section': 'body', 'authors': 'Cheng Xue, Lei Zhu, Huazhu Fu'}, page_content='nition. IEEE Journal of Biomedical and Health Informatics\\n22, 874â€“885.\\nZhang, H., Dana, K., Shi, J., Zhang, Z., Wang, X., Tyagi, A.,\\nAgrawal, A., 2018. Context encoding for semantic segmen-\\ntation, in: Proceedings of the IEEE conference on computer\\nvision and pattern recognition, pp. 7151â€“7160.\\nZhang, Z., Xie, Y ., Xing, F., McGough, M., Yang, L., 2017.\\nMDNet: A semantically and visually interpretable medi-\\ncal image diagnosis network, in: Proceedings of the IEEE\\nconference on computer vision and pattern recognition, pp.\\n6428â€“6436.\\nZhao, H., Shi, J., Qi, X., Wang, X., Jia, J., 2017. Pyramid scene\\nparsing network, in: Proceedings of the IEEE conference on\\ncomputer vision and pattern recognition, pp. 2881â€“2890.\\nZhou, Z., Siddiquee, M.M.R., Tajbakhsh, N., Liang, J., 2018.\\nUNet++: A nested U-Net architecture for medical image\\nsegmentation, in: Deep Learning in Medical Image Analy-\\nsis and Multimodal Learning for Clinical Decision Support.\\nSpringer, pp. 3â€“11.\\n16'),\n",
       " Document(metadata={'arxiv_id': '1805.00357v1', 'title': 'Domain and Geometry Agnostic CNNs for Left Atrium Segmentation in 3D Ultrasound', 'section': 'title_abstract', 'authors': 'Markus A. Degel, Nassir Navab, Shadi Albarqouni'}, page_content='Title: Domain and Geometry Agnostic CNNs for Left Atrium Segmentation in 3D Ultrasound\\n\\nAbstract: Segmentation of the left atrium and deriving its size can help to predict and detect various cardiovascular conditions. Automation of this process in 3D Ultrasound image data is desirable, since manual delineations are time-consuming, challenging and observer-dependent. Convolutional neural networks have made improvements in computer vision and in medical image analysis. They have successfully been applied to segmentation tasks and were extended to work on volumetric data. In this paper we introduce a combined deep-learning based approach on volumetric segmentation in Ultrasound acquisitions with incorporation of prior knowledge about left atrial shape and imaging device. The results show, that including a shape prior helps the domain adaptation and the accuracy of segmentation is further increased with adversarial learning.'),\n",
       " Document(metadata={'arxiv_id': '1805.00357v1', 'title': 'Domain and Geometry Agnostic CNNs for Left Atrium Segmentation in 3D Ultrasound', 'section': 'body', 'authors': 'Markus A. Degel, Nassir Navab, Shadi Albarqouni'}, page_content='Domain and Geometry Agnostic CNNs for Left\\nAtrium Segmentation in 3D Ultrasound\\nMarkus A. Degel 1,2, Nassir Navab1,3, and Shadi Albarqouni 1\\n1 Computer Aided Medical Procedures (CAMP), Technische UniversitÂ¨ at MÂ¨ unchen,\\nMunich, Germany\\n2 TOMTEC Imaging Systems GmbH, Unterschleissheim, Germany\\n3 Whiting School of Engineering, Johns Hopkins University, Baltimore, USA\\nAbstract. Segmentation of the left atrium and deriving its size can\\nhelp to predict and detect various cardiovascular conditions. Automation\\nof this process in 3D Ultrasound image data is desirable, since manual\\ndelineations are time-consuming, challenging and observer-dependent.\\nConvolutional neural networks have made improvements in computer\\nvision and in medical image analysis. They have successfully been applied\\nto segmentation tasks and were extended to work on volumetric data.\\nIn this paper we introduce a combined deep-learning based approach on\\nvolumetric segmentation in Ultrasound acquisitions with incorporation'),\n",
       " Document(metadata={'arxiv_id': '1805.00357v1', 'title': 'Domain and Geometry Agnostic CNNs for Left Atrium Segmentation in 3D Ultrasound', 'section': 'body', 'authors': 'Markus A. Degel, Nassir Navab, Shadi Albarqouni'}, page_content='In this paper we introduce a combined deep-learning based approach on\\nvolumetric segmentation in Ultrasound acquisitions with incorporation\\nof prior knowledge about left atrial shape and imaging device. The results\\nshow, that including a shape prior helps the domain adaptation and the\\naccuracy of segmentation is further increased with adversarial learning.\\n1 Introduction\\nQuantiï¬cation of cardiac chambers and their functions stay the most important\\nobjective of cardiac imaging [6]. Left atrium (LA) physiology and function have\\nan impact on the whole heart performance and its size is a valuable indica-\\ntor for various cardiovascular conditions, such as atrial ï¬brillation (AF), stroke\\nand diastolic dysfunction [6]. Echocardiography, cardiac computed tomography\\n(CCT) and cardiac magnetic resonance (CMR) are options to examine the heart.\\nEchocardiography is the best choice, due to its wide availability, safety and good'),\n",
       " Document(metadata={'arxiv_id': '1805.00357v1', 'title': 'Domain and Geometry Agnostic CNNs for Left Atrium Segmentation in 3D Ultrasound', 'section': 'body', 'authors': 'Markus A. Degel, Nassir Navab, Shadi Albarqouni'}, page_content='(CCT) and cardiac magnetic resonance (CMR) are options to examine the heart.\\nEchocardiography is the best choice, due to its wide availability, safety and good\\nspatial and temporal resolution, without exposing the patients to harmful radia-\\ntion. Volumetric measurements consider changes in all spatial dimensions, how-\\never, to obtain reproducible and accurate three-dimensional (3D) measurements,\\nrequires expert experience and is time consuming [3]. Automated segmentation\\nand quantiï¬cation could help to reduce inter/intra-observer variabilities [11] and\\nmight also save costs and time in echocardiographic laboratories [3].\\nPrevious automatic and semi-automatic approaches for LA segmentation\\nhave focused CCT and CMR as a planning and guidance tool for LA catheter\\ninterventions [1]. For 3D Ultrasound (US), the left ventricle (LV) was the seg-\\nmentation target, since its size and function remain the most important indica-'),\n",
       " Document(metadata={'arxiv_id': '1805.00357v1', 'title': 'Domain and Geometry Agnostic CNNs for Left Atrium Segmentation in 3D Ultrasound', 'section': 'body', 'authors': 'Markus A. Degel, Nassir Navab, Shadi Albarqouni'}, page_content='interventions [1]. For 3D Ultrasound (US), the left ventricle (LV) was the seg-\\nmentation target, since its size and function remain the most important indica-\\ntion for a cardiac study [5]. LA segmentation in 3D US data has not received\\nmuch attention, apart from commercially available methods, which were also\\narXiv:1805.00357v1  [cs.CV]  20 Apr 2018Fig. 1: Row 1: device EPIQ 7C (dice coeï¬ƒcient: 0.74, training: Vivid E9), Row 2:\\ndevice Vivid E9 (dice coeï¬ƒcient: 0.56, training: EPIQ 7C), Row 3: device iE33\\n(dice coeï¬ƒcient: 0.6, training: Vivid E9), Left: volume slice, Middle: ground\\ntruth delineation of LA, Right: prediction by C3 architecture (Table 3).\\nsuccessfully validated against the gold standard CMR and CCT [9,2]. Another\\napproach exists, adapted from a segmentation framework for LV, based on B-\\nspline explicit active surfaces [1]. For transesophageal echocardiography (TEE),\\nstatistical shape models from a CT database were used [12]. Those methods,'),\n",
       " Document(metadata={'arxiv_id': '1805.00357v1', 'title': 'Domain and Geometry Agnostic CNNs for Left Atrium Segmentation in 3D Ultrasound', 'section': 'body', 'authors': 'Markus A. Degel, Nassir Navab, Shadi Albarqouni'}, page_content='spline explicit active surfaces [1]. For transesophageal echocardiography (TEE),\\nstatistical shape models from a CT database were used [12]. Those methods,\\nhowever, require more or less manual interaction. Recently, fully automatic seg-\\nmentation software for the left heart was validated against CMR [3].\\nConvolutional neural networks (CNN) and their special architectures of fully\\nconvolutional networks (FCN) have successfully been applied to the problem of\\nmedical image segmentation [10]. Those networks are trained end-to-end, process\\nthe whole image and perform pixel-wise segmentation. The V-Net extends this\\nidea to volumetric MRI image data and enables 3D segmentation with the help\\nof spatial convolutions, instead of processing the volumes slice-wise [7].\\nAutomated segmentation in US images is challenging, due to artifacts ( e.g\\nrespiratory motion) or operator dependent errors (e.g shadows, signal-dropouts).'),\n",
       " Document(metadata={'arxiv_id': '1805.00357v1', 'title': 'Domain and Geometry Agnostic CNNs for Left Atrium Segmentation in 3D Ultrasound', 'section': 'body', 'authors': 'Markus A. Degel, Nassir Navab, Shadi Albarqouni'}, page_content='Automated segmentation in US images is challenging, due to artifacts ( e.g\\nrespiratory motion) or operator dependent errors (e.g shadows, signal-dropouts).\\nIncluding shape priors in this task can help algorithms to yield more accurate\\nand anatomically plausible results. Oktay et al. introduced a way to incorporate\\nsuch a prior with the help of an autoencoder network, that leads segmentation\\nmasks to follow an underlying shape representation [8].\\nImage data might be diï¬€erent ( e.g with respect to resolution, contrast), due\\nto varying imaging protocols and device manufacturers [4]. Although the seg-\\nmentation task is equivalent, neural networks perform poorly when applied to\\ndata that was not available during training. Generating ground truth maps and\\nretraining a new model for each domain is not a scalable solution. The problem\\nof models to generalize to new image data can be approached by domain adap-'),\n",
       " Document(metadata={'arxiv_id': '1805.00357v1', 'title': 'Domain and Geometry Agnostic CNNs for Left Atrium Segmentation in 3D Ultrasound', 'section': 'body', 'authors': 'Markus A. Degel, Nassir Navab, Shadi Albarqouni'}, page_content='retraining a new model for each domain is not a scalable solution. The problem\\nof models to generalize to new image data can be approached by domain adap-\\ntation. Kamnitsas et al. successfully introduced the application of unsupervisedFig. 2: Overview of the combined architecture: Image data Xi is processed by\\nV-Net[7]. Dice-loss (Eq. 1) is calculated from the resulting segmentation Ë†Yi and\\nthe ground truth Yi. Additionally, Ë†Yi and Yi are encoded (E) to get the shape\\nconstrain. The feature maps of Xi are extracted from V-Net to be processed in\\nthe classiï¬er (C), which predicts a domain Ë†di. Cross-entropy between Ë†di and the\\nreal domain di determines the adversarial loss.\\ndomain adaptation for brain lesion segmentation in diï¬€erent MRI databases,\\nwhen an adversarial neural network was inï¬‚uencing the feature maps of a CNN,\\nwhich was employed for the segmentation task [4].\\nIn this work, LA segmentation in 3D US volumes is performed with the help'),\n",
       " Document(metadata={'arxiv_id': '1805.00357v1', 'title': 'Domain and Geometry Agnostic CNNs for Left Atrium Segmentation in 3D Ultrasound', 'section': 'body', 'authors': 'Markus A. Degel, Nassir Navab, Shadi Albarqouni'}, page_content='which was employed for the segmentation task [4].\\nIn this work, LA segmentation in 3D US volumes is performed with the help\\nof neural networks. For the volumetric segmentation, V-Net will be trained,\\ncombined with additional losses, taking into account the geometrical constrain\\nintroduced by the shape of the LA and the desired ability to generalize to dif-\\nferent US devices and settings.\\n2 Methodology\\nOur framework, as depicted in Fig. 2, consists of three deep-learning blocks\\nworking jointly to achieve our objectives; LA segmentation with the help of\\nincorporating a shape prior, and further being able to generalize well on diï¬€erent\\ndomains.\\nSegmentation. For the segmentation task, we employ V-Net[7] as a fully con-\\nvolutional network, which processes an image volume of sizen,Xi ={x1,...,x n},\\nxiâˆˆX and yields a segmentation mask Ë†Yi ={Ë†y1,..., Ë†yn}, Ë†yiâˆˆ Ë†Y in the original\\nresolution.X represents the feature space of US acquisitions and Ë†Y describes'),\n",
       " Document(metadata={'arxiv_id': '1805.00357v1', 'title': 'Domain and Geometry Agnostic CNNs for Left Atrium Segmentation in 3D Ultrasound', 'section': 'body', 'authors': 'Markus A. Degel, Nassir Navab, Shadi Albarqouni'}, page_content='xiâˆˆX and yields a segmentation mask Ë†Yi ={Ë†y1,..., Ë†yn}, Ë†yiâˆˆ Ë†Y in the original\\nresolution.X represents the feature space of US acquisitions and Ë†Y describes\\nthe probability of a voxel belonging to the segmentation.\\nThe objective function of V-Net is adapted to the segmentation task. It is\\nbased on the Dice coeï¬ƒcient (Eq. 1), taking into account the possible imbalance\\nof foreground to background, alleviating the need to re-weight samples.\\nLseg = 1âˆ’ 2Â· âˆ‘\\niyiÂ· Ë†yiâˆ‘\\niy2\\ni + âˆ‘\\ni Ë†y2\\ni\\n, (1)with Ë†yi being the prediction and yi the voxels of the ground truth Yi from the\\nbinary distributionY.\\nShape prior. Incorporation of the shape prior to help the segmentation task\\nis realized with the approach of [8]. An autoencoder network is trained on the\\nsegmentation ground truth masks Y. The encoder reduces the label to a latent,\\nlow resolution representationE(Yi) and the decoder tries to retrieve the original\\nvolumeYi. Due to the resolution reduction of the encoder, the shape information'),\n",
       " Document(metadata={'arxiv_id': '1805.00357v1', 'title': 'Domain and Geometry Agnostic CNNs for Left Atrium Segmentation in 3D Ultrasound', 'section': 'body', 'authors': 'Markus A. Degel, Nassir Navab, Shadi Albarqouni'}, page_content='low resolution representationE(Yi) and the decoder tries to retrieve the original\\nvolumeYi. Due to the resolution reduction of the encoder, the shape information\\nis encoded in a compact fashion.\\nDuring training, the output of the segmentation network Ë†Yi is passed to the\\nencoder, along with the ground truth label Yi. Based on a distance metric d(Â·,Â·),\\na loss between the latent codes of both inputs is calculated as\\nLenc =d(E(Yi),E ( Ë†Yi)). (2)\\nThe gradient is then back-propagated to the segmentation network.\\nDomain adaptation. When a network is trained on one type of data XS\\n(source domain) and evaluated on anotherXT (target domain), the performance\\nis poor in most cases. Domain invariant features are desired to make the seg-\\nmentation network perform well on diï¬€erent data sets. Kamnitsas et al. propose\\nan approach to generate domain invariant features to increase the generalization\\ncapability [4].\\nProcessing an image volume in a CNN yields a latent representation hl(Xi)'),\n",
       " Document(metadata={'arxiv_id': '1805.00357v1', 'title': 'Domain and Geometry Agnostic CNNs for Left Atrium Segmentation in 3D Ultrasound', 'section': 'body', 'authors': 'Markus A. Degel, Nassir Navab, Shadi Albarqouni'}, page_content='an approach to generate domain invariant features to increase the generalization\\ncapability [4].\\nProcessing an image volume in a CNN yields a latent representation hl(Xi)\\nafter convolutional layerl. If the network is not domain invariant, those feature\\nmaps contain, as expected, information about the data type (source or target\\ndomain). The idea to solve this issue is to train a classiï¬erC, which takes feature\\nmaps of the segmentation network as input and returns a decision, if the input\\ndata was from source ( XS) or target ( XT ) domain: C(hl(Xi)) = Ë†diâˆˆ{ S,T}.\\nThe accuracy of this classiï¬er with respect to the real domain di is an indicator\\nof how domain invariant the features are.\\nCombination. The ideas introduced in the previous sections are now combined\\nto exploit the advantages of the individual approaches (Fig. 2). The loss of\\nthe domain classiï¬er is used as an adversarial loss term, since the goal of the'),\n",
       " Document(metadata={'arxiv_id': '1805.00357v1', 'title': 'Domain and Geometry Agnostic CNNs for Left Atrium Segmentation in 3D Ultrasound', 'section': 'body', 'authors': 'Markus A. Degel, Nassir Navab, Shadi Albarqouni'}, page_content='to exploit the advantages of the individual approaches (Fig. 2). The loss of\\nthe domain classiï¬er is used as an adversarial loss term, since the goal of the\\nsegmentation network is to lower the classiï¬cation accuracy ( i.e maximize the\\nclassiï¬cation loss). The inability of the classiï¬er to tell, which type of data was\\nsegmented means that the feature maps do not hold domain speciï¬c information.\\nAt the same time, the segmentation loss and the loss with respect to E(Yi) and\\nE( Ë†Yi) should be minimized. This yields the following combined loss function:\\nL =Lseg +Î»encÂ·L encâˆ’Î»advÂ·L adv (3)\\nwithLadv being the binary cross entropy loss of the classiï¬er C andLseg de-\\nscribing the dice loss (Eq. 1) with a weight regularization.Table 1: Data device and set distribution. iE33 datasets are only used for eval-\\nuation. Resolutions are equidistant. For resolution and opening angles (azimuth\\n& elevation) the meanÂ± standard deviation in the respective set are shown.\\nProperty EPIQ 7C Vivid E9 iE33'),\n",
       " Document(metadata={'arxiv_id': '1805.00357v1', 'title': 'Domain and Geometry Agnostic CNNs for Left Atrium Segmentation in 3D Ultrasound', 'section': 'body', 'authors': 'Markus A. Degel, Nassir Navab, Shadi Albarqouni'}, page_content='uation. Resolutions are equidistant. For resolution and opening angles (azimuth\\n& elevation) the meanÂ± standard deviation in the respective set are shown.\\nProperty EPIQ 7C Vivid E9 iE33\\ntrain/val/test 33/7/27 39/8/32 0/0/15\\nResolution (mm/voxel) 0 .95 Â± 0.10 0 .95 Â± 0.10 0 .96 Â± 0.11\\nAzimuth (deg) 87 .1 Â± 4.7 47 .3 Â± 10.4 80 .2 Â± 0.0\\nElevation (deg) 78 .2 Â± 0.1 47 .4 Â± 10.5 91 .6 Â± 0.0\\n3 Experiments and Results\\nTo evaluate the inï¬‚uence of the diï¬€erent loss terms, we apply it to 3D Ultrasound\\ndata to perform end-systolic LA segmentation. The network is trained with\\nimages and labels from one device and tested on diï¬€erent devices.\\nDataset. The data available for this work are 3D transthoracic echocardiog-\\nraphy (TTE) examinations taken from clinical routine. Multiple international\\ncenters contributed to a pool of 161 datasets, containing the LA ground truth\\nsegmentation in the recorded heart cycle, with the relevant phases for LA func-'),\n",
       " Document(metadata={'arxiv_id': '1805.00357v1', 'title': 'Domain and Geometry Agnostic CNNs for Left Atrium Segmentation in 3D Ultrasound', 'section': 'body', 'authors': 'Markus A. Degel, Nassir Navab, Shadi Albarqouni'}, page_content='centers contributed to a pool of 161 datasets, containing the LA ground truth\\nsegmentation in the recorded heart cycle, with the relevant phases for LA func-\\ntionality (end-diastole, end-systole and pre-atrial contraction) identiï¬ed.\\nThe image volumes were acquired with Ultrasound systems from GE (Vivid\\nE9, GE Vingmed Ultrasound) and Philips (EPIQ 7C and iE33, Philips Medical\\nSystems), each equipped with a matrix array transducer. Table 1 shows the data\\ndistribution with respective splits into training, validation and testing sets. Since\\nthere are only 15 datasets for device iE33, those examinations are not used for\\ntraining, but only for evaluation. For the data to be fed into the network, the\\nsize is down-sampled to 64 cubic volumes, preserving angles and ratios, by zero\\npadding datasets as visible in Fig. 1.\\nImplementation. Network architectures are implemented using the Tensor-\\nFlow4 library (version 1.4) with GPU support. For our approach, the V-Net'),\n",
       " Document(metadata={'arxiv_id': '1805.00357v1', 'title': 'Domain and Geometry Agnostic CNNs for Left Atrium Segmentation in 3D Ultrasound', 'section': 'body', 'authors': 'Markus A. Degel, Nassir Navab, Shadi Albarqouni'}, page_content='padding datasets as visible in Fig. 1.\\nImplementation. Network architectures are implemented using the Tensor-\\nFlow4 library (version 1.4) with GPU support. For our approach, the V-Net\\narchitecture is adapted such that volumes of size 64x64x64 can be processed.\\nThe autoencoder network architecture is inspired from the one proposed in [8].\\nTo generate the input for the classiï¬er, feature maps of V-Net at diï¬€erent levels\\nhave to be concatenated. The size of the bottleneck (4x4x4) for extracted feature\\nmaps is obtained by the (repeated) application of convolutions of ï¬lter size 2 and\\nstride 2.\\nTraining Details. The autoencoder network is trained before the combined\\ntraining procedure, to obtain a meaningful latent representation for the shape\\nprior. In the following training stages, the parameters of this network are frozen.\\n4 https://www.tensorflow.org/Table 2: Training procedure details. Each training uses a learning rate decay of'),\n",
       " Document(metadata={'arxiv_id': '1805.00357v1', 'title': 'Domain and Geometry Agnostic CNNs for Left Atrium Segmentation in 3D Ultrasound', 'section': 'body', 'authors': 'Markus A. Degel, Nassir Navab, Shadi Albarqouni'}, page_content='prior. In the following training stages, the parameters of this network are frozen.\\n4 https://www.tensorflow.org/Table 2: Training procedure details. Each training uses a learning rate decay of\\n0.99 after each epoch and a batch size of 4. X = XSâˆª XT , d: domain labels.\\n# Name (parameters) optimizer learning\\nrate\\nweight\\nreg. epochs data label\\n1 Autoencoder ( Î¸ae) Momentum Î²:0.9 5 Â· 10âˆ’4 0.1 100 YS YS\\n2 Segmentation ( Î¸seg) Adam\\nÎ²1: 0.99, Î²2: 0.999 1 Â· 10âˆ’5 5 Â· 10âˆ’4 50 XS YS\\n3 Classiï¬er ( Î¸adv) SGD 5 Â· 10âˆ’5 1 Â· 10âˆ’5 15 X d\\n4 Combination3 (Î¸seg) Momentum Î²:0.99 1 Â· 10âˆ’5 5 Â· 10âˆ’4\\n100 XS YS,d\\nClassiï¬er (Î¸adv) SGD 5 Â· 10âˆ’5 1 Â· 10âˆ’5 X d\\nThe segmentation network is shortly pre-trained, as well as the classiï¬er to\\nintroduce stability in the combined training. This way, the parameters of the\\nsingle networks are pre-adjusted and training can focus on realizing the scenario\\ndeï¬ned by the settings of Î»enc andÎ»adv. Feature maps L0, L2, M, R2 and R0 of'),\n",
       " Document(metadata={'arxiv_id': '1805.00357v1', 'title': 'Domain and Geometry Agnostic CNNs for Left Atrium Segmentation in 3D Ultrasound', 'section': 'body', 'authors': 'Markus A. Degel, Nassir Navab, Shadi Albarqouni'}, page_content='single networks are pre-adjusted and training can focus on realizing the scenario\\ndeï¬ned by the settings of Î»enc andÎ»adv. Feature maps L0, L2, M, R2 and R0 of\\nthe segmentation network are extracted for the classiï¬er (compare Fig. 2).\\nThe combined training procedure starts by adding the loss term for incorpo-\\nration of the shape prior to the segmentation loss. Adversarial inï¬‚uence begins\\nafter eadv = 10 epochs of combined training. Î»adv is increased linearly until its\\nmaximumÎ»adv,max = 0.001 (Î»adv = min((eâˆ’eadv + 1)Â·Î±, 1)Â·Î»adv,max, with\\ne the current epoch and Î± = 0.1 the adversarial inï¬‚uence growth factor). While\\nthe combined training adjusts the parameters of the segmentation network Î¸seg\\nonly, the classiï¬er parameters Î¸adv are continued to be trained in parallel to\\nretain a potent adversarial loss term. A training overview is given in Table 2.\\nDiï¬€erent parameters for the combined network are recorded in Table 3.'),\n",
       " Document(metadata={'arxiv_id': '1805.00357v1', 'title': 'Domain and Geometry Agnostic CNNs for Left Atrium Segmentation in 3D Ultrasound', 'section': 'body', 'authors': 'Markus A. Degel, Nassir Navab, Shadi Albarqouni'}, page_content='retain a potent adversarial loss term. A training overview is given in Table 2.\\nDiï¬€erent parameters for the combined network are recorded in Table 3.\\nEvaluation. The segmentation network returns a volume Ë†Yi of probabilities\\nfor the voxels to belong to the foreground, i.e the segmentation of the LA. The\\nthreshold for the cutoï¬€ probability to obtain a binary segmentation mask is\\ndetermined by the best Dice coeï¬ƒcient on the validation set, from which the\\nbiggest connected component is selected as the ï¬nal LA segmentation.\\nSegmentation metrics [1,8] are reported in Table 3 for the recommended phase\\nof LA segmentation (end-systole ES [6]). C1 describes the V-Net architecture\\nwith the additional loss term Lenc, calculated from the L2-distance ( d(p,q ) =\\nâˆ¥pâˆ’qâˆ¥2\\n2). To investigate the inï¬‚uence of a diï¬€erent distance metric, C2 uses\\nthe angular cosine distance (ACD, d(p,q ) = 1 âˆ’\\nâˆ‘\\ni piÂ·qi\\nâˆ¥pâˆ¥2Â·âˆ¥qâˆ¥2\\n). C3 leverages the'),\n",
       " Document(metadata={'arxiv_id': '1805.00357v1', 'title': 'Domain and Geometry Agnostic CNNs for Left Atrium Segmentation in 3D Ultrasound', 'section': 'body', 'authors': 'Markus A. Degel, Nassir Navab, Shadi Albarqouni'}, page_content='âˆ¥pâˆ’qâˆ¥2\\n2). To investigate the inï¬‚uence of a diï¬€erent distance metric, C2 uses\\nthe angular cosine distance (ACD, d(p,q ) = 1 âˆ’\\nâˆ‘\\ni piÂ·qi\\nâˆ¥pâˆ¥2Â·âˆ¥qâˆ¥2\\n). C3 leverages the\\nbetter performing distance metric (ACD) with the adversarial loss Ladv. We\\ndeï¬ne statistical signiï¬cance based on the paired two-sample t-test on a 5 %\\nsigniï¬cance level.\\nWhen training on EPIQ 7C, V-Net performs better than the other architec-\\ntures on the same device. However, those margins are not statistically signiï¬cant\\n(MSD: p = 0.65, HD: p = 0.24, DC: P = 0.66), compared to C3. The increasedTable 3: Results for ES LA segmentation. For completeness, results of ACNN\\nand V-Net are reported. C1: Î»adv = 0, d: L2-distance. C2: Î»adv = 0, d: ACD.\\nC3: Î»adv = 0.001, d: ACD. C1,C2 & C3: Î»enc = 0.001. Format: meanÂ± std.\\nTraining Test V-Net[7] ACNN[8] C1 C2 C3\\nMean Surface Distance (MSD)\\nEPIQ 7C\\nEPIQ 7C\\nVivid E9\\niE33\\n1.16Â±0.88\\n3.56 Â± 1.71\\n1.44 Â± 0.77\\n1.35 Â± 1.19\\n10.67 Â± 7.29\\n1.38Â±0.40\\n1.26 Â± 0.69\\n3.87 Â± 3.06\\n2.33 Â± 2.38'),\n",
       " Document(metadata={'arxiv_id': '1805.00357v1', 'title': 'Domain and Geometry Agnostic CNNs for Left Atrium Segmentation in 3D Ultrasound', 'section': 'body', 'authors': 'Markus A. Degel, Nassir Navab, Shadi Albarqouni'}, page_content='Mean Surface Distance (MSD)\\nEPIQ 7C\\nEPIQ 7C\\nVivid E9\\niE33\\n1.16Â±0.88\\n3.56 Â± 1.71\\n1.44 Â± 0.77\\n1.35 Â± 1.19\\n10.67 Â± 7.29\\n1.38Â±0.40\\n1.26 Â± 0.69\\n3.87 Â± 3.06\\n2.33 Â± 2.38\\n1.27 Â± 0.69\\n2.42 Â± 1.32\\n1.94 Â± 1.49\\n1.21 Â± 0.60\\n2.01Â±1.63\\n1.44 Â± 0.35\\nVivid E9\\nEPIQ 7C\\nVivid E9\\niE33\\n2.87 Â± 1.53\\n0.94Â±0.59\\n4.72 Â± 4.86\\n4.39 Â± 1.33\\n1.57 Â± 0.87\\n3.28 Â± 2.22\\n2.12 Â± 0.96\\n1.18 Â± 0.38\\n4.18 Â± 3.36\\n1.87 Â± 0.96\\n1.12 Â± 0.37\\n3.18 Â± 2.88\\n1.59Â±1.04\\n1.18 Â± 0.37\\n2.62Â±1.46\\nHausdorï¬€ Distance (HD)\\nEPIQ 7C\\nEPIQ 7C\\nVivid E9\\niE33\\n4.46Â±2.73\\n7.66 Â± 2.94\\n4.06Â±1.21\\n5.52 Â± 3.15\\n16.87 Â± 8.92\\n5.03 Â± 1.39\\n5.51 Â± 2.31\\n8.21 Â± 5.06\\n5.60 Â± 2.86\\n5.33 Â± 2.07\\n5.79 Â± 2.21\\n4.98 Â± 2.02\\n4.92 Â± 1.60\\n5.46Â±3.36\\n4.70 Â± 0.91\\nVivid E9\\nEPIQ 7C\\nVivid E9\\niE33\\n10.82 Â± 3.80\\n3.67Â±2.29\\n9.52 Â± 6.44\\n13.63 Â± 2.87\\n7.09 Â± 3.21\\n11.60 Â± 3.72\\n8.09 Â± 2.88\\n5.41 Â± 1.84\\n9.08 Â± 3.64\\n7.31 Â± 2.51\\n5.05 Â± 1.70\\n7.13 Â± 3.49\\n5.47Â±2.45\\n5.14 Â± 1.26\\n6.63Â±2.25\\nDice Coeï¬ƒcient (DC)\\nEPIQ 7C\\nEPIQ 7C\\nVivid E9\\niE33\\n0.75Â±0.17\\n0.10 Â± 0.21\\n0.57 Â± 0.31\\n0.69 Â± 0.20\\n0.15 Â± 0.25\\n0.64 Â± 0.11'),\n",
       " Document(metadata={'arxiv_id': '1805.00357v1', 'title': 'Domain and Geometry Agnostic CNNs for Left Atrium Segmentation in 3D Ultrasound', 'section': 'body', 'authors': 'Markus A. Degel, Nassir Navab, Shadi Albarqouni'}, page_content='9.08 Â± 3.64\\n7.31 Â± 2.51\\n5.05 Â± 1.70\\n7.13 Â± 3.49\\n5.47Â±2.45\\n5.14 Â± 1.26\\n6.63Â±2.25\\nDice Coeï¬ƒcient (DC)\\nEPIQ 7C\\nEPIQ 7C\\nVivid E9\\niE33\\n0.75Â±0.17\\n0.10 Â± 0.21\\n0.57 Â± 0.31\\n0.69 Â± 0.20\\n0.15 Â± 0.25\\n0.64 Â± 0.11\\n0.74 Â± 0.10\\n0.33 Â± 0.27\\n0.55 Â± 0.19\\n0.73 Â± 0.11\\n0.32 Â± 0.26\\n0.59 Â± 0.19\\n0.74 Â± 0.10\\n0.55Â±0.23\\n0.67Â±0.08\\nVivid E9\\nEPIQ 7C\\nVivid E9\\niE33\\n0.56 Â± 0.15\\n0.80Â±0.08\\n0.49 Â± 0.37\\n0.32 Â± 0.18\\n0.69 Â± 0.11\\n0.50Â±0.16\\n0.59 Â± 0.14\\n0.73 Â± 0.07\\n0.38 Â± 0.25\\n0.62 Â± 0.17\\n0.74 Â± 0.08\\n0.46 Â± 0.27\\n0.63Â±0.17\\n0.73 Â± 0.09\\n0.46 Â± 0.19\\nperformance of C3 compared to V-Net and ACNN is signiï¬cant with respect to\\nall metrics.\\nVivid E9 training yields V-Net with the best performance on the same device,\\nwith statistical signiï¬cance on all metrics. C3 is signiï¬cantly outperforming V-\\nNet on EPIQ 7C in terms of MSD and HD.\\nNo signiï¬cant diï¬€erences are observable on the evaluation of device iE33.\\nIndependent of the distance metric utilized, an improvement in generalizability'),\n",
       " Document(metadata={'arxiv_id': '1805.00357v1', 'title': 'Domain and Geometry Agnostic CNNs for Left Atrium Segmentation in 3D Ultrasound', 'section': 'body', 'authors': 'Markus A. Degel, Nassir Navab, Shadi Albarqouni'}, page_content='Net on EPIQ 7C in terms of MSD and HD.\\nNo signiï¬cant diï¬€erences are observable on the evaluation of device iE33.\\nIndependent of the distance metric utilized, an improvement in generalizability\\nis observable compared to V-Net when the shape prior is included (C1 & C2).\\n4 Discussion and Conclusion\\nThe results show, that including a shape prior for the segmentation task is\\nactually helpful for domain adaptation. The adversarial loss, represented by the\\nclassiï¬er accuracy is further improving the ability of the network to generalize.\\nWe show that the combination of loss terms for diï¬€erent objectives can have a\\ngreat potential for domain adaptation. The type of distance metric utilized for\\nthe geometrical constrain would be an interesting subject to further investigate.An average dice coeï¬ƒcient improvement to V-Net of 8.5 % was achieved on our\\nobjective of LA segmentation.\\nAcknowledgment\\nWe would like to thank Ozan Oktay for sharing his implementation of theACNN'),\n",
       " Document(metadata={'arxiv_id': '1805.00357v1', 'title': 'Domain and Geometry Agnostic CNNs for Left Atrium Segmentation in 3D Ultrasound', 'section': 'body', 'authors': 'Markus A. Degel, Nassir Navab, Shadi Albarqouni'}, page_content='objective of LA segmentation.\\nAcknowledgment\\nWe would like to thank Ozan Oktay for sharing his implementation of theACNN\\narchitecture as a baseline of our project. Further, we thank Georg Schummers\\nand Matthias Friedrichs from TOMTEC Imaging Systems GmbH for their sup-\\nport and helpful discussions.\\nReferences\\n1. Almeida, N., Friboulet, D., Sarvari, S.I., Bernard, O., Barbosa, D., Samset, E.,\\nDhooge, J.: Left-Atrial Segmentation From 3-D Ultrasound Using B-Spline Explicit\\nActive Surfaces With Scale Uncoupling. IEEE UFFC-S 63(2), 212â€“221 (2016)\\n2. Buechel, R.R., Sommer, G., Leibundgut, G., Rohner, A., Riede, F., Kessel-Schaefer,\\nA., Kaufmann, B.A., Zellweger, M.J., Bremerich, J., Handke, M.: Assessment of\\nleft atrial functional parameters using a novel dedicated analysis tool for real-\\ntime three-dimensional echocardiography: validation in comparison to magnetic\\nresonance imaging. INT J CARDIOVAS IMAG 29(3), 601â€“608 (2013)'),\n",
       " Document(metadata={'arxiv_id': '1805.00357v1', 'title': 'Domain and Geometry Agnostic CNNs for Left Atrium Segmentation in 3D Ultrasound', 'section': 'body', 'authors': 'Markus A. Degel, Nassir Navab, Shadi Albarqouni'}, page_content='time three-dimensional echocardiography: validation in comparison to magnetic\\nresonance imaging. INT J CARDIOVAS IMAG 29(3), 601â€“608 (2013)\\n3. van den Hoven, A.T., Mc-Ghie, J.S., Chelu, R.G., Duijnhouwer, A.L., Baggen,\\nV.J.M., Coenen, A., Vletter, W.B., Dijkshoorn, M.L., van den Bosch, A.E., Roos-\\nHesselink, J.W.: Transthoracic 3D echocardiographic left heart chamber quantiï¬-\\ncation in patients with bicuspid aortic valve disease. INT J CARDIOVAS IMAG\\n33(12), 1895â€“1903 (2017)\\n4. Kamnitsas, K., Baumgartner, C., Ledig, C., Newcombe, V., Simpson, J., Kane,\\nA., Menon, D., Nori, A., Criminisi, A., Rueckert, D., Glocker, B.: Unsupervised\\nDomain Adaptation in Brain Lesion Segmentation with Adversarial Networks. In:\\nNiethammer, M., Styner, M., Aylward, S., Zhu, H., Oguz, I., Yap, P.T., Shen, D.\\n(eds.) IPMI. pp. 597â€“609. Springer International Publishing (2017)\\n5. Knackstedt, C., Bekkers, S.C., Schummers, G., Schreckenberg, M., Muraru, D.,'),\n",
       " Document(metadata={'arxiv_id': '1805.00357v1', 'title': 'Domain and Geometry Agnostic CNNs for Left Atrium Segmentation in 3D Ultrasound', 'section': 'body', 'authors': 'Markus A. Degel, Nassir Navab, Shadi Albarqouni'}, page_content='(eds.) IPMI. pp. 597â€“609. Springer International Publishing (2017)\\n5. Knackstedt, C., Bekkers, S.C., Schummers, G., Schreckenberg, M., Muraru, D.,\\nBadano, L.P., Franke, A., Bavishi, C., Omar, A.M.S., Sengupta, P.P.: Fully Auto-\\nmated Versus Standard Tracking of Left Ventricular Ejection Fraction and Longi-\\ntudinal Strain: The FAST-EFs Multicenter Study. JACC 66(13), 1456â€“1466 (2015)\\n6. Lang, R.M., Badano, L.P., Mor-Avi, V., Aï¬lalo, J., Armstrong, A., Ernande, L.,\\nFlachskampf, F.A., Foster, E., Goldstein, S.A., Kuznetsova, T., Lancellotti, P.,\\nMuraru, D., Picard, M.H., Rietzschel, E.R., Rudski, L., Spencer, K.T., Tsang, W.,\\nVoigt, J.U.: Recommendations for Cardiac Chamber Quantiï¬cation by Echocardio-\\ngraphy in Adults: An Update from the American Society of Echocardiography and\\nthe European Association of Cardiovascular Imaging. Eur. Heart J. 16(3), 233â€“271\\n(2015)\\n7. Milletari, F., Navab, N., Ahmadi, S.: V-Net: Fully Convolutional Neural Networks'),\n",
       " Document(metadata={'arxiv_id': '1805.00357v1', 'title': 'Domain and Geometry Agnostic CNNs for Left Atrium Segmentation in 3D Ultrasound', 'section': 'body', 'authors': 'Markus A. Degel, Nassir Navab, Shadi Albarqouni'}, page_content='the European Association of Cardiovascular Imaging. Eur. Heart J. 16(3), 233â€“271\\n(2015)\\n7. Milletari, F., Navab, N., Ahmadi, S.: V-Net: Fully Convolutional Neural Networks\\nfor Volumetric Medical Image Segmentation. In: 2016 Fourth International Con-\\nference on 3D Vision (3DV). pp. 565â€“571 (2016), doi.ieeecomputersociety.org/\\n10.1109/3DV.2016.79\\n8. Oktay, O., Ferrante, E., Kamnitsas, K., Heinrich, M., Bai, W., Caballero, J., Cook,\\nS.A., de Marvao, A., Dawes, T., ORegan, D.P., Kainz, B., Glocker, B., Rueckert,\\nD.: Anatomically Constrained Neural Networks (ACNNs): Application to Cardiac\\nImage Enhancement and Segmentation. IEEE TMI 37(2), 384â€“395 (2018)9. Rohner, A., Brinkert, M., Kawel, N., Buechel, R.R., Leibundgut, G., Grize, L.,\\nKhne, M., Bremerich, J., Kaufmann, B.A., Zellweger, M.J., Buser, P., Osswald,\\nS., Handke, M.: Functional assessment of the left atrium by real-time three-\\ndimensional echocardiography using a novel dedicated analysis tool: initial val-'),\n",
       " Document(metadata={'arxiv_id': '1805.00357v1', 'title': 'Domain and Geometry Agnostic CNNs for Left Atrium Segmentation in 3D Ultrasound', 'section': 'body', 'authors': 'Markus A. Degel, Nassir Navab, Shadi Albarqouni'}, page_content='S., Handke, M.: Functional assessment of the left atrium by real-time three-\\ndimensional echocardiography using a novel dedicated analysis tool: initial val-\\nidation studies in comparison with computed tomography. Eur J Echocardiogr\\n12(7), 497â€“505 (2011)\\n10. Ronneberger, O., Fischer, P., Brox, T.: U-Net: Convolutional Networks for Biomed-\\nical Image Segmentation. In: Navab, N., Hornegger, J., Wells, W.M., Frangi, A.F.\\n(eds.) MICCAI 2015. pp. 234â€“241. Springer International Publishing (2015)\\n11. Tsang, W., Salgo, I.S., Medvedofsky, D., Takeuchi, M., Prater, D., Weinert, L.,\\nYamat, M., Mor-Avi, V., Patel, A.R., Lang, R.M.: Transthoracic 3D Echocardio-\\ngraphic Left Heart Chamber Quantiï¬cation Using an Automated Adaptive Ana-\\nlytics Algorithm. JACC: Cardiovascular Imaging 9(7), 769â€“782 (2016)\\n12. Voigt, I., Mansi, T., Mihalef, V., Ionasec, R.I., Calleja, A., Mengue, E.A., Sharma,\\nP., Houle, H., Georgescu, B., Hornegger, J., Comaniciu, D.: Patient-Speciï¬c Model'),\n",
       " Document(metadata={'arxiv_id': '1805.00357v1', 'title': 'Domain and Geometry Agnostic CNNs for Left Atrium Segmentation in 3D Ultrasound', 'section': 'body', 'authors': 'Markus A. Degel, Nassir Navab, Shadi Albarqouni'}, page_content='12. Voigt, I., Mansi, T., Mihalef, V., Ionasec, R.I., Calleja, A., Mengue, E.A., Sharma,\\nP., Houle, H., Georgescu, B., Hornegger, J., Comaniciu, D.: Patient-Speciï¬c Model\\nof Left Heart Anatomy, Dynamics and Hemodynamics from 4D TEE: A First\\nValidation Study. In: Metaxas, D.N., Axel, L. (eds.) FIMH. pp. 341â€“349. Springer\\nBerlin Heidelberg (2011)Appendix\\nThis section contains some supplementary images to show the diï¬€erent recorded\\nheart cycle phases, the down-sampling process and segmentation results.\\nUltrasound Data\\nFig. 3: Top: device Vivid E9, Bottom: device EPIQ 7C, Left: end-diastole, Mid-\\ndle: end-systole, Right: pre-atrial contraction.Preprocessing: down-sampling\\nFig. 4: Top: high resolution, Bottom: down-sampled to 64x64x64 with zero\\npadding, Left: device EPIQ 7C, Middle: device Vivid E9, Right: device iE33.Segmentation Result Images\\nFig. 5: Row 1: V-Net, Row 2: ACNN, Row 3: C1, Row 4: C2, Row 5: C3,\\nColumn 1 & 2: test result device Vivid E9 (training device EPIQ 7C), Col-'),\n",
       " Document(metadata={'arxiv_id': '1805.00357v1', 'title': 'Domain and Geometry Agnostic CNNs for Left Atrium Segmentation in 3D Ultrasound', 'section': 'body', 'authors': 'Markus A. Degel, Nassir Navab, Shadi Albarqouni'}, page_content='Fig. 5: Row 1: V-Net, Row 2: ACNN, Row 3: C1, Row 4: C2, Row 5: C3,\\nColumn 1 & 2: test result device Vivid E9 (training device EPIQ 7C), Col-\\numn 3 & 4: test result device EPIQ 7C (training device Vivid E9).Result plots\\nFig. 6: Top: Mean Surface Distance, Middle: Hausdorï¬€ Distance, Bottom: Dice\\nCoeï¬ƒcient, Left: EPIQ 7C training, Right: Vivid E9 training.'),\n",
       " Document(metadata={'arxiv_id': '2501.13193v2', 'title': 'Revisiting Data Augmentation for Ultrasound Images', 'section': 'title_abstract', 'authors': 'Adam Tupper, Christian GagnÃ©'}, page_content='Title: Revisiting Data Augmentation for Ultrasound Images\\n\\nAbstract: Data augmentation is a widely used and effective technique to improve the generalization performance of deep neural networks. Yet, despite often facing limited data availability when working with medical images, it is frequently underutilized. This appears to come from a gap in our collective understanding of the efficacy of different augmentation techniques across different tasks and modalities. One modality where this is especially true is ultrasound imaging. This work addresses this gap by analyzing the effectiveness of different augmentation techniques at improving model performance across a wide range of ultrasound image analysis tasks. To achieve this, we introduce a new standardized benchmark of 14 ultrasound image classification and semantic segmentation tasks from 10 different sources and covering 11 body regions. Our results demonstrate that many of the augmentations commonly used for tasks on natural images are also effective on ultrasound images, even more so than augmentations developed specifically for ultrasound images in some cases. We also show that diverse augmentation using TrivialAugment, which is widely used for natural images, is also effective for ultrasound images. Moreover, our proposed methodology represents a structured approach for assessing various data augmentations that can be applied to other contexts and modalities.'),\n",
       " Document(metadata={'arxiv_id': '2501.13193v2', 'title': 'Revisiting Data Augmentation for Ultrasound Images', 'section': 'body', 'authors': 'Adam Tupper, Christian GagnÃ©'}, page_content='Published in Transactions on Machine Learning Research (07/2025)\\nRevisiting Data Augmentation for Ultrasound Images\\nAdam Tupper adam.tupper.1@ulaval.ca\\nInstitut Intelligence et DonnÃ©es (IID), UniversitÃ© Laval, Mila\\nChristian GagnÃ© christian.gagne@gel.ulaval.ca\\nInstitut Intelligence et DonnÃ©es (IID), UniversitÃ© Laval\\nCanada-CIF AR AI Chair, Mila\\nReviewed on OpenReview: https: // openreview. net/ forum? id= iGcxlTLIL5\\nAbstract\\nData augmentation is a widely used and effective technique to improve the generalization\\nperformance of deep neural networks. Yet, despite often facing limited data availability\\nwhen working with medical images, it is frequently underutilized. This appears to come\\nfrom a gap in our collective understanding of the efficacy of different augmentation tech-\\nniques across different tasks and modalities. One modality where this is especially true is\\nultrasound imaging. This work addresses this gap by analyzing the effectiveness of different'),\n",
       " Document(metadata={'arxiv_id': '2501.13193v2', 'title': 'Revisiting Data Augmentation for Ultrasound Images', 'section': 'body', 'authors': 'Adam Tupper, Christian GagnÃ©'}, page_content='niques across different tasks and modalities. One modality where this is especially true is\\nultrasound imaging. This work addresses this gap by analyzing the effectiveness of different\\naugmentation techniques at improving model performance across a wide range of ultrasound\\nimage analysis tasks. To achieve this, we introduce a new standardized benchmark of 14\\nultrasound image classification and semantic segmentation tasks from 10 different sources\\nand covering 11 body regions. Our results demonstrate that many of the augmentations\\ncommonly used for tasks on natural images are also effective on ultrasound images, even\\nmore so than augmentations developed specifically for ultrasound images in some cases. We\\nalso show that diverse augmentation using TrivialAugment, which is widely used for natural\\nimages, is also effective for ultrasound images. Moreover, our proposed methodology repre-\\nsents a structured approach for assessing various data augmentations that can be applied'),\n",
       " Document(metadata={'arxiv_id': '2501.13193v2', 'title': 'Revisiting Data Augmentation for Ultrasound Images', 'section': 'body', 'authors': 'Adam Tupper, Christian GagnÃ©'}, page_content='images, is also effective for ultrasound images. Moreover, our proposed methodology repre-\\nsents a structured approach for assessing various data augmentations that can be applied\\nto other contexts and modalities.\\n1 Introduction\\nData augmentation is an essential component of deep learning. It not only improves generalization, but\\nit is also a core component of many self- and semi-supervised learning algorithms. However, while data\\naugmentation is ubiquitous for training deep neural networks on natural images (i.e., images of human-scale\\nscenes captured by ordinary digital cameras), when it comes to training such models on medical images its\\nproper usage is not as common and clearly understood (Chlap et al., 2021; Garcea et al., 2023). This is\\ndespite the difficulties we face collecting sufficient data due to privacy protections and high acquisition and\\nannotation costs.\\nThe under-utilization of augmentation when working with medical images suggests a weaker understanding'),\n",
       " Document(metadata={'arxiv_id': '2501.13193v2', 'title': 'Revisiting Data Augmentation for Ultrasound Images', 'section': 'body', 'authors': 'Adam Tupper, Christian GagnÃ©'}, page_content='annotation costs.\\nThe under-utilization of augmentation when working with medical images suggests a weaker understanding\\nof the effectiveness of different operations and strategies. Often, we simply apply photometric and geometric\\ntransformations proposed from natural images as is, without rigorous testing. However, the low uptake\\nindicates that findings from natural images may not translate well to medical images. This is not surprising\\ngiven that the size of the objects of interest and the relevance of specific textures may differ significantly for\\ndoing detection, classification or segmentation tasks from natural images compared to other modalities such\\nas microscopy, X-ray, and ultrasound, to name a few.\\nA lack of comparative studies featuring controlled experiments that evaluate various techniques over different\\ntasks, datasets, and imaging modalities has created a gap in our understanding of data augmentation for'),\n",
       " Document(metadata={'arxiv_id': '2501.13193v2', 'title': 'Revisiting Data Augmentation for Ultrasound Images', 'section': 'body', 'authors': 'Adam Tupper, Christian GagnÃ©'}, page_content='tasks, datasets, and imaging modalities has created a gap in our understanding of data augmentation for\\nmedical images. While there are several excellent literature surveys on this topic (Chlap et al., 2021; Garcea\\net al., 2023), relying solely on surveys leaves us at risk of falling foul of publication bias (i.e., the file-drawer\\n1\\narXiv:2501.13193v2  [eess.IV]  18 Jul 2025Published in Transactions on Machine Learning Research (07/2025)\\neffect). In addition, these surveys highlight the difficulty in drawing conclusions on which transforms are\\nmost effective, since there are many confounding variables. Ultimately, drawing conclusions from literature\\nsurveys alone is not enough. This problem needs to be addressed more rigorously and systematically through\\nan experimental approach. In this work, we evaluate the effectiveness of data augmentation techniques for\\ndeep neural networks in ultrasound image analysis.1 Our investigation reveals several key findings that'),\n",
       " Document(metadata={'arxiv_id': '2501.13193v2', 'title': 'Revisiting Data Augmentation for Ultrasound Images', 'section': 'body', 'authors': 'Adam Tupper, Christian GagnÃ©'}, page_content='deep neural networks in ultrasound image analysis.1 Our investigation reveals several key findings that\\nprovide practical guidance for implementing data augmentation in ultrasound image analysis:\\n1. Traditional domain-independent augmentations are effective, even more so in many cases than\\nultrasound-specific augmentations. They can be leveraged to achieve quick performance improve-\\nments before investing time and resources in developing custom techniques.\\n2. The impact of individual augmentations varies substantially across both domains (cardiac vs. liver\\nultrasound) and tasks (classification vs. semantic segmentation), with notable differences even be-\\ntween similar tasks on the same dataset.\\n3. While these variations might suggest the need for careful task-specific tuning of augmentation strate-\\ngies, we find that applying a diverse set of augmentations using the simple TrivialAugment strategy'),\n",
       " Document(metadata={'arxiv_id': '2501.13193v2', 'title': 'Revisiting Data Augmentation for Ultrasound Images', 'section': 'body', 'authors': 'Adam Tupper, Christian GagnÃ©'}, page_content='gies, we find that applying a diverse set of augmentations using the simple TrivialAugment strategy\\n(MÃ¼ller & Hutter, 2021) achieves substantial performance gains with limited tuning of the augmen-\\ntation set.\\nThe remainder of this paper is organized as follows. First, we discuss the prevalence of data augmentation\\nfor ultrasound image analysis using deep learning, ultrasound-specific augmentations, and previous studies\\nof data augmentations. Second, we present our benchmark that serves as the foundation for our analyses.\\nThird, we provide an in-depth description of the ultrasound-specific augmentations included in our study.\\nFourth, we present our analyses of individual augmentations and TrivialAugment. Finally, we discuss the\\nimplications of our results.\\n2 Background\\nAs previously mentioned, the use of data augmentation for ultrasound imaging is far less common than for\\nnatural image analysis. We start by presenting concrete analysis supporting this observation, then discuss'),\n",
       " Document(metadata={'arxiv_id': '2501.13193v2', 'title': 'Revisiting Data Augmentation for Ultrasound Images', 'section': 'body', 'authors': 'Adam Tupper, Christian GagnÃ©'}, page_content='natural image analysis. We start by presenting concrete analysis supporting this observation, then discuss\\nproposalsforultrasound-specificdataaugmentations, andfinallyexaminepriorstudiescomparingtheefficacy\\nof different data augmentations for medical image analysis.\\n2.1 Data Augmentation in Ultrasound Image Analysis\\nTo understand data augmentation practices for ultrasound image analysis with deep learning, we analyzed\\nthe use of data augmentation on 10 different publicly available ultrasound image datasets (Xu et al., 2023;\\nButterfly Network, 2018; Leclerc et al., 2019; Byra et al., 2018; Basu et al., 2022; Zhao et al., 2023; Singla\\net al., 2023; Born et al., 2021; Chen et al., 2024; Stanford AIMI Center, 2021) covering 11 regions of the\\nbody. We describe these datasets comprehensively in the following section as they form the basis of our\\nbenchmark. For now, we focus on the snapshot they provide of the use of data augmentation in ultrasound\\nimaging.'),\n",
       " Document(metadata={'arxiv_id': '2501.13193v2', 'title': 'Revisiting Data Augmentation for Ultrasound Images', 'section': 'body', 'authors': 'Adam Tupper, Christian GagnÃ©'}, page_content='benchmark. For now, we focus on the snapshot they provide of the use of data augmentation in ultrasound\\nimaging.\\nOf the 557 citations of the datasets catalogued by the Clarivate Web of Science platform2 as of August 2024,\\nwe identified 165 studies that used these datasets to train deep neural network models for classification and\\nsegmentation tasks. Among these studies, more than half (85 of 165) used no data augmentation at all\\nwhen training their models. Out of those remaining, 48 used three or less augmentations and only only\\n13 used six or more. This pales in comparison to the large sets of 14 augmentations used in common data\\naugmentation strategies such as AutoAugment (Cubuk et al., 2019), RandAugment (Cubuk et al., 2020),\\nand TrivialAugment (MÃ¼ller & Hutter, 2021).\\n1Our code, documentation and benchmark are available at https://github.com/adamtupper/ultrasound-augmentation.\\n2www.webofscience.com\\n2Published in Transactions on Machine Learning Research (07/2025)'),\n",
       " Document(metadata={'arxiv_id': '2501.13193v2', 'title': 'Revisiting Data Augmentation for Ultrasound Images', 'section': 'body', 'authors': 'Adam Tupper, Christian GagnÃ©'}, page_content='1Our code, documentation and benchmark are available at https://github.com/adamtupper/ultrasound-augmentation.\\n2www.webofscience.com\\n2Published in Transactions on Machine Learning Research (07/2025)\\nPerspective position adj.Low resolution simulationSpeckle parameter map\\nAcoustic shadow\\nWrap\\nNonlinear colour mapField-of-view masking\\nSharpen\\nBlur\\nMyocardium intensity adj.\\nHue adj.Occlusion\\nRandom erasing\\nSalt and pepper noise\\nMirrorCutMixClassMix\\nImage puzzle mixing\\nMixup\\nManifold mixup\\nFrequency domain mixingAdaptive gamma correction\\nUnsharp masking\\nMulti-level speckle noise\\nFan-shape preserving zoomSpeckle noise suppression\\nSpeckle noise\\nTime gain compensation\\nCone position adj.\\nImage resampling artifacts\\nJPEG compressionMultiplicative noise\\nNormalization\\nCutout\\nCenter crop\\nSpeckle reduction\\nColor jitter\\nShear\\nIntensity windowing\\nRandom noise\\nDepth attenuationElastic transformGaussian shadow\\nHaze artifact addition\\nGamma adj.\\nGaussian noiseBrightness adj.Contrast adj.Translation'),\n",
       " Document(metadata={'arxiv_id': '2501.13193v2', 'title': 'Revisiting Data Augmentation for Ultrasound Images', 'section': 'body', 'authors': 'Adam Tupper, Christian GagnÃ©'}, page_content='Color jitter\\nShear\\nIntensity windowing\\nRandom noise\\nDepth attenuationElastic transformGaussian shadow\\nHaze artifact addition\\nGamma adj.\\nGaussian noiseBrightness adj.Contrast adj.Translation\\nRandom crop\\nZoom\\nRotation\\nFlip\\n0\\n5\\n10\\n15\\n20\\n25\\n30\\n35\\n40\\n45\\n50\\n55\\n60Number of articles\\nGeneral\\nUltrasound-specific\\nFigure 1: The popularity of different augmentation techniques among 165 studies from the ultrasound liter-\\nature, showing moderate adoption of common methods but limited adoption of ultrasound-specific methods.\\nUpon examining the popularity of different augmentations, the list is dominated by natural image augmenta-\\ntions commonly found in deep learning frameworks. As presented in Fig. 1, the most popular augmentations\\nare classic geometric transforms that are known to perform well for natural image tasks, such as image flip-\\nping, rotation, zoom/scaling, random cropping, and translation. However, even among these most popular'),\n",
       " Document(metadata={'arxiv_id': '2501.13193v2', 'title': 'Revisiting Data Augmentation for Ultrasound Images', 'section': 'body', 'authors': 'Adam Tupper, Christian GagnÃ©'}, page_content='ping, rotation, zoom/scaling, random cropping, and translation. However, even among these most popular\\ntechniques, there is a steep decline in their use. The first augmentations designed specifically for ultrasound\\nimages, that is Gaussian shadowing (Smistad et al., 2018), haze artifact addition, depth attenuation and\\nspeckle reduction (Ostvik et al., 2021), are used in only three or four articles. In fact, these are the only\\nultrasound-specific augmentations used in multiple studies in our sample. We discuss these, along with other\\nultrasound-specific augmentations, in the following section.\\nAnother interesting observation is the lack of adoption of â€œmodernâ€ data augmentation strategies (e.g.,\\nRandAugment, TrivialAugment, etc.) among these studies, suggesting skepticism surrounding their efficacy\\nfrom researchers working on medical image analysis using deep learning. Instead, a common pattern we'),\n",
       " Document(metadata={'arxiv_id': '2501.13193v2', 'title': 'Revisiting Data Augmentation for Ultrasound Images', 'section': 'body', 'authors': 'Adam Tupper, Christian GagnÃ©'}, page_content='from researchers working on medical image analysis using deep learning. Instead, a common pattern we\\nfound is that researchers tend to focus on simple, hand-crafted, fixed sequences of augmentations that reflect\\nplausible differences that might arise in real-world settings. This is despite the fact that these stronger,\\nâ€œunrealisticâ€ strategies have proved effective in other modalities. The strategies adopted in deep learning\\nfor medical image analysis are reminiscent of the strategies used a decade or more ago for general computer\\nvision. This gap may reflect that ultrasound imaging simply lags behind natural image processing in adopting\\nmore recent techniques. Our work demonstrates the effectiveness of one such modern augmentation strategy\\nfor ultrasound image analysis, aiming to provide evidence that encourages wider adoption of these methods\\nin the field.\\n2.2 Ultrasound-Specific Data Augmentation'),\n",
       " Document(metadata={'arxiv_id': '2501.13193v2', 'title': 'Revisiting Data Augmentation for Ultrasound Images', 'section': 'body', 'authors': 'Adam Tupper, Christian GagnÃ©'}, page_content='for ultrasound image analysis, aiming to provide evidence that encourages wider adoption of these methods\\nin the field.\\n2.2 Ultrasound-Specific Data Augmentation\\nWhile domain-independent augmentations are the most frequently used, a variety of ultrasound-specific\\ntechniques have also been proposed. These target unique characteristics of ultrasound images to better\\nsimulate different machines and imaging conditions.\\nA common focus is noise manipulation. Techniques such as Multi-Level Speckle Noise (Monkam et al., 2023),\\nSpeckle Distortion (Ramakers et al., 2024), and Speckle Noise (Wang et al., 2022) all try to simulate realistic\\nspeckle noise, while Speckle Noise Suppression (Monkam et al., 2023) and Speckle Reduction (Ostvik et al.,\\n2021) aim to reduce it. Additional methods, including Haze Artifact (Ostvik et al., 2021) and multiplicative\\nnoise, simulate other realistic types of noise. Other methods simulate occlusions or variations in the field'),\n",
       " Document(metadata={'arxiv_id': '2501.13193v2', 'title': 'Revisiting Data Augmentation for Ultrasound Images', 'section': 'body', 'authors': 'Adam Tupper, Christian GagnÃ©'}, page_content='noise, simulate other realistic types of noise. Other methods simulate occlusions or variations in the field\\nof view, similar to Cutout (DeVries & Taylor, 2017) and random cropping. Some darken regions to mimic\\nacoustic shadows (Smistad et al., 2018; Singla et al., 2022; Ramakers et al., 2024), while Fan-Shape Preserv-\\n3Published in Transactions on Machine Learning Research (07/2025)\\ning Zoom (Singla et al., 2022), Field-of-View Masking (Pasdeloup et al., 2023), and Fan-Preserving Crop\\n(Ramakers et al., 2024) all reduce the field of view.\\nTo account for variability in probe orientation, Cone Position Adjustment and Perspective Position Ad-\\njustment (Sfakianakis et al., 2023) simulate changes in angle and rotation. Other methods adjust image\\nintensity and contrast. These include Myocardium Intensity Adjustment (Sfakianakis et al., 2023), which\\nenhances specific cardiac regions, Tirindelli et al.â€™s (2021) signal-to-noise ratio augmentation, which modifies'),\n",
       " ...]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings...\n",
      "Chunks to process: 9283\n",
      "This will take ~2-3 minutes\n",
      "\n",
      "\n",
      "Vector store created\n",
      "  Total chunks indexed: 9283\n",
      "  Ready for retrieval\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating embeddings...\")\n",
    "print(f\"Chunks to process: {len(all_chunks)}\")\n",
    "print(\"This will take ~2-3 minutes\\n\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\"\n",
    ")\n",
    "\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=all_chunks,\n",
    "    embedding=embeddings,\n",
    "    collection_name=\"litsearch_papers\"\n",
    ")\n",
    "\n",
    "print(f\"\\nVector store created\")\n",
    "print(f\"  Total chunks indexed: {len(all_chunks)}\")\n",
    "print(f\"  Ready for retrieval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test retrieval (sans LLM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Query: 'What are the best performing segmentation techniques on BraTS dataset?'\n",
      "============================================================\n",
      "\n",
      "[1] Score: 0.765\n",
      "    ArXiv: 2307.15872v1\n",
      "    Title: Cross-dimensional transfer learning in medical ima...\n",
      "    Section: body\n",
      "    Text: These methods have all performed more or less e fficiently, some better than others on\n",
      "the different challenge tasks. This shows the e ffectiveness of...\n",
      "\n",
      "[2] Score: 0.777\n",
      "    ArXiv: 2306.12510v2\n",
      "    Title: Comparative Analysis of Segment Anything Model and...\n",
      "    Section: body\n",
      "    Text: â€¢ DL for Medical Image Segmentation is used. \n",
      "â€¢ Hardware Acceleration on SBCs (Google's Edge \n",
      "TPU)  \n",
      "Scenario 1: \n",
      "BUSI: 0.995 \n",
      "UDIAT: 0.949 \n",
      "Scenario ...\n",
      "\n",
      "[3] Score: 0.785\n",
      "    ArXiv: 2102.04525v4\n",
      "    Title: Unified Focal loss: Generalising Dice and cross en...\n",
      "    Section: body\n",
      "    Text: 25described in (Table 1). For 3D binary segmentation, we used the BraTS20\n",
      "dataset. Here, images were pre-processed, with the skull stripped and images...\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "\n",
      "Query: 'What are the main challenges that remain unsolved in brain tumor segmentation?'\n",
      "============================================================\n",
      "\n",
      "[1] Score: 0.758\n",
      "    ArXiv: 2307.15872v1\n",
      "    Title: Cross-dimensional transfer learning in medical ima...\n",
      "    Section: body\n",
      "    Text: These methods have all performed more or less e fficiently, some better than others on\n",
      "the different challenge tasks. This shows the e ffectiveness of...\n",
      "\n",
      "[2] Score: 0.765\n",
      "    ArXiv: 1904.05191v3\n",
      "    Title: Weakly-Supervised White and Grey Matter Segmentati...\n",
      "    Section: body\n",
      "    Text: processes, drug perfusion and decision support systems. In this setting, segmen-\n",
      "tation of brain structures can initialize image based registration [1...\n",
      "\n",
      "[3] Score: 0.786\n",
      "    ArXiv: 1910.08978v2\n",
      "    Title: Attention Enriched Deep Learning Model for Breast ...\n",
      "    Section: body\n",
      "    Text: al. 2018; Ribli et al. 2017), a body of literature used MRI (Jaeger et al. 2018),  and histology images (Lin et al. 2018). \n",
      "U-Net (Ronneberger et al. ...\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n",
      "\n",
      "Query: 'What are the most commonly used public datasets for brain tumor detection?'\n",
      "============================================================\n",
      "\n",
      "[1] Score: 0.662\n",
      "    ArXiv: 2306.01827v2\n",
      "    Title: Active Learning on Medical Image...\n",
      "    Section: body\n",
      "    Text: Burren, Nicole Porz, Johannes Slotboom, Roland Wiest, et al. The multimodal brain tumor image segmentation\n",
      "benchmark (brats). IEEE transactions on med...\n",
      "\n",
      "[2] Score: 0.664\n",
      "    ArXiv: 2409.19483v4\n",
      "    Title: MedCLIP-SAMv2: Towards Universal Text-Driven Medic...\n",
      "    Section: body\n",
      "    Text: respectively.\n",
      "6â€¢ Brain Tumor MRI : The Brain Tumor dataset (Cheng,\n",
      "2017), comprising 1,462 T1-weighted MRI scans for train-\n",
      "ing, 1,002 for validation,...\n",
      "\n",
      "[3] Score: 0.676\n",
      "    ArXiv: 2102.04525v4\n",
      "    Title: Unified Focal loss: Generalising Dice and cross en...\n",
      "    Section: body\n",
      "    Text: Among medical imaging datasets, those involving tumour segmentation\n",
      "are associated with high degrees of class imbalance. Manual tumour delin-\n",
      "eation i...\n",
      "\n",
      "\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def test_retrieval(query, k=5):\n",
    "    \"\"\"Test semantic search\"\"\"\n",
    "    results = vectorstore.similarity_search_with_score(query, k=k)\n",
    "    \n",
    "    print(f\"\\nQuery: '{query}'\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    for i, (doc, score) in enumerate(results):\n",
    "        print(f\"[{i+1}] Score: {score:.3f}\")\n",
    "        print(f\"    ArXiv: {doc.metadata['arxiv_id']}\")\n",
    "        print(f\"    Title: {doc.metadata['title'][:50]}...\")\n",
    "        print(f\"    Section: {doc.metadata['section']}\")\n",
    "        print(f\"    Text: {doc.page_content[:150]}...\")\n",
    "        print()\n",
    "\n",
    "test_queries = [\n",
    "      \"What are the best performing segmentation techniques on BraTS dataset?\",\n",
    "      \"What are the main challenges that remain unsolved in brain tumor segmentation?\",\n",
    "      \"What are the most commonly used public datasets for brain tumor detection?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    test_retrieval(query, k=3)\n",
    "    print(\"\\n\" + \"â”€\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 7: Build Complete RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ RAG chain ready\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI research assistant. Use the context to answer. Cite sources as [arXiv:ID].\\n\\nContext: {context}\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([f\"[arXiv:{doc.metadata['arxiv_id']}]: {doc.page_content}\" for doc in docs])\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"âœ“ RAG chain ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Q: Can conformal predictions be used for unsupervised anomaly detection in images?\n",
      "============================================================\n",
      "\n",
      "The context provided does not mention the use of conformal predictions for unsupervised anomaly detection in images. The discussed methods for unsupervised anomaly detection in images include the use of generative models to synthesize healthy samples from diseased images, the use of an encoder network to replace the time-consuming iterative restoration process, and the creation of synthetic anomalies to train a discriminative model. However, it does not mention the use of conformal predictions in this process.\n",
      "\n",
      "\n",
      "============================================================\n",
      "Q: Are there machine learning methods to detect brain lesions using ultrasound?\n",
      "============================================================\n",
      "\n",
      "Yes, there are machine learning methods to detect brain lesions using ultrasound. For instance, the paper by H. Chen et al. discusses the use of iterative multi-domain regularized deep learning for anatomical structure detection and segmentation from ultrasound images [4]. However, the application of deep learning in ultrasound segmentation, particularly for brain lesions, is still an emerging field and more research is needed to make these methods more generalizable and applicable in various surgical settings [arXiv:1904.08655v1].\n",
      "\n",
      "\n",
      "============================================================\n",
      "Q: Transformers for multimodal image segmentation??Cross-attention for image classification?\n",
      "============================================================\n",
      "\n",
      "Transformers have been used for multimodal image segmentation. For instance, the TransBTS model uses transformers for multimodal brain tumor segmentation [30]. The model leverages the transformer's ability to capture contextual information and long-range dependencies in the input data, which is beneficial for medical image segmentation tasks.\n",
      "\n",
      "Cross-attention is also used in image classification tasks. In the context of ultrasound image segmentation, a Transformer decoder with cross-attention is proposed. This decoder consists of six decoder blocks and is trained with a set prediction objective. The cross-attention mechanism helps to capture long-range dependencies for better structure awareness [arXiv:2510.26568v1].\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_queries = [\n",
    "      \"Can conformal predictions be used for unsupervised anomaly detection in images?\",\n",
    "      \"Are there machine learning methods to detect brain lesions using ultrasound?\",\n",
    "      \"Transformers for multimodal image segmentation??\",\n",
    "      \"Cross-attention for image classification?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Q: {query}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    response = rag_chain.invoke(query)\n",
    "    print(response)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "import json\n",
    "\n",
    "def evaluate_faithfulness(question, answer, sources):\n",
    "    \"\"\"Evaluate if answer is faithful to sources using LLM judge\"\"\"\n",
    "    \n",
    "    context = \"\\n\\n\".join([f\"Source {i+1}: {s.page_content}\" for i, s in enumerate(sources)])\n",
    "    \n",
    "    prompt = f\"\"\"Rate faithfulness (0.0-1.0):\n",
    "\n",
    "Sources:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer: {answer}\n",
    "\n",
    "Is every claim supported by sources? Return only a number 0.0-1.0.\"\"\"\n",
    "    \n",
    "    judge = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "    score = judge.invoke(prompt).content.strip()\n",
    "    \n",
    "    return float(score)\n",
    "\n",
    "\n",
    "test_queries = [\n",
    "      \"What are the best performing segmentation techniques on BraTS dataset?\",\n",
    "      \"What are the main challenges that remain unsolved in brain tumor segmentation?\",\n",
    "      \"What are the most commonly used public datasets for brain tumor detection?\"\n",
    "]\n",
    "\n",
    "print(\"FAITHFULNESS EVALUATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "scores = []\n",
    "for query in test_queries:\n",
    "    answer = rag_chain.invoke(query)\n",
    "    sources = vectorstore.similarity_search(query, k=5)\n",
    "    score = evaluate_faithfulness(query, answer, sources)\n",
    "    scores.append(score)\n",
    "    \n",
    "    print(f\"{query[:45]}...\")\n",
    "    print(f\"  Faithfulness: {score:.2f}\\n\")\n",
    "\n",
    "print(f\"Average Faithfulness: {np.mean(scores):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
