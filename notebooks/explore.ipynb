{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LitSearch Setup and Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ““ LitSearch AI - Plan de DÃ©veloppement MVP\n",
    "\n",
    "## Phase 1: DATA (Aujourd'hui - 3h)\n",
    "\n",
    "### Step 1: Fetch 100 papers cs.AI\n",
    "- TÃ©lÃ©charger 100 papers rÃ©cents depuis arXiv\n",
    "- CatÃ©gorie: cs.AI\n",
    "- PÃ©riode: 6 derniers mois\n",
    "- **Deliverable:** Liste de 100 papers avec mÃ©tadonnÃ©es\n",
    "\n",
    "### Step 2: Parse PDFs (texte brut)\n",
    "- TÃ©lÃ©charger les PDFs depuis arXiv\n",
    "- Extraire le texte brut avec pypdf\n",
    "- GÃ©rer les erreurs (PDFs corrompus, etc.)\n",
    "- **Deliverable:** Texte complet pour chaque paper\n",
    "\n",
    "### Step 3: Valider qualitÃ© des donnÃ©es\n",
    "- VÃ©rifier que les PDFs sont lisibles\n",
    "- Analyser longueur moyenne des textes\n",
    "- Identifier papers problÃ©matiques\n",
    "- **Deliverable:** Dataset propre et validÃ©\n",
    "\n",
    "---\n",
    "\n",
    "## Phase 2: RAG PIPELINE (Aujourd'hui soir + Demain - 5h)\n",
    "\n",
    "### Step 4: Chunking intelligent\n",
    "- StratÃ©gie: Title+Abstract + Body chunks\n",
    "- Taille chunks: 1000 chars avec overlap 200\n",
    "- PrÃ©server mÃ©tadonnÃ©es (arXiv ID, section)\n",
    "- **Deliverable:** chunks avec metadata\n",
    "\n",
    "### Step 5: Embeddings + Vector store\n",
    "- CrÃ©er embeddings avec OpenAI\n",
    "- Stocker dans ChromaDB\n",
    "- Indexer avec mÃ©tadonnÃ©es\n",
    "- **Deliverable:** Vector store opÃ©rationnel\n",
    "\n",
    "### Step 6: Test retrieval (sans LLM)\n",
    "- Tester similarity search\n",
    "- VÃ©rifier pertinence des rÃ©sultats\n",
    "- Ajuster paramÃ¨tres (k, threshold)\n",
    "- **Deliverable:** Retrieval qui fonctionne\n",
    "\n",
    "### Step 7: RAG chain complet (avec LLM)\n",
    "- CrÃ©er prompt template scientifique\n",
    "- IntÃ©grer LLM (GPT-4)\n",
    "- Chain retrieval + generation\n",
    "- **Deliverable:** RAG end-to-end fonctionnel\n",
    "\n",
    "### Step 8: Tester avec questions\n",
    "- PrÃ©parer 10 questions test\n",
    "- Ã‰valuer qualitÃ© des rÃ©ponses\n",
    "- Identifier problÃ¨mes\n",
    "- **Deliverable:** 5+ questions qui marchent bien\n",
    "\n",
    "---\n",
    "\n",
    "## Phase 3: POLISH (Demain aprÃ¨s-midi - 3h)\n",
    "\n",
    "### Step 9: Optimiser prompts\n",
    "- AmÃ©liorer qualitÃ© des rÃ©ponses\n",
    "- RÃ©duire hallucinations\n",
    "- Forcer citations systÃ©matiques\n",
    "- **Deliverable:** RÃ©ponses de meilleure qualitÃ©\n",
    "\n",
    "### Step 10: AmÃ©liorer citations\n",
    "- Format: arXiv:ID, Section, Page\n",
    "- Affichage clair des sources\n",
    "- Relevance scores\n",
    "- **Deliverable:** Citations professionnelles\n",
    "\n",
    "### Step 11: Nettoyer le notebook\n",
    "- Markdown explicatif entre cells\n",
    "- Supprimer code mort\n",
    "- Organiser logiquement\n",
    "- **Deliverable:** Notebook prÃ©sentable\n",
    "\n",
    "---\n",
    "\n",
    "## Phase 4: PRÃ‰SENTATION (Samedi matin - 2h)\n",
    "\n",
    "### Step 12: README + documentation\n",
    "- Architecture diagram\n",
    "- Origin story (amie chercheuse)\n",
    "- Lien avec INSPIRE AI\n",
    "- **Deliverable:** README professionnel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment & Install necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install jupyter arxiv pypdf openai langchain langchain-core langchain-openai langchain-community langchain-text-splitters chromadb python-dotenv pandas requests"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nfrom dotenv import load_dotenv\n\nimport arxiv\n\nfrom pypdf import PdfReader\nimport io\nimport requests\n\nfrom langchain_openai import ChatOpenAI, OpenAIEmbeddings\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.documents import Document\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_community.vectorstores import Chroma\n\nimport pandas as pd\nfrom datetime import datetime, timedelta\nfrom typing import List, Dict\nimport json\nimport numpy as np"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "load_dotenv()\n\nopenai_key = os.getenv(\"OPENAI_API_KEY\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 1 : Data extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1. Fetch Papers from arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zm/zdbpwn4958j3hydxnlzjt9d00000gn/T/ipykernel_1967/2483580289.py:7: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n",
      "  for result in search.results():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ DR-Unet104 for Multimodal MRI brain tumor segmentation...\n",
      "âœ“ Robust Semantic Segmentation of Brain Tumor Regions from 3D ...\n",
      "âœ“ Reproducible Evaluation of Data Augmentation and Loss Functi...\n",
      "âœ“ Brain Tumor Segmentation using 3D-CNNs with Uncertainty Esti...\n",
      "âœ“ Optimizing Brain Tumor Classification: A Comprehensive Study...\n",
      "âœ“ Brain Tumor Sequence Registration with Non-iterative Coarse-...\n",
      "âœ“ Efficient Meningioma Tumor Segmentation Using Ensemble Learn...\n",
      "âœ“ Brain tumor multi classification and segmentation in MRI ima...\n",
      "âœ“ Brain Tumor Segmentation from MRI Images using Deep Learning...\n",
      "âœ“ MRI Brain Tumor Detection with Computer Vision...\n",
      "âœ“ Parameter-efficient Fine-tuning for improved Convolutional B...\n",
      "âœ“ Novel Deep Learning Architectures for Classification and Seg...\n",
      "âœ“ Analyzing Deep Learning Based Brain Tumor Segmentation with ...\n",
      "âœ“ Brain Tumor Detection in MRI Based on Federated Learning wit...\n",
      "âœ“ Deep Brain Net: An Optimized Deep Learning Model for Brain t...\n",
      "âœ“ Brain Tumor Detection Using Deep Learning Approaches...\n",
      "âœ“ Multimodal MRI brain tumor segmentation using random forests...\n",
      "âœ“ A Two-Stage Cascade Model with Variational Autoencoders and ...\n",
      "âœ“ Unified HT-CNNs Architecture: Transfer Learning for Segmenti...\n",
      "âœ“ Cross-Modality Deep Feature Learning for Brain Tumor Segment...\n",
      "âœ“ Brain MRI Tumor Segmentation with Adversarial Networks...\n",
      "âœ“ Towards Label-Free Brain Tumor Segmentation: Unsupervised Le...\n",
      "âœ“ A New Deep Hybrid Boosted and Ensemble Learning-based Brain ...\n",
      "âœ“ MRI brain tumor segmentation using informative feature vecto...\n",
      "âœ“ Brain Tumor Classification From MRI Images Using Machine Lea...\n",
      "âœ“ Deep and Statistical Learning in Biomedical Imaging: State o...\n",
      "âœ“ BiTr-Unet: a CNN-Transformer Combined Network for MRI Brain ...\n",
      "âœ“ MRI Brain Tumor Segmentation using Random Forests and Fully ...\n",
      "âœ“ Bayesian optimization assisted unsupervised learning for eff...\n",
      "âœ“ 3D AGSE-VNet: An Automatic Brain Tumor MRI Data Segmentation...\n",
      "âœ“ Robust 3D Brain MRI Inpainting with Random Masking Augmentat...\n",
      "âœ“ A Novel SLCA-UNet Architecture for Automatic MRI Brain Tumor...\n",
      "âœ“ An Optimization Framework for Processing and Transfer Learni...\n",
      "âœ“ Deep learning trends for focal brain pathology segmentation ...\n",
      "âœ“ Fully Automated Tumor Segmentation for Brain MRI data using ...\n",
      "âœ“ Transfer Learning for Brain Tumor Segmentation...\n",
      "âœ“ DIGEST: Deeply supervIsed knowledGE tranSfer neTwork learnin...\n",
      "âœ“ Deep Radiomics for Brain Tumor Detection and Classification ...\n",
      "âœ“ Multimodal CNN Networks for Brain Tumor Segmentation in MRI:...\n",
      "âœ“ Tumor Location-weighted MRI-Report Contrastive Learning: A F...\n",
      "âœ“ Multiclass Spinal Cord Tumor Segmentation on MRI with Deep L...\n",
      "âœ“ Brain Tumor classification and Segmentation using Deep Learn...\n",
      "âœ“ Brain Tumor Segmentation Based on Deep Learning, Attention M...\n",
      "âœ“ An Improved Deep Convolutional Neural Network by Using Hybri...\n",
      "âœ“ Towards fully automated deep-learning-based brain tumor segm...\n",
      "âœ“ Brain Tumor Classification from MRI Scans via Transfer Learn...\n",
      "âœ“ Incremental Learning for Heterogeneous Structure Segmentatio...\n",
      "âœ“ Unsupervised deep clustering and reinforcement learning can ...\n",
      "âœ“ Deep Convolutional Neural Networks Model-based Brain Tumor D...\n",
      "âœ“ Weakly Supervised Fine Tuning Approach for Brain Tumor Segme...\n",
      "âœ“ Detecting Glioma, Meningioma, and Pituitary Tumors, and Norm...\n",
      "âœ“ Detection and Classification of Glioblastoma Brain Tumor...\n",
      "âœ“ Latent Correlation Representation Learning for Brain Tumor S...\n",
      "âœ“ A Volumetric Convolutional Neural Network for Brain Tumor Se...\n",
      "âœ“ Machine learning approach to brain tumor detection and class...\n",
      "âœ“ Modality-Pairing Learning for Brain Tumor Segmentation...\n",
      "âœ“ A Deep Learning Approach for Brain Tumor Classification and ...\n",
      "âœ“ Multi-Resolution 3D CNN for MRI Brain Tumor Segmentation and...\n",
      "âœ“ A Review on End-To-End Methods for Brain Tumor Segmentation ...\n",
      "âœ“ Efficient Feature Extraction and Classification Architecture...\n",
      "âœ“ Enhancing Neuro-Oncology Through Self-Assessing Deep Learnin...\n",
      "âœ“ DeepSeg: Deep Neural Network Framework for Automatic Brain T...\n",
      "âœ“ Resource-Efficient Glioma Segmentation on Sub-Saharan MRI...\n",
      "âœ“ Non Parametric Data Augmentations Improve Deep-Learning base...\n",
      "âœ“ Brain Tumor Segmentation by Cascaded Deep Neural Networks Us...\n",
      "âœ“ Deep Learning for Brain Tumor Segmentation in Radiosurgery: ...\n",
      "âœ“ PK-YOLO: Pretrained Knowledge Guided YOLO for Brain Tumor De...\n",
      "âœ“ Training and Comparison of nnU-Net and DeepMedic Methods for...\n",
      "âœ“ United adversarial learning for liver tumor segmentation and...\n",
      "âœ“ Segmentation of brain tumor on magnetic resonance imaging us...\n",
      "âœ“ Advancing Brain Tumor Detection: A Thorough Investigation of...\n",
      "âœ“ An Explainable Deep Learning Framework for Brain Stroke and ...\n",
      "âœ“ Deep Learning in Medical Image Classification from MRI-based...\n",
      "âœ“ Brain MRI detection by Sematic Segmentation models- Transfer...\n",
      "âœ“ Fully-automated deep learning-powered system for DCE-MRI ana...\n",
      "âœ“ Election of Collaborators via Reinforcement Learning for Fed...\n",
      "âœ“ Neural Gas Network Image Features and Segmentation for Brain...\n",
      "âœ“ Dilated Inception U-Net (DIU-Net) for Brain Tumor Segmentati...\n",
      "âœ“ Transfer Learning and Explainable AI for Brain Tumor Classif...\n",
      "âœ“ Automatic brain tumor segmentation in 2D intra-operative ult...\n",
      "âœ“ ME-Net: Multi-Encoder Net Framework for Brain Tumor Segmenta...\n",
      "âœ“ A Novel Framework for Brain Tumor Detection Based on Convolu...\n",
      "âœ“ mmFormer: Multimodal Medical Transformer for Incomplete Mult...\n",
      "âœ“ Segmentation of Pediatric Brain Tumors using a Radiologicall...\n",
      "âœ“ Deep Learning-Based Brain Image Segmentation for Automated T...\n",
      "âœ“ Integrating Preprocessing Methods and Convolutional Neural N...\n",
      "âœ“ Precise measurement of CMB polarisation from Dome-C: the BRA...\n",
      "âœ“ Knowledge Distillation for Brain Tumor Segmentation...\n",
      "âœ“ Expectation-Maximization Regularized Deep Learning for Weakl...\n",
      "âœ“ Ensemble Learning and 3D Pix2Pix for Comprehensive Brain Tum...\n",
      "âœ“ Multi-class Brain Tumor Segmentation using Graph Attention N...\n",
      "âœ“ Demystifying Deep Learning-based Brain Tumor Segmentation wi...\n",
      "âœ“ Brain Tumor Synthetic Segmentation in 3D Multimodal MRI Scan...\n",
      "âœ“ Multiparametric Deep Learning and Radiomics for Tumor Gradin...\n",
      "âœ“ Automated ensemble method for pediatric brain tumor segmenta...\n",
      "âœ“ Hierarchical Deep Feature Fusion and Ensemble Learning for E...\n",
      "âœ“ Explainable Deep Learning in Medical Imaging: Brain Tumor an...\n",
      "âœ“ BC-MRI-SEG: A Breast Cancer MRI Tumor Segmentation Benchmark...\n",
      "âœ“ Generating 3D Brain Tumor Regions in MRI using Vector-Quanti...\n",
      "âœ“ Graph-based Multi-Modal Interaction Lightweight Network for ...\n",
      "âœ“ Within-Brain Classification for Brain Tumor Segmentation...\n",
      "âœ“ Brain Tumor MRI Classification using a Novel Deep Residual a...\n",
      "âœ“ Brain Tumor Detection using Swin Transformers...\n",
      "âœ“ Multi-channel MRI Embedding: An EffectiveStrategy for Enhanc...\n",
      "âœ“ Brain Tumor Detection through Thermal Imaging and MobileNET...\n",
      "âœ“ Fed-MUnet: Multi-modal Federated Unet for Brain Tumor Segmen...\n",
      "âœ“ 3D Brainformer: 3D Fusion Transformer for Brain Tumor Segmen...\n",
      "âœ“ Predicting Hypoxia in Brain Tumors from Multiparametric MRI...\n",
      "âœ“ Cross-modality (CT-MRI) prior augmented deep learning for ro...\n",
      "âœ“ Brain Tumor Classification by Cascaded Multiscale Multitask ...\n",
      "âœ“ Assistive Diagnostic Tool for Brain Tumor Detection using Co...\n",
      "âœ“ Deep Learning for Longitudinal Gross Tumor Volume Segmentati...\n",
      "âœ“ Synthetic Poisoning Attacks: The Impact of Poisoned MRI Imag...\n",
      "âœ“ HI-Net: Hyperdense Inception 3D UNet for Brain Tumor Segment...\n",
      "âœ“ Reinforcement learning using Deep Q Networks and Q learning ...\n",
      "âœ“ MRI brain tumor segmentation and uncertainty estimation usin...\n",
      "âœ“ Swin UNETR: Swin Transformers for Semantic Segmentation of B...\n",
      "âœ“ Lung tumor segmentation in MRI mice scans using 3D nnU-Net w...\n",
      "âœ“ A Transformer-based Generative Adversarial Network for Brain...\n",
      "âœ“ Impact of Spherical Coordinates Transformation Pre-processin...\n",
      "âœ“ MRI-Based Brain Tumor Detection through an Explainable Effic...\n",
      "âœ“ Automated Ensemble-Based Segmentation of Adult Brain Tumors:...\n",
      "âœ“ BRISC: Annotated Dataset for Brain Tumor Segmentation and Cl...\n",
      "âœ“ Redundancy Reduction in Semantic Segmentation of 3D Brain Tu...\n",
      "âœ“ Minimally Interactive Segmentation of Soft-Tissue Tumors on ...\n",
      "âœ“ Enhancing Brain Tumor Classification Using TrAdaBoost and Mu...\n",
      "âœ“ An Ensemble Approach for Brain Tumor Segmentation and Synthe...\n",
      "âœ“ Advanced Brain Tumor Segmentation Using EMCAD: Efficient Mul...\n",
      "âœ“ Prototype-Based Approach for One-Shot Segmentation of Brain ...\n",
      "âœ“ Hypergraph Tversky-Aware Domain Incremental Learning for Bra...\n",
      "âœ“ Distributed Federated Learning-Based Deep Learning Model for...\n",
      "âœ“ Generative Adversarial Synthesis and Deep Feature Discrimina...\n",
      "âœ“ Learning Multi-Modal Brain Tumor Segmentation from Privilege...\n",
      "âœ“ Magnetic Resonance Imaging Feature-Based Subtyping and Model...\n",
      "âœ“ Attention-Enhanced Hybrid Feature Aggregation Network for 3D...\n",
      "âœ“ A Novel Mask R-CNN Model to Segment Heterogeneous Brain Tumo...\n",
      "âœ“ SKIPNet: Spatial Attention Skip Connections for Enhanced Bra...\n",
      "âœ“ Evaluating the Impact of Sequence Combinations on Breast Tum...\n",
      "âœ“ Deep reinforcement learning-based image classification achie...\n",
      "âœ“ Multimodal Fusion at Three Tiers: Physics-Driven Data Genera...\n",
      "âœ“ M3AE: Multimodal Representation Learning for Brain Tumor Seg...\n",
      "âœ“ Light Weight CNN for classification of Brain Tumors from MRI...\n",
      "âœ“ Computational Modeling of Deep Multiresolution-Fractal Textu...\n",
      "âœ“ M-Net: MRI Brain Tumor Sequential Segmentation Network via M...\n",
      "âœ“ Deep neuroevolution to predict primary brain tumor grade fro...\n",
      "âœ“ Domain Knowledge Based Brain Tumor Segmentation and Overall ...\n",
      "âœ“ A Systematic Approach for MRI Brain Tumor Localization, and ...\n",
      "âœ“ NNDM: NN_UNet Diffusion Model for Brain Tumor Segmentation...\n",
      "âœ“ CAVM: Conditional Autoregressive Vision Model for Contrast-E...\n",
      "âœ“ Negligible effect of brain MRI data preprocessing for tumor ...\n",
      "\n",
      "150 papers fetched (RAG-focused)\n"
     ]
    }
   ],
   "source": [
    "search = arxiv.Search(\n",
    "      query='brain tumor AND MRI AND deep learning AND (detection OR segmentation)',\n",
    "      max_results=150,\n",
    "      sort_by=arxiv.SortCriterion.Relevance)\n",
    "\n",
    "papers = []\n",
    "for result in search.results():\n",
    "    paper = {\n",
    "        'article_id': result.entry_id.split('/')[-1],\n",
    "        'title': result.title,\n",
    "        'authors': [author.name for author in result.authors],\n",
    "        'published': result.published,\n",
    "        'summary': result.summary,\n",
    "        'pdf_url': result.pdf_url\n",
    "    }\n",
    "    papers.append(paper)\n",
    "    print(f\"âœ“ {paper['title'][:60]}...\")\n",
    "\n",
    "print(f\"\\n{len(papers)} papers fetched (RAG-focused)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 2: Parse PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def parse_pdf(pdf_url):\n    try:\n        response = requests.get(pdf_url, timeout=30)\n        pdf = PdfReader(io.BytesIO(response.content))\n        text = \"\".join(page.extract_text() for page in pdf.pages)\n        return text if len(text) > 500 else None\n    except:\n        return None\n\nprint(f\"\\nParsing {len(papers)} PDFs...\")\n\nfor i, paper in enumerate(papers):\n    text = parse_pdf(paper['pdf_url'])\n    \n    if text:\n        paper['full_text'] = text\n    else:\n        paper['full_text'] = f\"{paper['title']}\\n\\n{paper['summary']}\"\n    \n    if i % 10 == 0:\n        print(f\"{i}/{len(papers)}\")\n\nsuccess = sum(1 for p in papers if len(p['full_text']) > 1000)\nprint(f\"\\nDone: {success}/{len(papers)} parsed\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 3: Data Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Length Statistics:\n",
      "count       150.000000\n",
      "mean      36151.133333\n",
      "std       17251.731701\n",
      "min        1556.000000\n",
      "25%       25077.750000\n",
      "50%       31661.000000\n",
      "75%       45653.750000\n",
      "max      109250.000000\n",
      "Name: text_length, dtype: float64\n",
      "\n",
      "Short papers (<2k chars): 2\n",
      "\n",
      "âœ“ Data validated\n"
     ]
    }
   ],
   "source": [
    "df_analysis = pd.DataFrame(papers)\n",
    "df_analysis['text_length'] = df_analysis['full_text'].str.len()\n",
    "\n",
    "print(\"Text Length Statistics:\")\n",
    "print(df_analysis['text_length'].describe())\n",
    "\n",
    "print(f\"\\nShort papers (<2k chars): {(df_analysis['text_length'] < 2000).sum()}\")\n",
    "\n",
    "print(\"\\nâœ“ Data validated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "      <th>published</th>\n",
       "      <th>summary</th>\n",
       "      <th>pdf_url</th>\n",
       "      <th>full_text</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011.02840v2</td>\n",
       "      <td>DR-Unet104 for Multimodal MRI brain tumor segm...</td>\n",
       "      <td>[Jordan Colman, Lei Zhang, Wenting Duan, Xujio...</td>\n",
       "      <td>2020-11-04 01:24:26+00:00</td>\n",
       "      <td>In this paper we propose a 2D deep residual Un...</td>\n",
       "      <td>https://arxiv.org/pdf/2011.02840v2</td>\n",
       "      <td>\\n \\n \\nDR-Unet104 for Multimodal MRI brain t...</td>\n",
       "      <td>25003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001.02040v1</td>\n",
       "      <td>Robust Semantic Segmentation of Brain Tumor Re...</td>\n",
       "      <td>[Andriy Myronenko, Ali Hatamizadeh]</td>\n",
       "      <td>2020-01-06 07:47:42+00:00</td>\n",
       "      <td>Multimodal brain tumor segmentation challenge ...</td>\n",
       "      <td>https://arxiv.org/pdf/2001.02040v1</td>\n",
       "      <td>Robust Semantic Segmentation of Brain Tumor\\nR...</td>\n",
       "      <td>18633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2510.08617v1</td>\n",
       "      <td>Reproducible Evaluation of Data Augmentation a...</td>\n",
       "      <td>[Saumya B]</td>\n",
       "      <td>2025-10-08 06:15:28+00:00</td>\n",
       "      <td>Brain tumor segmentation is crucial for diagno...</td>\n",
       "      <td>https://arxiv.org/pdf/2510.08617v1</td>\n",
       "      <td>Reproducible Evaluation of Data Augmentation a...</td>\n",
       "      <td>28235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2009.12188v1</td>\n",
       "      <td>Brain Tumor Segmentation using 3D-CNNs with Un...</td>\n",
       "      <td>[Laura Mora Ballestar, Veronica Vilaplana]</td>\n",
       "      <td>2020-09-24 10:50:12+00:00</td>\n",
       "      <td>Automation of brain tumors in 3D magnetic reso...</td>\n",
       "      <td>https://arxiv.org/pdf/2009.12188v1</td>\n",
       "      <td>Brain Tumor Segmentation using 3D-CNNs with\\nU...</td>\n",
       "      <td>24661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2308.06821v1</td>\n",
       "      <td>Optimizing Brain Tumor Classification: A Compr...</td>\n",
       "      <td>[Raza Imam, Mohammed Talha Alam]</td>\n",
       "      <td>2023-08-13 17:30:32+00:00</td>\n",
       "      <td>Deep learning has emerged as a prominent field...</td>\n",
       "      <td>https://arxiv.org/pdf/2308.06821v1</td>\n",
       "      <td>Optimizing Brain Tumor Classification: A Compr...</td>\n",
       "      <td>38656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2211.07876v1</td>\n",
       "      <td>Brain Tumor Sequence Registration with Non-ite...</td>\n",
       "      <td>[Mingyuan Meng, Lei Bi, Dagan Feng, Jinman Kim]</td>\n",
       "      <td>2022-11-15 03:58:47+00:00</td>\n",
       "      <td>In this study, we focus on brain tumor sequenc...</td>\n",
       "      <td>https://arxiv.org/pdf/2211.07876v1</td>\n",
       "      <td>Brain Tumor Sequence Registration with Non-ite...</td>\n",
       "      <td>25657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2510.21040v1</td>\n",
       "      <td>Efficient Meningioma Tumor Segmentation Using ...</td>\n",
       "      <td>[Mohammad Mahdi Danesh Pajouh, Sara Saeedi]</td>\n",
       "      <td>2025-10-23 22:51:22+00:00</td>\n",
       "      <td>Meningiomas represent the most prevalent form ...</td>\n",
       "      <td>https://arxiv.org/pdf/2510.21040v1</td>\n",
       "      <td>In loving memory of a wonderful grandma whose ...</td>\n",
       "      <td>27877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2304.10039v2</td>\n",
       "      <td>Brain tumor multi classification and segmentat...</td>\n",
       "      <td>[Belal Amin, Romario Sameh Samir, Youssef Tare...</td>\n",
       "      <td>2023-04-20 01:32:55+00:00</td>\n",
       "      <td>This study proposes a deep learning model for ...</td>\n",
       "      <td>https://arxiv.org/pdf/2304.10039v2</td>\n",
       "      <td>BRAIN TUMOR MULTI CLASSIFICATION AND\\nSEGMENTA...</td>\n",
       "      <td>29201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2305.00257v1</td>\n",
       "      <td>Brain Tumor Segmentation from MRI Images using...</td>\n",
       "      <td>[Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra,...</td>\n",
       "      <td>2023-04-29 13:33:21+00:00</td>\n",
       "      <td>A brain tumor, whether benign or malignant, ca...</td>\n",
       "      <td>https://arxiv.org/pdf/2305.00257v1</td>\n",
       "      <td>Brain Tumor Segmentation from MRI Images using...</td>\n",
       "      <td>37212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2510.10250v1</td>\n",
       "      <td>MRI Brain Tumor Detection with Computer Vision</td>\n",
       "      <td>[Jack Krolik, Jake Lynn, John Henry Rudden, Dm...</td>\n",
       "      <td>2025-10-11 15:07:52+00:00</td>\n",
       "      <td>This study explores the application of deep le...</td>\n",
       "      <td>https://arxiv.org/pdf/2510.10250v1</td>\n",
       "      <td>MRI Brain Tumor Detection with Computer Vision...</td>\n",
       "      <td>28734</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     article_id                                              title  \\\n",
       "0  2011.02840v2  DR-Unet104 for Multimodal MRI brain tumor segm...   \n",
       "1  2001.02040v1  Robust Semantic Segmentation of Brain Tumor Re...   \n",
       "2  2510.08617v1  Reproducible Evaluation of Data Augmentation a...   \n",
       "3  2009.12188v1  Brain Tumor Segmentation using 3D-CNNs with Un...   \n",
       "4  2308.06821v1  Optimizing Brain Tumor Classification: A Compr...   \n",
       "5  2211.07876v1  Brain Tumor Sequence Registration with Non-ite...   \n",
       "6  2510.21040v1  Efficient Meningioma Tumor Segmentation Using ...   \n",
       "7  2304.10039v2  Brain tumor multi classification and segmentat...   \n",
       "8  2305.00257v1  Brain Tumor Segmentation from MRI Images using...   \n",
       "9  2510.10250v1     MRI Brain Tumor Detection with Computer Vision   \n",
       "\n",
       "                                             authors  \\\n",
       "0  [Jordan Colman, Lei Zhang, Wenting Duan, Xujio...   \n",
       "1                [Andriy Myronenko, Ali Hatamizadeh]   \n",
       "2                                         [Saumya B]   \n",
       "3         [Laura Mora Ballestar, Veronica Vilaplana]   \n",
       "4                   [Raza Imam, Mohammed Talha Alam]   \n",
       "5    [Mingyuan Meng, Lei Bi, Dagan Feng, Jinman Kim]   \n",
       "6        [Mohammad Mahdi Danesh Pajouh, Sara Saeedi]   \n",
       "7  [Belal Amin, Romario Sameh Samir, Youssef Tare...   \n",
       "8  [Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra,...   \n",
       "9  [Jack Krolik, Jake Lynn, John Henry Rudden, Dm...   \n",
       "\n",
       "                  published  \\\n",
       "0 2020-11-04 01:24:26+00:00   \n",
       "1 2020-01-06 07:47:42+00:00   \n",
       "2 2025-10-08 06:15:28+00:00   \n",
       "3 2020-09-24 10:50:12+00:00   \n",
       "4 2023-08-13 17:30:32+00:00   \n",
       "5 2022-11-15 03:58:47+00:00   \n",
       "6 2025-10-23 22:51:22+00:00   \n",
       "7 2023-04-20 01:32:55+00:00   \n",
       "8 2023-04-29 13:33:21+00:00   \n",
       "9 2025-10-11 15:07:52+00:00   \n",
       "\n",
       "                                             summary  \\\n",
       "0  In this paper we propose a 2D deep residual Un...   \n",
       "1  Multimodal brain tumor segmentation challenge ...   \n",
       "2  Brain tumor segmentation is crucial for diagno...   \n",
       "3  Automation of brain tumors in 3D magnetic reso...   \n",
       "4  Deep learning has emerged as a prominent field...   \n",
       "5  In this study, we focus on brain tumor sequenc...   \n",
       "6  Meningiomas represent the most prevalent form ...   \n",
       "7  This study proposes a deep learning model for ...   \n",
       "8  A brain tumor, whether benign or malignant, ca...   \n",
       "9  This study explores the application of deep le...   \n",
       "\n",
       "                              pdf_url  \\\n",
       "0  https://arxiv.org/pdf/2011.02840v2   \n",
       "1  https://arxiv.org/pdf/2001.02040v1   \n",
       "2  https://arxiv.org/pdf/2510.08617v1   \n",
       "3  https://arxiv.org/pdf/2009.12188v1   \n",
       "4  https://arxiv.org/pdf/2308.06821v1   \n",
       "5  https://arxiv.org/pdf/2211.07876v1   \n",
       "6  https://arxiv.org/pdf/2510.21040v1   \n",
       "7  https://arxiv.org/pdf/2304.10039v2   \n",
       "8  https://arxiv.org/pdf/2305.00257v1   \n",
       "9  https://arxiv.org/pdf/2510.10250v1   \n",
       "\n",
       "                                           full_text  text_length  \n",
       "0   \\n \\n \\nDR-Unet104 for Multimodal MRI brain t...        25003  \n",
       "1  Robust Semantic Segmentation of Brain Tumor\\nR...        18633  \n",
       "2  Reproducible Evaluation of Data Augmentation a...        28235  \n",
       "3  Brain Tumor Segmentation using 3D-CNNs with\\nU...        24661  \n",
       "4  Optimizing Brain Tumor Classification: A Compr...        38656  \n",
       "5  Brain Tumor Sequence Registration with Non-ite...        25657  \n",
       "6  In loving memory of a wonderful grandma whose ...        27877  \n",
       "7  BRAIN TUMOR MULTI CLASSIFICATION AND\\nSEGMENTA...        29201  \n",
       "8  Brain Tumor Segmentation from MRI Images using...        37212  \n",
       "9  MRI Brain Tumor Detection with Computer Vision...        28734  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_analysis.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase 2 : Rag pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 1: Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def chunk_paper(paper):\n    \"\"\"Create chunks with metadata\"\"\"\n    \n    title_abstract = f\"Title: {paper['title']}\\n\\nAbstract: {paper['summary']}\"\n    \n    chunks = [Document(\n        page_content=title_abstract,\n        metadata={\n            'arxiv_id': paper['article_id'],\n            'title': paper['title'],\n            'section': 'title_abstract',\n            'authors': ', '.join(paper['authors'][:3])\n        }\n    )]\n    \n    splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,\n        chunk_overlap=200\n    )\n    \n    body_chunks = splitter.create_documents(\n        texts=[paper['full_text']],\n        metadatas=[{\n            'arxiv_id': paper['article_id'],\n            'title': paper['title'],\n            'section': 'body',\n            'authors': ', '.join(paper['authors'][:3])\n        }]\n    )\n    \n    chunks.extend(body_chunks)\n    return chunks\n\nprint(\"Chunking papers...\")\n\nall_chunks = []\nfor i, paper in enumerate(papers):\n    paper_chunks = chunk_paper(paper)\n    all_chunks.extend(paper_chunks)\n    \n    if i % 20 == 0:\n        print(f\"{i}/{len(papers)} - {len(all_chunks)} chunks so far\")\n\nprint(f\"\\nDone: {len(all_chunks)} total chunks\")\nprint(f\"Avg chunks per paper: {len(all_chunks)/len(papers):.1f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'arxiv_id': '2011.02840v2', 'title': 'DR-Unet104 for Multimodal MRI brain tumor segmentation', 'section': 'title_abstract', 'authors': 'Jordan Colman, Lei Zhang, Wenting Duan'}, page_content=\"Title: DR-Unet104 for Multimodal MRI brain tumor segmentation\\n\\nAbstract: In this paper we propose a 2D deep residual Unet with 104 convolutional layers (DR-Unet104) for lesion segmentation in brain MRIs. We make multiple additions to the Unet architecture, including adding the 'bottleneck' residual block to the Unet encoder and adding dropout after each convolution block stack. We verified the effect of introducing the regularisation of dropout with small rate (e.g. 0.2) on the architecture, and found a dropout of 0.2 improved the overall performance compared to no dropout, or a dropout of 0.5. We evaluated the proposed architecture as part of the Multimodal Brain Tumor Segmentation (BraTS) 2020 Challenge and compared our method to DeepLabV3+ with a ResNet-V2-152 backbone. We found that the DR-Unet104 achieved a mean dice score coefficient of 0.8862, 0.6756 and 0.6721 for validation data, whole tumor, enhancing tumor and tumor core respectively, an overall improvement on 0.8770, 0.65242 and 0.68134 achieved by DeepLabV3+. Our method produced a final mean DSC of 0.8673, 0.7514 and 0.7983 on whole tumor, enhancing tumor and tumor core on the challenge's testing data. We produced a competitive lesion segmentation architecture, despite only 2D convolutions, having the added benefit that it can be used on lower power computers than a 3D architecture. The source code and trained model for this work is openly available at https://github.com/jordan-colman/DR-Unet104.\"),\n",
       " Document(metadata={'arxiv_id': '2011.02840v2', 'title': 'DR-Unet104 for Multimodal MRI brain tumor segmentation', 'section': 'body', 'authors': 'Jordan Colman, Lei Zhang, Wenting Duan'}, page_content='DR-Unet104 for Multimodal MRI brain tumor  \\nsegmentation  \\nJordan Colman1,2, Lei Zhang2, Wenting Duan2, Xujiong Ye2 \\n1Ashford and St Peterâ€™s Hospitals NHS Foundation Trust, Surrey, UK \\njordan.colman@nhs.net,  \\n2 School of Computer Science, University of Lincoln, Lincoln, UK LZhang@lincoln.ac.uk, \\nwduan@lincoln.ac.uk, XYe@lincoln.ac.uk \\nAbstract. In this paper we propose a 2D deep residual Unet with 104 convolutional layers (DR-Unet104) for \\nlesion segmentation in brain MRIs. We make multiple additions to the Unet architecture, including adding the \\nâ€˜bottleneckâ€™ residual block to the Unet encoder and adding dropout after each convolution block stack. We \\nverified the effect of including the regularization of dropout with small rate (e.g. 0.2) on the architecture, and \\nfound a dropout of 0.2 improved the overall performance compared to no dropout, or a dropout of 0.5. We \\nevaluated the proposed architecture as part of the  Multimodal Brain Tumor Segmentation (BraTS) 20 20'),\n",
       " Document(metadata={'arxiv_id': '2011.02840v2', 'title': 'DR-Unet104 for Multimodal MRI brain tumor segmentation', 'section': 'body', 'authors': 'Jordan Colman, Lei Zhang, Wenting Duan'}, page_content='evaluated the proposed architecture as part of the  Multimodal Brain Tumor Segmentation (BraTS) 20 20 \\nChallenge and compared our method to DeepLabV3+ wit h a ResNet -V2-152 backbone. We found the DR-\\nUnet104 achieved a mean dice score coefficient of 0.8862, 0.6756 and 0.6721 for validation data, whole tumor, \\nenhancing tumor and tumor core respectively, an overall improvement on 0.8770, 0.65242 and 0.68134 \\nachieved by DeepLabV3+. Our method produced a final mean DSC of 0.8673, 0.7514 and 0.7983 on whole \\ntumor, enhancing tumor and tumor core on the challenge â€™s testing data.  We pr oduce a competitive  lesion \\nsegmentation architecture, despite only using 2D convolutions, having the added benefit that it can be used on \\nlower power computers than a 3D architecture.  The source code and trained model for this work is openly \\navailable at https://github.com/jordan-colman/DR-Unet104. \\n \\nKeywords: Deep learning, Brain Tumor Segmentation, BraTS, ResNet, Unet, Dropout'),\n",
       " Document(metadata={'arxiv_id': '2011.02840v2', 'title': 'DR-Unet104 for Multimodal MRI brain tumor segmentation', 'section': 'body', 'authors': 'Jordan Colman, Lei Zhang, Wenting Duan'}, page_content='available at https://github.com/jordan-colman/DR-Unet104. \\n \\nKeywords: Deep learning, Brain Tumor Segmentation, BraTS, ResNet, Unet, Dropout \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFinal authenticated publication is available online at https://doi.org/10.1007/978-3-030-72087-2_36 \\n \\nCite this paper as: \\nColman J., Zhang L., Duan W., Ye X. (2021) DR-Unet104 for Multi-modal MRI Brain Tumor Segmentation. \\nIn: Crimi A., Bakas S. (eds) Brainlesion: Glioma, Multiple Sclerosis, Stroke and Trau -matic Brain Injuries. \\nBrainLes 2020. Lecture Notes in Computer Science, vol 12659. Springer, Cham. https://doi.org/10.1007/978-\\n3-030-72087-2_36  2             Colman et al. \\n \\n1 Introduction \\nLesion segmentation is an important area of research necessary to progress the field of radiomics, using imaging \\nto infer biomarkers, that can be used to aid prognosis prediction and treatment of patients [1]. Segmentation of'),\n",
       " Document(metadata={'arxiv_id': '2011.02840v2', 'title': 'DR-Unet104 for Multimodal MRI brain tumor segmentation', 'section': 'body', 'authors': 'Jordan Colman, Lei Zhang, Wenting Duan'}, page_content='to infer biomarkers, that can be used to aid prognosis prediction and treatment of patients [1]. Segmentation of \\ngliomas, the most common form of primary brain malignancy [2], is a highly useful application of lesion segmen-\\ntation. Accurate brain tumor segmentation in MRI produces useful volumetric information, and in the future may \\nbe used to derive biomarkers to grade gliomas and predict prognosis. Manual brain tumor segmentation, however, \\nis a skilled and time -consuming task. Therefore, automa ted brain tumor segmentation would be o f great benefit \\nto progress this area. However, accurate segmentation remains a challenging task due to the high variation of brain \\ntumor size, shape, position and inconsistent intensity and contrast in various image modalities. This has contrib-\\nuted to the development of automatic segmentation methods, for which several methods have been proposed. The'),\n",
       " Document(metadata={'arxiv_id': '2011.02840v2', 'title': 'DR-Unet104 for Multimodal MRI brain tumor segmentation', 'section': 'body', 'authors': 'Jordan Colman, Lei Zhang, Wenting Duan'}, page_content='uted to the development of automatic segmentation methods, for which several methods have been proposed. The \\nMultimodal Brain Tumor Segmentation (BraTS) challenge is an annual challenge set to act as a benchmark of the \\ncurrent state-of-the-art brain tumor segmentation algorithms. \\n \\nIn the recent years deep neural networks have achieved the top performance in the BraTS challenge. Many existing \\nmethods consider the Unet [ 3] as a base architecture , a basic but effective form of the encoder -decoder network \\ndesign, with at least two winners of the BraTS challenge utilising a variation of the Unet in 2017 and 2019 [ 4,5]. \\nOther current state -of-the-art segmentation algorithms use the ResNet [6,7] as an encoder in such architecture , \\nsuch as the DeepLabV3+, which uses ResNet-101 as the initial encoder and spatial pyramid pooling module in \\nthe final layer [8]. The ResNet uses identity mapping, a residual connection which skips multiple network layers'),\n",
       " Document(metadata={'arxiv_id': '2011.02840v2', 'title': 'DR-Unet104 for Multimodal MRI brain tumor segmentation', 'section': 'body', 'authors': 'Jordan Colman, Lei Zhang, Wenting Duan'}, page_content='the final layer [8]. The ResNet uses identity mapping, a residual connection which skips multiple network layers \\nto aid back propagation and allows deep er networks to be made. Additionally, the ResNet uses a â€˜bottleneckâ€™ \\nresidual block which uses a 1x1 convolution to reduce the number of imag e features prior to a spatial , or 3x3 \\nconvolution, and then uses another 1x1 convolution to increase the number of feature s. This is done to increase \\ncomputational efficiency and at the same time increase the number of image features represented in the network. \\nThe ResNet and DeeplabV3+ do not use random dropout, a commonly used regularizer, which randomly removes \\nthe signal of a given proportion of neurons in a layer in order to reduce overfitting of training data [9].  \\n \\nA common approach of improving the performance of current architectures in medical image segmentation is by \\nextending 2D image segmentation to 3D, using 3D convolutional networks on whole images as opposed to a'),\n",
       " Document(metadata={'arxiv_id': '2011.02840v2', 'title': 'DR-Unet104 for Multimodal MRI brain tumor segmentation', 'section': 'body', 'authors': 'Jordan Colman, Lei Zhang, Wenting Duan'}, page_content='extending 2D image segmentation to 3D, using 3D convolutional networks on whole images as opposed to a \\nsingle 2D MRI slice. A n example is the  BraTS 2019 1 st place solution for the brain tumor segmentation task, \\nwhich used a two-staged cascaded 3D Unet [4]. The paper used a 3D Unet architecture with the addition of residual \\nconnections in convolutional blocks, in stacks of 1, 2 or 4. The first cascade has fewer total feature channel s and \\ndetects â€˜coarseâ€™ features, the second cascade detects finer details as it has more total feature channels. This archi-\\ntecture achieved a mean dice score coefficient (DSC) of 0.8880 for whole tumor segmentation of the testing data, \\n0.8370 for tumor core and 0.8327 for enhancing tumor. However, this architecture required cropping 3D images \\nto run in a batch size of 1 on a graphics card with 12Gb of memory due to the high memory usages of 3D pro-\\ncessing and the large image size.'),\n",
       " Document(metadata={'arxiv_id': '2011.02840v2', 'title': 'DR-Unet104 for Multimodal MRI brain tumor segmentation', 'section': 'body', 'authors': 'Jordan Colman, Lei Zhang, Wenting Duan'}, page_content='to run in a batch size of 1 on a graphics card with 12Gb of memory due to the high memory usages of 3D pro-\\ncessing and the large image size. \\n   \\nIn this paper, given the success of the Unet in BraST challeng e [4,5] and inspired by th e â€˜bottleneckâ€™ residual \\nblock from the ResNet  [6,7] we proposed a new architecture to couple the strengths o f the â€˜bottleneckâ€™ residual \\nblock and the Unet for brain tumor segmentation in the BraTS 2020 challenge. The proposed network has a total \\nof 104 convolutional layers, so is named, deep residual Unet 104 (DR-Unet104). We additionally include dropout \\nand investigate if this improves architecture performance , as we mimic the ResNet in our encoder and it is sug-\\ngested by its creators additional regularization may improve performance [ 7].    \\n \\n \\n \\n DR-Unet104 for Multimodal MRI brain tumor segmentation             3 \\n \\n2 Methods \\n2.1 Architecture'),\n",
       " Document(metadata={'arxiv_id': '2011.02840v2', 'title': 'DR-Unet104 for Multimodal MRI brain tumor segmentation', 'section': 'body', 'authors': 'Jordan Colman, Lei Zhang, Wenting Duan'}, page_content='gested by its creators additional regularization may improve performance [ 7].    \\n \\n \\n \\n DR-Unet104 for Multimodal MRI brain tumor segmentation             3 \\n \\n2 Methods \\n2.1 Architecture \\n  \\nThe proposed architecture DR -Unet104 overview is shown below in Figure 1 and 2. The detailed architecture \\ndescription is shown in Figure 3 , in order to display the number of image feature channels in each layer . It com-\\nprises of three main components: encoder, decoder and bridge that forms a typical U-shape network. In this design, \\nfive stacked residual block levels with convolution layers and identify mapping are deployed in both encoder and \\ndecoder components, which are connected by the bridge component. The feature representations are encoded by \\nthe encoder path, which are recovered in the decoder path to a pixel-wise classification. Figure 2 shows the outline \\nof the residual blocks in the encoder and decoder path. In the encoder path and bridge connection, the bottleneck'),\n",
       " Document(metadata={'arxiv_id': '2011.02840v2', 'title': 'DR-Unet104 for Multimodal MRI brain tumor segmentation', 'section': 'body', 'authors': 'Jordan Colman, Lei Zhang, Wenting Duan'}, page_content='of the residual blocks in the encoder and decoder path. In the encoder path and bridge connection, the bottleneck \\ndesign is applied, which consists of a 1 Ã—1 convolution layer for reducing the depth of feature channel , a 3Ã—3 \\nconvolution layer, and then a 1Ã—1 convolution layer for restoring dimension [ 6]. In the decoder path, the typical \\nresidual block consists of two stacked 3x3 2D convolutions . The batch no rmalisation (BN) and rectified linear \\nunit (ReLU) activation are used in all residual blocks ( Figure 2), we use â€˜pre-activationâ€™ residual connections as \\nused in ResNet-V2 and described in He et al. 2016 [7]. \\n \\nGiven a deep architecture has many layers, the issues regarding overfitting and dead neurons in activation need to \\nbe considered for training the network. In our method, following the work [ 9], we employ a regularization using \\ndropout with a small rate (e.g. dropout rate of 0.2) after each level. In our method, the input of each level (after'),\n",
       " Document(metadata={'arxiv_id': '2011.02840v2', 'title': 'DR-Unet104 for Multimodal MRI brain tumor segmentation', 'section': 'body', 'authors': 'Jordan Colman, Lei Zhang, Wenting Duan'}, page_content='dropout with a small rate (e.g. dropout rate of 0.2) after each level. In our method, the input of each level (after \\nupsampling) of the decoder is added with a concatenation connection from the output of the encoder to aid feature \\nmapping. Downsampling is performed with a stride of 2 in the first convolution of each level (except for level 1). \\nUpsampling is performed with 2D transposed convolution with a kernel of 2x2 and stride of 2. The final layer is \\nconvoluted with a 1x1 kernel and generating pixel-wise classification scores to represent the 3 tumor classes, and \\nbackground class, the class of the pixel is decided by the channel with the largest output (argmax), softmax is used \\nduring training. The input is a 2D image slice with 4 channels representing each MRI modality.  The proposed \\nnetwork code is publicly available at https://github.com/jordan-colman/DR-Unet104.'),\n",
       " Document(metadata={'arxiv_id': '2011.02840v2', 'title': 'DR-Unet104 for Multimodal MRI brain tumor segmentation', 'section': 'body', 'authors': 'Jordan Colman, Lei Zhang, Wenting Duan'}, page_content='during training. The input is a 2D image slice with 4 channels representing each MRI modality.  The proposed \\nnetwork code is publicly available at https://github.com/jordan-colman/DR-Unet104. \\n \\n \\nFigure 1. Overview of the DR-Unet104 architecture showing the bottleneck residual block in the encoder and typical residual \\nblock in the decoder. The number of stacks of the bottleneck block is also shown. The downsampling is performed by a step \\nof 2 in the initial 1x1 convolution in the first bottleneck block of the level. \\n \\n \\n4             Colman et al. \\n \\n \\nFigure 2. The outline of the typical residual blocks in the decoder and the bottleneck residual block of the encoder path. The \\ntypical residual block can be seen to be formed of two 3x3 2D convolutions with batch normalization and rectified linear unit \\n(Relu) activation before each convolution. The Bottleneck residual block has a 1x1 2D convolution, which reduces the number'),\n",
       " Document(metadata={'arxiv_id': '2011.02840v2', 'title': 'DR-Unet104 for Multimodal MRI brain tumor segmentation', 'section': 'body', 'authors': 'Jordan Colman, Lei Zhang, Wenting Duan'}, page_content='(Relu) activation before each convolution. The Bottleneck residual block has a 1x1 2D convolution, which reduces the number \\nof image feature channels (F) to Â¼ of the number. This is  followed by a 3x3 2 D convolution then a 1x1 convolution  which \\nincreases the feature channel number by 4 times to the input number. Both blocks input is added to the output for the identity \\nmapping to aid backpropagation [6,7]. \\n \\n \\n \\nDR-Unet104 for Multimodal MRI brain tumor segmentation             5 \\n \\n \\nFigure 3. Our Proposed architecture for the DR-Unet104. The [] brackets denote the typical or bottleneck residual block with \\n3x3,64 representing a 2D convolution with a 3x3 kernel and 64 layers.  ð‘¥ð‘ denotes the number, N, of stacked residual blocks \\nin that layer. â†“ denotes reduction in spatial resolution performed by a stride of 2 in the first 1x1 2D convolution of the initial'),\n",
       " Document(metadata={'arxiv_id': '2011.02840v2', 'title': 'DR-Unet104 for Multimodal MRI brain tumor segmentation', 'section': 'body', 'authors': 'Jordan Colman, Lei Zhang, Wenting Duan'}, page_content='in that layer. â†“ denotes reduction in spatial resolution performed by a stride of 2 in the first 1x1 2D convolution of the initial \\nbottleneck residual block of that level. â†‘ denotes upsampling via the 2D transposed convolution with a kernel of 3x3 and stride \\nof 2. â†’ denotes a skip connection between the output of the encoder with the input of the decoder at the same level, joined \\nvia concatenation to the unsampled output of the previous level. \\n2.2 Loss function \\n   The loss function used was sparse categorical cross entropy (CE) , which is calculated with Eq (1).  This was \\nchosen for simplicity, however, better performance may have  been produced using other loss functions such as \\nâ€˜soft Dice lossâ€™ [4]. \\n ð¶ð¸ =  âˆ’ \\n1\\nð‘ âˆ‘ âˆ‘ ð‘Œð‘¡ð‘Ÿð‘¢ð‘’ ð‘\\nð‘›  Ã— log (ð‘Œð‘ð‘Ÿð‘’ð‘‘ð‘\\nð‘›)ð¶\\nð‘\\nð‘\\nð‘›  (1) \\n \\nWhere ð‘ is number of examples and ð¶ represents the classes, ð‘Œð‘¡ð‘Ÿð‘¢ð‘’  is the truth label and ð‘Œð‘ð‘Ÿð‘’ð‘‘  the softmax of the \\nprediction [10].  \\nDR-Unet104 \\n6             Colman et al. \\n \\n3 Experiment'),\n",
       " Document(metadata={'arxiv_id': '2011.02840v2', 'title': 'DR-Unet104 for Multimodal MRI brain tumor segmentation', 'section': 'body', 'authors': 'Jordan Colman, Lei Zhang, Wenting Duan'}, page_content='Where ð‘ is number of examples and ð¶ represents the classes, ð‘Œð‘¡ð‘Ÿð‘¢ð‘’  is the truth label and ð‘Œð‘ð‘Ÿð‘’ð‘‘  the softmax of the \\nprediction [10].  \\nDR-Unet104 \\n6             Colman et al. \\n \\n3 Experiment \\n3.1 Dataset, Pre-processing and data augmentation \\nThe data used for training and evaluation of our model consisted of the BraTS 2020  dataset including training \\ndata with 369 subjects, validation data with 125 subjects and testing data with 166 subjects, all contained low and \\nhigh grade gliomas [11-15]. Each subject has a T2 weighted FLAIR, T1 weighted, T1 weighted post contrast, and \\na T2 weighted MRI. Each image is interp olated to a 1x1x1 mm voxel sized giving images sized 240x240x155 \\nvoxels. All subject images are aligned into a common space and skull stripped prior to data sharing. The training \\ndata additionally contains manually drawn tumor segmentation mask, annotated by experts and checked by a'),\n",
       " Document(metadata={'arxiv_id': '2011.02840v2', 'title': 'DR-Unet104 for Multimodal MRI brain tumor segmentation', 'section': 'body', 'authors': 'Jordan Colman, Lei Zhang, Wenting Duan'}, page_content='data additionally contains manually drawn tumor segmentation mask, annotated by experts and checked by a \\nneuroradiologist, the segmentation is labelled with numbers denoting  edema, tumor core and enhancing tumor. \\nFigure 4 shows an example of a subjects imaging modalities and tumor mask. \\n \\n \\n \\nFigure 4. Shows the different MRI modalities provided on one subject with a glioma, a ) shows a T2 weighted MRI, b ) a T2 \\nweighted FLAIR MRI, d) a T1 weighted MRI, e) a T1 weighted MRI post contrast. c) and f) show the manual tumor segmen-\\ntation in an axial and corona l slice respectively on a T2 weighted FLAIR MRI . Green is edema, red tumor core and yellow \\nenhancing tumor. \\n \\nWe pre-processed the images using data standardisation on a whole 3D MRI modality wise basis, normalised \\nusing Eq (2) where ð‘£ is a given voxel and the ð‘šð‘’ð‘Žð‘› and standard deviation (ð‘†ð·) are of the image of voxels > 0. \\nThis is so the image was rescaled prior to conversion to 2D image.  \\n \\n ð‘£ =  {'),\n",
       " Document(metadata={'arxiv_id': '2011.02840v2', 'title': 'DR-Unet104 for Multimodal MRI brain tumor segmentation', 'section': 'body', 'authors': 'Jordan Colman, Lei Zhang, Wenting Duan'}, page_content='using Eq (2) where ð‘£ is a given voxel and the ð‘šð‘’ð‘Žð‘› and standard deviation (ð‘†ð·) are of the image of voxels > 0. \\nThis is so the image was rescaled prior to conversion to 2D image.  \\n \\n ð‘£ =  {\\n            254                           ð‘£ > ð‘šð‘’ð‘Žð‘› + 3 âˆ— ð‘†ð· \\n(ð‘£âˆ’ð‘šð‘’ð‘Žð‘›)+127\\n3âˆ—ð‘†ð· 128â„                   ð‘œð‘¡â„Žð‘’ð‘Ÿð‘¤ð‘–ð‘ ð‘’           \\n              0                             ð‘£ < ð‘šð‘’ð‘Žð‘› âˆ’ 3 âˆ— ð‘†ð·\\n} (2) \\n \\nDR-Unet104 for Multimodal MRI brain tumor segmentation             7 \\n \\nEach slice of the image is then saved as individual png image, with each channel representing an MRI modality. \\nThe only image augmentation applied was randomly flipping the images in the left -right and anterior-posterior \\norientation with a 50% probability. \\n3.2 Setup and training  \\nFor training, we used ADAM optimiser [16] with a learning rate of 1e -4 and He initialization [17]. We used a \\nbatch sizes of 10 and ran the training for 50 epochs. All subjectsâ€™ images were converted to single RGBA images'),\n",
       " Document(metadata={'arxiv_id': '2011.02840v2', 'title': 'DR-Unet104 for Multimodal MRI brain tumor segmentation', 'section': 'body', 'authors': 'Jordan Colman, Lei Zhang, Wenting Duan'}, page_content='batch sizes of 10 and ran the training for 50 epochs. All subjectsâ€™ images were converted to single RGBA images \\nfor each slice as described in the methods and inputted to the model in a random order. We evaluated our method \\nwith varying dropout rates and compared our proposed architecture to DeeplabV3+ as the architecture is openly \\navailable online and a current state -of-the-art segmentation architecture. We use a ResNet-V2-152 backbone for \\nthe BraTS evaluation and not the originally suggested ResNet-V1-101 in order to improve the performance and \\nmake the architecture more analogous to our proposed method [6]. The architectures are implemented in Keras \\nwith a tensorflow backend [18-19]. This was performed using a NVIDIA GTX 2080Ti graphics card with 12 GB \\nof memory.  \\n4 Results \\nWe initially evaluated our proposed DR-Unet104 architecture on the 125 validation subjects, pre-processed in the'),\n",
       " Document(metadata={'arxiv_id': '2011.02840v2', 'title': 'DR-Unet104 for Multimodal MRI brain tumor segmentation', 'section': 'body', 'authors': 'Jordan Colman, Lei Zhang, Wenting Duan'}, page_content='of memory.  \\n4 Results \\nWe initially evaluated our proposed DR-Unet104 architecture on the 125 validation subjects, pre-processed in the \\nsame way as the training data, however, without data augmentation . The resulting 2D masks  are reconstructed \\ninto 3D nii images and evaluated on the IPP website [https://ipp.cbica.upenn.edu/], which computed and evaluated \\nthe lesion masks outputting  the metrics, DSC, sensitivity, specificity and Hausdorff distance 95% (HD95). We \\nevaluated our proposed architecture with varying random dropout rate after each level, the rate we trailed being \\n0.2, 0.5 and no dropout.  We can observe from the Table 1, the architecture applying the dropout of rate 0.2 has \\nsuperior performance than the architecture with a rate of 0.5 or without dropout, with all tumor componentsâ€™ DSC \\nand HD95 being greater . These observations verify that dropout in our architecture allows the network to learn'),\n",
       " Document(metadata={'arxiv_id': '2011.02840v2', 'title': 'DR-Unet104 for Multimodal MRI brain tumor segmentation', 'section': 'body', 'authors': 'Jordan Colman, Lei Zhang, Wenting Duan'}, page_content='and HD95 being greater . These observations verify that dropout in our architecture allows the network to learn \\ninformative features from the data, but only with the small rate (e.g. 0.2), as the network with a large (0.5) rate \\nperforms poor and even worse than the setting witho ut dropout. Comparing ou r proposed architecture  with a \\ndroupout of 0.2 to DeepLabV3+ shows improvement of segmentation results in all areas except the mean tumor \\ncore DSC. \\n \\n \\nTable 1. Table showing \\nthe mean results of the \\nDice Score coefficient \\n(DSC) and Hausdorff dis-\\ntance 95% (HD95) with \\nthe Standard deviation \\n(SD) in brackets results on \\nthe BraTS20 validation \\ndata. We show results for  \\nthe whole tumor (WT), \\nenhancing tumor (ET) and \\ntumor core (TC). We compare the results of our own architecture, the deep residual Unet 104 (DR-Unet104) with no dropout, \\na dropout of 0.2 and of 0.5 after each level. We additionally compare our model to DeepLabV3+ with a ResNet101 backbone.'),\n",
       " Document(metadata={'arxiv_id': '2011.02840v2', 'title': 'DR-Unet104 for Multimodal MRI brain tumor segmentation', 'section': 'body', 'authors': 'Jordan Colman, Lei Zhang, Wenting Duan'}, page_content='a dropout of 0.2 and of 0.5 after each level. We additionally compare our model to DeepLabV3+ with a ResNet101 backbone. \\nWe show the best results in bold. \\nWe finally evaluated our model on the 166 BraTS testing subjects as part of the 2020 challenge with the final \\nresults shown in Table 2. Our mean whole tumor DSC is lower than the validation results at 0.8673, however the \\nenhancing tumor a nd tumor core DSC are much higher than validation  results achieving 0.7514 and 0.7983 re-\\nspectively. The proposed methods overall better performance on the testing data  shows it generalizability aided \\nby random dropout. Our modelâ€™s performance is additionally competitive with 3D models. \\n \\n \\n Validation Results \\n DSC HD95 \\nArchitecture WT (SD) ET (SD) TC (SD) WT (SD) ET (SD) TC (SD) \\nDR-UNET104 \\ndropout (0) \\n0.8763 \\n(0.0859) \\n0.6549 \\n(0.3256) \\n0.6693 \\n(0.3357) \\n18.39 \\n(24.87) \\n53.61 \\n(115.9) \\n16.19 \\n(20.88) \\nDR-UNET104 \\ndropout (0.2) \\n0.8862 \\n(0.0886) \\n0.6756 \\n(0.3171) \\n0.6721 \\n(0.3462)'),\n",
       " Document(metadata={'arxiv_id': '2011.02840v2', 'title': 'DR-Unet104 for Multimodal MRI brain tumor segmentation', 'section': 'body', 'authors': 'Jordan Colman, Lei Zhang, Wenting Duan'}, page_content='dropout (0) \\n0.8763 \\n(0.0859) \\n0.6549 \\n(0.3256) \\n0.6693 \\n(0.3357) \\n18.39 \\n(24.87) \\n53.61 \\n(115.9) \\n16.19 \\n(20.88) \\nDR-UNET104 \\ndropout (0.2) \\n0.8862 \\n(0.0886) \\n0.6756 \\n(0.3171) \\n0.6721 \\n(0.3462) \\n12.11 \\n(20.82) \\n47.62 \\n(112.7) \\n15.74 \\n(36.04) \\nDR-UNET104 \\ndropout (0.5) \\n0.8723 \\n(0.0977) \\n0.6701 \\n(0.3245) \\n0.6489 \\n(0.3651) \\n23.80 \\n(27.58) \\n51.53 \\n(108.5) \\n28.56 \\n(51.69) \\nDeepLabV3+ 0.8771 \\n(0.0853) \\n0.6524 \\n(0.3101) \\n0.6813 \\n(0.3213) \\n14.87 \\n(23.20) \\n49.10 \\n(112.6) \\n17.96 \\n(37.54) 8             Colman et al. \\n \\nTable 2. Table show-\\ning the mean results of \\nthe Dice Score coeffi-\\ncient (DSC) and \\nHausdorff distance \\n95% (HD95) with the \\nStandard deviation (SD) in brackets results on the BraTS20 Testing data. We show results for the whole tumor (WT), enhanc-\\ning tumor (ET) and tumor core (TC) evaluated using the deep residual Unet 104 (DR-Unet104) with dropout of 0.2. \\n5 Discussion'),\n",
       " Document(metadata={'arxiv_id': '2011.02840v2', 'title': 'DR-Unet104 for Multimodal MRI brain tumor segmentation', 'section': 'body', 'authors': 'Jordan Colman, Lei Zhang, Wenting Duan'}, page_content='ing tumor (ET) and tumor core (TC) evaluated using the deep residual Unet 104 (DR-Unet104) with dropout of 0.2. \\n5 Discussion \\nIn this paper, we present a 2D deep residual Unet with 104 convolution layers for automated brain tumor segmen-\\ntation in multimodal MRI. The proposed network couples the strengths of deep residual blocks and the Unet with \\nencoder-decoder structure. The regulari zation of dropout is i ncluded into the network, allowing it to learn more \\nrepresentative features than the plain architectures without regulari zation, producing improved validation results \\nas shown in table 1.  \\n \\nThe results show that our method achieves promising performance when comparing to a state-of-the-art method \\n(i.e. DeeplabV3+), and performs reasonably well despite being a 2D architecture, having minimal training data \\naugmentation and being trained for only 50 epochs without any complex learning rate scheduling. The 2D archi-'),\n",
       " Document(metadata={'arxiv_id': '2011.02840v2', 'title': 'DR-Unet104 for Multimodal MRI brain tumor segmentation', 'section': 'body', 'authors': 'Jordan Colman, Lei Zhang, Wenting Duan'}, page_content='augmentation and being trained for only 50 epochs without any complex learning rate scheduling. The 2D archi-\\ntecture has the added benefit of meaning the model can be evaluated on lower powered computers/GPUs, only \\nneeding a GPU with 1-2 GBs of memory (when evaluated with a batch size of 1), unlike many other 3D architec-\\ntures [4]. A limitation is that  for simplicity we used the commonly used cross-entropy loss function, but would \\nlikely have received a better performance using a â€˜soft Dice lossâ€™ function, as used by the BraTS 2019 1st place \\nmethod [4], due to DSC being used for the evaluation, and this could be included in future work. \\n \\nUnusually, on the testing data set our architecture performed worse on whole tumor segmentation, but much better \\non enhancing tumor and tumor core  labelling, compared to the validation data set. This is likely due a greater \\nnumber of difficult to segment enhancing areas in the validation data set, which would also affect the tumor core'),\n",
       " Document(metadata={'arxiv_id': '2011.02840v2', 'title': 'DR-Unet104 for Multimodal MRI brain tumor segmentation', 'section': 'body', 'authors': 'Jordan Colman, Lei Zhang, Wenting Duan'}, page_content='number of difficult to segment enhancing areas in the validation data set, which would also affect the tumor core \\nDSC if mislabelled as one another. A greater number of difficult to label enhancing tumor areas in the validation \\ndata set is supported by a larger standard deviation of the mean DSC, 0.25 in testing vs. 0.32 on validation data.  \\n \\n6 Conclusion \\nWe propose a variant of the Unet taking advantage of bottleneck residual block  to produce a deeply stacked \\nencoder. We additionally show the benefit of using dropout in our architecture. Our method has a competitive \\nperformance despite being a 2D architecture and having simple and limited training.  \\nReferences \\n1. Lambin, P., Leijenaar, R., Deist, T., et al. Radiomics: the bridge between medical imaging and personalized medicine . \\nNat Rev Clin Oncol 14, 749â€“762 (2017). DOI: 10.1038/nrclinonc.2017.141 \\n2. Ricard, D., Idbaih, A., Ducray, F., et al. Primary brain tumours in adults. The Lancet 379:9830, 1984-1996 (2012). DOI:'),\n",
       " Document(metadata={'arxiv_id': '2011.02840v2', 'title': 'DR-Unet104 for Multimodal MRI brain tumor segmentation', 'section': 'body', 'authors': 'Jordan Colman, Lei Zhang, Wenting Duan'}, page_content='Nat Rev Clin Oncol 14, 749â€“762 (2017). DOI: 10.1038/nrclinonc.2017.141 \\n2. Ricard, D., Idbaih, A., Ducray, F., et al. Primary brain tumours in adults. The Lancet 379:9830, 1984-1996 (2012). DOI: \\n10.1016/S0140-6736(11)61346-9 \\n3. Ronneberger, O., Fischer, P., Brox, T. U-Net: Convolutional Networks for Biomedical Image Segmentation . In: Navab, \\nN., Hornegger, J., Wells, W., Frangi, A. (eds) Medical Image Computing and Computer-Assisted Intervention â€“ MICCAI \\n2015. MICCAI 2015. Lecture Notes in Computer Science, vol 9351. Springer, Cham. DOI: 10.1007/978-3-319-24574-\\n4_28 \\n4. Jiang Z., Ding C., Liu M., Tao D. Two-Stage Cascaded U-Net: 1st Place Solution to BraTS Challenge 2019 Segmentation \\nTask. In: Crimi A., Bakas S. (eds) Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries. BrainLes \\n2019. Lecture Notes in Computer Science, vol 11992. Springer, Cham. DOI: 10.1007/978-3-030-46640-4_22'),\n",
       " Document(metadata={'arxiv_id': '2011.02840v2', 'title': 'DR-Unet104 for Multimodal MRI brain tumor segmentation', 'section': 'body', 'authors': 'Jordan Colman, Lei Zhang, Wenting Duan'}, page_content='2019. Lecture Notes in Computer Science, vol 11992. Springer, Cham. DOI: 10.1007/978-3-030-46640-4_22 \\n5. Kamnitsas K. et al. (2018) Ensembles of Multiple Models and Architectures for Robust Brain Tumour Segmentation. In: \\nCrimi A., Bakas S., Kuijf H., Menze B., Reyes M. (eds) Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic \\n Testing Data \\n DSC HD95 \\nArchitecture WT (SD) ET (SD) TC (SD) WT (SD) ET (SD) TC (SD) \\nDR-UNET104 \\ndropout (0.2) \\n0.8673 \\n(0.1279) \\n0.7514 \\n(0.2478) \\n0.7983 \\n(0.2757) \\n10.41 \\n(16.59) \\n24.68 \\n(83.99) \\n21.84 \\n(74.35) DR-Unet104 for Multimodal MRI brain tumor segmentation             9 \\n \\nBrain Injuries. BrainLes 2017. Lectur e Notes in Computer Science, vol 10670. Springer, Cham.  DOI: 10.1007/978-3-\\n319-75238-9_38 \\n6. He K., Zhang X., Ren S., Sun J. (2016) Identity Mappings in Deep Residual Networks. In: Leibe B., Matas J., Sebe N., \\nWelling M. (eds) Computer Vision â€“ ECCV 2016. ECCV 2016. Lecture Notes in Computer Science, vol 9908. Springer,'),\n",
       " Document(metadata={'arxiv_id': '2011.02840v2', 'title': 'DR-Unet104 for Multimodal MRI brain tumor segmentation', 'section': 'body', 'authors': 'Jordan Colman, Lei Zhang, Wenting Duan'}, page_content='Welling M. (eds) Computer Vision â€“ ECCV 2016. ECCV 2016. Lecture Notes in Computer Science, vol 9908. Springer, \\nCham. DOI: 10.1007/978-3-319-46493-0_38 \\n7. He. K., Zhang. X., Ren. S., Sun. J., Deep Residual Learning for Image Recognition, Proceedings of the IEEE Conference \\non Computer Vision and Pattern Recognition (CVPR), 770-778 (2016). DOI: 10.1109/CVPR.2016.90 \\n8. Chen, L-C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H. , Encoder-Decoder with Atrous Separable Convolution for \\nSemantic Image Segmentation. Proceedings of the European Conference on Computer Vision (ECCV), 801-818 (2018). \\nDOI: 10.1007/978-3-030-01234-2_49 \\n9. Park S., Kwak N. Analysis on the Dropout Effect in Convolutional Neural Networks. In: Lai SH., Lepetit V., Nishino K., \\nSato Y. (eds) Computer Vision â€“ ACCV 2016. ACCV 2016. Lecture Notes in Compu ter Science, vol 10112. Springer, \\nCham. DOI: 10.1007/978-3-319-54184-6_12'),\n",
       " Document(metadata={'arxiv_id': '2011.02840v2', 'title': 'DR-Unet104 for Multimodal MRI brain tumor segmentation', 'section': 'body', 'authors': 'Jordan Colman, Lei Zhang, Wenting Duan'}, page_content='Sato Y. (eds) Computer Vision â€“ ACCV 2016. ACCV 2016. Lecture Notes in Compu ter Science, vol 10112. Springer, \\nCham. DOI: 10.1007/978-3-319-54184-6_12 \\n10. Ho, Y. and Wookey, S. The Real-World-Weight Cross-Entropy Loss Function: Modeling the Costs of Mislabeling . in \\nIEEE Access, vol. 8, pp. 4806-4813, (2020). DOI: 10.1109/ACCESS.2019.2962617 \\n11. Menze, B.H., Jakab, A., Bauer, S., Kalpathy-Cramer, J., Farahani, K., Kirby, J., et al. The Multimodal Brain Tumor Image \\nSegmentation Benchmark (BRATS). IEEE Transactions on Medical Imaging 34(10), 1993 -2024 (2015) . DOI: \\n10.1109/TMI.2014.2377694 \\n12. Bakas, S., Akbari, H., Sotiras, A., Bilello, M., Rozycki, M., Kirby, J.S., et al. Advancing The Can cer Genome Atlas \\nglioma MRI collections with expert segmentation labels and radiomic features. Nature Scientific Data, 4:170117 (2017). \\nDOI: 10.1038/sdata.2017.117 \\n13. Bakas, S., Reyes, M., Jakab, A., Bauer, S., Rempfler, M., Crimi, A., et al. Identifying the Best Machine Learning Algo-'),\n",
       " Document(metadata={'arxiv_id': '2011.02840v2', 'title': 'DR-Unet104 for Multimodal MRI brain tumor segmentation', 'section': 'body', 'authors': 'Jordan Colman, Lei Zhang, Wenting Duan'}, page_content='DOI: 10.1038/sdata.2017.117 \\n13. Bakas, S., Reyes, M., Jakab, A., Bauer, S., Rempfler, M., Crimi, A., et al. Identifying the Best Machine Learning Algo-\\nrithms for Brain Tumor Segmentation, Progression Assessment, and Overall Survival Prediction in the BRATS Challenge. \\narXiv preprint arXiv:1811.02629 (2018). \\n14. Bakas, S., Akbari, H., Sotiras, A., Bilello, M., Rozycki, M., Kirby, J., et al. Segmentation Labels and Radiomic Features \\nfor the Pre -operative Scans of the TCGA -GBM collection. The Cancer Imaging Archive (2017). DOI: \\n10.7937/K9/TCIA.2017.KLXWJJ1Q \\n15. Bakas, S., Akbari, H., Sotiras, A., Bilello, M., Rozycki, M., Kirby, J., et al. Segmentation Labels and Radiomic Features \\nfor the Pre -operative Scans of the TCGA -LGG collection.  The Cancer Imaging Archive, (2017). DOI: \\n10.7937/K9/TCIA.2017.GJQ7R0EF \\n16. Kingma, D., Ba, J. Adam: A method for stochastic optimization. arXiv 1412.6980 (2014).'),\n",
       " Document(metadata={'arxiv_id': '2011.02840v2', 'title': 'DR-Unet104 for Multimodal MRI brain tumor segmentation', 'section': 'body', 'authors': 'Jordan Colman, Lei Zhang, Wenting Duan'}, page_content='10.7937/K9/TCIA.2017.GJQ7R0EF \\n16. Kingma, D., Ba, J. Adam: A method for stochastic optimization. arXiv 1412.6980 (2014). \\n17. He, K., Zhang, X., Ren, S. and Sun, J. Delving deep into rectifiers: Surpassing human -level performance on ImageNet \\nclassification.  In International Conference on Computer Vision, (2015). DOI: 10.1109/ICCV.2015.123 \\n18. Chollet, F., et al. (2015). Keras. GitHub. Retrieved from https://github.com/fchollet/keras \\n19. Abadi et al. TensorFlow: Large -scale machine learning on heterogeneous systems . (2015). Software available from \\nhttps://tensorflow.org'),\n",
       " Document(metadata={'arxiv_id': '2001.02040v1', 'title': 'Robust Semantic Segmentation of Brain Tumor Regions from 3D MRIs', 'section': 'title_abstract', 'authors': 'Andriy Myronenko, Ali Hatamizadeh'}, page_content='Title: Robust Semantic Segmentation of Brain Tumor Regions from 3D MRIs\\n\\nAbstract: Multimodal brain tumor segmentation challenge (BraTS) brings together researchers to improve automated methods for 3D MRI brain tumor segmentation. Tumor segmentation is one of the fundamental vision tasks necessary for diagnosis and treatment planning of the disease. Previous years winning methods were all deep-learning based, thanks to the advent of modern GPUs, which allow fast optimization of deep convolutional neural network architectures. In this work, we explore best practices of 3D semantic segmentation, including conventional encoder-decoder architecture, as well combined loss functions, in attempt to further improve the segmentation accuracy. We evaluate the method on BraTS 2019 challenge.'),\n",
       " Document(metadata={'arxiv_id': '2001.02040v1', 'title': 'Robust Semantic Segmentation of Brain Tumor Regions from 3D MRIs', 'section': 'body', 'authors': 'Andriy Myronenko, Ali Hatamizadeh'}, page_content='Robust Semantic Segmentation of Brain Tumor\\nRegions from 3D MRIs\\nAndriy Myronenko, Ali Hatamizadeh\\nNVIDIA, Santa Clara, CA\\namyronenko@nvidia.com, ahatamizadeh@nvidia.com\\nAbstract. Multimodal brain tumor segmentation challenge (BraTS)\\nbrings together researchers to improve automated methods for 3D MRI\\nbrain tumor segmentation. Tumor segmentation is one of the fundamen-\\ntal vision tasks necessary for diagnosis and treatment planning of the\\ndisease. Previous years winning methods were all deep-learning based,\\nthanks to the advent of modern GPUs, which allow fast optimization\\nof deep convolutional neural network architectures. In this work, we ex-\\nplore best practices of 3D semantic segmentation, including conventional\\nencoder-decoder architecture, as well combined loss functions, in attempt\\nto further improve the segmentation accuracy. We evaluate the method\\non BraTS 2019 challenge.\\n1 Introduction\\nBrain tumors are categorized into primary and secondary tumor types. Primary'),\n",
       " Document(metadata={'arxiv_id': '2001.02040v1', 'title': 'Robust Semantic Segmentation of Brain Tumor Regions from 3D MRIs', 'section': 'body', 'authors': 'Andriy Myronenko, Ali Hatamizadeh'}, page_content='to further improve the segmentation accuracy. We evaluate the method\\non BraTS 2019 challenge.\\n1 Introduction\\nBrain tumors are categorized into primary and secondary tumor types. Primary\\nbrain tumors originate from brain cells, whereas secondary tumors metastasize\\ninto the brain from other organs. The most common type of primary brain tu-\\nmors are gliomas, which arise from brain glial cells. Gliomas can be of low-grade\\n(LGG) and high-grade (HGG) subtypes. High grade gliomas are an aggressive\\ntype of malignant brain tumor that grow rapidly, usually require surgery and\\nradiotherapy and have poor survival prognosis. Magnetic Resonance Imaging\\n(MRI) is a key diagnostic tool for brain tumor analysis, monitoring and surgery\\nplanning. Usually, several complimentary 3D MRI modalities are acquired - such\\nas T1, T1 with contrast agent (T1c), T2 and Fluid Attenuation Inversion Re-\\ncover (FLAIR) - to emphasize diï¬€erent tissue properties and areas of tumor'),\n",
       " Document(metadata={'arxiv_id': '2001.02040v1', 'title': 'Robust Semantic Segmentation of Brain Tumor Regions from 3D MRIs', 'section': 'body', 'authors': 'Andriy Myronenko, Ali Hatamizadeh'}, page_content='as T1, T1 with contrast agent (T1c), T2 and Fluid Attenuation Inversion Re-\\ncover (FLAIR) - to emphasize diï¬€erent tissue properties and areas of tumor\\nspread. For example the contrast agent, usually gadolinium, emphasizes hyper-\\nactive tumor subregions in T1c MRI modality.\\nAutomated segmentation of 3D brain tumors can save physicians time and\\nprovide an accurate reproducible solution for further tumor analysis and moni-\\ntoring. Recently, deep learning based segmentation techniques surpassed tradi-\\ntional computer vision methods for dense semantic segmentation. Convolutional\\nneural networks (CNN) are able to learn from examples and demonstrate state-\\nof-the-art segmentation accuracy both in 2D natural images [5,7] and in 3D\\nmedical image modalities [15].\\nMultimodal Brain Tumor Segmentation Challenge (BraTS) aims to evaluate\\nstate-of-the-art methods for the segmentation of brain tumors by providing a 3D\\narXiv:2001.02040v1  [eess.IV]  6 Jan 20202 A. Myronenko'),\n",
       " Document(metadata={'arxiv_id': '2001.02040v1', 'title': 'Robust Semantic Segmentation of Brain Tumor Regions from 3D MRIs', 'section': 'body', 'authors': 'Andriy Myronenko, Ali Hatamizadeh'}, page_content='state-of-the-art methods for the segmentation of brain tumors by providing a 3D\\narXiv:2001.02040v1  [eess.IV]  6 Jan 20202 A. Myronenko\\nMRI dataset with ground truth tumor segmentation labels annotated by physi-\\ncians [4,14,3,1,2]. This year, BraTS 2019 training dataset included 335 cases,\\neach with four 3D MRI modalities (T1, T1c, T2 and FLAIR) rigidly aligned,\\nresampled to 1x1x1 mm isotropic resolution and skull-stripped. The input image\\nsize is 240x240x155. The data were collected from multiple institutions, using\\nvarious MRI scanners. Annotations include 3 tumor subregions: the enhancing\\ntumor, the peritumoral edema, and the necrotic and non-enhancing tumor core.\\nThe annotations were combined into 3 nested subregions: whole tumor (WT),\\ntumor core (TC) and enhancing tumor (ET), as shown in Figure 1. Two addi-\\ntional datasets without the ground truth labels were provided for validation and\\ntesting. These datasets required participants to upload the segmentation masks'),\n",
       " Document(metadata={'arxiv_id': '2001.02040v1', 'title': 'Robust Semantic Segmentation of Brain Tumor Regions from 3D MRIs', 'section': 'body', 'authors': 'Andriy Myronenko, Ali Hatamizadeh'}, page_content='tional datasets without the ground truth labels were provided for validation and\\ntesting. These datasets required participants to upload the segmentation masks\\nto the organizersâ€™ server for evaluations. The validation dataset (125 cases) al-\\nlowed multiple submissions and was designed for intermediate evaluations. The\\ntesting dataset allowed only a single submission, and is used to calculate the\\nï¬nal challenge ranking.\\nIn this work, we describe our semantic segmentation approach for volumetric\\n3D brain tumor segmentation from multimodal 3D MRIs and participate in\\nBraTS 2019 challenge.\\n2 Related work\\nPrevious year, BraTS 2018 top submissions included Myronenko [16], Isensee\\net al. [11], McKinly et al. [13] and Zhou et al. [19]. In our previous work [16],\\nwe explored how an additional decoder for a secondary task get impose addi-\\ntional structure on the network. Isensee et al. [11] demonstrated that a generic\\nU-net architecture with a few minor modiï¬cations is enough to achieve compet-'),\n",
       " Document(metadata={'arxiv_id': '2001.02040v1', 'title': 'Robust Semantic Segmentation of Brain Tumor Regions from 3D MRIs', 'section': 'body', 'authors': 'Andriy Myronenko, Ali Hatamizadeh'}, page_content='tional structure on the network. Isensee et al. [11] demonstrated that a generic\\nU-net architecture with a few minor modiï¬cations is enough to achieve compet-\\nitive performance. McKinly et al. [13] proposed a segmentation CNN in which\\na DenseNet [9] structure with dilated convolutions was embedded in U-net-like\\nnetwork. Finally, Zhou et al. [19] proposed to use an ensemble of diï¬€erent net-\\nworks: taking into account multi-scale context information, segmenting 3 tumor\\nsubregions in cascade with a shared backbone weights and adding an attention\\nblock.\\nHere, we generally follow the previous year submission [16], but instead of\\nsecondary task decoder we explore various architecture design choices and com-\\nplimentary loss functions. We also utilize multi-gpu systems for data parallelism\\nto be able to use larger batch sizes.\\n3 Methods\\nOur segmentation approach generally follows [16] with encoder-decoder based\\nCNN architecture.\\n3.1 Encoder part'),\n",
       " Document(metadata={'arxiv_id': '2001.02040v1', 'title': 'Robust Semantic Segmentation of Brain Tumor Regions from 3D MRIs', 'section': 'body', 'authors': 'Andriy Myronenko, Ali Hatamizadeh'}, page_content='to be able to use larger batch sizes.\\n3 Methods\\nOur segmentation approach generally follows [16] with encoder-decoder based\\nCNN architecture.\\n3.1 Encoder part\\nThe encoder part uses ResNet [8] blocks, where each block consists of two convo-\\nlutions with normalization and ReLU, followed by additive identity skip connec-Robust Semantic Segmentation of Brain Tumor Regions from 3D MRIs 3\\ntion. For normalization, we experimented with Group Normalization (GN) [18],\\nInstance Normalization [17] and Batch Normalization [10]. We follow a common\\nCNN approach to progressively downsize image dimensions by 2 and simultane-\\nously increase feature size by 2. For downsizing we use strided convolutions. All\\nconvolutions are 3x3x3 with initial number of ï¬lters equal to 32. The encoder\\npart structure is shown in Table 1. The encoder endpoint has size 256x20x24x16,\\nand is 8 times spatially smaller than the input image. We decided against further\\ndownsizing to preserve more spatial content.'),\n",
       " Document(metadata={'arxiv_id': '2001.02040v1', 'title': 'Robust Semantic Segmentation of Brain Tumor Regions from 3D MRIs', 'section': 'body', 'authors': 'Andriy Myronenko, Ali Hatamizadeh'}, page_content='and is 8 times spatially smaller than the input image. We decided against further\\ndownsizing to preserve more spatial content.\\nTable 1.Encoder structure, where GN stands for group normalization (with group size\\nof 8), Conv - 3x3x3 convolution, AddId - addition of identity/skip connection. Repeat\\ncolumn shows the number of repetitions of the block. We refer to the ï¬nal output of\\nthe encoder, as the encoder endpoint\\nName Ops Repeat Output size\\nInput 4x160x192x128\\nInitConv Conv 1 32x160x192x128\\nEncoderBlock0 GN,ReLU,Conv,GN,ReLU,Conv, AddId 1 32x160x192x128\\nEncoderDown1 Conv stride 2 1 64x80x96x64\\nEncoderBlock1 GN,ReLU,Conv,GN,ReLU,Conv, AddId 2 64x80x96x64\\nEncoderDown2 Conv stride 2 1 128x40x48x32\\nEncoderBlock2 GN,ReLU,Conv,GN,ReLU,Conv, AddId 2 128x40x48x32\\nEncoderDown3 Conv stride 2 1 256x20x24x16\\nEncoderBlock3 GN,ReLU,Conv,GN,ReLU,Conv, AddId 4 256x20x24x16\\n3.2 Decoder part\\nThe decoder structure is similar to the encoder one, but with a single block per'),\n",
       " Document(metadata={'arxiv_id': '2001.02040v1', 'title': 'Robust Semantic Segmentation of Brain Tumor Regions from 3D MRIs', 'section': 'body', 'authors': 'Andriy Myronenko, Ali Hatamizadeh'}, page_content='EncoderBlock3 GN,ReLU,Conv,GN,ReLU,Conv, AddId 4 256x20x24x16\\n3.2 Decoder part\\nThe decoder structure is similar to the encoder one, but with a single block per\\neach spatial level. Each decoder level begins with upsizing: reducing the number\\nof features by a factor of 2 (using 1x1x1 convolutions) and doubling the spatial\\ndimension (using 3D bilinear upsampling), followed by an addition of encoder\\noutput of the equivalent spatial level. The end of the decoder has the same\\nspatial size as the original image, and the number of features equal to the initial\\ninput feature size, followed by 1x1x1 convolution into 3 channels and a sigmoid\\nfunction. The decoder structure is shown in Table 2.\\n3.3 Loss\\nWe use a hybrid loss function that consists of the following terms:\\nL = Ldice + Lfocal + Lacl (1)4 A. Myronenko\\nTable 2. Decoder structure, where GN stands for group normalization (with group\\nsize of 8), Conv - 3x3x3 convolution, Conv1 - 1x1x1 convolution, AddId - addition of'),\n",
       " Document(metadata={'arxiv_id': '2001.02040v1', 'title': 'Robust Semantic Segmentation of Brain Tumor Regions from 3D MRIs', 'section': 'body', 'authors': 'Andriy Myronenko, Ali Hatamizadeh'}, page_content='Table 2. Decoder structure, where GN stands for group normalization (with group\\nsize of 8), Conv - 3x3x3 convolution, Conv1 - 1x1x1 convolution, AddId - addition of\\nidentity/skip connection, UpLinear - 3D linear spatial upsampling\\nName Ops Repeat Output size\\nDecoderUp2 Conv1, UpLinear, +EncoderBlock2 1 128x40x48x32\\nDecoderBlock2 GN,ReLU,Conv,GN,ReLU,Conv, AddId 1 128x40x48x32\\nDecoderUp1 Conv1, UpLinear, +EncoderBlock1 1 64x80x96x64\\nDecoderBlock1 GN,ReLU,Conv,GN,ReLU,Conv, AddId 1 64x80x96x64\\nDecoderUp0 Conv1, UpLinear, +EncoderBlock0 1 32x160x192x128\\nDecoderBlock0 GN,ReLU,Conv,GN,ReLU,Conv, AddId 1 32x160x192x128\\nDecoderEnd Conv1, Sigmoid 1 1x160x192x144\\nLdice is a soft dice loss [15] applied to the decoder output ppred to match the\\nsegmentation mask ptrue:\\nLdice = 1 âˆ’ 2 âˆ—âˆ‘ptrue âˆ—ppredâˆ‘p2\\ntrue + âˆ‘p2\\npred + Ïµ (2)\\nwhere summation is voxel-wise, and Ïµis a small constant to avoid zero division.\\nSince the output of the segmentation decoder has 3 channels (predictions for'),\n",
       " Document(metadata={'arxiv_id': '2001.02040v1', 'title': 'Robust Semantic Segmentation of Brain Tumor Regions from 3D MRIs', 'section': 'body', 'authors': 'Andriy Myronenko, Ali Hatamizadeh'}, page_content='true + âˆ‘p2\\npred + Ïµ (2)\\nwhere summation is voxel-wise, and Ïµis a small constant to avoid zero division.\\nSince the output of the segmentation decoder has 3 channels (predictions for\\neach tumor subregion), we simply add the three dice loss functions together.\\nLacl is the 3D extension of supervised active contour loss [6] that consists of\\nvolumetric and length terms:\\nLacl = Lvol + Llength (3)\\nin which :\\nLvol =|\\nâˆ‘\\nppred(c1 âˆ’ptrue)2 |+ |\\nâˆ‘\\n(1 âˆ’ppred)(c2 âˆ’ptrue)2 | (4)\\nLlength =\\nâˆ‘âˆš\\n|(âˆ‡ppred,x)2 + (âˆ‡ppred,y)2 + (âˆ‡ppred,z)2 |+Ïµ (5)\\nWhere c1 and c2 represent the energy of the foreground and background.\\nLfocal is a focal loss function [12] deï¬ned as:\\nLfocal = âˆ’1\\nN\\nâˆ‘\\n(1 âˆ’ppred)Î³ptruelog (ppred + Ïµ) (6)\\nWhere N is the total number of voxels, and Î³ is set to 2.Robust Semantic Segmentation of Brain Tumor Regions from 3D MRIs 5\\n3.4 Optimization\\nWe use Adam optimizer with initial learning rate ofÎ±0 = 1eâˆ’4 and progressively\\ndecrease it according to:\\nÎ±= Î±0 âˆ—\\n(\\n1 âˆ’ e\\nNe\\n)0.9\\n(7)'),\n",
       " Document(metadata={'arxiv_id': '2001.02040v1', 'title': 'Robust Semantic Segmentation of Brain Tumor Regions from 3D MRIs', 'section': 'body', 'authors': 'Andriy Myronenko, Ali Hatamizadeh'}, page_content='3.4 Optimization\\nWe use Adam optimizer with initial learning rate ofÎ±0 = 1eâˆ’4 and progressively\\ndecrease it according to:\\nÎ±= Î±0 âˆ—\\n(\\n1 âˆ’ e\\nNe\\n)0.9\\n(7)\\nwhere e is an epoch counter, and Ne is a total number of epochs (300 in our\\ncase). We draw input images in random order (ensuring that each training image\\nis drawn once per epoch).\\n3.5 Regularization\\nWe use L2 norm regularization on the convolutional kernel parameters with a\\nweight of 1 eâˆ’5. We also use the spatial dropout with a rate of 0 .2 after the\\ninitial encoder convolution.\\n3.6 Data preprocessing and augmentation\\nWe normalize all input images to have zero mean and unit std (based on non-\\nzero voxels only). We apply a random (per channel) intensity shift ( âˆ’0.1..0.1 of\\nimage std) and scale (0.9..1.1) on input image channels. We also apply a random\\naxis mirror ï¬‚ip (for all 3 axes) with a probability 0 .5.\\n4 Results\\nWe implemented our network in PyTorch1 and trained it on NVIDIA Tesla V100'),\n",
       " Document(metadata={'arxiv_id': '2001.02040v1', 'title': 'Robust Semantic Segmentation of Brain Tumor Regions from 3D MRIs', 'section': 'body', 'authors': 'Andriy Myronenko, Ali Hatamizadeh'}, page_content='axis mirror ï¬‚ip (for all 3 axes) with a probability 0 .5.\\n4 Results\\nWe implemented our network in PyTorch1 and trained it on NVIDIA Tesla V100\\n32GB GPUs using BraTS 2019 training dataset (335 cases) without any addi-\\ntional in-house data. During training we used a random crop of size 160x192x128,\\nwhich ensures that most image content remains within the crop area. We con-\\ncatenated 4 available 3D MRI modalities into the 4 channel image as an input.\\nThe output of the network is 3 nested tumor subregions (after the sigmoid).\\nWe report the results of our approach on BraTS 2019 validation (125 cases).\\nWe uploaded our segmentation results to the BraTS 2019 server for evaluation\\nof per class dice, sensitivity, speciï¬city and Hausdorï¬€ distances.\\nThe results of our model on the BratTS 2019 data are shown in Table 3 for\\nthe validation dataset and in Table 4 for the testing dataset.\\nTime-wise, each training epoch (335 cases) on a single GPU (NVIDIA Tesla'),\n",
       " Document(metadata={'arxiv_id': '2001.02040v1', 'title': 'Robust Semantic Segmentation of Brain Tumor Regions from 3D MRIs', 'section': 'body', 'authors': 'Andriy Myronenko, Ali Hatamizadeh'}, page_content='the validation dataset and in Table 4 for the testing dataset.\\nTime-wise, each training epoch (335 cases) on a single GPU (NVIDIA Tesla\\nV100 32GB) takes 10min. Training the model for 300 epochs takes 2 days.\\nWe trained the model on NVIDIA DGX-1 server (that includes 8 V100 GPUs\\ninterconnected with NVLink); this allowed to train the model in 8 hours. The\\ninference time is 0.4 sec for a single model on a single V100 GPU.\\n1 https://pytorch.org/6 A. Myronenko\\nFig. 1.A typical segmentation example with true and predicted labels overlaid over\\nT1c MRI axial, sagittal and coronal slices. The whole tumor (WT) class includes all\\nvisible labels (a union of green, yellow and red labels), the tumor core (TC) class is a\\nunion of red and yellow, and the enhancing tumor core (ET) class is shown in yellow (a\\nhyperactive tumor part). The predicted segmentation results match the ground truth\\nwell.\\nTable 3.BraTS 2019 validation dataset results. Mean Dice and Hausdorï¬€ measure-'),\n",
       " Document(metadata={'arxiv_id': '2001.02040v1', 'title': 'Robust Semantic Segmentation of Brain Tumor Regions from 3D MRIs', 'section': 'body', 'authors': 'Andriy Myronenko, Ali Hatamizadeh'}, page_content='hyperactive tumor part). The predicted segmentation results match the ground truth\\nwell.\\nTable 3.BraTS 2019 validation dataset results. Mean Dice and Hausdorï¬€ measure-\\nments of the proposed segmentation method. EN - enhancing tumor core, WT - whole\\ntumor, TC - tumor core.\\nDice Hausdorï¬€ (mm)\\nValidation dataset ET WT TC ET WT TC\\nSingle Model (batch 8) 0.800 0.894 0.834 3.921 5.89 6.562\\n5 Discussion and Conclusion\\nIn this work, we described a semantic segmentation network for brain tumor\\nsegmentation from multimodal 3D MRIs for BraTS 2019 challenge. We have\\nexperimented with various normalization functions, and found groupnorm and\\ninstancenorm to perform equivalent, whereas batchnorm was always inferior,\\nwhich could be due the fact of the largest batch size attempted being only\\n16. Since instancenorm is simpler to understand and implement, we used it for\\nnormalization by default. Multi-gpu systems, such as DGX-1 server, contains 8'),\n",
       " Document(metadata={'arxiv_id': '2001.02040v1', 'title': 'Robust Semantic Segmentation of Brain Tumor Regions from 3D MRIs', 'section': 'body', 'authors': 'Andriy Myronenko, Ali Hatamizadeh'}, page_content='16. Since instancenorm is simpler to understand and implement, we used it for\\nnormalization by default. Multi-gpu systems, such as DGX-1 server, contains 8\\nGPU, which allows data-parallel implementation of batch size of 1 (where each\\neach GPU get a batch of 1). We found the performance of multi-gpu system\\nto be equivalent to a single gpu (batch 1) case, thus we used a batch of 8 by\\ndefault, since it is almost 8 times faster to train. We have also experimented withRobust Semantic Segmentation of Brain Tumor Regions from 3D MRIs 7\\nTable 4.BraTS 2019 testing dataset results. Mean Dice and Hausdorï¬€ measurements\\nof the proposed segmentation method. EN - enhancing tumor core, WT - whole tumor,\\nTC - tumor core.\\nDice Hausdorï¬€ (mm)\\nTesting dataset ET WT TC ET WT TC\\nEnsemble 0.826 0.882 0.837 2.203 4.713 3.968\\nmore sophisticated data augmentation techniques, including random histogram\\nmatching, aï¬ƒne image transforms, rotations, random image ï¬ltering, which did'),\n",
       " Document(metadata={'arxiv_id': '2001.02040v1', 'title': 'Robust Semantic Segmentation of Brain Tumor Regions from 3D MRIs', 'section': 'body', 'authors': 'Andriy Myronenko, Ali Hatamizadeh'}, page_content='Ensemble 0.826 0.882 0.837 2.203 4.713 3.968\\nmore sophisticated data augmentation techniques, including random histogram\\nmatching, aï¬ƒne image transforms, rotations, random image ï¬ltering, which did\\nnot demonstrate any additional improvements. Increasing the network depth\\nfurther did not improve the performance, but increasing the network width (the\\nnumber of features/ï¬lters) consistently improved the results. Our BraTS 2019\\nï¬nal testing dataset results were 0.826, 0.882 and 0.837 average dice for enhanced\\ntumor core, whole tumor and tumor core, respectively.\\nReferences\\n1. Bakas, S., Akbari, H., Sotiras, A., Bilello, M., Rozycki, M., Kirby, J., John Frey-\\nmann, K.F., Davatzikos, C.: Segmentation labels and radiomic features for the pre-\\noperative scans of the tcga-gbm collection. The Cancer Imaging Archive (2017),\\nhttps://doi.org/10.7937/K9/TCIA.2017.KLXWJJ1Q\\n2. Bakas, S., Akbari, H., Sotiras, A., Bilello, M., Rozycki, M., Kirby, J., John Frey-'),\n",
       " Document(metadata={'arxiv_id': '2001.02040v1', 'title': 'Robust Semantic Segmentation of Brain Tumor Regions from 3D MRIs', 'section': 'body', 'authors': 'Andriy Myronenko, Ali Hatamizadeh'}, page_content='https://doi.org/10.7937/K9/TCIA.2017.KLXWJJ1Q\\n2. Bakas, S., Akbari, H., Sotiras, A., Bilello, M., Rozycki, M., Kirby, J., John Frey-\\nmann, K.F., Davatzikos, C.: Segmentation labels and radiomic features for the\\npre-operative scans of the tcga-lgg collection. The Cancer Imaging Archive (2017),\\nhttps://doi.org/10.7937/K9/TCIA.2017.GJQ7R0EF\\n3. Bakas, S., Akbari, H., Sotiras, A., Bilello, M., Rozycki, M., Kirby, J., Freymann,\\nJ., Farahani, K., Davatzikos, C.: Advancing the cancer genome atlas glioma mri\\ncollections with expert segmentation labels and radiomic features. Scientiï¬c data\\n4 (9 2017)\\n4. Bakas, S., Reyes, M., et Int, Menze, B.: Identifying the best machine learning algo-\\nrithms for brain tumor segmentation, progression assessment, and overall survival\\nprediction in the BRATS challenge. In: arXiv:1811.02629 (2018)\\n5. Chen, L.C., Zhu, Y., Papandreou, G., Schroï¬€, F., Adam, H.: Encoder-decoder with\\natrous separable convolution for semantic image segmentation. arXiv:1802.02611\\n(2018)'),\n",
       " Document(metadata={'arxiv_id': '2001.02040v1', 'title': 'Robust Semantic Segmentation of Brain Tumor Regions from 3D MRIs', 'section': 'body', 'authors': 'Andriy Myronenko, Ali Hatamizadeh'}, page_content='5. Chen, L.C., Zhu, Y., Papandreou, G., Schroï¬€, F., Adam, H.: Encoder-decoder with\\natrous separable convolution for semantic image segmentation. arXiv:1802.02611\\n(2018)\\n6. Chen, X., Williams, B.M., Vallabhaneni, S.R., Czanner, G., Williams, R., Zheng,\\nY.: Learning active contour models for medical image segmentation. In: Proceed-\\nings of the IEEE Conference on Computer Vision and Pattern Recognition. pp.\\n11632â€“11640 (2019)\\n7. Hatamizadeh, A., Sengupta, D., Terzopoulos, D.: End-to-end deep convolutional\\nactive contours for image segmentation. arXiv preprint arXiv:1909.13359 (2019)\\n8. He, K., Zhang, X., Ren, S., Sun, J.: Identity mappings in deep residual networks.\\nIn: European Conference on Computer Vision (ECCV) (2016)\\n9. Huang, G., Liu, Z., van der Maaten, L., Weinberger, K.Q.: Densely connected\\nconvolutional networks. In: Proceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition. pp. 2261â€“2269 (2017)8 A. Myronenko'),\n",
       " Document(metadata={'arxiv_id': '2001.02040v1', 'title': 'Robust Semantic Segmentation of Brain Tumor Regions from 3D MRIs', 'section': 'body', 'authors': 'Andriy Myronenko, Ali Hatamizadeh'}, page_content='convolutional networks. In: Proceedings of the IEEE Conference on Computer\\nVision and Pattern Recognition. pp. 2261â€“2269 (2017)8 A. Myronenko\\n10. Ioï¬€e, S., Szegedy, C.: Batch normalization: Accelerating deep network training by\\nreducing internal covariate shift. In: International Conference on Machine Learning\\n(ICML). pp. 448â€“456 (2015)\\n11. Isensee, F., Kickingereder, P., Wick, W., Bendszus, M., Maier-Hein, K.H.: No new-\\nnet. In: International Conference on Medical Image Computing and Computer As-\\nsisted Intervention (MICCAI 2018). Multimodal Brain Tumor Segmentation Chal-\\nlenge (BraTS 2018). BrainLes 2018 workshop. LNCS, Springer (2018)\\n12. Lin, T.Y., Goyal, P., Girshick, R., He, K., DollÂ´ ar, P.: Focal loss for dense object\\ndetection. In: Proceedings of the IEEE international conference on computer vision.\\npp. 2980â€“2988 (2017)\\n13. McKinley, R., Meier, R., Wiest, R.: Ensembles of densely-connected cnns with'),\n",
       " Document(metadata={'arxiv_id': '2001.02040v1', 'title': 'Robust Semantic Segmentation of Brain Tumor Regions from 3D MRIs', 'section': 'body', 'authors': 'Andriy Myronenko, Ali Hatamizadeh'}, page_content='detection. In: Proceedings of the IEEE international conference on computer vision.\\npp. 2980â€“2988 (2017)\\n13. McKinley, R., Meier, R., Wiest, R.: Ensembles of densely-connected cnns with\\nlabel-uncertainty for brain tumor segmentation. In: International Conference on\\nMedical Image Computing and Computer Assisted Intervention (MICCAI 2018).\\nMultimodal Brain Tumor Segmentation Challenge (BraTS 2018). BrainLes 2018\\nworkshop. LNCS, Springer (2018)\\n14. Menze, B.H., Jakab, A., Bauer, S., Kalpathy-Cramer, J., Farahani, K., Kirby, J.,\\nBurren, Y., Porz, N., Slotboom, J., Wiest, R., Lanczi, L., Gerstner, E.R., Weber,\\nM.A., Arbel, T., Avants, B.B., Ayache, N., Buendia, P., Collins, D.L., Cordier,\\nN., Corso, J.J., Criminisi, A., Das, T., Delingette, H., Demiralp, C., Durst, C.R.,\\nDojat, M., Doyle, S., Festa, J., Forbes, F., Geremia, E., Glocker, B., Golland, P.,\\nGuo, X., Hamamci, A., Iftekharuddin, K.M., Jena, R., John, N.M., Konukoglu,'),\n",
       " Document(metadata={'arxiv_id': '2001.02040v1', 'title': 'Robust Semantic Segmentation of Brain Tumor Regions from 3D MRIs', 'section': 'body', 'authors': 'Andriy Myronenko, Ali Hatamizadeh'}, page_content='Dojat, M., Doyle, S., Festa, J., Forbes, F., Geremia, E., Glocker, B., Golland, P.,\\nGuo, X., Hamamci, A., Iftekharuddin, K.M., Jena, R., John, N.M., Konukoglu,\\nE., Lashkari, D., Mariz, J.A., Meier, R., Pereira, S., Precup, D., Price, S.J., Raviv,\\nT.R., Reza, S.M.S., Ryan, M.T., Sarikaya, D., Schwartz, L.H., Shin, H.C., Shotton,\\nJ., Silva, C.A., Sousa, N., Subbanna, N.K., Szekely, G., Taylor, T.J., Thomas,\\nO.M., Tustison, N.J., Unal, G.B., Vasseur, F., Wintermark, M., Ye, D.H., Zhao,\\nL., Zhao, B., Zikic, D., Prastawa, M., Reyes, M., Leemput, K.V.: The multimodal\\nbrain tumor image segmentation benchmark (brats). IEEE Trans. Med. Imaging\\n34(10), 1993â€“2024 (2015)\\n15. Milletari, F., Navab, N., Ahmadi, S.A.: V-net: Fully convolutional neural networks\\nfor volumetric medical image segmentation. In: Fourth International Conference\\non 3D Vision (3DV) (2016)\\n16. Myronenko, A.: 3D MRI brain tumor segmentation using autoencoder regulariza-'),\n",
       " Document(metadata={'arxiv_id': '2001.02040v1', 'title': 'Robust Semantic Segmentation of Brain Tumor Regions from 3D MRIs', 'section': 'body', 'authors': 'Andriy Myronenko, Ali Hatamizadeh'}, page_content='for volumetric medical image segmentation. In: Fourth International Conference\\non 3D Vision (3DV) (2016)\\n16. Myronenko, A.: 3D MRI brain tumor segmentation using autoencoder regulariza-\\ntion. In: BrainLes, Medical Image Computing and Computer Assisted Intervention\\n(MICCAI). pp. 311â€“320. LNCS, Springer (2018), https://arxiv.org/abs/1810.\\n11654\\n17. Ulyanov, D., Vedaldi, A., Lempitsky, V.S.: Instance normalization: The missing\\ningredient for fast stylization. In: CVPR (2016)\\n18. Wu, Y., He, K.: Group normalization. In: European Conference on Computer Vi-\\nsion (ECCV) (2018)\\n19. Zhou, C., Chen, S., Ding, C., Tao, D.: Learning contextual and attentive informa-\\ntion for brain tumor segmentation. In: International Conference on Medical Im-\\nage Computing and Computer Assisted Intervention (MICCAI 2018). Multimodal\\nBrain Tumor Segmentation Challenge (BraTS 2018). BrainLes 2018 workshop.\\nLNCS, Springer (2018)'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'title_abstract', 'authors': 'Saumya B'}, page_content='Title: Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation\\n\\nAbstract: Brain tumor segmentation is crucial for diagnosis and treatment planning, yet challenges such as class imbalance and limited model generalization continue to hinder progress. This work presents a reproducible evaluation of U-Net segmentation performance on brain tumor MRI using focal loss and basic data augmentation strategies. Experiments were conducted on a publicly available MRI dataset, focusing on focal loss parameter tuning and assessing the impact of three data augmentation techniques: horizontal flip, rotation, and scaling. The U-Net with focal loss achieved a precision of 90%, comparable to state-of-the-art results. By making all code and results publicly available, this study establishes a transparent, reproducible baseline to guide future research on augmentation strategies and loss function design in brain tumor segmentation.'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='Reproducible Evaluation of Data Augmentation and\\nLoss Functions for Brain Tumor Segmentation\\nSaumya B\\nProject Associate\\nDESE, Indian Institute of Science\\nBengaluru, India\\nsaumya.b@fsid-iisc.in\\nAbstractâ€”Brain tumor segmentation is crucial for diagnosis\\nand treatment planning, yet challenges such as class imbalance\\nand limited model generalization continue to hinder progress.\\nThis work presents a reproducible evaluation of U-Net segmenta-\\ntion performance on brain tumor MRI using focal loss and basic\\ndata augmentation strategies. Experiments were conducted on a\\npublicly available MRI dataset, focusing on focal loss parameter\\ntuning and assessing the impact of three data augmentation\\ntechniques: horizontal flip, rotation, and scaling. The U-Net with\\nfocal loss achieved a precision of 90%, comparable to state-of-the-\\nart results. By making all code and results publicly available, this\\nstudy establishes a transparent, reproducible baseline to guide'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='art results. By making all code and results publicly available, this\\nstudy establishes a transparent, reproducible baseline to guide\\nfuture research on augmentation strategies and loss function\\ndesign in brain tumor segmentation.\\nIndex Termsâ€”brain tumor segmentation, data augmentation,\\nU-Net, focal loss\\nI. INTRODUCTION\\nBrain tumors are among the most challenging medical\\nconditions to diagnose and treat, often requiring precise iden-\\ntification of tumor boundaries for effective treatment plan-\\nning. Magnetic Resonance Imaging (MRI) is a widely used\\nimaging modality for detecting brain tumors, providing de-\\ntailed anatomical information essential for accurate diagnosis.\\nHowever, manual delineation of tumor regions by radiologists\\nis time-consuming, prone to inter-observer variability, and\\ndifficult to scale in clinical settings. These challenges highlight\\nthe need for automated methods that can accurately segment\\nbrain tumors and provide interpretable predictions.'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='difficult to scale in clinical settings. These challenges highlight\\nthe need for automated methods that can accurately segment\\nbrain tumors and provide interpretable predictions.\\nDeep learning-based approaches, particularly convolutional\\nneural networks (CNNs), have shown significant promise in\\nmedical image segmentation tasks due to their ability to learn\\nhierarchical spatial features. Among these, the U-Net archi-\\ntecture is widely regarded as the gold standard for biomedical\\nimage segmentation, owing to its encoder-decoder structure\\nwith skip connections that effectively capture fine-grained\\nspatial details [1].While U-Net models have achieved strong\\nsegmentation performance, they continue to face challenges\\nwith class imbalance, a factor that can significantly affect\\nmodel accuracy in medical imaging.\\nThis study presents a reproducible evaluation of U-Net\\nperformance on brain tumor MRI with focal loss and basic\\ndata augmentation strategies. Focal loss is commonly applied'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='This study presents a reproducible evaluation of U-Net\\nperformance on brain tumor MRI with focal loss and basic\\ndata augmentation strategies. Focal loss is commonly applied\\nto address class imbalance by increasing the weight of hard-to-\\nclassify regions, such as tumor boundaries. We systematically\\nevaluated the effect of focal loss parameters alongside three\\naugmentation techniques â€” horizontal flip, rotation, and scal-\\ning â€” to assess their impact on segmentation accuracy and\\ngeneralization.\\nThe major contributions of this work are given below:\\nâ€¢A baseline implementation of U-Net with focal loss for\\nbrain tumor MRI segmentation\\nâ€¢Analysis of the impact of varying focal loss parameters\\non model performance\\nâ€¢Evaluation of the effect of three different data augmen-\\ntation techniques on model performance\\nThough this study does not introduce new architectures, it\\nprovides a transparent, reproducible evaluation of focal loss\\nand data augmentation strategies for brain tumor segmentation.'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='Though this study does not introduce new architectures, it\\nprovides a transparent, reproducible evaluation of focal loss\\nand data augmentation strategies for brain tumor segmentation.\\nBy making the results and code publicly available, this work\\nserves as a reproducible baseline and reference point for future\\nresearch in model training, augmentation strategies, and loss\\nfunction design.\\nII. LITERATURESURVEY\\nAccurate quantification of brain tumors is critical for di-\\nagnosis, treatment planning, and monitoring of therapeutic\\nresponse. Among imaging techniques, MRI remains the pre-\\nferred modality because of its superior soft tissue contrast,\\nenabling more precise tumor delineation than CT or ultra-\\nsound. Traditional segmentation methods, such as thresholding\\n[2], [3], boundary detection via active contour models [4],\\nand various forms of region growing [5]â€“[7], depend on\\nhandcrafted intensity heuristics [7] or feature engineering [5].'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='[2], [3], boundary detection via active contour models [4],\\nand various forms of region growing [5]â€“[7], depend on\\nhandcrafted intensity heuristics [7] or feature engineering [5].\\nFor example, Sujan et al. (2016) [3] combined thresholding\\nwith morphological operations, reporting approximately 85%\\naccuracy for tumor detection on BRATS MRI scans, while\\nMeier et al. (2016) [8] demonstrated clinical validation of\\na fully automated volumetry system in longitudinal studies.\\nNevertheless, such rule-based or classical approaches are fre-\\nquently constrained by noise sensitivity and poor generaliza-\\ntion across datasets, thus motivating the shift toward deep\\nlearning-based segmentation techniques.\\nConvolutional neural networks (CNNs) have transformed\\nmedical image analysis by automatically learning hierarchical\\nfeatures, outperforming traditional handcrafted approaches in\\narXiv:2510.08617v1  [cs.CV]  8 Oct 2025tumor detection and segmentation [9]. Among these, U-Net'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='features, outperforming traditional handcrafted approaches in\\narXiv:2510.08617v1  [cs.CV]  8 Oct 2025tumor detection and segmentation [9]. Among these, U-Net\\n(Ronneberger et al., 2015) [1] became the standard for biomed-\\nical segmentation due to its encoderâ€“decoder structure with\\nskip connections, enabling precise localization from limited\\ntraining data. However, brain tumor segmentation remains\\nchallenging because of severe class imbalance, where tumor\\npixels are sparse compared to the background. To address\\nthis, several loss functions have been explored: binary cross-\\nentropy (BCE) provides pixel-level supervision but struggles\\nwith imbalance, while Dice loss emphasizes overlap but can\\nbe unstable for small structures. More recently, focal loss [10]\\nhas been proposed to focus training on hard-to-classify pixels,\\nand studies comparing focal loss with BCE demonstrate its\\nadvantage in medical segmentation tasks [11].\\nData scarcity and variability across scanners often limit'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='and studies comparing focal loss with BCE demonstrate its\\nadvantage in medical segmentation tasks [11].\\nData scarcity and variability across scanners often limit\\nthe generalization of brain tumor segmentation models. Data\\naugmentation is therefore widely applied to synthetically en-\\nlarge training sets and improve robustness. Common tech-\\nniques include geometric transformations such as flipping,\\nrotation, and scaling, as well as elastic deformations and\\nintensity perturbations [12]. Prior studies have shown that\\neven simple augmentations can significantly boost Dice scores\\nand reduce overfitting in medical segmentation tasks. Since\\ndata augmentation is already known to improve brain tumor\\nsegmentation, this study focuses on systematically comparing\\ndifferent augmentation strategies in this context.\\nIII. METHODOLOGY\\nTo achieve the objectives mentioned in section 1, the study\\nwas conducted in two phases, using the same U-Net model:\\ni.Focal Loss Parameter Tuning:'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='III. METHODOLOGY\\nTo achieve the objectives mentioned in section 1, the study\\nwas conducted in two phases, using the same U-Net model:\\ni.Focal Loss Parameter Tuning:\\nIn the initial set of experiments, the goal was to identify\\nthe optimal parameters for focal loss that yielded the\\nbest segmentation performance. These experiments were\\nconducted using the original dataset without applying any\\ndata augmentation techniques.\\nii.Data Augmentation Analysis:\\nAfter determining the best-performing focal loss param-\\neters, they were fixed for the subsequent experiments.\\nThree different data augmentation techniques were then\\napplied individually to the dataset, and the modelâ€™s per-\\nformance was evaluated for each augmentation technique.\\nThis systematic approach ensured a comprehensive analysis\\nof both focal loss parameter tuning and the impact of data\\naugmentation on the modelâ€™s segmentation performance.\\nA. Dataset\\nFor this study, a brain tumor dataset containing 3064 T1-'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='of both focal loss parameter tuning and the impact of data\\naugmentation on the modelâ€™s segmentation performance.\\nA. Dataset\\nFor this study, a brain tumor dataset containing 3064 T1-\\nweighted contrast-enhanced MRI images was used. The data\\nwas collected from Nanfang Hospital and Tianjing Medical\\nUniversity, China, from 2005 to 2010, by Jun Cheng, who\\noriginally used the dataset in his study [13] and [14], and\\nhad uploaded the entire dataset with its metadata to Figshare.\\nThe dataset consists of T1-weighted contrast-enhanced MRI\\nscans from 233 patients, and has three kinds of brain tumors\\n- 708 cases of Meningiomas, 1426 gliomas and 930 cases of\\npituitary tumors. The source states that the tumor borders were\\nmanually delineated by three experienced radiologists. A copy\\nof this dataset was taken from Kaggle [15], where the scans\\nand corresponding binary masks were uploaded as 256x256\\npixel images. A summary of the details of the dataset are\\ngiven in Table 1.\\nTABLE I\\nDATASET DETAILS'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='and corresponding binary masks were uploaded as 256x256\\npixel images. A summary of the details of the dataset are\\ngiven in Table 1.\\nTABLE I\\nDATASET DETAILS\\nTumor type Dimensions No. of images\\nMeningioma 256x256 708\\nGlioma 256x256 1426\\nPituitary 256x256 930\\nB. Pre-processing\\nFig. 1. Pre-processing workflow\\nThe preprocessing steps began by loading both the images\\nand masks in grayscale format, hence reducing the complexity\\nby using fewer channels compared to color images. The\\nimages and masks were then resized to a standardized size\\n(256x256) to ensure uniformity across the dataset. Following\\nthis, the pixel values were normalized by dividing each pixel\\nby 255, which not only facilitates faster convergence [16]\\nduring training but also makes the computations more efficient\\nand less memory-intensive. Since the model is designed to\\nwork with binary masks, an important step was performed\\nto verify that all mask pixel values were either 0 or 1. Any'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='and less memory-intensive. Since the model is designed to\\nwork with binary masks, an important step was performed\\nto verify that all mask pixel values were either 0 or 1. Any\\npixel with a value between 0 and 1 (non-binary) was converted\\nto 0 if it was greater than 0 but less than 1. Finally, the\\ndataset, which contained 3064 samples, was randomly split\\ninto training, validation, and test sets, with 1838 samples\\nassigned to the training set, and 613 samples each assigned\\nto the validation and test sets.\\nC. Model Architecture\\nThe model used in the study is the U-Net architecture,\\noriginally designed for biomedical image segmentation tasks.The U-Net architecture was selected due to its effectiveness\\nin biomedical image segmentation, particularly in tasks with\\nlimited labeled data and the need for precise spatial localiza-\\ntion, which is critical for tumor segmentation. It employs an\\nencoder-decoder structure with skip connections to preserve'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='limited labeled data and the need for precise spatial localiza-\\ntion, which is critical for tumor segmentation. It employs an\\nencoder-decoder structure with skip connections to preserve\\nspatial information. The encoder extracts hierarchical features\\nthrough downsampling, while the decoder reconstructs the\\nsegmentation mask by upsampling the feature maps.\\nThe encoder consists of four downsampling blocks, each\\nwith two convolutional layers (with padding = â€œsameâ€, kernel\\nsize of 3x3) to balance the need for fine-grained segmentation\\naccuracy and model generalization. This is followed by a max\\npooling layer with a pool size of 2x2. The convolutional\\nlayers use ReLU activation and He normal initialization.\\nReLU was chosen because it introduces non-linearity, avoids\\nvanishing gradient issues, and promotes faster convergence,\\nwhile He initialization complements it by maintaining variance\\nthroughout the network, ensuring stable and efficient training.'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='vanishing gradient issues, and promotes faster convergence,\\nwhile He initialization complements it by maintaining variance\\nthroughout the network, ensuring stable and efficient training.\\nAfter the pooling layers, a dropout layer with a rate of 0.3 is\\napplied to regularize the network and prevent overfitting. The\\nnumber of filters doubles in each block, starting from 64 in\\nthe first block and increasing to 1024 in the bottleneck layer.\\nThe decoder mirrors the encoder with four upsampling\\nblocks. Each block begins with a transposed convolution layer\\nto upsample the feature maps, followed by skip connections\\nthat concatenate feature maps from the corresponding encoder\\nblock, preserving spatial details lost during down-sampling.\\nThese are followed by two convolutional layers with kernel\\nsize 3x3 and with ReLU activation and He initialization.\\nA dropout layer with the same rate of 0.3 is applied after\\nthe concatenation step to maintain regularization and support'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='size 3x3 and with ReLU activation and He initialization.\\nA dropout layer with the same rate of 0.3 is applied after\\nthe concatenation step to maintain regularization and support\\nuncertainty estimation. At the bottleneck, the deepest part\\nof the U-Net, two convolutional layers with 1024 filters\\ncapture high-level feature representations essential for accurate\\nsegmentation. The output layer is a 1Ã—1 convolution with a\\nsigmoid activation function, producing a binary segmentation\\nmap to identify tumor regions.\\nD. Choosing focal loss as loss function\\nFocal loss was selected as the loss function for this model\\ndue to its effectiveness in addressing class imbalance, which\\nis a common issue in segmentation tasks. In this case, the seg-\\nmentation masks contain significantly fewer foreground pixels\\n(tumor regions) compared to the background pixels. Standard\\nloss functions, such as cross-entropy loss, may struggle in\\nsuch scenarios because the loss tends to be dominated by the'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='(tumor regions) compared to the background pixels. Standard\\nloss functions, such as cross-entropy loss, may struggle in\\nsuch scenarios because the loss tends to be dominated by the\\nmajority class (background).\\nFocal Loss mitigates this issue by dynamically scaling the\\ncontribution of each pixelâ€™s loss based on its classification dif-\\nficulty. Hard-to-classify examples (e.g., noisy textures, small\\ntumor regions, or partial objects) are given higher weights,\\nwhile easy-to-classify examples (e.g., background pixels) are\\ndown-weighted, allowing the model to focus on learning\\nchallenging patterns. Focal Loss is an extension of Cross-\\nEntropy Loss, formulated as in Eq. 1.\\nFL(pt) =âˆ’Î±(1âˆ’p t)Î³ log(pt)(1)\\nwherep t is the modelâ€™s predicted probability for the true\\nclass,Î±is a weighting factor for balancing the importance\\nof different classes, (1 -p t) is the modulating factor, which\\ngives more weight to misclassified or hard examples, and\\nÎ³is the focusing parameter that adjusts how much the loss'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='of different classes, (1 -p t) is the modulating factor, which\\ngives more weight to misclassified or hard examples, and\\nÎ³is the focusing parameter that adjusts how much the loss\\nfocuses on hard examples. The parametersÎ±andÎ³play\\ncrucial roles in the behavior of Focal Loss.Î±is the weighting\\nfactor which controls the balance between the foreground and\\nbackground classes. For instance, setting a higherÎ±for the\\ntumor (foreground) class ensures that the model assigns greater\\nimportance to learning from tumor pixels, which are typically\\nunderrepresented.Î³is the focusing parameter and determines\\nthe degree of focus on hard examples. WhenÎ³= 0, Focal Loss\\nis equivalent to standard Cross-Entropy Loss. IncreasingÎ³\\n(e.g., 2.0) amplifies the contribution of harder examples while\\nreducing the weight of well-classified ones. Higher values ofÎ³\\nare particularly useful in datasets with severe class imbalance.\\nE. Hyperparamenters\\nTABLE II\\nMODELHYPERPARAMETERS ANDTRAININGCONFIGURATION'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='are particularly useful in datasets with severe class imbalance.\\nE. Hyperparamenters\\nTABLE II\\nMODELHYPERPARAMETERS ANDTRAININGCONFIGURATION\\nCategory Hyperparameter Value\\nModel Architecture\\nInput Shape (256, 256, 1)\\nKernel Size (Initial) (3, 3)\\nKernel Size (Subsequent) (3, 3)\\nActivation function ReLU\\nKernel Initializer He Normal\\nDropout Rate 0.3\\nFinal Activation Sigmoid\\nLoss and Metrics\\nLoss function Focal Loss\\nOptimizer Adam\\nTraining metrics Accuracy\\nTraining Parameters\\nBatch Size 8\\nLearning Rate1Ã—10 âˆ’4\\nNo. of epochs 200\\nThe Adam optimizer (Adaptive Momentum Estimator) was\\nchosen for training the model due to its effectiveness in\\nbiomedical image segmentation tasks [17] and its ability to\\nbalance computational efficiency, minimal memory require-\\nments, and ease of implementation. Unlike traditional stochas-\\ntic gradient descent, Adam adapts the learning rate dynam-\\nically for each parameter based on past gradients, making\\nit well-suited for complex models like U-Net. The learning'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='tic gradient descent, Adam adapts the learning rate dynam-\\nically for each parameter based on past gradients, making\\nit well-suited for complex models like U-Net. The learning\\nrate is a critical hyperparameter that directly affects training\\nstability and speed. A learning rate that is too low can result\\nin slow convergence, while one that is too high may cause the\\nmodel to miss the optimal solution. For this model, an initial\\nlearning rate of1Ã—10 âˆ’4 was selected, which provided a good\\nbalance between convergence speed and stability. The rest of\\nthe hyperparameters chosen are given in detail in the Table 2.\\nF . Metrics\\nThe evaluation was focused on assessing segmentation qual-\\nity. Segmentation quality was assessed using standard metrics\\nsuch as Dice Coefficient, Intersection Over Union (IoU), pre-\\ncision and recall. These metrics provide a quantitative measureof the overlap and accuracy of the predicted segmentation\\nmasks compared to the ground truth.\\nG. Training'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='cision and recall. These metrics provide a quantitative measureof the overlap and accuracy of the predicted segmentation\\nmasks compared to the ground truth.\\nG. Training\\nAll training was performed using Google Colabâ€™s free TPU,\\nthe TPUv2-8. The Tensorflow library was used to enable\\nthe model to utilise the TPU during training. The model\\nwas trained for 200 epochs using Adam optimizer with a\\nlearning rate of1Ã—10 âˆ’4. A fixed training schedule of 200\\nepochs was used across all experiments to ensure consistency\\nand to analyze when overfitting tendencies emerged. While\\nearly stopping or best-model checkpointing are commonly\\napplied, in this study the extended training allowed us to\\nobserve the effect of different loss parameters and augmenta-\\ntions on convergence and stability. Validation was done using\\nthe standard training-validation-test split of 60%-20%-20%.\\nDuring training, accuracy was used as the primary evaluation\\nmetric, which provides a general measure of how well the'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='the standard training-validation-test split of 60%-20%-20%.\\nDuring training, accuracy was used as the primary evaluation\\nmetric, which provides a general measure of how well the\\nmodelâ€™s predictions align with the ground truth. However, for a\\nmore comprehensive evaluation of segmentation performance,\\nadditional metrics were computed on the test dataset, including\\nthe Dice coefficient, Intersection over Union (IoU), precision,\\nand recall. These metrics are particularly relevant for assessing\\nthe quality of segmentation masks, especially in imbalanced\\ndatasets where pixel-level accuracy may not fully reflect model\\nperformance.\\nH. Experiments conducted\\nPhase 1: Focal Loss Parameter Tuning\\nIn this phase of the experiments, theÎ±andÎ³parameters\\nfor focal loss were varied to evaluate their impact on segmen-\\ntation performance. These experiments were conducted using\\nthe original dataset, without applying any data augmentation\\ntechniques. Two sets of parameter combinations were tested:'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='tation performance. These experiments were conducted using\\nthe original dataset, without applying any data augmentation\\ntechniques. Two sets of parameter combinations were tested:\\nâ€¢Î±= 0.25, Î³= 2.0\\nâ€¢Î±= 2.0, Î³= 0.75\\nFor the first set of parameters (Î±= 0.25,Î³= 2.0), the lowerÎ±\\nplaces more weight on the background class, while the higher\\nÎ³emphasizes hard-to-classify pixels, such as tumor boundaries\\nor ambiguous regions. In contrast, the second set of parameters\\n(Î±= 2.0,Î³= 0.75) increases the importance of the minority\\nclass (tumor) with a higherÎ±, while the smallerÎ³of 0.75\\nreduces the focus on difficult examples and instead prioritizes\\nthe overall distribution, giving weightage to easier- to-classify\\nregions too.\\nPhase 2: Data Augmentation Evaluation\\nIn this phase, three different data augmentation techniques\\nwere evaluated on the proposed model, while keeping the\\nfocal loss parameters constant atÎ±= 0.25 andÎ³= 2.0.\\nThe techniques evaluated are: Horizontal Flip, Rotation and'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='were evaluated on the proposed model, while keeping the\\nfocal loss parameters constant atÎ±= 0.25 andÎ³= 2.0.\\nThe techniques evaluated are: Horizontal Flip, Rotation and\\nScaling. The details of these augmentation techniques are\\nprovided in Table 3. It is important to note that, since this\\nis a segmentation task, the augmentation transformations were\\napplied to both the image and its corresponding mask to ensure\\nconsistency between the input and target.\\nTABLE III\\nDATAAUGMENTATIONEXPERIMENT DETAILS\\nTechnique % of training dataset Parameters\\nHorizontal Flip 50% none\\nRotation 50% Angle:Â±15 â—¦\\nRandom Scaling 50% Range: 0.8 - 1.2\\nIV. RESULTS AND DISCUSSION\\nThis section presents the results of the experiments con-\\nducted to evaluate the segmentation performance of the model,\\nthe impact of focal loss parameter tuning and the effect\\nof data augmentation techniques. The model was evaluated\\nusing multiple segmentation metrics, including accuracy, loss,'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='the impact of focal loss parameter tuning and the effect\\nof data augmentation techniques. The model was evaluated\\nusing multiple segmentation metrics, including accuracy, loss,\\nDice coefficient, Intersection over Union (IoU), precision, and\\nrecall. Experiments were conducted using both the original\\ndataset and augmented datasets to assess the impact of data\\naugmentation techniques on performance. The model showed\\ncompetitive results across all metrics, with Horizontal Flip\\nemerging as the most effective data augmentation technique,\\nachieving the highest Dice coefficient and IoU scores. Rota-\\ntion also contributed positively, whereas Scaling had minimal\\nimpact on model performance.\\nA. Impact of focal loss parameters\\nThe results obtained in the first set of experiments of\\ntuning the focal loss are presented in Table 4. In the first\\ncase, the configuration prioritized hard-to-classify examples,\\nparticularly tumor boundaries, resulting in better performance.'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='tuning the focal loss are presented in Table 4. In the first\\ncase, the configuration prioritized hard-to-classify examples,\\nparticularly tumor boundaries, resulting in better performance.\\nIn the second case, the configuration emphasized the minor\\nclass (tumor regions) more but focused less on boundary\\ndetails, leading to weaker performance in challenging regions.\\nThe experiments demonstrated that the choice of focal loss\\nparameters play a significant role in balancing foreground and\\nbackground segmentation accuracy.\\nB. Impact of Data Augmentation\\nThree data augmentation techniques - Horizontal flip, Ro-\\ntation and Scaling - were experimented with to evaluate their\\neffect on model generalization and robustness. The results of\\nthe experiments are summarized in the Table 5. Horizontal\\nflip consistently improved performance across all metrics,\\nmaking it the most effective augmentation technique. Rotation\\nimproved the Dice coefficient and IoU, indicating its effective-'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='flip consistently improved performance across all metrics,\\nmaking it the most effective augmentation technique. Rotation\\nimproved the Dice coefficient and IoU, indicating its effective-\\nness. Scaling, however, showed negligible or no improvement\\nin segmentation performance, suggesting that size variations\\nare less significant for this dataset. Visualization of ground\\ntruth and predictions further supported these findings (refer\\nfigure 2), with horizontal flip and rotation demonstrating\\nclearer and more accurate segmentation results compared to\\nscaling.\\nC. Inference drawn from Training graphs\\nFigure 3 shows the training graphs for the different aug-\\nmentation techniques compared with the baseline model.\\nHorizontal flip and rotation augmentations produced moreTABLE IV\\nRESULTS FORLOSSPARAMETEREXPERIMENTS\\nLoss Function Parameters Accuracy Loss Precision Recall IoU Dice Co-efficient\\nFocal Loss\\nÎ±= 0.25\\nÎ³= 2.0 0.9941 0.0082 0.9014 0.7681 0.7082 0.7867\\nFocal Loss\\nÎ±= 2.0'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='RESULTS FORLOSSPARAMETEREXPERIMENTS\\nLoss Function Parameters Accuracy Loss Precision Recall IoU Dice Co-efficient\\nFocal Loss\\nÎ±= 0.25\\nÎ³= 2.0 0.9941 0.0082 0.9014 0.7681 0.7082 0.7867\\nFocal Loss\\nÎ±= 2.0\\nÎ³= 0.75 0.9939 0.0154 0.8778 0.7789 0.7004 0.7839\\nTABLE V\\nRESULTS FORDATAAUGMENTATIONEXPERIMENTS\\nAugmentation type Accuracy Loss Precision Recall IoU Dice Co-efficient\\nNone0.9941 0.0082 0.9014 0.7681 0.7082 0.7867\\nHorizontal Flip0.9942 0.0053 0.9001 0.7779 0.7152 0.8041\\nRotation0.9940 0.0029 0.8774 0.7892 0.7090 0.7955\\nRandom Scaling0.9934 0.0064 0.9097 0.7106 0.6643 0.7486\\nFig. 2. Ground Truth v/s Model Prediction: Green - Prediction, Blue - GT\\nstable validation curves with smaller gaps between training\\nand validation performance, suggesting better robustness to\\nunseen data. In contrast, scaling augmentation resulted in\\nhigher fluctuations in validation loss, indicating reduced sta-\\nbility and weaker generalization. The no-augmentation setting'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='unseen data. In contrast, scaling augmentation resulted in\\nhigher fluctuations in validation loss, indicating reduced sta-\\nbility and weaker generalization. The no-augmentation setting\\nshowed smoother curves but a clear gap between training and\\nvalidation, pointing to mild overfitting. Overall, the results\\nsuggest that simple geometric augmentations such as flip and\\nrotation contribute more effectively to generalization compared\\nto scaling or no augmentation. Extending training to 200\\nepochs further highlighted these differences, showing that\\nhorizontal flip consistently accelerated convergence with stable\\nbehavior, rotation offered moderate but steady gains, while\\nscaling produced noisy training patterns and minimal benefit.\\nD. Comparison with state-of-the-art\\nThe model achieved comparable performance with state-\\nof-the-art methods for brain tumor segmentation. A brief\\ncomparison with state-of-the-art models is shown in Table 7.\\nTABLE VI\\nCOMPARISON WITHSTATE-OF-THE-ART(SOTA)'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='of-the-art methods for brain tumor segmentation. A brief\\ncomparison with state-of-the-art models is shown in Table 7.\\nTABLE VI\\nCOMPARISON WITHSTATE-OF-THE-ART(SOTA)\\nModel Precision Recall IoU Dice co-efficient\\nOur model0.9001 0.7779 0.7152 0.8041\\nArafat et al. [18]0.82 0.74 0.68 0.94\\nGupta et al. [19]0.89 0.91-0.90\\nV. CONCLUSION AND FUTURE WORK\\nThe aim of this study was to systematically evaluate the\\nimpact of focal loss parameters and basic data augmentation\\nstrategies on U-Netâ€“based brain tumor segmentation. Changes\\nin focal loss parameters significantly impacted the model\\nbehavior, with better results obtained when parameters were\\ntuned to give more weightage on minority class (tumor re-\\ngion) and hard-to-classify examples. Among the augmentation\\ntechniques, Horizontal flip was the most effective, followed\\nby Rotation, while Scaling showed minimal improvement.\\nThese findings highlight the importance of careful loss func-\\ntion design and augmentation choice in developing robust'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='by Rotation, while Scaling showed minimal improvement.\\nThese findings highlight the importance of careful loss func-\\ntion design and augmentation choice in developing robust\\nsegmentation pipelines.\\nFuture work will extend this baseline study to include more\\nadvanced augmentation strategies such as elastic deformations,\\nmodality-specific transformations, etc. Artificially synthesized\\ndata using generative models like GANs can also be experi-\\nmented with to evaluate its effect on model performance when\\nused as an augmentation technique. Furthermore, classification\\nof tumor type can also be integrated into the project pipeline\\nsuch that when given an image, the model segments it and\\nclassifies the tumor into specific categories.\\nData and code availability:All code and\\nexperimental configurations are publicly available at\\ngithub.com/Saumya4321/2d-brain-tumor-segmentation.\\nREFERENCES\\n[1] Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. â€œU-net: Convo-'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='experimental configurations are publicly available at\\ngithub.com/Saumya4321/2d-brain-tumor-segmentation.\\nREFERENCES\\n[1] Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. â€œU-net: Convo-\\nlutional networks for biomedical image segmentation.â€ In International\\nConference on Medical image computing and computer-assisted inter-\\nvention, pp. 234-241. Cham: Springer international publishing, 2015.\\n[2] Otsu, Nobuyuki. â€œA threshold selection method from gray-level his-\\ntograms.â€ Automatica 11, no. 285-296 (1975): 23-27.\\n[3] Sujan, Md, Nashid Alam, Syed Abdullah Noman, and Mohammed\\nJahirul Islam. â€œA segmentation based automated system for brain tumor\\ndetection.â€ International Journal of Computer Applications 153, no. 10\\n(2016): 41-49.\\n[4] Derraz, Foued, Mohamed Beladgham, and Mâ€™hamed Khelif. â€œAppli-\\ncation of active contour models in medical image segmentation.â€ In\\nInternational Conference on Information Technology: Coding and Com-'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='cation of active contour models in medical image segmentation.â€ In\\nInternational Conference on Information Technology: Coding and Com-\\nputing, 2004. Proceedings. ITCC 2004., vol. 2, pp. 675-681. IEEE, 2004.Fig. 3. Training graphs - (a) Horizontal flip (b) Rotation (c) Scaling (d) No aug; blue - train, orange - validation\\n[5] Adams, Rolf, and Leanne Bischof. â€œSeeded region growing.â€ IEEE\\nTransactions on pattern analysis and machine intelligence 16, no. 6\\n(1994): 641-647.\\n[6] Shih, Frank Y ., and Shouxian Cheng. â€œAutomatic seeded region growing\\nfor color image segmentation.â€ Image and vision computing 23, no. 10\\n(2005): 877-886.\\n[7] Chaibou, Mahaman Sani, Pierre-Henri Conze, Karim Kalti, Basel So-\\nlaiman, and Mohamed Ali Mahjoub. â€œAdaptive strategy for superpixel-\\nbased region-growing image segmentation.â€ Journal of Electronic Imag-\\ning 26, no. 6 (2017): 061605-061605.\\n[8] Meier, Raphael, Urspeter Knecht, Tina Loosli, Stefan Bauer, Johannes'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='based region-growing image segmentation.â€ Journal of Electronic Imag-\\ning 26, no. 6 (2017): 061605-061605.\\n[8] Meier, Raphael, Urspeter Knecht, Tina Loosli, Stefan Bauer, Johannes\\nSlotboom, Roland Wiest, and Mauricio Reyes. â€œClinical evaluation of\\na fully-automatic segmentation method for longitudinal brain tumor\\nvolumetry.â€ Scientific reports 6, no. 1 (2016): 23376.\\n[9] Wang, Guotai, Wenqi Li, S Â´ebastien Ourselin, and Tom Vercauteren. â€œAu-\\ntomatic brain tumor segmentation using cascaded anisotropic convolu-\\ntional neural networks.â€ In International MICCAI brainlesion workshop,\\npp. 178-190. Cham: Springer International Publishing, 2017.\\n[10] Lin, Tsung-Yi, Priya Goyal, Ross Girshick, Kaiming He, and Piotr\\nDollÂ´ar. â€œFocal loss for dense object detection.â€ In Proceedings of the\\nIEEE international conference on computer vision, pp. 2980-2988. 2017.\\n[11] Neyestanak, Mahdi Shafiei, Hamid Jahani, Mohsen Khodarahmi, Javad\\nZahiri, Mostafa Hosseini, Amirali Fatoorchi, and Mir Saeed Yekanine-'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='[11] Neyestanak, Mahdi Shafiei, Hamid Jahani, Mohsen Khodarahmi, Javad\\nZahiri, Mostafa Hosseini, Amirali Fatoorchi, and Mir Saeed Yekanine-\\njad. â€œA quantitative comparison between focal loss and binary cross-\\nentropy loss in brain tumor auto-segmentation using U-Net.â€ Journal of\\nBiostatistics and Epidemiology 11, no. 1 (2025): 15-35.\\n[12] Nalepa, Jakub, Michal Marcinkiewicz, and Michal Kawulok. â€œData\\naugmentation for brain-tumor segmentation: a review.â€ Frontiers in\\ncomputational neuroscience 13 (2019): 83.\\n[13] Cheng, Jun, et al. â€œEnhanced Performance of Brain Tumor Classification\\nvia Tumor Region Augmentation and Partition.â€ PloS one 10.10 (2015)\\n[14] Cheng, Jun, et al. â€œRetrieval of Brain Tumors by Adaptive Spatial\\nPooling and Fisher Vector Representation.â€ PloS one 11.6\\n[15] The Kaggle dataset link - https://www.kaggle.com/datasets/nikhilroxtomar/\\nbrain-tumor-segmentation\\n[16] Du, Getao, Xu Cao, Jimin Liang, Xueli Chen, and Yonghua Zhan.'),\n",
       " Document(metadata={'arxiv_id': '2510.08617v1', 'title': 'Reproducible Evaluation of Data Augmentation and Loss Functions for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Saumya B'}, page_content='[15] The Kaggle dataset link - https://www.kaggle.com/datasets/nikhilroxtomar/\\nbrain-tumor-segmentation\\n[16] Du, Getao, Xu Cao, Jimin Liang, Xueli Chen, and Yonghua Zhan.\\nâ€œMedical Image Segmentation based on U-Net: A Review.â€ Journal of\\nImaging Science Technology 64, no. 2 (2020).\\n[17] Kingma, Diederik P. â€œAdam: A method for stochastic optimization.â€\\narXiv preprint arXiv:1412.6980 (2014).\\n[18] Arafat, Ali, Dipesh Mamtani, and K. R. Jansi. â€œBrain tumor MRI image\\nsegmentation and classification based on deep learning techniques.â€ In\\n2023 2nd international conference on smart technologies and systems\\nfor next generation computing (ICSTSN), pp. 1-6. IEEE, 2023.\\n[19] S. Gupta and M. Gupta, â€œDeep Learning for Brain Tumor Seg-\\nmentation using Magnetic Resonance Imagesâ€, 2021 IEEE Confer-\\nence on Computational Intelligence in Bioinformatics and Computa-\\ntional Biology (CIBCB), Melbourne, Australia, 2021, pp. 1-6, doi:\\n10.1109/CIBCB49929.2021.9562890.'),\n",
       " Document(metadata={'arxiv_id': '2009.12188v1', 'title': 'Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation', 'section': 'title_abstract', 'authors': 'Laura Mora Ballestar, Veronica Vilaplana'}, page_content='Title: Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation\\n\\nAbstract: Automation of brain tumors in 3D magnetic resonance images (MRIs) is key to assess the diagnostic and treatment of the disease. In recent years, convolutional neural networks (CNNs) have shown improved results in the task. However, high memory consumption is still a problem in 3D-CNNs. Moreover, most methods do not include uncertainty information, which is specially critical in medical diagnosis. This work proposes a 3D encoder-decoder architecture, based on V-Net \\\\cite{vnet} which is trained with patching techniques to reduce memory consumption and decrease the effect of unbalanced data. We also introduce voxel-wise uncertainty, both epistemic and aleatoric using test-time dropout and data-augmentation respectively. Uncertainty maps can provide extra information to expert neurologists, useful for detecting when the model is not confident on the provided segmentation.'),\n",
       " Document(metadata={'arxiv_id': '2009.12188v1', 'title': 'Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation', 'section': 'body', 'authors': 'Laura Mora Ballestar, Veronica Vilaplana'}, page_content='Brain Tumor Segmentation using 3D-CNNs with\\nUncertainty Estimation\\nLaura Mora Ballestar and Veronica Vilaplana â‹†\\nSignal Theory and Communications Department, Universitat Politcnica de\\nCatalunya. BarcelonaTech, Spain\\nlmoraballestar@gmail.com, veronica.vilaplana@upc.edu\\nAbstract. Automation of brain tumors in 3D magnetic resonance im-\\nages (MRIs) is key to assess the diagnostic and treatment of the disease.\\nIn recent years, convolutional neural networks (CNNs) have shown im-\\nproved results in the task. However, high memory consumption is still\\na problem in 3D-CNNs. Moreover, most methods do not include uncer-\\ntainty information, which is specially critical in medical diagnosis. This\\nwork proposes a 3D encoder-decoder architecture, based on V-Net [6]\\nwhich is trained with patching techniques to reduce memory consump-\\ntion and decrease the eï¬€ect of unbalanced data. We also introduce voxel-\\nwise uncertainty, both epistemic and aleatoric using test-time dropout'),\n",
       " Document(metadata={'arxiv_id': '2009.12188v1', 'title': 'Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation', 'section': 'body', 'authors': 'Laura Mora Ballestar, Veronica Vilaplana'}, page_content='tion and decrease the eï¬€ect of unbalanced data. We also introduce voxel-\\nwise uncertainty, both epistemic and aleatoric using test-time dropout\\nand data-augmentation respectively. Uncertainty maps can provide extra\\ninformation to expert neurologists, useful for detecting when the model is\\nnot conï¬dent on the provided segmentation.\\nKeywords: brain tumor segmentation Â· deep learning Â· uncertainty Â·\\nconvolutional neural networks\\n1 Introduction\\nBrain tumors are categorized into primary, brain originated; and secondary, tu-\\nmors that have spread from elsewhere and are known as brain metastasis tumors.\\nAmong malignant primary tumors, gliomas are the most common in adults, rep-\\nresenting 81% of brain tumors [7]. The World Health Organization (WHO) cat-\\negorizes gliomas into grades I-IV which can be simpliï¬ed into two types (1) low\\ngrade gliomas (LGG), grades I-II, which are less common and are characterized\\nby low blood concentration and slow growth and (2) high grade gliomas (HGG),'),\n",
       " Document(metadata={'arxiv_id': '2009.12188v1', 'title': 'Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation', 'section': 'body', 'authors': 'Laura Mora Ballestar, Veronica Vilaplana'}, page_content='grade gliomas (LGG), grades I-II, which are less common and are characterized\\nby low blood concentration and slow growth and (2) high grade gliomas (HGG),\\ngrades III-IV, which have a faster growth rate and aggressiveness.\\nThe extend of the disease is composed of four heterogeneous histological\\nsub-regions, i.e. the peritumoral edematous/invaded tissue, the necrotic core\\n(ï¬‚uid-ï¬lled), the enhancing and no-enhancing tumor (solid) core. Each region is\\ndescribed by varying intensity proï¬les across MRI modalities (T1-weighted, post-\\ncontrast T1-weighted, T2-weighted, and Fluid-Attenuated Inversion Recovery-\\nFLAIR), which reï¬‚ect the diverse tumor biological properties and are commonly\\nâ‹† This work has been partially supported by the project MALEGRA TEC2016-75976-\\nR ï¬nanced by the Spanish Ministerio de EconomÂ´ Ä±a y Competitividad.\\narXiv:2009.12188v1  [eess.IV]  24 Sep 20202 L. Mora et al.\\nused to assess the diagnosis, treatment and evaluation of the disease. These MRI'),\n",
       " Document(metadata={'arxiv_id': '2009.12188v1', 'title': 'Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation', 'section': 'body', 'authors': 'Laura Mora Ballestar, Veronica Vilaplana'}, page_content='arXiv:2009.12188v1  [eess.IV]  24 Sep 20202 L. Mora et al.\\nused to assess the diagnosis, treatment and evaluation of the disease. These MRI\\nmodalities facilitate tumor analysis, but at the expense of performing manual\\ndelineation of the tumor regions which is a challenging and time-consuming\\nprocess. For this reason, automatic mechanisms for region tumor segmentation\\nhave appeared in the last decade thanks to the advancement of deep learning\\nmodels in computer vision tasks.\\nThe Brain Tumor Segmentation (BraTS) [1â€“5] challenge started in 2012 with\\na focus on evaluating state-of-the-art methods for glioma segmentation in multi-\\nmodal MRI scans. BraTS 2020 training dataset includes 369 cases (293 HGG and\\n76 LGG), each with four 3D MRI modalities rigidly aligned, re-sampled to 1mm3\\nisotropic resolution and skull-stripped with size 240 x240x155. Each provides\\na manual segmentation approved by experienced neuro-radiologists. Training'),\n",
       " Document(metadata={'arxiv_id': '2009.12188v1', 'title': 'Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation', 'section': 'body', 'authors': 'Laura Mora Ballestar, Veronica Vilaplana'}, page_content='isotropic resolution and skull-stripped with size 240 x240x155. Each provides\\na manual segmentation approved by experienced neuro-radiologists. Training\\nannotations comprise the enhancing tumor (ET, label 4), the peritumoral edema\\n(ED, label 2), and the necrotic and non-enhancing tumor core (NCR/NET,\\nlabel 1). The nested sub-regions considered for evaluation are: whole tumor WT\\n(label 1, 2, 4), tumor core TC (label 1, 4) and enhancing tumor ET (label 4).\\nThe validation set includes 125 cases, with unknown grade nor ground truth\\nannotation.\\nThis work describes a semantic segmentation approach for 3D brain tumor\\nsegmentation and uncertainty estimation. We use the V-Net [6] architecture\\nto segment the three sub-regions and we estimate both epistemic and aleatory\\n[8] uncertainties using test-time dropout (TTD) [9] and data augmentations,\\nrespectively.\\n2 Related Work\\n2.1 Semantic Segmentation\\nBrain tumor segmentation methods include generative and discriminative ap-'),\n",
       " Document(metadata={'arxiv_id': '2009.12188v1', 'title': 'Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation', 'section': 'body', 'authors': 'Laura Mora Ballestar, Veronica Vilaplana'}, page_content='respectively.\\n2 Related Work\\n2.1 Semantic Segmentation\\nBrain tumor segmentation methods include generative and discriminative ap-\\nproaches. Generative methods try to incorporate prior knowledge and model\\nprobabilistic distributions whereas discriminative methods extract features from\\nimage representations. This latter approach has thrived in recent years thanks\\nto the advancement in CNNs, as demonstrated in the winners of the previous\\nBraTS. The biggest break through in this area was introduced by DeepMedic [10]\\na 3D CNN that exploits multi-scale features using parallel pathways and incor-\\nporates a fully connected conditional random ï¬eld (CRF) to remove false pos-\\nitives. [11] compares the performances of three 3D CNN architectures showing\\nthe importance of the multi-resolution connections to obtain ï¬ne details in the\\nsegmentation of tumor sub-regions. More recently, EMMA [12] creates and en-\\nsemble at inference time which reduces overï¬tting but at high computational'),\n",
       " Document(metadata={'arxiv_id': '2009.12188v1', 'title': 'Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation', 'section': 'body', 'authors': 'Laura Mora Ballestar, Veronica Vilaplana'}, page_content='segmentation of tumor sub-regions. More recently, EMMA [12] creates and en-\\nsemble at inference time which reduces overï¬tting but at high computational\\ncost, and [13] proposes a cascade of two CNNs, where the ï¬rst network produces\\nraw tumor masks and the second network is trained on the vecinity of the tumor\\nto predict tumor regions. BraTS 2018 winner [14] proposed an asymmetrically\\nencoder-decoder architecture with a variational autoencoder to reconstruct the\\nimage during training, which is used as a regularizer. Isensee, F [15] uses a regu-\\nlar 3D-U-Net optimized on the evaluation metrics and co-trained with externalBrain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation 3\\ndata. BraTS 2019 winners [16] use a two-stage cascade U-Net trained end-to-\\nend. Finally, [17] applies several tricks in three categories: data processing, model\\ndevising and optimization process to boost the model performance.\\n2.2 Uncertainty'),\n",
       " Document(metadata={'arxiv_id': '2009.12188v1', 'title': 'Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation', 'section': 'body', 'authors': 'Laura Mora Ballestar, Veronica Vilaplana'}, page_content='end. Finally, [17] applies several tricks in three categories: data processing, model\\ndevising and optimization process to boost the model performance.\\n2.2 Uncertainty\\nUncertainty information of segmentation results is important, specially in medi-\\ncal imaging, to guide the clinical decisions and help understand the reliability of\\nthe provided segmentation, hence being able to identify more challenging cases\\nwhich may require expert review. Segmentation models for brain tumor MRIs\\ntend to label voxels with less conï¬dence in the surrounding tissues of the segmen-\\ntation targets [19], thus indicating regions that may have been miss-segmented.\\nLast yearâ€™s BraTS challenge already started introducing uncertainty measure-\\nments. [18] computes epistemic uncertainty using TTD. They obtain a posterior\\ndistribution generated after running several epochs for each image at test-time.\\nThen, mean and variance are used to evaluate the model uncertainty. A diï¬€erent'),\n",
       " Document(metadata={'arxiv_id': '2009.12188v1', 'title': 'Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation', 'section': 'body', 'authors': 'Laura Mora Ballestar, Veronica Vilaplana'}, page_content='distribution generated after running several epochs for each image at test-time.\\nThen, mean and variance are used to evaluate the model uncertainty. A diï¬€erent\\napproach is proposed by Wang G [19], who uses TTD and data augmentation to\\nestimate the voxel-wise uncertainty by computing the entropy instead of the vari-\\nance. Finally, [20] proposes to incorporate uncertainty measures during training\\nas they deï¬ne a loss function that models label noise and uncertainty.\\n3 Method\\nThe following section details the network architecture as well as the training\\nschemes and data processing techniques used to train our model.\\n3.1 Data Pre-processing and Augmentation\\nMRI intensity values are not standardized as the data is obtained from diï¬€erent\\ninstitutions, scanners and protocols. When training a neural network it is par-\\nticularly important to use normalized data, even more as MRI modalities are\\ntreated like color channels. For this reason, we normalize each modality of each'),\n",
       " Document(metadata={'arxiv_id': '2009.12188v1', 'title': 'Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation', 'section': 'body', 'authors': 'Laura Mora Ballestar, Veronica Vilaplana'}, page_content='ticularly important to use normalized data, even more as MRI modalities are\\ntreated like color channels. For this reason, we normalize each modality of each\\npatient independently to have zero mean and unit std based on non-zero voxels\\nonly, which represent the brain region.\\nWe also apply data augmentation techniques to prevent over-ï¬tting by trying\\nto disrupt minimally the data. For this, we apply Random Flip (for all 3 axes)\\nwith a 50% probability, Random Intensity Shift between ( âˆ’0.1..0.1 of data std)\\nand Random Intensity Scale on all input channels at range (0.9..1.1).\\n3.2 Network Architecture\\nV-Net [6] and U-Net [25] architectures have proven to be successful and reli-\\nable encoder-decoder architectures for medical image segmentation, as they are\\nable to segment ï¬ne image structures. Moreover, may past years participants\\nachieve great results using this architectures as baselines. With this in mind,4 L. Mora et al.'),\n",
       " Document(metadata={'arxiv_id': '2009.12188v1', 'title': 'Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation', 'section': 'body', 'authors': 'Laura Mora Ballestar, Veronica Vilaplana'}, page_content='able to segment ï¬ne image structures. Moreover, may past years participants\\nachieve great results using this architectures as baselines. With this in mind,4 L. Mora et al.\\nour work uses a V-Net architecture with four output channels and some minor\\nmodiï¬cations, such as the usage of Instance Normalization [21] in contrast of\\nBatch Normalization, which normalizes across each channel for each training\\nexample. Moreover we have doubled the number of features maps at each level\\nof the network as proposed in [15], having 32 feature channels at the highest\\nresolution.\\nThe network follows a common approach that progressively downsizes the\\nimage and feature dimensions by a factor of 2 using strided convolutions instead\\nof pooling layers, see Figure 1.\\nFig. 1: We use the V-Net [6] architecture with instance normalization, ELU non-\\nlinearities and doubled number of feature channels. Feature dimensionality is\\ndenoted at each block. The network outputs the segmentation in four channels'),\n",
       " Document(metadata={'arxiv_id': '2009.12188v1', 'title': 'Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation', 'section': 'body', 'authors': 'Laura Mora Ballestar, Veronica Vilaplana'}, page_content='linearities and doubled number of feature channels. Feature dimensionality is\\ndenoted at each block. The network outputs the segmentation in four channels\\nusing a Softmax.\\n3.3 Training\\nThe network is trained end-to-end with randomly selected patches which have a\\n50% probability being centered on healthy tissue and 50% probability on tumor\\n[22]. Due to memory constraints, we need to make a trade-oï¬€ between the size\\nof each patch and the batch size. With this, we found that the best results\\nare achieved with patches of size 64x64x64 and a batch size of 8. In our case,\\nbigger patch sizes required smaller batches and were more likely to overï¬t, thus\\nachieving worst results on the validation set.\\nAs optimization, we use SGD with 0 .99 momentum and a learning rate of\\n1eâˆ’2. We use the Pytorch learning scheduler ReduceLROnPlateau, which will\\ndecrease by a factor of 0 .1 whenever the validation loss has not improved in the\\npassed 10 epochs.'),\n",
       " Document(metadata={'arxiv_id': '2009.12188v1', 'title': 'Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation', 'section': 'body', 'authors': 'Laura Mora Ballestar, Veronica Vilaplana'}, page_content='1eâˆ’2. We use the Pytorch learning scheduler ReduceLROnPlateau, which will\\ndecrease by a factor of 0 .1 whenever the validation loss has not improved in the\\npassed 10 epochs.\\nFor the loss function we use the dice loss as deï¬ned in [6] which can be written\\nasBrain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation 5\\nLdice = 2 âˆ—âˆ‘N\\ni pigi\\nâˆ‘N\\ni p2\\ni + âˆ‘N\\ni g2\\ni + Ïµ\\nwhere N is the number of voxels, pi and gi correspond to the predicted and\\nground truth labels per voxel respectively, and Ïµis added to avoid zero division.\\nWe also train a version of the model where the loss is optimized on the three\\nnested regions, whole tumor, tumor core and enhancing tumor together with the\\nprevious dice loss.\\n3.4 Post-Processing\\nThe proposed model shows several false positives in the form of small and sepa-\\nrated connected components. Therefore, we keep the two biggest connected com-\\nponents if their proportion is bigger than some threshold (obtained by analysing'),\n",
       " Document(metadata={'arxiv_id': '2009.12188v1', 'title': 'Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation', 'section': 'body', 'authors': 'Laura Mora Ballestar, Veronica Vilaplana'}, page_content='rated connected components. Therefore, we keep the two biggest connected com-\\nponents if their proportion is bigger than some threshold (obtained by analysing\\nthe training set), as some of the subjects may have several regions with unhealthy\\ntissue.\\n3.5 Uncertainty\\nThis yearâ€™s BraTS includes a third task to evaluate the model uncertainty and\\nreward methods with predictions that are: (a) conï¬dent when correct and (b)\\nuncertain when incorrect. In this work, we model the voxel-wise uncertainty of\\nour method at test time, using test time dropout (TTD) and test-time data\\naugmentation for epistemic and aleatoric uncertainty respectively.\\nWe compute epistemic uncertainty as proposed in Gal et.al [23], who uses\\ndropout as a Bayesian Approximation in order to simplify the task. Therefore,\\nthe idea is to use dropout both at training and testing time. The paper suggests\\nto repeat the prediction a few hundred times with random dropout. Then, the'),\n",
       " Document(metadata={'arxiv_id': '2009.12188v1', 'title': 'Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation', 'section': 'body', 'authors': 'Laura Mora Ballestar, Veronica Vilaplana'}, page_content='the idea is to use dropout both at training and testing time. The paper suggests\\nto repeat the prediction a few hundred times with random dropout. Then, the\\nï¬nal prediction is the average of all estimations and the uncertainty is modelled\\nby computing the variance of the predictions. In this work we perform B = 50\\niterations and use dropout with a 50% probability to zero out a channel. As\\nsaid, the uncertainty map is estimated with the variance for each sub-region\\nindependently. Let Yi =\\n{\\nyi\\n1,yi\\n2...yi\\nB\\n}\\nbe the vector that represents the i-th\\nvoxelâ€™s predicted labels, the voxel-wise uncertainty map, for each evaluation\\nregion, is obtained as the variance:\\nvar = 1\\nB\\nBâˆ‘\\nb=1\\n(yi\\nb âˆ’yi\\nmean)2\\nUncertainty can also be estimated with the entropy, as [19] showed. However,\\nthe entropy will provide a global measure instead of map for sub-region. In this\\ncase, the voxel-wise uncertainty is calculated as:\\nH(Yi|X) â‰ˆâˆ’\\nMâˆ‘\\nm=1\\nË†pi\\nmln(Ë†pi\\nm)6 L. Mora et al.\\nwhere Ë†pi'),\n",
       " Document(metadata={'arxiv_id': '2009.12188v1', 'title': 'Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation', 'section': 'body', 'authors': 'Laura Mora Ballestar, Veronica Vilaplana'}, page_content='the entropy will provide a global measure instead of map for sub-region. In this\\ncase, the voxel-wise uncertainty is calculated as:\\nH(Yi|X) â‰ˆâˆ’\\nMâˆ‘\\nm=1\\nË†pi\\nmln(Ë†pi\\nm)6 L. Mora et al.\\nwhere Ë†pi\\nm is the frequency of the m-th unique value in Yi and X represents\\nthe input image.\\nFinally, we model aleatoric uncertainty by applying the same augmentation\\ntechniques from training plus random Gaussian noise. The ï¬nal prediction and\\nuncertainty maps are computed following the same strategy as in the epistemic\\nuncertainty.\\n4 Results\\nThe model has been implemented in Pytorch [24] and trained on the GPI 1\\nservers, based on 2 Intel(R) Xeon(R) @ 2.40GHz CPUs using 16GB RAM and\\na 12GB NVIDIA GPU, using BraTS 2020 training dataset. We report results\\non both training (369 cases) and validation (125 cases) datasets. All results,\\nprediction and uncertainty maps, are uploaded to the CBICAs Image Processing\\nPortal (IPP)2 for evaluation of Dice score, Hausdorï¬€ distance (95th percentile),'),\n",
       " Document(metadata={'arxiv_id': '2009.12188v1', 'title': 'Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation', 'section': 'body', 'authors': 'Laura Mora Ballestar, Veronica Vilaplana'}, page_content='prediction and uncertainty maps, are uploaded to the CBICAs Image Processing\\nPortal (IPP)2 for evaluation of Dice score, Hausdorï¬€ distance (95th percentile),\\nsensitivity and speciï¬city per each class. Speciï¬c uncertainty evaluation metrics\\nare the ratio of ï¬ltered TN (FTN) and the ratio of ï¬ltered TP.\\n4.1 Segmentation\\nThe principal metrics to evaluate the segmentation performance are the Dice\\nScore, which is an overlap measure for pairwise comparison of segmentation\\nmask X and ground truth Y:\\nDSC = 2 âˆ— |Xâˆ©Y|\\n|X|+ |Y|\\nand the Hausdorï¬€ distance, which is the maximum distance of a set to the\\nnearest point in the other set, deï¬ned as:\\nDH(X,Y ) = max\\n{\\nsupxÏµX inf\\nyÏµY\\nd(x,y)),supyÏµY inf\\nxÏµX\\nd(x,y))\\n}\\nwhere sup represents the supremum and inf the inï¬mum. In order to have\\nmore robust results and to avoid issues with noisy segmentation, the evaluation\\nscheme uses the 95th percentile.\\nTable 1 and Table 2 show results for training and validation sets respectively.'),\n",
       " Document(metadata={'arxiv_id': '2009.12188v1', 'title': 'Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation', 'section': 'body', 'authors': 'Laura Mora Ballestar, Veronica Vilaplana'}, page_content='more robust results and to avoid issues with noisy segmentation, the evaluation\\nscheme uses the 95th percentile.\\nTable 1 and Table 2 show results for training and validation sets respectively.\\nWe show the segmentation maps obtain directly from our model and the ones\\nobtained from averaging the predictions from uncertainty estimation, annotated\\nwith (avg).\\nIn both sets, the best results are obtained with the V-Net model directly when\\npost-processing is applied. However, both Dice score and Hausdorï¬€ distance have\\nlower performance on the validation set, being more noticeable in the ET region,\\n1 The UPC Image and Video Processing Group (GPI) is a research group of the Signal\\nTheory and Communications department.\\n2 BraTS20 leaderboard: https://www.cbica.upenn.edu/BraTS20/Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation 7\\nTable 1: Segmentation Results on Training Dataset (369 cases).\\nMethod Dice Hausdorï¬€ (mm)\\nWT TC ET WT TC ET'),\n",
       " Document(metadata={'arxiv_id': '2009.12188v1', 'title': 'Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation', 'section': 'body', 'authors': 'Laura Mora Ballestar, Veronica Vilaplana'}, page_content='Table 1: Segmentation Results on Training Dataset (369 cases).\\nMethod Dice Hausdorï¬€ (mm)\\nWT TC ET WT TC ET\\nV-Net 0.8421 0.7837 0.6752 21.9354 11.7565 31.1815\\nV-Net+post 0.8513 0.7852 0.6765 16.8425 10.8923 30.6484\\nV-Net (avg) 0.8414 0.7727 0.6482 22.7816 11.8985 34.0967\\nV-Net+post (avg) 0.8342 0.7860 0.6635 19.2272 11.3911 34.2888\\nTable 2: Segmentation Results on Validation Dataset (125 cases)\\nMethod Dice Hausdorï¬€ (mm)\\nWT TC ET WT TC ET\\nV-Net 0.8368 0.7499 0.6159 26.4085 13.3398 49.7425\\nV-Net+post 0.8463 0.7526 0.6179 20.4073 12.1752 47.7020\\nV-Net (avg) 0.8335 0.7547 0.6215 26.1902 13.1222 50.1156\\nV-Net+post (avg) 0.8421 0.7503 0.6175 20.4005 12.9294 48.7698\\nwere it increments from 31 to 47. This high distance is caused because we obtain\\n13 predictions with the highest Hausdorï¬€ distance and 0 dice, indicating we are\\npredicting ET in tumors that should not have it.\\n4.2 Uncertainty\\nBraTS requires to upload three uncertainty maps, one for each subregion (WT,'),\n",
       " Document(metadata={'arxiv_id': '2009.12188v1', 'title': 'Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation', 'section': 'body', 'authors': 'Laura Mora Ballestar, Veronica Vilaplana'}, page_content='predicting ET in tumors that should not have it.\\n4.2 Uncertainty\\nBraTS requires to upload three uncertainty maps, one for each subregion (WT,\\nTC, ET) together with the prediction map. Moreover, uncertainty maps must be\\nnormalized from 0-100 such that â€0â€ represents the most certain prediction and\\nâ€100â€ represents the most uncertain. Moreover they measure the FTP deï¬ned\\nas FTP = ( TP100 âˆ’TPT)/TP100, where T represents the threshold used to\\nï¬lter the more uncertain values. The ratio of ï¬ltered true negatives (FTN) is\\ncalculated in a similar manner.\\nTable 3: Uncertainty Results on Training and Validation Dataset\\nMethod DICE AUC FTP RATIO AUC FTN RATIO AUC\\nWT TC ET WT TC ET WT TC ET\\n(train) V-Net+post 0.8506 0.7890 0.6779 0.0029 0.0104 0.0179 0.0008 0.0002 0.0002\\n(valid) V-Net+post 0.8505 0.7583 0.6274 0.0118 0.0515 0.0815 0.0027 0.0008 0.00058 L. Mora et al.\\nFig. 2: Training results on patients: 223 and 325 (top-bottom). Image order: (1)'),\n",
       " Document(metadata={'arxiv_id': '2009.12188v1', 'title': 'Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation', 'section': 'body', 'authors': 'Laura Mora Ballestar, Veronica Vilaplana'}, page_content='(valid) V-Net+post 0.8505 0.7583 0.6274 0.0118 0.0515 0.0815 0.0027 0.0008 0.00058 L. Mora et al.\\nFig. 2: Training results on patients: 223 and 325 (top-bottom). Image order: (1)\\nFlair (2) GT (3) Prediction (4) Average Prediction from uncertainty (5) WT\\nuncertainty map (6) TC uncertainty map (7) ET uncertainty map\\nFig. 3: Validation results on patients: 007 and 035 (top-bottom). Image order: (1)\\nFlair (2) Prediction (3) Average Prediction from uncertainty (4) WT uncertainty\\nmap (5) TC uncertainty map (5) ET uncertainty map\\nResults on Table 3 show that our model has low FTP and FTN, meaning\\nthat it is certain of those predictions.\\n4.3 Visual Analysis\\nFigure 2 and ï¬gure 3 show some visual results from the training and validation\\nsets respectively. As it can be seen, in some of the subjects we obtain a fairly\\ngood segmentation, whereas in others we have too many false positives in the\\nWT and ET regions. However, the uncertainty values show that the model is'),\n",
       " Document(metadata={'arxiv_id': '2009.12188v1', 'title': 'Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation', 'section': 'body', 'authors': 'Laura Mora Ballestar, Veronica Vilaplana'}, page_content='good segmentation, whereas in others we have too many false positives in the\\nWT and ET regions. However, the uncertainty values show that the model is\\nmore uncertain in the tumor surroundings and areas where the prediction has\\nbeen miss-classiï¬ed.\\n5 Discussion and Conclusions\\nIn this paper we use a V-Net architecture with minor modiï¬cations: number of\\nfeature maps and instance normalization instead of using batch normalization.Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation 9\\nResults, on both training and validation sets, prove that our model has severe\\nproblems to correctly segment whole tumor and, specially enchancing tumor.\\nIn both cases, we have a large number of false positives, as can be seen in the\\nHaurdorï¬€ distance results. In the case of ET, the distance is much bigger as a\\nsingle false positive voxel in a patient where no enhancing tumor is present in\\nthe ground truth results in a Dice score of 0 and a Hausdorï¬€ distance of approx-'),\n",
       " Document(metadata={'arxiv_id': '2009.12188v1', 'title': 'Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation', 'section': 'body', 'authors': 'Laura Mora Ballestar, Veronica Vilaplana'}, page_content='single false positive voxel in a patient where no enhancing tumor is present in\\nthe ground truth results in a Dice score of 0 and a Hausdorï¬€ distance of approx-\\nimately 373, which is the case for some of the subjects which has a dramatically\\neï¬€ect on the mean value.\\nIn order to improve the results, we plan on adding ResNet blocks to the\\nnetwork baseline as well as using the Generalised Dice Loss [26], more suited\\nfor unbalanced data. Moreover, we believe that some post-processing techniques\\nshould also improve the results.\\nReferences\\n1. B. H. Menze, A. Jakab, S. Bauer, J. Kalpathy-Cramer, K. Farahani, J. Kirby, et\\nal.: â€The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS)â€,\\nIEEE Transactions on Medical Imaging 34(10), 1993-2024 (2015) https://doi.org/\\n10.1109/TMI.2014.2377694\\n2. S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J.S. Kirby, et al.: â€Ad-\\nvancing The Cancer Genome Atlas glioma MRI collections with expert segmen-'),\n",
       " Document(metadata={'arxiv_id': '2009.12188v1', 'title': 'Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation', 'section': 'body', 'authors': 'Laura Mora Ballestar, Veronica Vilaplana'}, page_content='10.1109/TMI.2014.2377694\\n2. S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J.S. Kirby, et al.: â€Ad-\\nvancing The Cancer Genome Atlas glioma MRI collections with expert segmen-\\ntation labels and radiomic featuresâ€, Nature Scientiï¬c Data, 4:170117 (2017)\\nhttps://doi.org/10.1038/sdata.2017.117\\n3. S. Bakas, M. Reyes, A. Jakab, S. Bauer, M. Rempï¬‚er, A. Crimi, et al.: â€Identifying\\nthe Best Machine Learning Algorithms for Brain Tumor Segmentation, Progres-\\nsion Assessment, and Overall Survival Prediction in the BRATS Challengeâ€, arXiv\\npreprint arXiv:1811.02629 (2018)\\n4. S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J. Kirby, et al.,\\nâ€Segmentation Labels and Radiomic Features for the Pre-operative Scans\\nof the TCGA-GBM collectionâ€, The Cancer Imaging Archive, 2017. DOI:\\n10.7937/K9/TCIA.2017.KLXWJJ1Q\\n5. S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J. Kirby, et al.,\\nâ€Segmentation Labels and Radiomic Features for the Pre-operative Scans'),\n",
       " Document(metadata={'arxiv_id': '2009.12188v1', 'title': 'Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation', 'section': 'body', 'authors': 'Laura Mora Ballestar, Veronica Vilaplana'}, page_content='10.7937/K9/TCIA.2017.KLXWJJ1Q\\n5. S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J. Kirby, et al.,\\nâ€Segmentation Labels and Radiomic Features for the Pre-operative Scans\\nof the TCGA-LGG collectionâ€, The Cancer Imaging Archive, 2017. DOI:\\n10.7937/K9/TCIA.2017.GJQ7R0EF\\n6. Milletari, Fausto, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convo- lu-\\ntional neural networks for volumetric medical image segmentation. 3D Vision (3DV),\\n2016 Fourth International Conference on. IEEE, 2016.\\n7. Morgan, L Lloyd: The epidemiology of glioma in adults: A â€state of the scienceâ€\\nreview. Neuro-oncology vol.17 01-2015 https://doi.org/10.1093/neuonc/nou358\\n8. Armen Der Kiureghian and Ove Ditlevsen: Aleatory or epistemic? does it matter?\\nStructural safety, 31(2):105112, 2009.\\n9. Yarin Gal and Zoubin Ghahramani: Dropout as a bayesian approximation: Repre-\\nsenting model uncertainty in deep learning. arXiv preprint arXiv:1506.02142, 2015'),\n",
       " Document(metadata={'arxiv_id': '2009.12188v1', 'title': 'Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation', 'section': 'body', 'authors': 'Laura Mora Ballestar, Veronica Vilaplana'}, page_content='9. Yarin Gal and Zoubin Ghahramani: Dropout as a bayesian approximation: Repre-\\nsenting model uncertainty in deep learning. arXiv preprint arXiv:1506.02142, 2015\\n10. Konstantinos Kamnitsas, Christian Ledig, Virginia F.J. Newcombe, Joanna P.\\nSimpson, Andrew D. Kane, David K. Menon, Daniel Rueckert, Ben Glocker:\\nEï¬ƒcient multi-scale 3D CNN with fully connected CRF for accurate brain\\nlesion segmentation, Medical Image Analysis, Volume 36, 2017, pages 61-78,\\nhttps://doi.org/10.1016/j.media.2016.10.00410 L. Mora et al.\\n11. Casamitjana, A., Puch, S., Aduriz, A., Vilaplana, V., â€3D Convolutional Neural\\nNetworks for Brain Tumor Segmentation: a comparison of multi-resolution archi-\\ntecturesâ€. In: Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain\\nInjuries. BrainLes 2016. Lecture Notes in Computer Science, vol 10154. Springer,\\n2017.\\n12. Kamnitsas, K., Bai, W., Ferrante, E., McDonagh, S., Sinclair, M., Pawlowski, N:'),\n",
       " Document(metadata={'arxiv_id': '2009.12188v1', 'title': 'Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation', 'section': 'body', 'authors': 'Laura Mora Ballestar, Veronica Vilaplana'}, page_content='Injuries. BrainLes 2016. Lecture Notes in Computer Science, vol 10154. Springer,\\n2017.\\n12. Kamnitsas, K., Bai, W., Ferrante, E., McDonagh, S., Sinclair, M., Pawlowski, N:\\nEnsembles of Multiple Models and Architectures for Robust Brain Tumour Segmen-\\ntation in International MICCAI Brainlesion Workshop (Quebec, QC), 450462 arXiv\\npreprint arXiv:1711.01468, 2017\\n13. Casamitjana, A., Cat, M., Snchez, I., Combalia, M., Vilaplana, V., â€Cascaded V-\\nNet Using ROI Masks for Brain Tumor Segmentationâ€. In: Brainlesion: Glioma,\\nMultiple Sclerosis, Stroke and Traumatic Brain Injuries. BrainLes 2017. Lecture\\nNotes in Computer Science, vol 10670. Springer, 2018.\\n14. Andriy Myronenko: 3D MRI brain tumor segmentation using autoencoder regu-\\nlarization. arXiv preprint arXiv:1810.11654, 2016\\n15. Isensee, F., et al.: No new-net. International MICCAI Brainlesion Workshop, pp.\\n234244. Springer (2018)\\n16. Jiang Z., Ding C., Liu M., Tao D. (2020) Two-Stage Cascaded U-Net: 1st Place'),\n",
       " Document(metadata={'arxiv_id': '2009.12188v1', 'title': 'Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation', 'section': 'body', 'authors': 'Laura Mora Ballestar, Veronica Vilaplana'}, page_content='15. Isensee, F., et al.: No new-net. International MICCAI Brainlesion Workshop, pp.\\n234244. Springer (2018)\\n16. Jiang Z., Ding C., Liu M., Tao D. (2020) Two-Stage Cascaded U-Net: 1st Place\\nSolution to BraTS Challenge 2019 Segmentation Task. In: Crimi A., Bakas S. (eds)\\nBrainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries. Brain-\\nLes 2019. Lecture Notes in Computer Science, vol 11992. Springer, Cham\\n17. Zhao YX., Zhang YM., Liu CL. (2020) Bag of Tricks for 3D MRI Brain Tumor\\nSegmentation. In: Crimi A., Bakas S. (eds) Brainlesion: Glioma, Multiple Sclerosis,\\nStroke and Traumatic Brain Injuries. BrainLes 2019. Lecture Notes in Computer\\nScience, vol 11992. Springer, Cham\\n18. Natekar Parth, Kori Avinash, Krishnamurthi Ganapathy AUTHOR=Natekar\\nParth, Kori Avinash, Krishnamurthi Ganapathy: Demystifying Brain Tumor Seg-\\nmentation Networks: Interpretability and Uncertainty Analysis. Frontiers in Com-'),\n",
       " Document(metadata={'arxiv_id': '2009.12188v1', 'title': 'Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation', 'section': 'body', 'authors': 'Laura Mora Ballestar, Veronica Vilaplana'}, page_content='Parth, Kori Avinash, Krishnamurthi Ganapathy: Demystifying Brain Tumor Seg-\\nmentation Networks: Interpretability and Uncertainty Analysis. Frontiers in Com-\\nputational Neuroscience vol.14 page 6 https://doi.org/10.3389/fncom.2020.00006,\\n2020\\n19. Wang G., Li W., Ourselin S. and Vercauteren T. Automatic Brain Tumor\\nSegmentation Based on Cascaded Convolutional Neural Networks With Un-\\ncertainty Estimation. Frontiers in Computational Neuroscience vol.13 pages 56\\nhttps://doi.org/10.3389/fncom.2019.00056, 2019\\n20. McKinley R., Meier R., Wiest R. (2019) Ensembles of Densely-Connected CNNs\\nwith Label-Uncertainty for Brain Tumor Segmentation. In: Crimi A., Bakas S.,\\nKuijf H., Keyvan F., Reyes M., van Walsum T. (eds) Brainlesion: Glioma, Multiple\\nSclerosis, Stroke and Traumatic Brain Injuries. BrainLes 2018. Lecture Notes in\\nComputer Science, vol 11384. Springer\\n21. Dmitry Ulyanov and Andrea Vedaldi and Victor Lempitsky. Instance Normaliza-'),\n",
       " Document(metadata={'arxiv_id': '2009.12188v1', 'title': 'Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation', 'section': 'body', 'authors': 'Laura Mora Ballestar, Veronica Vilaplana'}, page_content='Sclerosis, Stroke and Traumatic Brain Injuries. BrainLes 2018. Lecture Notes in\\nComputer Science, vol 11384. Springer\\n21. Dmitry Ulyanov and Andrea Vedaldi and Victor Lempitsky. Instance Normaliza-\\ntion: The Missing Ingredient for Fast Stylization. arXiv preprint arXiv:1607.08022,\\n2016\\n22. Kamnitsas, K., Ledig, C., Newcombe, V.F., Simpson, J.P., Kane, A.D., Menon,\\nD.K., Rueckert, D., Glocker, B.: Eï¬ƒcient multi-scale 3D CNN with fully connected\\nCRF for accurate brain lesion segmentation. Med. Image Anal. 36 (2017) 6178\\n23. Yarin Gal and Zoubin Ghahraman.Dropout as a Bayesian Approximation: Rep-\\nresenting Model Uncertainty in Deep Learning. arXiv preprint arXiv:1506.02142,\\n2015\\n24. Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and\\nYang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and\\nAntiga, Luca and Lerer, Adam. Automatic diï¬€erentiation in PyTorch, NIPS-W 2017Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation 11'),\n",
       " Document(metadata={'arxiv_id': '2009.12188v1', 'title': 'Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation', 'section': 'body', 'authors': 'Laura Mora Ballestar, Veronica Vilaplana'}, page_content='Antiga, Luca and Lerer, Adam. Automatic diï¬€erentiation in PyTorch, NIPS-W 2017Brain Tumor Segmentation using 3D-CNNs with Uncertainty Estimation 11\\n25. Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. U-net: Convolu-\\ntional net- works for biomedical image segmentation. MICCAI. Springer, 2015.\\nhttps://doi.org/10.1007/978-3-319-24574-4 28\\n26. Sudre, C.H., Li, W., Vercauteren, T., Ourselin, S., Jorge Cardoso, M.Generalised\\nDice Overlap as a Deep Learning Loss Function for Highly Unbalanced Segmenta-\\ntions. Lecture Notes in Computer Science 240-248, Springer International Publishing\\n2017. https://doi.org/10.1007/978-3-319-67558-9 28'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'title_abstract', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='Title: Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models\\n\\nAbstract: Deep learning has emerged as a prominent field in recent literature, showcasing the introduction of models that utilize transfer learning to achieve remarkable accuracies in the classification of brain tumor MRI images. However, the majority of these proposals primarily focus on balanced datasets, neglecting the inherent data imbalance present in real-world scenarios. Consequently, there is a pressing need for approaches that not only address the data imbalance but also prioritize precise classification of brain cancer. In this work, we present a novel deep learning-based approach, called Transfer Learning-CNN, for brain tumor classification using MRI data. The proposed model leverages the predictive capabilities of existing publicly available models by utilizing their pre-trained weights and transferring those weights to the CNN. By leveraging a publicly available Brain MRI dataset, the experiment evaluated various transfer learning models for classifying different tumor types, including meningioma, glioma, and pituitary tumors. We investigate the impact of different loss functions, including focal loss, and oversampling methods, such as SMOTE and ADASYN, in addressing the data imbalance issue. Notably, the proposed strategy, which combines VGG-16 and CNN, achieved an impressive accuracy rate of 96%, surpassing alternative approaches significantly.'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer\\nLearning and Imbalance Handling in Deep Learning Models\\nRaza Imam1,3 Mohammed Talha Alam2,3\\n1Aligarh Muslim University, Aligarh, India\\n2Jamia Hamdard University, New Delhi, India\\n3Mohamed Bin Zayed University of Artificial Intelligence, Abu Dhabi, UAE\\nAbstract\\nDeep learning has emerged as a prominent field\\nin recent literature, showcasing the introduction\\nof models that utilize transfer learning to achieve\\nremarkable accuracies in the classification of brain\\ntumor MRI images. However, the majority of these\\nproposals primarily focus on balanced datasets, ne-\\nglecting the inherent data imbalance present in real-\\nworld scenarios. Consequently, there is a press-\\ning need for approaches that not only address the\\ndata imbalance but also prioritize precise classifi-\\ncation of brain cancer. In this work, we present a\\nnovel deep learning-based approach, called Trans-\\nfer Learning-CNN, for brain tumor classification'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='cation of brain cancer. In this work, we present a\\nnovel deep learning-based approach, called Trans-\\nfer Learning-CNN, for brain tumor classification\\nusing MRI data. The proposed model leverages the\\npredictive capabilities of existing publicly avail-\\nable models by utilizing their pre-trained weights\\nand transferring those weights to the CNN. By\\nleveraging a publicly available Brain MRI dataset,\\nthe experiment evaluated various transfer learn-\\ning models for classifying different tumor types,\\nincluding meningioma, glioma, and pituitary tu-\\nmors. We investigate the impact of different loss\\nfunctions, including focal loss, and oversampling\\nmethods, such as SMOTE and ADASYN, in ad-\\ndressing the data imbalance issue. Notably, the pro-\\nposed strategy, which combines VGG-16 and CNN,\\nachieved an impressive accuracy rate of 96%, sur-\\npassing alternative approaches significantly. Our\\ncode is available at Github.\\n1 INTRODUCTION\\nImpactful solutions are being offered by the context-aware'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='passing alternative approaches significantly. Our\\ncode is available at Github.\\n1 INTRODUCTION\\nImpactful solutions are being offered by the context-aware\\ndeployment of deep learning methodologies to enhance med-\\nical diagnostics. The World Health Organization (WHO)\\nstates that a correct diagnosis of a brain tumor entails its\\ndiscovery, localization, and classification based on its de-\\ngree, kind, and severity. This research comprises finding\\nthe tumor, grading it according to type and location, and\\nclassifying it according to grade in the diagnosis of brain\\ntumors using magnetic resonance imaging (MRI). This ap-\\nproach has experimented with using several models rather\\nthan a single model for classification task in order to cat-\\negorize brain MRI data [Veeramuthu et al., 2022]. Deep\\nlearning and transfer learning models have been proposed\\nwith higher accuracies in recent literature for classifying\\nBrain Tumor MRI images. However most of such proposals'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='learning and transfer learning models have been proposed\\nwith higher accuracies in recent literature for classifying\\nBrain Tumor MRI images. However most of such proposals\\nare focused on balanced data. Hence, approaches to account\\nfor the imbalance in data, as well as focusing on the precise\\nclassification of brain cancer in real world scenarios, are\\nneeded. We introduce a â€™Transfer Learning + CNN modelâ€™\\nrather of just â€™Transfer Learning model with Fine Tuningâ€™\\nand compared 8 of such Transfer Learning models using\\nvarious approaches on imbalance MRI images [Li et al.,\\n2022].\\nFigure 1: Conventional Classification of Brain Tumors from\\nMRI images using Convolutional Neural Networks\\nUsing three pathogenic forms of brain tumor (glioma, menin-\\ngioma, and pituitary tumor), we aim to propose an accurate\\nand automated classification scheme in this work. Towards\\nthis classification task, our goal is to empirically assess how\\nwell the most common benchmark models perform. Our'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='and automated classification scheme in this work. Towards\\nthis classification task, our goal is to empirically assess how\\nwell the most common benchmark models perform. Our\\ngoal of offering the finest classification model will open up\\nnew research directions in terms of choosing the right model\\nfor real-world brain tumor classification deployment. Utiliz-\\ning tested models, the acquired characteristics are classified.\\nThe approach of our experiments that we will be acquiring\\nis also represented in Figure 1. Subsequently, a thorough\\nPaper accepted at E-pi UAI workshop (UAI 2023).\\narXiv:2308.06821v1  [cs.CV]  13 Aug 2023assessment of the suggested system is performed utilizing\\nseveral effective evaluation metrics of classification tasks\\nalong with comparing several analytical factors, such as how\\nwell each model performs with less training samples from\\npracticality aspect and how overfitting with lower training\\nsamples affects performance of the classifier.\\n1.1 MOTIV ATION'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='well each model performs with less training samples from\\npracticality aspect and how overfitting with lower training\\nsamples affects performance of the classifier.\\n1.1 MOTIV ATION\\nEarly brain tumor identification and classification represent\\na significant area of research in the field of medical imag-\\ning. It helps in choosing the most appropriate line of action\\nfor treatments to save patientsâ€™ lives. In both children and\\nadults, brain tumors are regarded as one of the most severe\\ndisorders. Brain tumors account for 85% to 90% of all ma-\\njor malignancies of the Central Nervous System (CNS). An\\nestimated 11,700 people receive a brain tumor diagnosis\\neach year. For those with a malignant brain or CNS tumor,\\nthe 5-year survival rate is around 34% for males and 36%\\nfor women [Wang et al., 2020]. It is difficult to treat brain\\ntumors as we know that our brain has a very complex struc-\\nture having tissues that are linked to each other in a complex'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='for women [Wang et al., 2020]. It is difficult to treat brain\\ntumors as we know that our brain has a very complex struc-\\nture having tissues that are linked to each other in a complex\\nmanner. Often, producing MRI results is very difficult and\\ntime-consuming in underdeveloped nations due to a shortage\\nof skilled medical professionals and a lack of understanding\\nof malignancies.\\nDepending on the tumorâ€™s severityâ€”that is, its location,\\nsize, and typeâ€”different treatment methods are possible.\\nThe most common technique for treating brain tumors at the\\nmoment is surgery since it has no adverse implications on\\nthe brain. There are different medical imaging techniques\\nthat are used to view the internal structures of a human body\\nin order to discover any abnormalities Imam et al. [2022].\\nThe most often used of them to identify brain tumors is\\nMagnetic Resonance Imaging (MRI) since it can show ab-\\nnormalities that may not be seen or just dimly visible on'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='The most often used of them to identify brain tumors is\\nMagnetic Resonance Imaging (MRI) since it can show ab-\\nnormalities that may not be seen or just dimly visible on\\ncomputed tomography (CT) [Kibriya et al., 2022]. How-\\never, the rush of patients makes it difficult, chaotic, and\\nperhaps error-prone to manually review these images. Auto-\\nmated classification methods based on machine learning and\\nartificial intelligence have regularly outperformed manual\\nclassification in terms of accuracy in order to solve such\\nissues. Therefore, recommending a system that does detec-\\ntion and classification utilizing deep learning algorithms\\nemploying the above-mentioned benchmark models would\\nbe helpful for radiologists and other medical professionals.\\n1.2 CONTRIBUTIONS\\nIn this work, we focus on imbalance problem of Brain Tu-\\nmor Classification as it is a real-world scenario in deploy-\\nment. To solve this imbalance problem, we experiment with'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='1.2 CONTRIBUTIONS\\nIn this work, we focus on imbalance problem of Brain Tu-\\nmor Classification as it is a real-world scenario in deploy-\\nment. To solve this imbalance problem, we experiment with\\nseveral loss functions including focal loss. Along with this,\\na comparative study is conducted on the performance of\\ndifferent oversampling methods including augmentation,\\nSMOTE, and ADASYN. The models are experimented on\\nare 8 Transfer Learning-CNN models that incorporates the\\nbase Transfer learning model, followed by the integration\\nof proposed CNN layers. Following a detailed evaluation of\\neach Transfer Learning-CNN model, we finally conclude\\nVGG16-CNN is the novel proposal of this study.\\nâ€¢ Developed a Transfer learning-CNN framework for\\nbrain tumor MRI classification, utilizing pre-trained\\nmodels, where the produced weights transfer to an\\n8-layer CNN head for effective training.\\nâ€¢ Employed 8 different transfer learning models inte-\\ngrated with CNNs to increase the classification ac-'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='models, where the produced weights transfer to an\\n8-layer CNN head for effective training.\\nâ€¢ Employed 8 different transfer learning models inte-\\ngrated with CNNs to increase the classification ac-\\ncuracy on different types of brain cancer (no tumor,\\nglioma, meningioma, and pituitary cancer).\\nâ€¢ Experimented with 5 different approaches to deal with\\nimbalanced datasets such as- Changing loss functions:\\n(1) Focal loss (2) Cross Entropy, and Oversampling\\nmethods: (3) Data Augmentation, (4) SMOTE (5)\\nADASYN.\\nâ€¢ Assessed empirical evaluation of the models under the\\ndifferent approaches on metrics including Accuracy,\\nPrecision, Recall, and F1 Score.\\n2 RELATED WORKS\\nThe classification of brain tumors using MRI data has been\\nthe subject of numerous studies based on convolutional neu-\\nral networks in recent years. Many of these methods make\\nuse of hybrid approaches, and many also offer technical\\nvariations on widely used deep learning models Alam et al.'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='ral networks in recent years. Many of these methods make\\nuse of hybrid approaches, and many also offer technical\\nvariations on widely used deep learning models Alam et al.\\n[2021]. In [Deepak and Ameer, 2019], the authors describe\\na classification method for the 3-class classification issue\\nthat combines transfer learning with GoogleNets. They used\\na number of evaluation metrics, with a mean classification\\naccuracy of 98%, including area under the curve (AUC),\\nprecision, recall, F-score, and specificity. Abd El Kader\\net al. [2021] have made use of the benefit of differential\\nCNN by generating extra differential feature maps. With the\\ncapacity to categorize a sizable database of pictures with\\nhigh accuracy of about 98%, their approach demonstrated\\na considerable improvement for the brain MRI classifica-\\ntion problem. In [Raza et al., 2022], the authors proposed\\na hybrid architecture by adopting GoogleNet as a based\\nCNN model while tweaking the last few layers for the spe-'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='tion problem. In [Raza et al., 2022], the authors proposed\\na hybrid architecture by adopting GoogleNet as a based\\nCNN model while tweaking the last few layers for the spe-\\ncific Brain Tumor Classification. Their proposal attained the\\nclassification accuracy of 99.67%. Moreover, Irmak [2021]\\nconducted a multi-class study of Brain Tumor MRI Images\\nas they propose a CNN model for early diagnoses purposes\\nwith fully optimized framework. Compared to the conven-\\ntional CNN models, their solution attained an accuracy of\\n98.14%.Figure 2: Difference between traditional Transfer Learning vs Transfer Learning-CNN\\n3 METHODOLOGY\\n3.1 TRANSFER LEARNING AND CNN\\nWe observed that using the most recent transfer learning\\nmodels that have been presented in the most recent liter-\\nature produced relatively decent results when trained on\\nthe balanced Brain MRI datasets. This empirical evaluation\\nof brain tumor classification is focused on the Imbalance'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='ature produced relatively decent results when trained on\\nthe balanced Brain MRI datasets. This empirical evaluation\\nof brain tumor classification is focused on the Imbalance\\nproblem. However, in training such Transfer Learning mod-\\nels with fine tuning on the imbalance dataset did not show\\nas good results in comparison to the Transfer models inte-\\ngrated with newly added CNN layers [Li et al., 2022]. Since\\nthe later case demonstrated improved predicted accuracies\\non the unbalanced dataset, we experiment with 8 of these\\nTransfer Learning models that are combined with newly\\nadded CNN layers. The ImageNet training dataset was used\\nto train the traditional pre-train models, namely VGG16,\\nEfficientNetB0, EfficientNetB3, ResNet50, DenseNet201,\\nMobileNet, GoogleNet, XceptionNet. For effective train-\\ning, each modelâ€™s trained parameter weights were trans-\\nferred to the newly added layers of the additional CNN\\nmodel. The CNN model was then fine-tuned using the'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='ing, each modelâ€™s trained parameter weights were trans-\\nferred to the newly added layers of the additional CNN\\nmodel. The CNN model was then fine-tuned using the\\nbrain tumor augmented MRI data set for final classification\\ninto 4 classes. CNN, VGG16-CNN, EfficientNetB0-CNN,\\nEfficientNetB3-CNN, ResNet50-CNN, DenseNet201-CNN,\\nMobileNet-CNN, GoogleNet-CNN, XceptionNet-CNN are\\nthe resultant models we empirically evaluated the training\\nperformance on the imbalanced dataset using various ap-\\nproaches.\\n3.2 PROPOSED METHOD\\nThe proposed Transfer Learning-CNN (VGG-CNN) model\\nincorporates the base Transfer learning model, i.e., VGG16,\\nfollowed by the basic structure of the CNNs. It might take\\ndays to weeks to train a raw CNN completely from scratch,\\nmaking it a difficult task [Raza et al., 2022]. Therefore, it\\nwould be preferable to train the suggested deep learning\\napproach using a pre-trained classifier rather than creating\\na new deep learning classifier from start [Petmezas et al.,'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='would be preferable to train the suggested deep learning\\napproach using a pre-trained classifier rather than creating\\na new deep learning classifier from start [Petmezas et al.,\\n2022]. Additionally, in terms of predictive accuracy, our\\nTransfer Learning models combined with CNN performed\\nsignificantly better on the imbalanced Brain MRI dataset\\nthan the base CNN as well as the base Transfer Learning\\nmodel with Fine tuning. In order to do this, we chose the\\nmost accurate current Transfer Learning model on the bal-\\nanced dataset, VGG16, as our foundation model. CNN ar-\\nchitecture VGG16 was employed to win the 2014 ILSVR\\n(Imagenet) competition. It is regarded as one of the best\\nvision model architectures created to date. The most distinc-\\ntive feature of VGG16 is that it prioritized having convolu-\\ntion layers of 3x3 filters with a stride 1 and always utilized\\nthe same padding and maxpool layer of 2x2 filters with a\\nstride 2. Throughout the entire architecture, convolution and'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='tion layers of 3x3 filters with a stride 1 and always utilized\\nthe same padding and maxpool layer of 2x2 filters with a\\nstride 2. Throughout the entire architecture, convolution and\\nmax pool layers are arranged in the same manner. It con-\\ncludes with two fully connected layers (FC) and a softmax\\nfor output.\\nMoreover, the final four layers of VGG-16 were dropped\\nfrom the planned VGG-CNN architecture, and 8 new levels\\nwere added in their place. After these adjustments, there\\nwere 148 layers overall instead of 144. Addition to the lay-\\ners of transfer learning model, the first convolution layer of\\nthe CNN portion, uses a filter size of 3x3 with the depth of\\n32, which immediately reduces the image size followed by\\na max pooling layer of 2x2 filter. The second convolution\\nlayer has a depth of 64 with the same filter size of 3x3,\\nfollowed by a max pooling layer of 2x2 filter. Again, a 2D\\nconvolution layer of depth 32 with filter size 3x3 is incorpo-'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='layer has a depth of 64 with the same filter size of 3x3,\\nfollowed by a max pooling layer of 2x2 filter. Again, a 2D\\nconvolution layer of depth 32 with filter size 3x3 is incorpo-\\nrated followed by a Flatten layer. Dense layer of 16 hidden\\nunits with relu activation and the final output layer of 4 units\\nwith softmax activation are used. Compared to the initial 4\\nlayers of transfer learning models, adding the extra convo-\\nlutional layers in the CNN portion gave us more detailed,\\naccurate, and robust features. These 3 later convolutional\\nlayers extracted high-level features compared with the initial\\nlayer, which extracted low-level features. The choice of this\\nadditional 8-layer architecture as the head on the pre-trainedFigure 3: The proposed framework for the classification of brain tumor MRIs. Here, the pre-trained CNN models (VGG16-\\nCNN, EfficientNetB0-CNN, EfficientNetB3-CNN, ResNet50-CNN, DenseNet201-CNN, MobileNet-CNN, GoogleNet-CNN,'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='CNN, EfficientNetB0-CNN, EfficientNetB3-CNN, ResNet50-CNN, DenseNet201-CNN, MobileNet-CNN, GoogleNet-CNN,\\nand XceptionNet-CNN) are trained using the Imagenet dataset, and the produced weights of these pre-trained models are\\nindividually transferred to the suggested CNN model for effective training\\nnetwork was based on several considerations. We aimed\\nto strike a balance between model complexity and perfor-\\nmance. This 8-layer architecture was found to be effective\\nin capturing the necessary features for our specific task,\\nwhile not being overly complex, which could lead to overfit-\\nting or increased computational requirements. Our methodâ€™s\\nsuperior robustness is supported by evaluating various ar-\\nchitectures during experimentation, where the 8-layer head\\nconsistently demonstrated better performance across multi-\\nple metrics and outperformed other architectures in diverse\\ndatasets and scenarios. Hence, compared to the traditional'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='consistently demonstrated better performance across multi-\\nple metrics and outperformed other architectures in diverse\\ndatasets and scenarios. Hence, compared to the traditional\\nTransfer learning model, the proposed Transfer Learning-\\nCNN model achieves higher accuracies in comparison as\\nmore intricate, exclusionary, and deep features have been\\nacquired in the proposed method. The specific architecture\\nof the proposed methodology is also shown in Figure 3.\\n3.3 EXPERIMENTS\\nThe experiments have been performed on a combination of\\nthree distinct datasets taken from Kaggle namely figshare,\\nSARTAJ and Br35H. This accumulated custom imbalance\\ndataset contains about 4200 images of human brain MRI\\nimages which are classified into 4 classes: no-tumor (1760),\\nglioma (858), meningioma (1265) and pituitary (341) can-\\ncer, whereas no-tumor class images were taken from the\\nBr35H dataset. The dataset images have been normalized\\nand resized as a part of pre-processing according to the input'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='cer, whereas no-tumor class images were taken from the\\nBr35H dataset. The dataset images have been normalized\\nand resized as a part of pre-processing according to the input\\nsize of the proposed model, i.e., 128x128. In addition, the\\naccumulated dataset is divided into train and test with 90:10\\nratio for validation purpose. The results mentioned in this\\nstudy only apply to the test set. The testing dataset is then\\nused to evaluate all of the predictive metric claims made\\nabout the models in the results and discussion section.\\nPredictive modeling is challenged by imbalanced classifica-\\ntions because the majority of machine learning methods for\\nclassification were devised on the premise that there should\\nbe a similar number of samples in each category. The distri-\\nbution of examples among the recognized classes is skewed\\nor biased in imbalance classes. These kind of issues with a\\nstrong to weak bias in the dataset are typical in real-world'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='bution of examples among the recognized classes is skewed\\nor biased in imbalance classes. These kind of issues with a\\nstrong to weak bias in the dataset are typical in real-world\\napplications. As a result of such imbalance, models perform\\npoorly in terms of prediction, particularly for the minor-\\nity class [Deepak and Ameer, 2019], [Deepak and Ameer,\\n2022]. This is a complication since, in general, the minority\\nclass is more significant and, as a result, the issue is more\\nsusceptible to errors in classifying for the minority class\\nthan the classes with the higher samples. In the real-world\\nscenarios of a model deployment, imbalanced classification\\nis a huge complication [BadÅ¾a and Barjaktarovi Â´c, 2020].\\nHence, in our experiments, we tried to empirically evalu-\\nate majority of the existing solutions that can be used for\\nimbalanced or long tail classification problems.\\nFocal Loss and Cross Entropy. For each of the 8 deep\\nlearning models, we comparatively examined 5 distinct ap-'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='imbalanced or long tail classification problems.\\nFocal Loss and Cross Entropy. For each of the 8 deep\\nlearning models, we comparatively examined 5 distinct ap-\\nproaches on each of them in order to address the imbalance\\nscenarios that arise during the classification of brain tumors.\\nWe explore with changing the loss function in our modelingbecause itâ€™s one of the main reasons that simple examples\\nwill divert training in an unbalanced class scenario. We\\ncompared the results of test accuracy on our models by\\nchanging Cross Entropy with Focal loss in order to obtain\\nbetter results. Focal Loss, one of the effective solutions in\\nterms of loss functions, deals with this imbalance issue and\\nis created in a way that allows the network to concentrate\\non training the challenging cases by lowering the loss (or\\n\"down-weight\") for the simple examples. In other words,\\nFocal loss reduces the importance of the simple examples\\nand emphasizes the difficult ones; thus, smaller class counts'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='\"down-weight\") for the simple examples. In other words,\\nFocal loss reduces the importance of the simple examples\\nand emphasizes the difficult ones; thus, smaller class counts\\nwould have heavier weights in errors than cross-entropy.\\nThe Cross-Entropy loss is multiplied by a modifying factor\\nin focal loss [Chatterjee et al., 2022]. When a sample is in-\\ncorrectly classified, the modulating factor is close to 1, p is\\nlow, and the loss is unchanged. The modulating factor gets\\ncloser to zero as p approaches 1 and the loss for correctly\\ncategorized samples gets down-weighted.\\nF L(Pt) =âˆ’Î±t(1 âˆ’ Pt)Î³log(Pt)\\nHere, Pt = P if Î³ = 1, else (1-P); where Î³ = 0, Î±t = 1 then\\nFL is Cross Entropy Loss.\\nIn addition, oversampling, which uses artificial data cre-\\nation to increase the number of samples in the data set,\\nis one of the most fundamental approaches in the state of\\nthe art to handle the imbalance problem, other than loss\\nfunctions [Xie et al., 2022]. By producing synthetic obser-'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='is one of the most fundamental approaches in the state of\\nthe art to handle the imbalance problem, other than loss\\nfunctions [Xie et al., 2022]. By producing synthetic obser-\\nvations based on the minority observations already there,\\noversampling aims to grow the minority class so that the\\ndata set becomes balanced. In our experimental evaluations,\\nwe used 3 oversampling approaches, namely Augmentation,\\nSMOTE, and ADASYN methods.\\nData Augmentation. Data augmentation is used to expand\\nthe volume of data by introducing slightly altered versions\\nof either existing data or brand-new synthetic data that is\\nderived from available data. It also serves as a regulariza-\\ntion term and aids in lowering overfitting when a model is\\nbeing trained [Irmak, 2021]. In our experiments, we tried to\\naugment the samples from minority classes to achieve the\\nFigure 4: Data distribution of different classes for imbal-\\nanced Brain MRI dataset\\nFigure 5: Train-test split of the original dataset'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='augment the samples from minority classes to achieve the\\nFigure 4: Data distribution of different classes for imbal-\\nanced Brain MRI dataset\\nFigure 5: Train-test split of the original dataset\\nnumber of the samples equal to the majority class. The aug-\\nmentation tasks we applied to such minority class samples\\nare brightness, contrast, and sharpness alteration each with\\na random intensity of 80% to 120%. This was followed by\\nthe normalization process as a part of preprocessing.\\nSMOTE. Second oversampling method we implemented\\non our dataset was SMOTE (Synthetic Minority Oversam-\\npling Technique). By creating artificial data samples that are\\nmarginally different from the current data points on the basis\\nof the existing data points, SMOTE performs oversampling.\\nFollowing is how the SMOTE algorithm operates:\\nâ€¢ A random sample is picked from the minority class.\\nâ€¢ Locate the k nearest neighbors for the observations in\\nthis minority class sample.'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='Following is how the SMOTE algorithm operates:\\nâ€¢ A random sample is picked from the minority class.\\nâ€¢ Locate the k nearest neighbors for the observations in\\nthis minority class sample.\\nâ€¢ The vector (line) between the existing data point and\\none of those neighbors will then be determined using\\nthat neighbor.\\nâ€¢ The vector is then multiplied by a randomized range\\nunit between 0 and 1.\\nâ€¢ We combine this with the existing data point to get the\\nsynthetic data sample in the space.\\nADASYN. ADASYN is an improvised version of SMOTE.\\nIt accomplishes the same things as SMOTE, albeit slightly\\nbetter. It then adds a random tiny value to the points to make\\nit more realistic after creating those synthetic samples [Raza\\net al., 2022]. In other words, the sampleâ€™s variance, or degree\\nFigure 6: Distribution of the incorporated approaches to\\nresolve data imbalance problemFigure 7: Implementation of data imbalance approaches\\nthroughout the phases of experimentation on models'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='Figure 6: Distribution of the incorporated approaches to\\nresolve data imbalance problemFigure 7: Implementation of data imbalance approaches\\nthroughout the phases of experimentation on models\\nFigure 8: Working of SMOTE vs ADASYN algorithm\\nof dispersion, is a little higher than its linear correlation\\nto the parent. Moreover, ADASYN adaptively change the\\nweights of different minority samples to compensate for\\nthe skewed distributions. Difference between SMOTE and\\nADASYN oversampling approaches is also shown in Figure\\n8.\\nWe examined each of our 8 TL-CNN models on each of\\nthese above-mentioned 5 approaches - Focal Loss, Cross En-\\ntropy, Data Augmentation, SMOTE, and ADASYN, with an\\naim to achieve higher test accuracy on the imbalance dataset.\\nFocal loss is initially compared with the Cross Entropy\\nloss on unbalanced dataset, whereas Data Augmentation,\\nSMOTE, and ADASYN are compared on the unbalanced\\ndataset too but with cross entropy as these later 3 approaches'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='loss on unbalanced dataset, whereas Data Augmentation,\\nSMOTE, and ADASYN are compared on the unbalanced\\ndataset too but with cross entropy as these later 3 approaches\\nare oversampling approaches [Abd El Kader et al., 2021].\\nWith the parameters listed in Table 1, we conducted tests\\nusing a trial-and-error methodology. In order to determine\\nthe best convergence for each TL-CNN model, we contin-\\nuously tracked the development of the training validation\\naccuracy and error. With a mini-batch size of 20 images and\\nan initial learning rate ranging between 0.001 and 0.0001\\ndepending on the model, we utilized mini-batch GD to train\\nthe TL-CNN models. To get the results that are explained in\\nthe following section, the evaluated models were trained on\\n5â€“12 epochs for brain tumor classification.\\n3.4 EV ALUATION\\nFor the typical assessment of a classifier, numerous perfor-\\nmance metrics are specified, including Classification Ac-\\ncuracy, Precision, Recall, and F1-Score Alam et al. [2022].'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='3.4 EV ALUATION\\nFor the typical assessment of a classifier, numerous perfor-\\nmance metrics are specified, including Classification Ac-\\ncuracy, Precision, Recall, and F1-Score Alam et al. [2022].\\nThe evaluation employs criteria besides the total classifica-\\ntion accuracy due to the unbalanced dataset. The mean of\\ncorrectly classified samples from each class is used to com-\\nTable 1: Several parameters utilised during the training\\nphase\\nParameter Values\\nMini Batch Size 20\\nNumber of Epochs 5 to 12\\nLearning Rate(s) 0.001 to 0.0001\\nShuffle True\\nSteps Per Epoch Train Size (1600) / 20\\nDense Activations RELU\\nOptimizer ADAM\\npute balanced accuracy. A classâ€™s F-score is determined by\\ntaking the harmonic mean of its precision and recall values.\\nP recision= True Positive\\nTrue Positive + False Positive\\nRecall = True Positive\\nTrue Positive + False Negative\\nAccuracy = TN + TP\\nTN + FP + TP + FN\\nF1 Score = 2Ã— Precision Ã— Recall\\nPrecision + Recall\\nWhen the test dataset has an identical amount of samples'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='Recall = True Positive\\nTrue Positive + False Negative\\nAccuracy = TN + TP\\nTN + FP + TP + FN\\nF1 Score = 2Ã— Precision Ã— Recall\\nPrecision + Recall\\nWhen the test dataset has an identical amount of samples\\nfrom each category, classification accuracy is a useful met-\\nric to assess performance. Nevertheless, the dataset we aim\\nto use for this categorization problem under discussion is\\nadequately imbalance. This calls for a more thorough assess-\\nment of the suggested system using more evaluation metrics.\\nTo evaluate the effectiveness of our tumor categorization\\nmethod, we employed other metrics including Precision,\\nRecall, and F1-Score. Tables 2, 3, 4, and 5 presents a sum-\\nmary of such metrics on each of the 8 models in terms of\\nvarious approaches we employed to resolve imbalance clas-\\nsification. The effectiveness of the relevant implemented\\nmodels that have been implemented in our categorization\\nare empirically evaluated using these classification metrics.'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='sification. The effectiveness of the relevant implemented\\nmodels that have been implemented in our categorization\\nare empirically evaluated using these classification metrics.\\nDue to the existence of imbalance among the 4 classes, we\\nalso showed individual precision values for each class that\\nhave been implemented using different approach.\\nPrecision, recall (or sensitivity), and F1-Score are crucial\\nindicators, and they are determined using the relations de-\\nscribed above. For each class, the harmonic mean of recall\\nand precision yields the F1-score, another significant statis-\\ntical classification metric. If we obtain high F1-Score values\\nacross all classes, this suggests that we have successfully\\nidentified samples free of any class of brain tumor. Due to\\nthe existence of imbalance among the 4 classes, we also\\nshowed individual precision values for each class that have'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='identified samples free of any class of brain tumor. Due to\\nthe existence of imbalance among the 4 classes, we also\\nshowed individual precision values for each class that have\\nbeen implemented using different approach. We also intendTable 2: Evaluation of Integrated frameworks performance on imbalanced dataset when trained with Cross Entropy Loss.\\nThe space complexity of each model is the number of trainable parameters. M = Million. The space complexity increases\\nwith increasing number of trainable parameters. The time complexity is the training time (in hours) of the models\\nModel Space\\nComplexity\\nTime (hrs)\\nComplexityLR Acc. Precision Recall F1 Score\\nP M G NT P M G NT P M G NT\\nCNN 440k 0.091 0.001 0.8125 1.00 0.67 0.73 0.98 0.48 0.91 0.51 0.96 0.65 0.77 0.60 0.97\\nVGG16-CNN 15.7M 0.26 0.0001 0.83 0.73 0.66 0.94 0.99 0.73 0.66 0.41 1.00 0.60 0.78 0.57 1.00\\nEfficientNetB0-CNN 4.05M 0.27 0.001 0.37 0.00 0.17 0.00 0.39 0.04 0.04 0.00 0.86 0.00 0.07 0.00 0.54'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='VGG16-CNN 15.7M 0.26 0.0001 0.83 0.73 0.66 0.94 0.99 0.73 0.66 0.41 1.00 0.60 0.78 0.57 1.00\\nEfficientNetB0-CNN 4.05M 0.27 0.001 0.37 0.00 0.17 0.00 0.39 0.04 0.04 0.00 0.86 0.00 0.07 0.00 0.54\\nEfficientNetB3-CNN 10.8M 0.47 1.6e-5 0.60 0.50 0.60 0.38 0.65 0.06 0.46 0.27 0.96 0.11 0.52 0.31 0.78\\nResNet50-CNN 23.5M 0.31 0.001 0.78 1.00 0.87 0.60 0.83 0.39 0.59 0.82 0.97 0.56 0.70 0.69 0.90\\nDenseNet201-CNN 51.8M 0.57 0.001 0.47 0.33 0.43 0.67 0.49 0.23 0.32 0.05 0.82 0.27 0.37 0.10 0.61\\nMobileNet-CNN 3M 0.126 0.001 0.30 0.00 0.30 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.46 0.00 0.00\\nGoogleNet-CNN 6M 0.14 0.0001 0.77 0.71 0.64 0.58 0.97 0.65 0.67 0.56 0.96 0.68 0.65 0.57 0.97\\nXceptionNet-CNN 25M 0.15 0.0001 0.83 0.90 0.66 0.89 0.99 0.61 0.95 0.42 0.99 0.73 0.78 0.57 0.99\\nTable 3: Evaluation of Integrated frameworks performance on imbalanced dataset when trained with Focal Loss\\nModel Space\\nComplexity\\nTime (hrs)\\nComplexityLR Acc. Precision Recall F1 Score\\nP M G NT P M G NT P M G NT'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='Model Space\\nComplexity\\nTime (hrs)\\nComplexityLR Acc. Precision Recall F1 Score\\nP M G NT P M G NT P M G NT\\nCNN 440k 0.084 0.001 0.80 0.88 0.68 0.61 0.99 0.45 0..81 0.56 0.98 0.60 0.74 0.59 0.98\\nVGG16-CNN 15.7M 0.267 0.0001 0.85 0.93 0.68 0.88 1.00 0.87 0.98 0.37 0.98 0.90 0.81 0.52 0.99\\nEfficientNetB0-CNN 4.05M 0.27 0.001 0.42 0.00 0.00 0.00 0.42 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.59\\nEfficientNetB3-CNN 10.8M 0.47 1.6e-5 0.32 0.00 0.22 0.18 0.37 0.00 0.14 0.09 0.62 0.00 0.17 0.12 0.46\\nResNet50-CNN 23.5M 0.31 0.001 0.62 0.69 0.45 1.00 0.97 0.71 0.95 0.17 0.59 0.70 0.61 0.29 0.73\\nDenseNet201-CNN 51.8M 0.57 0.001 0.66 0.57 0.50 0.24 0.96 0.39 0.68 0.15 0.94 0.46 0.58 0.19 0.95\\nMobileNet-CNN 3M 0.13 0.001 0.42 0.00 0.00 0.00 0.42 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.59\\nGoogleNet-CNN 6M 0.14 0.0001 0.78 0.73 0.62 0.74 0.97 0.61 0.88 0.33 0.97 0.67 0.72 0.46 0.97\\nXceptionNet-CNN 25M 0.20 0.0001 0.81 0.83 0.62 0.84 1.00 0.61 0.92 0.33 1.00 0.70 0.74 0.48 1.00'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='GoogleNet-CNN 6M 0.14 0.0001 0.78 0.73 0.62 0.74 0.97 0.61 0.88 0.33 0.97 0.67 0.72 0.46 0.97\\nXceptionNet-CNN 25M 0.20 0.0001 0.81 0.83 0.62 0.84 1.00 0.61 0.92 0.33 1.00 0.70 0.74 0.48 1.00\\nTable 4: Evaluation of Integrated frameworks performance on imbalanced dataset when trained with SMOTE oversampling\\nModel Space\\nComplexity\\nTime (hrs)\\nComplexityLR Acc. Precision Recall F1 Score\\nP M G NT P M G NT P M G NT\\nCNN 440k 0.11 0.001 0.82 0.86 0.71 0.71 0.93 0.77 0.83 0.45 0.99 0.81 0.76 0.55 0.96\\nVGG16-CNN 15.7M 0.52 0.0001 0.86 0.96 0.71 0.83 0.99 0.81 0.96 0.50 0.97 0.88 0.81 0.62 0.98\\nEfficientNetB0-CNN 4.05M 0.45 0.001 0.47 0.00 0.38 0.00 0.51 0.00 0.41 0.00 0.82 0.00 0.39 0.00 0.63\\nEfficientNetB3-CNN 10.8M 1.04 1.6e-5 0.50 0.00 0.39 0.00 0.88 0.00 0.97 0.00 0.51 0.00 0.55 0.00 0.64\\nResNet50-CNN 23.5M 0.77 0.001 0.66 1.00 0.97 0.37 0.96 0.19 0.27 0.95 0.89 0.32 0.42 0.54 0.92\\nDenseNet201-CNN 51.8M 2.08 0.001 0.86 0.96 0.70 0.97 0.99 0.77 0.98 0.45 0.99 0.86 0.82 0.61 0.99'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='ResNet50-CNN 23.5M 0.77 0.001 0.66 1.00 0.97 0.37 0.96 0.19 0.27 0.95 0.89 0.32 0.42 0.54 0.92\\nDenseNet201-CNN 51.8M 2.08 0.001 0.86 0.96 0.70 0.97 0.99 0.77 0.98 0.45 0.99 0.86 0.82 0.61 0.99\\nMobileNet-CNN 3M 0.27 0.001 0.42 0.00 0.00 0.00 0.42 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.59\\nGoogleNet-CNN 6M 0.14 0.0001 0.79 0.88 0.68 0.57 1.00 0.74 0.66 0.72 0.93 0.81 0.67 0.64 0.96\\nXceptionNet-CNN 25M 0.27 0.0001 0.83 0.91 0.66 0.82 1.00 0.68 0.94 0.41 0.99 0.78 0.78 0.55 1.00\\nTable 5: Evaluation of Integrated frameworks performance on imbalanced dataset when trained with ADASYN oversampling\\nModel Space\\nComplexity\\nTime (hrs)\\nComplexityLR Acc. Precision Recall F1 Score\\nP M G NT P M G NT P M G NT\\nCNN 440k 0.12 0.001 0.83 0.83 0.75 0.64 0.97 0.77 0.77 0.59 1.00 0.80 0.76 0.61 0.98\\nVGG16-CNN 15.7M 0.60 0.0001 0.87 0.91 0.75 0.81 1.00 0.94 0.92 0.54 0.99 0.92 0.82 0.65 0.99\\nEfficientNetB0-CNN 4.05M 0.63 0.001 0.42 0.00 0.00 0.00 0.42 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.59'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='VGG16-CNN 15.7M 0.60 0.0001 0.87 0.91 0.75 0.81 1.00 0.94 0.92 0.54 0.99 0.92 0.82 0.65 0.99\\nEfficientNetB0-CNN 4.05M 0.63 0.001 0.42 0.00 0.00 0.00 0.42 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.59\\nEfficientNetB3-CNN 10.8M 1.11 1.6e-5 0.52 1.00 0.36 0.33 0.86 0.13 0.15 0.87 0.69 0.23 0.21 0.48 0.69\\nResNet50-CNN 23.5M 0.83 0.001 0.42 0.00 0.00 0.00 0.42 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.59\\nDenseNet201-CNN 51.8M 1.67 0.001 0.90 1.00 0.85 0.94 0.85 0.77 0.90 0.76 1.00 0.87 0.87 0.84 0.95\\nMobileNet-CNN 3M 0.30 0.001 0.42 0.00 0.00 0.00 0.42 0.00 0.00 0.00 1.00 0.00 0.00 0.00 0.59\\nGoogleNet-CNN 6M 0.15 0.0001 0.77 0.82 0.66 0.49 1.00 0.74 0.58 0.63 0.97 0.78 0.62 0.55 0.98\\nXceptionNet-CNN 25M 0.31 0.0001 0.86 0.96 0.70 0.90 0.99 0.81 0.97 0.45 0.99 0.88 0.81 0.60 0.99Table 6: Accuracy Comparison on Unbalanced Dataset us-\\ning several imbalance solution approaches. The highlighted\\nrow indicates the best performing version of our proposed\\nTransfer Learning-CNN approach'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='ing several imbalance solution approaches. The highlighted\\nrow indicates the best performing version of our proposed\\nTransfer Learning-CNN approach\\nModel CE Focal Loss Augment CE SMOTE CE ADASYN CE\\nCNN 0.8125 0.80 0.91 0.82 0.83\\nVGG16-CNN 0.83 0.85 0.96 0.86 0.87\\nEfficientNetB0-CNN 0.37 0.42 0.31 0.47 0.42\\nEfficientNetB3-CNN 0.60 0.32 0.32 0.50 0.52\\nResNet50-CNN 0.78 0.62 0.80 0.66 0.42\\nDenseNet201-CNN 0.47 0.66 0.94 0.86 0.90\\nMobileNet-CNN 0.30 0.42 0.23 0.42 0.42\\nGoogleNet-CNN 0.77 0.78 0.80 0.79 0.77\\nXceptionNet-CNN 0.83 0.81 0.94 0.83 0.86\\nto compare several analytical factors, such as how well each\\nmodel performs with less training samples from practical-\\nity aspect and how overfitting with lower training samples\\naffects performance of the classifier.\\n4 RESULTS AND DISCUSSION\\nThis study aimed to appropriately categorize four distinct\\nkind of brain tumor MRI scans. We compared 5 different\\nmethodologiesâ€”augmentation, focal loss, SMOTE, and'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='4 RESULTS AND DISCUSSION\\nThis study aimed to appropriately categorize four distinct\\nkind of brain tumor MRI scans. We compared 5 different\\nmethodologiesâ€”augmentation, focal loss, SMOTE, and\\nADASYN â€” while using 8 transfer learning + CNN models.\\nWe used a variety of evaluation criteria, including accuracy,\\nprecision, recall, and F1-score, to evaluate the effectiveness\\nof our suggested model. This unbalanced dataset was\\ncategorized using deep learning models that have already\\nbeen trained as well as the suggested transfer learning +\\nCNN model, with 90% of the dataset being used for training\\nand 10% being utilized for testing. The performance of\\ndeep learning networks for classification can be measured\\nusing various methods. In the CNN process, classification\\ntasks are frequently carried out using a confusion matrix.\\nThe accuracy comparison on imbalanced dataset utilized in\\nthis research is shown in the Table 6.\\nThe result of the above experiments showed that VGG-16'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='The accuracy comparison on imbalanced dataset utilized in\\nthis research is shown in the Table 6.\\nThe result of the above experiments showed that VGG-16\\nperformed best overall across all techniques for the accuracy\\nevaluation metric. With augmentation, 0.96 was the highest\\naccuracy ever attained overall. In this instance, MobileNet\\nand EfficientNets had the worst results with an overall\\naccuracy of 0.23 and 0.31, respectively. For the relevant\\nunbalanced dataset on VGG16, EfficientNets, DenseNet201,\\nMobileNet, and GoogleNet, it was found that focal loss\\nobtained higher accuracies than cross entropy loss when\\nthe loss functions were compared having values 0.85, 0.66,\\n0.42 and 0.78, respectively. With the cross-entropy loss, the\\nremaining models fared better. Data augmentation produced\\nthe overall best accuracy of the techniques used to address\\nthe data imbalance issue.\\nIn all classes where VGG-16 + CNN with data augmentation\\noutperformed other approaches, we computed the average'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='the overall best accuracy of the techniques used to address\\nthe data imbalance issue.\\nIn all classes where VGG-16 + CNN with data augmentation\\noutperformed other approaches, we computed the average\\nprecision, recall, and F1 scores. To summarize, despite hav-\\ning far less layers than other models, transfer learning using\\nthe VGG-16 model performed the best overall. The perfor-\\nmance measures employing various transfer learning + CNN\\nmodels on our unbalanced dataset are compared in Figure\\n9. Here, we note that performance on an augmented dataset\\nwith cross-entropy loss outperforms alternative methods\\nacross a range of evaluation parameters.\\n5 CONCLUSION AND FUTURE WORK\\nMost computer-aided modeling techniques for the inves-\\ntigation of medical image data heavily rely on the CNN\\nmodel for precise and reliable medical image classification\\nImam et al. [2023]. We have proposed a deep learning-based\\ndetection and identification technique for classifying brain'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='model for precise and reliable medical image classification\\nImam et al. [2023]. We have proposed a deep learning-based\\ndetection and identification technique for classifying brain\\ntumors in our research study. With the help of brain tumor\\nMRI imaging data, we have examined various transfer learn-\\ning models for the classification of tumor kinds, including\\nmeningioma, glioma, and pituitary. We have added various\\ntransfer learning techniques, followed by a CNN model\\nfor each method, to improve the CNN modelâ€™s predictive\\npower. According to the experimental findings, the proposed\\nstrategy, which combines VGG-16 and CNN, has a 96% ac-\\ncuracy rate which is significantly higher when compared\\nwith other approaches.\\nThe proposed methodâ€™s notable prediction accuracy was\\nmostly due to data augmentation; further contributing as-\\npects included altering the number of layers, using optimiz-\\ners, and experimenting with various activation functions. In'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='mostly due to data augmentation; further contributing as-\\npects included altering the number of layers, using optimiz-\\ners, and experimenting with various activation functions. In\\nthe future, we will concentrate on optimizing the VGG16\\nand DenseNet models through rigorous GridSearchCV and\\nhyperparameter tuning, which requires a lot of effort and\\ncomputing power. We also want to focus on MobileNet due\\nto its simplicity and increased use in real-world settings.\\nACKNOWLEDGEMENT\\nWe sincerely thank Dr. Rao Anwer, our course advisor at\\nMohamed Bin Zayed University of Artificial Intelligence,\\nfor his invaluable guidance and support throughout this\\nwork. Additionally, we extend our special appreciation to\\nMohamed Bin Zayed University of Artificial Intelligence\\nfor providing us with the required computational resources\\nto conduct the experiments featured in this study. Their\\nsupport has been pivotal in the successful completion of our'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='for providing us with the required computational resources\\nto conduct the experiments featured in this study. Their\\nsupport has been pivotal in the successful completion of our\\nresearch.Figure 9: Performance comparison using various transfer learning models + CNN on different techniques for handling data\\nimbalance\\nReferences\\nIsselmou Abd El Kader, Guizhi Xu, Zhang Shuai, Sani\\nSaminu, Imran Javaid, and Isah Salim Ahmad. Differen-\\ntial deep convolutional neural network model for brain\\ntumor classification. Brain Sciences, 11(3):352, 2021.\\nMohammed Talha Alam, Syed Ubaid, Shakil, Sha-\\nhab Saquib Sohail, Maryam Nadeem, Shiraz Hussain,\\nand Jamshed Siddiqui. Comparative analysis of ma-\\nchine learning based filtering techniques using movielens\\ndataset. Procedia Computer Science, 194:210â€“217, 2021.\\ndoi: https://doi.org/10.1016/j.procs.2021.10.075.\\nMohammed Talha Alam, Shahab Saquib Sohail, Syed Ubaid,\\nShakil, Zafar Ali, Mohammad Hijji, Abdul Khader Jilani'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='doi: https://doi.org/10.1016/j.procs.2021.10.075.\\nMohammed Talha Alam, Shahab Saquib Sohail, Syed Ubaid,\\nShakil, Zafar Ali, Mohammad Hijji, Abdul Khader Jilani\\nSaudagar, and Khan Muhammad. Its your turn, are you\\nready to get vaccinated? towards an exploration of vaccine\\nhesitancy using sentiment analysis of instagram posts.\\nMathematics, 10(22), 2022. doi: 10.3390/math10224165.\\nMilica M BadÅ¾a and Marko Ë‡C BarjaktaroviÂ´c. Classification\\nof brain tumors from mri images using a convolutional\\nneural network. Applied Sciences, 10(6):1999, 2020.\\nSoumick Chatterjee, Faraz Ahmed Nizamani, Andreas NÃ¼rn-\\nberger, and Oliver Speck. Classification of brain tumours\\nin mr images using deep spatiospatial models. Scientific\\nReports, 12(1):1â€“11, 2022.\\nS Deepak and PM Ameer. Brain tumor classification using\\ndeep cnn features via transfer learning. Computers in\\nbiology and medicine, 111:103345, 2019.\\nS Deepak and PM Ameer. Brain tumor categorization from\\nimbalanced mri dataset using weighted loss and deep'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='biology and medicine, 111:103345, 2019.\\nS Deepak and PM Ameer. Brain tumor categorization from\\nimbalanced mri dataset using weighted loss and deep\\nfeature fusion. Neurocomputing, 2022.\\nRaza Imam, Kaushal Kumar, Syed Mehran Raza, Rumi\\nSadaf, Faisal Anwer, Noor Fatima, Mohammad Nadeem,\\nMohamed Abbas, and Obaidur Rahman. A system-\\natic literature review of attribute based encryptionin health services. Journal of King Saud Univer-\\nsity - Computer and Information Sciences, 34(9):6743â€“\\n6774, 2022. doi: https://doi.org/10.1016/j.jksuci.2022.06.\\n018. URL https://www.sciencedirect.com/\\nscience/article/pii/S1319157822002269.\\nRaza Imam, Muhammad Huzaifa, and Mohammed El-\\nAmine Azz. On enhancing the robustness of vision trans-\\nformers: Defensive diffusion, 2023.\\nEmrah Irmak. Multi-classification of brain tumor mri im-\\nages using deep convolutional neural network with fully\\noptimized framework. Iranian Journal of Science and\\nTechnology, Transactions of Electrical Engineering, 45'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='ages using deep convolutional neural network with fully\\noptimized framework. Iranian Journal of Science and\\nTechnology, Transactions of Electrical Engineering, 45\\n(3):1015â€“1036, 2021.\\nHareem Kibriya, Rashid Amin, Asma Hassan Alshehri,\\nMomina Masood, Sultan S Alshamrani, and Abdullah\\nAlshehri. A novel and effective brain tumor classification\\nmodel using deep feature fusion and famous machine\\nlearning classifiers. Computational Intelligence and Neu-\\nroscience, 2022, 2022.\\nJian Ping Li, Shakir Khan, Mohammed Ali Alshara,\\nReemiah Muneer Alotaibi, CobbinahBernard Mawuli,\\net al. Dacbt: deep learning approach for classification of\\nbrain tumors using mri data in iot healthcare environment.\\nScientific Reports, 12(1):1â€“14, 2022.\\nGeorgios Petmezas, Grigorios-Aris Cheimariotis, Leandros\\nStefanopoulos, Bruno Rocha, Rui Pedro Paiva, Aggelos K\\nKatsaggelos, and Nicos Maglaveras. Automated lung\\nsound classification using a hybrid cnn-lstm network and\\nfocal loss function. Sensors, 22(3):1232, 2022.'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='Katsaggelos, and Nicos Maglaveras. Automated lung\\nsound classification using a hybrid cnn-lstm network and\\nfocal loss function. Sensors, 22(3):1232, 2022.\\nAsaf Raza, Huma Ayub, Javed Ali Khan, Ijaz Ahmad,\\nAhmed S. Salama, Yousef Ibrahim Daradkeh, Danish\\nJaveed, Ateeq Ur Rehman, and Habib Hamam. A hybrid\\ndeep learning-based approach for brain tumor classifica-\\ntion. Electronics, 11(7):1146, 2022.\\nA Veeramuthu, S Meenakshi, G Mathivanan, Ketan Kotecha,\\nJatinderkumar R Saini, V Vijayakumar, and V Subra-\\nmaniyaswamy. Mri brain tumor image classification using\\na combined feature and image-based classifier. Frontiers\\nin Psychology, 13, 2022.\\nTao Wang, Changhua Lu, Mei Yang, Feng Hong, and Chun\\nLiu. A hybrid method for heartbeat classification via\\nconvolutional neural networks, multilayer perceptrons\\nand focal loss. PeerJ Computer Science, 6:e324, 2020.\\nYuting Xie, Fulvio Zaccagna, Leonardo Rundo, Claudia\\nTesta, Raffaele Agati, Raffaele Lodi, David Neil Man-'),\n",
       " Document(metadata={'arxiv_id': '2308.06821v1', 'title': 'Optimizing Brain Tumor Classification: A Comprehensive Study on Transfer Learning and Imbalance Handling in Deep Learning Models', 'section': 'body', 'authors': 'Raza Imam, Mohammed Talha Alam'}, page_content='and focal loss. PeerJ Computer Science, 6:e324, 2020.\\nYuting Xie, Fulvio Zaccagna, Leonardo Rundo, Claudia\\nTesta, Raffaele Agati, Raffaele Lodi, David Neil Man-\\nners, and Caterina Tonon. Convolutional neural network\\ntechniques for brain tumor classification (from 2015 to\\n2022): Review, challenges, and future perspectives. Di-\\nagnostics, 12(8):1850, 2022.'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'title_abstract', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='Title: Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision\\n\\nAbstract: In this study, we focus on brain tumor sequence registration between pre-operative and follow-up Magnetic Resonance Imaging (MRI) scans of brain glioma patients, in the context of Brain Tumor Sequence Registration challenge (BraTS-Reg 2022). Brain tumor registration is a fundamental requirement in brain image analysis for quantifying tumor changes. This is a challenging task due to large deformations and missing correspondences between pre-operative and follow-up scans. For this task, we adopt our recently proposed Non-Iterative Coarse-to-finE registration Networks (NICE-Net) - a deep learning-based method for coarse-to-fine registering images with large deformations. To overcome missing correspondences, we extend the NICE-Net by introducing dual deep supervision, where a deep self-supervised loss based on image similarity and a deep weakly-supervised loss based on manually annotated landmarks are deeply embedded into the NICE-Net. At the BraTS-Reg 2022, our method achieved a competitive result on the validation set (mean absolute error: 3.387) and placed 4th in the final testing phase (Score: 0.3544).'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision Mingyuan Meng1, Lei Bi1, Dagan Feng1,2, and Jinman Kim1 1 School of Computer Science, The University of Sydney, Sydney, Australia. 2 Med-X Research Institute, Shanghai Jiao Tong University, Shanghai, China. lei.bi@sydney.edu.au'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='Abstract. In this study, we focus on brain tumor sequence registration between pre-operative and follow-up Magnetic Resonance Imaging (MRI) scans of brain glioma patients, in the context of Brain Tumor Sequence Registration challenge (BraTS-Reg 2022). Brain tumor registration is a fundamental requirement in brain image analysis for quantifying tumor changes. This is a challenging task due to large deformations and missing correspondences between pre-operative and follow-up scans. For this task, we adopt our recently proposed Non-Iterative Coarse-to-finE registration Networks (NICE-Net) â€“ a deep learning-based method for coarse-to-fine registering images with large deformations. To overcome missing correspondences, we extend the NICE-Net by introducing dual deep supervision, where a deep self-supervised loss based on image simi-larity and a deep weakly-supervised loss based on manually annotated land-marks are deeply embedded into the NICE-Net. At the BraTS-Reg 2022, our method'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='deep self-supervised loss based on image simi-larity and a deep weakly-supervised loss based on manually annotated land-marks are deeply embedded into the NICE-Net. At the BraTS-Reg 2022, our method achieved a competitive result on the validation set (mean absolute error: 3.387) and placed 4th in the final testing phase (Score: 0.3544). Keywords: Image Registration, Coarse-to-fine Networks, Deep Supervision.'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='1 Introduction Deformable image registration aims to establish a dense, non-linear correspondence between a pair of images, which is a crucial step in a variety of clinical tasks such as organ atlas creation and tumor monitoring [1]. Due to pathological changes (e.g., tumor growth) or inter-patient anatomy variations, medical images usually carry many non-linear local deformations. Therefore, unlike the commonly used natural image registration tasks such as photo stitching [2], medical image analysis heavily relies on deformable image registration. However, deformable image registration is still an intractable problem due to image shape and appearance variations [3], especially for image pairs containing pathology-affected tissue changes.  To establish a fair benchmark environment for deformable registration methods, Baheti et al. [4] organized a Brain Tumor Sequence Registration challenge (BraTS-Reg 2022), focusing on brain tumor sequence registration between pre-operative and'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='deformable registration methods, Baheti et al. [4] organized a Brain Tumor Sequence Registration challenge (BraTS-Reg 2022), focusing on brain tumor sequence registration between pre-operative and follow-up Magnetic Resonance Imaging (MRI) scans of brain glioma patients. Brain tumor registration is clinically important as it can advance the understanding of glio-2'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='mas and aids in analyzing the tissue resulting in tumor relapse [5]. However, this is a challenging task because: (i) brain tumors usually cause large deformations in brain anatomy, and (ii) there are missing correspondences between the tumor in the pre-operative scan and the resection cavity in the follow-up scan [4]. Traditional methods attempt to solve deformable registration as an iterative optimi-zation problem [6], which is usually time-consuming and has inspired a tendency toward faster deep registration methods based on deep learning [7]. To register image pairs with large deformations, coarse-to-fine deep registration methods were widely used and are regarded as the state-of-the-art [8-13]. Typically, coarse-to-fine registra-tion was implemented by iteratively warping an image with multiple cascaded net-works [8, 9] or with multiple network iterations [10]. Recently, non-iterative coarse-to-fine registration methods were proposed to perform coarse-to-fine registration with a'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='multiple cascaded net-works [8, 9] or with multiple network iterations [10]. Recently, non-iterative coarse-to-fine registration methods were proposed to perform coarse-to-fine registration with a single network in a single iteration [11-13], which have demonstrated state-of-the-art registration accuracy even when compared with methods using multiple cascaded networks or multiple network iterations. In this study, we adopt our recently proposed deep registration method â€“ Non-Iterative Coarse-to-finE registration Networks (NICE-Net) [13]. The NICE-Net per-forms multiple steps of coarse-to-fine registration in a single network iteration, which is optimized for image pairs with large deformations and has shown state-of-the-art performance on inter-patient brain MRI registration [13]. However, the NICE-Net was not optimized for brain tumor registration with missing correspondences. We extend the NICE-Net by introducing dual deep supervision, where a deep self-supervised loss based on'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='the NICE-Net was not optimized for brain tumor registration with missing correspondences. We extend the NICE-Net by introducing dual deep supervision, where a deep self-supervised loss based on image similarity and a deep weakly-supervised loss based on manually annotated landmarks (ground truth) are deeply embedded into each coarse-to-fine registration step of the NICE-Net. The deep self-supervised loss can leverage the information within image appearance (e.g., texture and intensity pattern), while the deep weakly-supervised loss can leverage ground truth information to overcome the missing correspondences between pre-operative and follow-up scans.'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='2 Materials and Methods 2.1 Dataset and Data Preprocessing The BraTS-Reg 2022 dataset was curated from multiple institutions, containing pairs of pre-operative and follow-up brain MRI scans of the same patient diagnosed and treated for gliomas. The training set contains 140 pairs of multi-parametric MRI scans along with landmark annotations. The multi-parametric MRI sequences include native T1-weighted (T1), contrast-enhanced T1-weighted (T1CE), T2-weighted (T2), and T2 Fluid Attenuated Inversion Recovery (FLAIR) images. The landmark annotations include the landmark coordinates in the baseline scan and their corresponding coordi-nates in the follow-up scan, which were manually annotated by clinical experts and are regarded as ground truth (refer to [4] for more details about landmark annota-tions). The validation set contains 20 pairs of MRI scans and was provided to validate the developed registration methods. Finally, the registration methods are submitted in a containerized form to'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='The validation set contains 20 pairs of MRI scans and was provided to validate the developed registration methods. Finally, the registration methods are submitted in a containerized form to be evaluated on a hidden testing set. 3'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='We preprocessed the provided images with the following steps: (i) we cropped and padded each MRI image from 240Ã—240Ã—155 to 144Ã—192Ã—160 with the coordinates of (48:192, 32:224, -5:155), and (ii) we normalized each MRI image within the range of 0 and 1 through min-max normalization.'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='2.2 Non-iterative Coarse-to-fine Registration Networks Image registration aims to find a spatial transformation ! that warps a moving image \"! to a fixed image \"\", so that the warped image \"!âˆ˜! is spatially aligned with the fixed image \"\". In this study, the \"! and \"\" are two volumes defined in a 3D spatial domain Î©âŠ‚â„#, and the ! is parameterized as a displacement field [14]. Our method is based on our recently proposed NICE-Net (Non-Iterative Coarse-to-finE registra-tion Networks) [13]. The architecture of the NICE-Net is shown in Fig. 1, which con-sists of a feature learning encoder and a coarse-to-fine registration decoder. We em-ployed the NICE-Net to perform four steps of coarse-to-fine registration. At the \\'$% step for \\'âˆˆ{1,2,3,4}, a transformation !& is produced, with the !\\' as the coarsest transformation and the !( as the finest transformation. We created two image pyra-mids as the input, downsampling the \"\" and \"! with trilinear interpolation by a factor of 0.5((*&) to obtain'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='coarsest transformation and the !( as the finest transformation. We created two image pyra-mids as the input, downsampling the \"\" and \"! with trilinear interpolation by a factor of 0.5((*&) to obtain \"\"& and \"!& for \\'âˆˆ{1,2,3,4} with \"\"(=\"\" and \"!(=\"!. In addition, the coordinates of the landmark in the \"\" and \"! are denoted as 4\" and 4!. Along with the downsampling of \"\" and \"!, the 4\" and 4! were also downsampled as 4\"& and 4!& for \\'âˆˆ{1,2,3,4} with 4\"(=4\" and 4!(=4!.'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='Fig. 1. Architecture of the NICE-Net that performs four steps of coarse-to-fine registration. +Element-wisesummation\\n!!\\n3Ã—3Ã—3 Convolution,1Ã—1Ã—1stride3Ã—3Ã—3 Convolution,2Ã—2Ã—2strideUpsamplingandrescaleÃ—2Skipconnection\\n!\" #! + +#\" ##\\n$#! $#$\\nW%%\"\\nâ„‚Concatenation\\nâ„‚ W%%#â„‚\\nFeatureLearningEncoderCoarse-to-fineRegistrationDecoder\\nUpsamplingÃ—2WWarping\\n+#$$#\"\\nW%%$â„‚\\nâ„’&\\'()/â„’*\\'+,%%-,%)-,\\'%-,\\')-4'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='The feature learning encoder has two paths to separately extract features from the \"\" and \"!, which is different from the deep registration methods that learn coupled features from the concatenated \"\" and \"! [8, 9, 14, 15]. Specifically, the encoder has two identical, weight-shared paths that take \"\" and \"! as input. Each path consists of five successive 3Ã—3Ã—3 convolutional layers, followed by LeakyReLU activation with parameter 0.2. Except for the first convolutional layer, each convolutional layer has a stride of 2 to reduce the resolution of feature maps. The coarse-to-fine registration decoder performs four steps of registration in a coarse-to-fine manner. Specifically, the decoder has five successive 3Ã—3Ã—3 convo-lutional layers to cumulate features from different sources, followed by LeakyReLU activation with parameter 0.2. Except for the last convolutional layer, an upsampling layer is used after each convolutional layer to increase the resolution of feature maps by a factor of'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='LeakyReLU activation with parameter 0.2. Except for the last convolutional layer, an upsampling layer is used after each convolutional layer to increase the resolution of feature maps by a factor of 2. The first convolutional layer is used to cumulate features from the encoder, while the first (coarsest) registration step is performed at the second convo-lutional layer and produces the !\\'. The !\\' is upsampled by a factor of 2 (as !\\'6) and then warps the \"!,. The warped image \"!,âˆ˜!\\'6 and the !\\'6 are fed into a convolu-tional layer to extract features and then are leveraged at the second registration step. The second registration step produces a displacement field based on the cumulated'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='Fig. 2. Exemplified registration results with NICE-Net. Red boxes highlight the regions with major changes, gradually being closer to the fixed image !! after each registration step. The same red bounding box has been placed at the same location for better visual comparison. \\n!! !\"!!âˆ˜##!!âˆ˜#$!!âˆ˜#%!!âˆ˜#&\\nT1T1CET2FLAIR\\n5 \\nfeatures at the third convolution layer, and voxel-wisely add the displacement field to !\\'6 to obtain !,. We repeat this process until !( is obtained. During training, all the output !\\', !,, !#, and !( are supervised by a deep self-supervised loss â„’-./\" and/or a deep weakly-supervised loss â„’0.12 (detailed in Section 2.3 and Section 2.4). During inference, only !( is used to warp the \"! to align with the \"\". We present exemplified registration results of the NICE-Net in Fig. 2, which shows that the NICE-Net can perform coarse-to-fine registration to make the moving image \"!  gradually closer to the fixed image \"\".'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='2.3 Inter-patient Pretraining As the provided training set is relatively small (140 image pairs), we pretrained our networks with inter-patient registration to avoid overfitting. Specifically, all images in the training set were shuffled and randomly paired, resulting in a total of 280Ã—279 inter-patient image pairs. Then, the networks were pretrained with these inter-patient image pairs using a deep self-supervised loss â„’-./\" as follows: â„’-./\"=âˆ’âˆ‘\\',(\"#$)(&3\\' :;;0(\"!&âˆ˜!&,\\t\"\"&),                                (1) where the :;;0 is the local normalized cross-correlation [16] with window size ?#. The â„’-./\" calculates the negative :;;0 between the fixed and warped images at four registration steps, which measures the image similarity without ground truth labels.  In addition, we imposed L2 regularization on the !& to encourage its smoothness. Consequently, the total pretraining loss â„’45. is defined as: â„’65.=â„’-./\"+âˆ‘\\',(\"#$)(&3\\' (âˆ‘A|âˆ‡!&(D)|A,p âˆˆ Î© ).                           (2)'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='2.4 Intra-patient Training with Dual Deep Supervision After the inter-patient pretraining, our networks were further trained for intra-patient registration with the 140 intra-patient image pairs in the training set. In addition to the deep self-supervised loss â„’-./\" used during pretraining, we also introduced a deep weakly-supervised loss â„’0.12 as follows: â„’0.12=âˆ‘2((*&)(&3\\' EFG(4!&âˆ˜!&,\\t4\"&),                                 (3) where the EFG is the mean square error between the coordinates of two sets of land-marks. The â„’0.12 penalizes the distance between the fixed and warped landmarks at four registration steps, which leverages the information within the landmark labels to overcome the missing correspondences between pre-operative and follow-up scans. In addition, as the landmark labels are sparse, the displacement fields should be smooth so that the landmark labels can impose more influence on the displacement fields. Therefore, in addition to the L2 regularization used during'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='are sparse, the displacement fields should be smooth so that the landmark labels can impose more influence on the displacement fields. Therefore, in addition to the L2 regularization used during pretraining, we adopted an additional regularization loss that explicitly penalizes the negative Jacobi-an determinants [17], making the total regularization loss â„’5.8 as:  6'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='â„’5.8=âˆ‘\\',(\"#$)(&3\\' [Iâˆ™KL(!&)+âˆ‘A|âˆ‡!&(D)|A,p âˆˆ Î© ],                       (4) where the KL is regularization loss penalizing the negative Jacobian determinants of !& (refer to [17]) and the I is a regularization parameter that adjusts the smoothness of displacement fields. Finally, the total training loss â„’$51&9 is defined as: â„’:51&9=â„’-./\"+Nâ„’0.12+Oâ„’5.8,                                      (5) where the N and O are two parameters that adjust the magnitude of each loss term. \\n2.5 Pair-specific Fine-tuning We fine-tuned the trained networks for each inferred image pair and then used the fine-tuned networks for inference. The loss used for pair-specific fine-tuning is: â„’;&9.=â„’-./\"+Oâ„’5.8.                                                (6) As ground truth labels are not required, pair-specific fine-tuning can be used for any unseen image pairs and this has been demonstrated to improve the registration accu-racy of deep registration methods [14,18].'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='2.6 Implementation Details Our method was implemented using Keras with a Tensorflow backend on a 12 GB Titan V GPU. We used an Adam optimizer with a batch size of 1. Our networks were (inter-patient) pretrained for 50,000 iterations with a learning rate of 10-4 and then were (intra-patient) trained for another 14,000 iterations (100 epochs) with a learning rate of 10-5. We trained four networks for four MRI sequences (T1, T1CE, T2, and FLAIR) and combined them by averaging their outputs. During inference, for each inferred image pair, we fine-tuned the trained networks for 20 iterations with a learn-ing rate of 10-5. The O and N were set as 1.0 and 0.01 to ensure that the â„’-./\", Oâ„’5.8, and Nâ„’0.12 had close values, while the I and ? were optimized based on validation results. Our method achieved the best validation results when I=10-4 and ?=3.0.'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='2.7 Evaluation Metrics The registration accuracy was evaluated based on manually annotated landmarks in terms of Mean Absolute Error (MAE) and Robustness. MAE is calculated between the landmark coordinates in the pre-operative scan and in the warped follow-up scan, where a lower MAE generally indicates more accurate registration. Robustness is a successful-rate metric in the range of [0, 1], describing the percentage of landmarks improving their MAE after registration. In addition, the smoothness of the displace-ment field was evaluated by the number of negative Jacobian determinants (NJD). As the ! is smooth and invertible at the voxel P where the Jacobian determinant is posi-tive (|K!(P)|>0) [19], a lower NJD indicates a smoother displacement field. 7'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='3 Results and Discussion Our validation results are summarized in Table 1. We trained four networks with four MRI sequences (T1, T1CE, T2, and FLAIR) and report their validation results respec-tively. Among the four networks, the network using T1CE achieved the best valida-tion results (MAE: 3.486; Robustness: 0.818), followed by the networks using T1 (MAE: 3.917; Robustness: 0.785), FLAIR (MAE: 4.127; Robustness: 0.787), and T2 (MAE: 4.156; Robustness: 0.748). We combined these networks by averaging their outputs, in which we first combined all four networks and then removed the networks with relatively lower validation results one by one, resulting in three network ensem-bles (T1/T1CE/T2/FLAIR, T1/T1CE/FLAIR, and T1/T1CE). We found that all three ensembles achieved better validation results and produced smoother displacement fields, which suggests that combining different MRI sequences improves brain tumor registration. Combining more MRI sequences consistently improved NJDs as'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='results and produced smoother displacement fields, which suggests that combining different MRI sequences improves brain tumor registration. Combining more MRI sequences consistently improved NJDs as averag-ing more displacement fields together naturally resulted in a smoother displacement field. However, combining more MRI sequences cannot guarantee better registration accuracy. Among different ensembles, the T1/T1CE ensemble achieved the best vali-dation results (MAE: 3.392; Robustness: 0.827), while further combining T2 and FLAIR networks resulted in lower validation results. In addition, combining the T1/T1CE networks by 0.3/0.7-weighted averaging further improved the performance and achieved our best validation results (MAE: 3.387; Robustness: 0.831). Therefore, we submitted this ensemble to be evaluated on the testing set, which made us place 4th in the final testing phase (Score: 0.3544) [4]. We also attempted to train a single net-work with multi-channel image pairs'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='this ensemble to be evaluated on the testing set, which made us place 4th in the final testing phase (Score: 0.3544) [4]. We also attempted to train a single net-work with multi-channel image pairs (concatenating MRI sequences into multiple channels), but this approach resulted in worse validation results.  In addition, we performed an ablation study to explore the contributions of dual deep supervision and pair-specific fine-tuning. In the ablation study, all four MRI sequences were used, and the deep weakly-supervised loss â„’0.12 and/or the pair-specific fine-tuning process were excluded. The MAE results of the ablation study are shown in Table 2. Since pair-specific fine-tuning was not performed on the training Table 1. Validation results of our method using different MRI sequences. MRI sequence MAE Robustness NJD T1 3.917 0.785 94.70 T1CE 3.486 0.818 68.65 T2 4.156 0.748 69.70 FLAIR 4.127 0.787 100.00 T1/T1CE/T2/FLAIR 3.445 0.826 0.30 T1/T1CE/FLAIR 3.432 0.826 0.65 T1/T1CE 3.392'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='sequence MAE Robustness NJD T1 3.917 0.785 94.70 T1CE 3.486 0.818 68.65 T2 4.156 0.748 69.70 FLAIR 4.127 0.787 100.00 T1/T1CE/T2/FLAIR 3.445 0.826 0.30 T1/T1CE/FLAIR 3.432 0.826 0.65 T1/T1CE 3.392 0.827 2.65 T1/T1CE (weighted by 0.3/0.7) 3.387 0.831 3.60 Bold: the lowest MAE, the highest Robustness, and the lowest NJD are in bold.  8'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='set, the corresponding results are missing in Table 2. We found that using both â„’-./\" and â„’0.12 (i.e. dual deep supervision) resulted in lower MAE than merely using â„’-./\". This is because ground truth information (landmark labels) was leveraged through the â„’0.12, which helped to find the existing correspondences between images and thus relieved the challenges introduced by missing correspondences. However, when â„’0.12 was used, the training MAE (=1.026) became much lower than the vali-dation MAE (=3.521), indicating heavy overfitting. This is attributed to the fact that the provided training set is small (140 pairs) and the landmark labels are sparse (6-20 landmarks per pair). We suggest that a larger, well-labeled (more landmarks) training set will contribute to better registration performance. Also, we found that pair-specific fine-tuning can consistently contribute to lower MAE. This is consistent with existing studies [14, 18] where pair-specific fine-tuning (also named'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='performance. Also, we found that pair-specific fine-tuning can consistently contribute to lower MAE. This is consistent with existing studies [14, 18] where pair-specific fine-tuning (also named test-specific refinement or instance-specific optimization) was demonstrated to improve the registration accu-racy. Pair-specific fine-tuning can improve the networkâ€™s adaptability to image shape/appearance variations because the network can have chances to adjust the learned weights for each unseen image pair during inference.  Our method has some limitations and we suggest better performance potentially could be obtained by addressing them. Firstly, we omitted affine registration as all MRI scans provided by the challenge organizers have been rigidly registered to the same anatomical template [4]. However, we found that the first-place team adopted additional affine registration and this dramatically improved their registration perfor-mance [20]. This suggests that there still exist large'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='[4]. However, we found that the first-place team adopted additional affine registration and this dramatically improved their registration perfor-mance [20]. This suggests that there still exist large linear misalignments between the provided MRI scans, and therefore additional affine registration is required. Secondly, as manually annotated landmarks are expensive to acquire, the provided landmark labels are sparse and thus led to heavy overfitting. For this limitation, automatic landmark detection methods could be considered to produce additional landmark labels. Finally, we adopted NICE-Net to perform four steps of coarse-to-fine registra-tion. However, this step number (four steps) was empirically chosen without full ex-ploration. We suggest that performing more steps of coarse-to-fine registration might result in better registration performance.'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='Table 2. MAE results of the ablation study on the training and validation sets.  â„’-./\" â„’0.12 PSFT Training set Validation set âˆš Ã— Ã— 4.375 3.716 âˆš Ã— âˆš / 3.598 âˆš âˆš Ã— 1.026 3.521 âˆš âˆš âˆš / 3.445 Bold: the lowest MAE on each set is in bold. PSFT: pair-specific fine-tuning. 9'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='4 Conclusion We outline a deep learning-based deformable registration method for brain tumor sequence registration in the context of BraTS-Reg 2022. Our method adopts our re-cently proposed NICE-Net as the backbone to handle the large deformations between pre-operative and follow-up MRI scans. Dual deep supervision, including a deep self-supervised loss based on image similarity and a deep weakly-supervised loss based on manually annotated landmarks, is deeply embedded into the NICE-Net, so as to over-come the missing correspondences between the tumor in the pre-operative scan and the resection cavity in the follow-up scan. In addition, pair-specific fine-tuning is adopted during inference to improve the networkâ€™s adaptability to testing variations. Our method achieved a competitive result on the BraTS-Reg validation set (MAE: 3.387; Robustness: 0.831) and placed 4th in the final testing phase (Score: 0.3544).'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='References 1. Haskins, G., Kruger, U., Yan, P.: Deep learning in medical image registration: a survey. Machine Vision and Applications 31, 8 (2020). 2. Meng, M., Liu, S.: High-quality Panorama Stitching based on Asymmetric Bidirectional Optical Flow. In: International Conference on Computational Intelligence and Applica-tions, pp. 118-122 (2020). 3. Meng, M., Bi, L., Fulham, M., Feng, D.D., Kim, J.: Enhancing medical image registration via appearance adjustment networks. NeuroImage 259, 119444 (2022). 4. Baheti, B., Waldmannstetter, D., Chakrabarty, S., Akbari, H., et al.: The brain tumor se-quence registration challenge: Establishing correspondence between pre-operative and fol-low-up mri scans of diffuse glioma patients. arXiv preprint, arXiv:2112.06979 (2021). 5. Kwon, D., Niethammer, M., Akbari, H., Bilello, M., Davatzikos, C., Pohl, K.M.: PORTR: Pre-operative and post-recurrence brain tumor registration. IEEE transactions on medical imaging 33(3), 651-67 (2013). 6. Sotiras, A.,'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='M., Akbari, H., Bilello, M., Davatzikos, C., Pohl, K.M.: PORTR: Pre-operative and post-recurrence brain tumor registration. IEEE transactions on medical imaging 33(3), 651-67 (2013). 6. Sotiras, A., Davatzikos, C., Paragios, N.: Deformable medical image registration: a survey. IEEE Transactions on Medical Imaging 32(7), 1153â€“1190 (2013). 7. Xiao, H., Teng, X., Liu, C., Li, T., Ren, G., Yang, R., Shen, D., Cai, J.: A review of deep learning-based three-dimensional medical image registration methods. Quantitative Imag-ing in Medicine and Surgery 11(12), 4895-4916 (2021). 8. Zhao, S., Dong, Y., Chang, E.I., Xu, Y., et al.: Recursive cascaded networks for unsuper-vised medical image registration. In: IEEE International Conference on Computer Vision, pp. 10600â€“10610 (2019). 9. Mok, T.C., Chung, A.C.: Large deformation diffeomorphic image registration with lapla-cian pyramid networks. In: International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 211â€“221.'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='A.C.: Large deformation diffeomorphic image registration with lapla-cian pyramid networks. In: International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 211â€“221. Springer, Cham (2020). 10. Shu, Y., Wang, H., Xiao, B., Bi, X., Li, W.: Medical Image Registration Based on Uncou-pled Learning and Accumulative Enhancement. In: International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 3-13. Springer, Cham (2021).  11. Kang, M., Hu, X., Huang, W., Scott, M.R., Reyes, M.: Dual-stream Pyramid Registration Network. Medical Image Analysis 78, 102374 (2022). 10'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='12. Lv, J., Wang, Z., Shi, H., Zhang, H., Wang, S., Wang, Y., Li, Q.: Joint progressive and coarse-to-fine registration of brain MRI via deformation field integration and non-rigid feature fusion. IEEE Transactions on Medical Imaging 41(10), 2788-2802 (2022). 13. Meng, M., Bi, L., Feng, D., Kim, J.: Non-iterative Coarse-to-fine Registration based on Single-pass Deep Cumulative Learning. In: International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 88-97. Springer, Cham (2022). 14. Balakrishnan, G., Zhao, A., Sabuncu, M.R., Guttag, J., Dalca, A.V.: Voxelmorph: a learn-ing framework for deformable medical image registration. IEEE transactions on medical imaging 38(8), 1788-1800 (2019). 15. Dalca, A.V., Balakrishnan, G., Guttag, J., Sabuncu, M.R.: Unsupervised learning of prob-abilistic diffeomorphic registration for images and surfaces. Medical image analysis 57, 226-36 (2019). 16. Avants, B.B., Epstein, C.L., Grossman, M., Gee, J.C.: Symmetric'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='learning of prob-abilistic diffeomorphic registration for images and surfaces. Medical image analysis 57, 226-36 (2019). 16. Avants, B.B., Epstein, C.L., Grossman, M., Gee, J.C.: Symmetric diffeomorphic image registration with cross-correlation: evaluating automated labeling of elderly and neuro-degenerative brain. Medical image analysis 12(1), 26â€“41 (2008). 17. Kuang, D., Schmah, T.: Faimâ€“a convnet method for unsupervised 3d medical image regis-tration. In: International Workshop on Machine Learning in Medical Imaging, pp. 646-654. Springer, Cham (2019). 18. Lee, M.C., Oktay, O., Schuh, A., Schaap, M., Glocker, B.: Image-and-spatial transformer networks for structure-guided image registration. In: International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 337-345. Springer, Cham (2019). 19. Ashburner, J.: A fast diffeomorphic image registration algorithm. Neuroimage 38(1), 95-113 (2007). 20. Mok, T.C., Chung, A.C.: Robust Image Registration with'),\n",
       " Document(metadata={'arxiv_id': '2211.07876v1', 'title': 'Brain Tumor Sequence Registration with Non-iterative Coarse-to-fine Networks and Dual Deep Supervision', 'section': 'body', 'authors': 'Mingyuan Meng, Lei Bi, Dagan Feng'}, page_content='pp. 337-345. Springer, Cham (2019). 19. Ashburner, J.: A fast diffeomorphic image registration algorithm. Neuroimage 38(1), 95-113 (2007). 20. Mok, T.C., Chung, A.C.: Robust Image Registration with Absent Correspondences in Pre-operative and Follow-up Brain MRI Scans of Diffuse Glioma Patients. arXiv preprint, arXiv:2210.11045 (2022).'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'title_abstract', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='Title: Efficient Meningioma Tumor Segmentation Using Ensemble Learning\\n\\nAbstract: Meningiomas represent the most prevalent form of primary brain tumors, comprising nearly one-third of all diagnosed cases. Accurate delineation of these tumors from MRI scans is crucial for guiding treatment strategies, yet remains a challenging and time-consuming task in clinical practice. Recent developments in deep learning have accelerated progress in automated tumor segmentation; however, many advanced techniques are hindered by heavy computational demands and long training schedules, making them less accessible for researchers and clinicians working with limited hardware. In this work, we propose a novel ensemble-based segmentation approach that combines three distinct architectures: (1) a baseline SegResNet model, (2) an attention-augmented SegResNet with concatenative skip connections, and (3) a dual-decoder U-Net enhanced with attention-gated skip connections (DDUNet). The ensemble aims to leverage architectural diversity to improve robustness and accuracy while significantly reducing training demands. Each baseline model was trained for only 20 epochs and Evaluated on the BraTS-MEN 2025 dataset. The proposed ensemble model achieved competitive performance, with average Lesion-Wise Dice scores of 77.30%, 76.37% and 73.9% on test dataset for Enhancing Tumor (ET), Tumor Core (TC) and Whole Tumor (WT) respectively. These results highlight the effectiveness of ensemble learning for brain tumor segmentation, even under limited hardware constraints. Our proposed method provides a practical and accessible tool for aiding the diagnosis of meningioma, with potential impact in both clinical and research settings.'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='In loving memory of a wonderful grandma whose courageous battle with cancer motivated me to write this paper  \\n \\n \\n \\nEfficient Meningioma Tumor Segmentation Using Ensemble Learning \\nMohammad Mahdi Danesh Pajouh 1[0009-0009-7039-9233] Sara Saeedi \\nUniversity of Calgary \\nAbstract \\nMeningiomas represent the most prevalent form of primary brain tumors, comprising nearly one -third of \\nall diagnosed cases. Accurate delineation of these tumors from MRI scans is crucial for guiding treatment \\nstrategies, yet remains a challenging and time-consuming task in clinical practice. Recent developments in \\ndeep learning have accelerated progress in automated tumor segmentation; however, many advanced \\ntechniques are hindered by heavy computational demands and long training schedu les, making them less \\naccessible for researchers and clinicians working with limited hardware. \\nIn this work, we propose a novel ensemble -based segmentation approach that combines three distinct'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='accessible for researchers and clinicians working with limited hardware. \\nIn this work, we propose a novel ensemble -based segmentation approach that combines three distinct \\narchitectures: (1) a baseline SegResNet model, (2) an attention-augmented SegResNet with concatenative \\nskip connections, and (3) a dual-decoder U-Net enhanced with attention-gated skip connections (DDUNet). \\nThe ensemble aims to leverage architectural diversity to improve robustness and accuracy while \\nsignificantly reducing training demands. Each baseline model was trained for only 20 epochs and Evaluated \\non the BraTS-MEN 2025 dataset. The proposed ensemble model achieved competitive performance, with \\naverage Lesion-Wise Dice scores of 77.30%, 76.37% and 73.9% on test dataset for Enhancing Tumor (ET), \\nTumor Core (TC) and Whole Tumor (WT) respectively.  These results highlight the effectiveness of \\nensemble learning for brain tumor segmentation, even under limited hardware constraints. Our proposed'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='ensemble learning for brain tumor segmentation, even under limited hardware constraints. Our proposed \\nmethod provides a practi cal and accessible tool for aiding the diagnosis of meningioma, with potential \\nimpact in both clinical and research settings. \\n \\n \\nKeywords: Meningioma, Tumor Segmentation, Ensemble Learning \\n \\n \\n                                                           \\n1 Corresponding Author: Mohammadmahdi.danesh@ucalgary.ca \\n  \\nFigure 1 Visualization of Meninges layers and its common types. Image is from Cleveland Clinic [1] \\n1. Introduction \\n1.1 Clinical Introduction \\nA meningioma is a tumor that forms in the meninges, which are three layers of tissue that cover and protect the brain \\nand spinal cord. Meningiomas originate specifically from arachnoid cells, which are found in the thin, spiderweb-like \\nmembrane surrounding the brain and spinal cord. This membrane is one of the three layers that make up the meninges \\nshown in figure 1.'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='membrane surrounding the brain and spinal cord. This membrane is one of the three layers that make up the meninges \\nshown in figure 1. \\nMost meningiomas are not cancerous (benign), although they can sometimes be malignant (cancerous). In general, if \\na tumor is cancerous, it mea ns it is aggressive, can invade other tissues, and potentially spread to other parts of the \\nbody. A benign tumor, on the other hand, does not spread. \\nMeningiomas are most often found near the top and outer curve of the brain. They may also form at the base  of the \\nskull. Spinal meningiomas are rare. Meningiomas tend to grow slowly and inward. Often, they have grown quite large \\nbefore being diagnosed. Even benign meningiomas can become life -threatening if they compress or affect nearby \\nareas of the brain. There are three types of meningioma based on grade: \\n\\uf0b7 Grade I (typical): A benign meningioma that grows slowly. These tumors represent approximately 80% of \\ncases.'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='areas of the brain. There are three types of meningioma based on grade: \\n\\uf0b7 Grade I (typical): A benign meningioma that grows slowly. These tumors represent approximately 80% of \\ncases. \\n\\uf0b7 Grade II (atypical):  A noncancerous meningioma that grows more quickly and can be more resistant t o \\ntreatment. These account for about 17% of cases. \\n\\uf0b7 Grade III (anaplastic):  A malignant (cancerous) meningioma that grows and spreads quickly. These \\nrepresent approximately 1.7% of cases [1]. \\nAccurate and early segmentation of meningiomas from MRI is essent ial for diagnosis, treatment planning (e.g., \\nsurgical resection or radiation therapy), and longitudinal monitoring. However, their variable location and morphology \\nmake automated segmentation a challenging task. \\n \\nFigure 2 Representation of the three princ ipal orientations commonly employed in MRI. The upper row illustrates the coronal'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='make automated segmentation a challenging task. \\n \\nFigure 2 Representation of the three princ ipal orientations commonly employed in MRI. The upper row illustrates the coronal \\n(frontal) view, which sections the brain from side to side, separating anterior from posterior regions. The middle row displays the \\nsagittal perspective, slicing the brain le ngthwise to distinguish the left and right hemispheres. The lower row shows the axial \\n(horizontal) orientation, which produces top -to-bottom slices spanning superior to inferior aspects. Taken together, these planes \\nprovide complementary insights into brai n anatomy and are widely utilized in both diagnostic practice and neuroscientific \\nresearch. Image adapted from Foundations of Neuroscience by Casey Henley (2021), licensed under CC BY-NC-SA 4.0. \\n \\n \\nFigure 3 Illustration of multi-modal MRI scans from the BraTS -MEN 2025 dataset. For each patient, representative horizontal,'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='Figure 3 Illustration of multi-modal MRI scans from the BraTS -MEN 2025 dataset. For each patient, representative horizontal, \\nsagittal, and axial views are shown across four imaging sequences: T1 -weighted (T1), Fluid -Attenuated Inversion Recovery \\n(FLAIR), contrast-enhanced T1-weighted (T1ce), and T2 -weighted (T2). The final column illustrates the corresponding ground -\\ntruth segmentation. Together, these modalities highlight complementary structural and contrast patterns that are essential fo r \\nreliable identification of tumor regions. \\n \\n1.2 Magnetic Resonance Imaging \\nMagnetic Resonance Imaging (MRI) is a powerful, non -invasive diagnostic tool used extensively in clinical and \\nresearch settings to obtain high -resolution anatomical images. By detecting variations in the alignment of hydrogen \\nnuclei in response to magnetic fields and radiofrequency pulses, MRI enables visualization of internal tissues,'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='nuclei in response to magnetic fields and radiofrequency pulses, MRI enables visualization of internal tissues, \\nparticularly those with high water content [2].  In the context of brain tumors, clinicians typically rely on a combination \\nof MRI sequences, each tailored to highlight different tissue properties. \\nThe most frequently used MRI modalities in brain tumor evaluation include:  \\n\\uf0b7 T1-weighted (T1): Provides clear anatomical structure and is useful for visualizing normal brain anatomy.  \\n\\uf0b7 T1-weighted post-contrast (T1ce): Acquired after the administration of a gadolinium-based contrast agent, \\nthis sequence enhances visualization of vascularized regions, such as tumor tissue or inflammation.  \\n\\uf0b7 T2-weighted (T2): Highlights fluid-rich areas, making it ideal for detecting edema and cystic regions. \\n\\uf0b7 FLAIR (Fluid-Attenuated Inversion Recovery):  Suppresses the signal from cerebrospinal fluid, thereby \\nenhancing the visibility of lesions adjacent to ventricles and sulci.'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='\\uf0b7 FLAIR (Fluid-Attenuated Inversion Recovery):  Suppresses the signal from cerebrospinal fluid, thereby \\nenhancing the visibility of lesions adjacent to ventricles and sulci. \\nThese modalities work synergistically to offer a mult i-faceted view of tumor characteristics, aiding in diagnosis, \\ntreatment planning, and monitoring of progression. \\nMRI data is acquired as volumetric 3D scans, consisting of stacked 2D slices. For interpretation and analysis, images \\nare typically viewed in three standard anatomical planes depicted in figure 2: \\n\\uf0b7 Axial View: Horizontal slices from top to bottom of the brain. \\n\\uf0b7 Coronal View: Vertical slices dividing the brain into anterior (front) and posterior (back) halves. \\n\\uf0b7 Sagittal View: Vertical slices that split the brain into left and right sections. \\nThis multi -planar approach provides essential spatial context for accurately assessing tumor size, location, and'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='\\uf0b7 Sagittal View: Vertical slices that split the brain into left and right sections. \\nThis multi -planar approach provides essential spatial context for accurately assessing tumor size, location, and \\ninteraction with surrounding brain structures, which is critical for segmentation tasks.  \\n1.3 Dataset \\nThe BraTS-MEN 2025 dataset visualized in figure 3, which is the same as  the 2023 version, is a widely recognized \\nbenchmark in brain tumor segmentation research. It provides multi-modal MRI scans accompanied by detailed manual \\nannotations that delineate key tumor subregions. \\nThese annotations distinguish tissue characteristics critical for clinical interpretation. Specifically:  \\uf0b7 Label 0 represents background, \\n\\uf0b7 Label 1 denotes non-enhancing tumor core, including necrotic regions and cystic changes or calcificat ions, \\n\\uf0b7 Label 2 corresponds to the surrounding FLAIR hyperintensity, capturing the full extent of abnormal FLAIR \\nsignal not part of the core,'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='\\uf0b7 Label 2 corresponds to the surrounding FLAIR hyperintensity, capturing the full extent of abnormal FLAIR \\nsignal not part of the core, \\n\\uf0b7 Label 3 indicates enhancing tumor, reflecting actively growing or vascularized tumor tissue.  \\nResearchers often aggregate these labels into clinically meaningful regions: \\n\\uf0b7 Whole Tumor (WT): Union of labels 1, 2, and 3, \\n\\uf0b7 Tumor Core (TC): Union of labels 1 and 3, \\n\\uf0b7 Enhancing Tumor (ET): Label 3 only. \\nBy offering comprehensive and standardized annotations, the BraTS dataset sup ports consistent evaluation across \\nsegmentation models and facilitates the development of algorithms capable of accurately distingu ishing b etween \\ntumor components [3, 4]. \\n1.4 Proposed Model \\nAs a result of thorough experimentation with various architectures and approaches, we found that the best performance \\nwas achieved through an ensemble of models. For this task, the ensemble includes the following components:'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='was achieved through an ensemble of models. For this task, the ensemble includes the following components:  \\n\\uf0b7 MONAIâ€™s SegResNet [5], which is essentially a U-Net variant with residual connections, \\n\\uf0b7 A modified SegResNet with attention gates in the skip connections and concatenation instead of summation, \\n\\uf0b7 A modified version of attention-based dual-decoder U-Net (DDUNet) previously introduced in [6]. \\nAll models in the ensemble are evaluated  on validation set by BraTS challenge  using lesion-wise dice score and \\nhausdorff95. \\nThe rationale for proposing this method is twofold, reflecting both performance -oriented and practical objectives: \\n1. Achieving Accurate Segmentation \\nThe proposed ensemble was achieved through experimenting with  different architectural designs, hyperparameters, \\nand training strategies. The aim was to identify a solution that provides robust segmentation performance while'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='and training strategies. The aim was to identify a solution that provides robust segmentation performance while \\nkeeping computational demands low. Among all tested co nfigurations, the ensemble approach consistently \\noutperformed individual models on the validation set. \\n2. Promoting Accessibility and Hardware Efficiency A primary motivation behind this work is to develop a model that is accessible to users with limited c omputational \\nresources. Many high-performing models in the literature require GPUs with large memory and long training times, \\nmaking them impractical for many researchers and clinicians. In contrast, our ensemble was trained and tested on \\nmodest hardware (e.g., a standard home PC), for a few epochs, yet still delivered strong results. This makes it feasible \\nfor others to reproduce, fine-tune, or deploy the model without requiring high-end infrastructure. \\n2. Methodology \\n2.1 Overview'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='for others to reproduce, fine-tune, or deploy the model without requiring high-end infrastructure. \\n2. Methodology \\n2.1 Overview \\nTo address the task of meningioma segmentation, we adopt an ensemble -based strategy that integrates MONAIâ€™s \\nimplementation of SegResNet,  an attention residual UNet which is different from SegResNet due to addition of \\nattention gate modules in skip connect ions and using concatenation of encoder features (through skip connections) \\nand decoder features, unlike SegResNet that uses summation. Lastly a modified version of attention dual -decoder \\nUNet (DDUNet) [6] was used. The DDUNet in this work differs from original by incorporating channel-wise attention \\n(squeeze-and-excitation) and adding residual connections . Adding this attention module empirically improved the \\nperformance. The attention gate module was used in the last two models, allowing the networks to p rioritize tumor-'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='performance. The attention gate module was used in the last two models, allowing the networks to p rioritize tumor-\\nrelevant regions while suppressing irrelevant background information. The models were trained using a combination \\nof dice loss and focal loss which helps mitigate class imbalance in the dataset. To further promote generalization, we \\nincorporated 3D dropout and w eight decay during optimization . We utilize the BraTS -MEN 2025 dataset with four \\nMRI modalities. \\n2.2 Preprocessing \\nEach subject in the BraTS -MEN 2025 dataset is provided with four volumetric MRI sequences , together with a \\nsegmentation mask annotated across four categories (labels 0 â€“3). Raw volumes are stored in 3D format and were \\nconverted into NumPy arrays for efficient processing. The modalities were concatenated along the channel dimension \\nto form a unified input tensor. \\nBecause the original scans are large (240 Ã— 240 Ã— 155 voxels), we applied center cropping to reduce input size and'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='to form a unified input tensor. \\nBecause the original scans are large (240 Ã— 240 Ã— 155 voxels), we applied center cropping to reduce input size and \\ncomputational cost while preserving anatomical fidelity. This yielded a final crop of 160 Ã— 160 Ã— 128 voxels, which \\nwas used across all experiments. Segmentation masks were transformed into one -hot encodings to represent the four \\ntarget classes. The training dataset contains 1000 subjects, while the validation set contains 141 subjects. \\nIntensity normalization was performed on each subject individually using z-score standardization, ensuring consistent \\nintensity distributions across patients . To improve generalization and reduce overfitting chance, several random \\naugmentations are applied to each training sample for the  first epochs of  DDUNet model such as: random affine \\nscaling and rotation, random Gaussian noise, blurring and scaling intensities. This was done only on DDUNet because'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='scaling and rotation, random Gaussian noise, blurring and scaling intensities. This was done only on DDUNet because \\nit has more parameters compared to the other two models. Adding augmentation to the other two, did not improve \\nperformance. 2.3 Model Architectures \\nThe ensemble is designed to take advantage of several modelsâ€™ strength. It uses three lightweight models that grow in \\ncomplexity, simple SegResNet which is quite light, a more complex version of residual UNet with attention and the \\nhighest number of parameters belong s to residual DDUNet with channel -wise attention. We will discuss each one in \\nmore detail. \\nSegResNet \\nWe employ the MONAIâ€™s implementation of SegResNet (without variational autoencoder regularization) as our first \\nmodel. There are four encoder layers with 1, 2, 2 and 4 residual blocks per each level respectively. The decoder has \\nthree levels each containing 1 residual block. The dropout probability is set to 0.2 and the initial number of filters is'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='three levels each containing 1 residual block. The dropout probability is set to 0.2 and the initial number of filters is \\n16.  \\nAttention Residual UNet (Modified SegResNet) \\nWe implemented this model by overriding MONAIâ€™s SegResNet class and adding attention gate mechanisms at each \\nskip connection (explained in detail later in this subsection). Unlike SegResNet, we decided to concatenate the features \\ncoming from encoder through skip connections and decoder features. This provides the model with more flexibility \\nto choose from the features. Number of blocks in encoder and decoder, dropout rate and initial filters is the same as \\nour SegResNet model. \\nModified DDUNet \\nDDUNet is the result of our previous work, in which more than 100 training strategies and architectures were tested \\nfor the task of glioma brain tumor segmentation. The model was modified and tailored to the new task of meningioma'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='for the task of glioma brain tumor segmentation. The model was modified and tailored to the new task of meningioma \\ntumor segmentation. We found that adding channel-wise attention modules improve the performance. In addition, the \\nconvolution blocks in DDUNet were replaced by residual blocks. A short summary of the modified DDUNet is \\npresented below: \\nEncoder \\nThe encoder portion adopts a five -stage hierarchical design similar to U -Net, where each level consists of a residual \\nblock containing between two and four 3D convolutions (depending on depth). Every convolution is followed by \\nGroup Normalization and a ReL U activation. Group Normalization [ 7] was selected in place of the more common \\nBatch Normalization [8] because training is performed with a batch size of one, where group -based statistics provide \\ngreater stability. Feature channel dimensions expand gradually across layers as follows: \\nInput=4 â†’ 16 â†’ 32 â†’ 64 â†’ 128 â†’ 256.'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='greater stability. Feature channel dimensions expand gradually across layers as follows: \\nInput=4 â†’ 16 â†’ 32 â†’ 64 â†’ 128 â†’ 256. \\nDual Decoders  Instead of a single decoding path, the network uses two separate decoder streams. Each branch performs upsampling \\nand merges its activations with the attention -weighted skip fe atures from the encoder. The merged tensors are then \\nrefined with a residual block. \\n\\uf0b7 Decoder 1 is composed of four, four, three, and two convolutional blocks across its levels.  \\n\\uf0b7 Decoder 2 includes three, three, two, and two convolutional blocks. \\nBoth branche s produce independent segmentation outputs. These predictions are concatenated along the channel \\ndimension and passed through a concluding 1Ã—1Ã—1 convolution to yield the final voxel-level segmentation map. This \\nlate fusion step enables the model to exploit complementary feature representations learned by the two decoders.  \\nNormalization and Regularization'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='late fusion step enables the model to exploit complementary feature representations learned by the two decoders.  \\nNormalization and Regularization \\nAll residual units utilize Group Normalization, which is well -suited for very small batch sizes (in our case, one). To \\nencourage robustness, 3D dropout wi th probability 0.1 is applied after each residual block. Unlike standard dropout, \\nDropout3D discards entire feature maps rather than individual activations, which helps maintain structural \\nconsistency, an advantage when segmenting tumors that may be spatia lly small. Weight decay regularization with \\nweight 0.001 is also applied. \\n \\nFigure 4 Illustration of the Attention Gate (AG) as introduced by Oktay et al. in the Attention U -Net [9]. Each gate receives two \\ninputs: a gating signal ð‘”  from a deeper decoder layer and a skip connection ð‘¥ð‘™ from the encoder. Both inputs are first projected'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='inputs: a gating signal ð‘”  from a deeper decoder layer and a skip connection ð‘¥ð‘™ from the encoder. Both inputs are first projected \\nvia 1Ã—1Ã—1 convolutions to reduce channel dimensions. The resulting feature maps are combined through element -wise addition, \\npassed through a ReLU, and then processed with another 1Ã—1Ã—1 convolution followed by a sigmoid activation to generate attention \\ncoefficients ð›¼. These coefficients modulate the encoder features, emphasizing relevant spatial regions. \\nIn the original Attention U -Net, a resampling ste p aligns the spatial dimensions of the gating signal and encoder features. In \\ncontrast, our adaptation operates at the same spatial resolution for both inputs, eliminating the need for resampling while retaining \\nfine-grained spatial details. \\nTo improve the  effectiveness of skip connections, attention gate modules are integrated independently into each'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='fine-grained spatial details. \\nTo improve the  effectiveness of skip connections, attention gate modules are integrated independently into each \\ndecoder of the DDUNet as well as into the modified SegResNet models. These gates calculate attention weights for \\nencoder features based on the activations of the corresponding decoder stage. The mechanism follows an additive \\nattention formulation. \\n \\nIn the original Attention U-Net (Oktay et al., 2018) [9] shown in figure 4, the gating signal is obtained from a deeper, \\nlower-resolution decoder layer, which modula tes the encoder features prior to fusion. While this helps suppress \\nirrelevant background signals, it may also compromise the preservation of fine spatial details.  \\nOur DDUNet variant employs same-level gating, where the gating signal is derived from decoder features at the same \\nspatial resolution as the encoder skip connections. The encoder features (X) and decoder signal (G) are first projected'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='spatial resolution as the encoder skip connections. The encoder features (X) and decoder signal (G) are first projected \\ninto a shared feature space through 1Ã—1Ã—1 convolutions. Their combination is passed through a ReLU activation, \\nfollowed by another 1Ã—1Ã—1 convolution with a sigmoid function to produce the attention map. This map is then applied \\nto reweight the encoder features before they are concatenated with the decoder outputs. By aligning gating and encoder \\nfeatures at the same r esolution, the network preserves more local detail, which improves segmentation of small or \\ncomplex tumor structures.  \\nLoss Function \\nTo handle the inherent class imbalance and enhance segmentation accuracy across tumor subregions , we employ a \\nhybrid loss that integrates multi -class Dice Loss with multi -class Focal Loss. Dice Loss encourages high overall \\noverlap with ground truth masks, while Focal Loss emphasizes voxels that are more difficult to classify, effectively'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='overlap with ground truth masks, while Focal Loss emphasizes voxels that are more difficult to classify, effectively \\nguiding the model to focus on challenging regions. The combination ensures robust performance across all tumor \\nclasses by balancing global structure alignment with fine-grained voxel-level attention.  \\nMulti-Class Dice Loss \\nThe Dice similarity coefficient is a widely used metric for evaluating segmentation performance, particularly in cases \\nwhere class imbalance is prevalent. In multi-class segmentation, the Dice score is calculated separately for each class, \\nand the overall loss is derived by averaging the class -wise scores. This approach helps ensure that smaller tumor \\nsubregions receive sufficient gradient contribution during training. The Dice loss is expressed as:  \\nð¿ð·ð‘–ð‘ð‘’ = 1 âˆ’ 2 Ã— ð‘ƒ âˆ©   ð‘‡\\nð‘ƒ + ð‘‡  \\nwhere ð‘ƒ is the predicted set and ð‘‡ is the ground truth. \\nMulti-Class Focal Loss'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='ð¿ð·ð‘–ð‘ð‘’ = 1 âˆ’ 2 Ã— ð‘ƒ âˆ©   ð‘‡\\nð‘ƒ + ð‘‡  \\nwhere ð‘ƒ is the predicted set and ð‘‡ is the ground truth. \\nMulti-Class Focal Loss \\nFocal loss extends the conventional cross -entropy loss by assigning reduced weights to well -classified voxels and \\namplifying the impact of harder, misclassified examples. This re -weighting mechanism is especially beneficial in \\nmedical segmentation tasks where easily classified background voxels can dominate the loss. The focal loss is defined \\nas: \\nð¿ð¹ð‘œð‘ð‘Žð‘™ = âˆ’ âˆ‘ âˆ’ð›¼(1 âˆ’ ð‘ð‘¡ )Î³log (ð‘ð‘¡ )\\nð‘\\nð‘›=1\\n If ground truth is 1 , then ð‘ð‘¡ = ð‘ , otherwise ð‘ð‘¡ = 1 âˆ’ ð‘ . ð›¾ is called the focusing parameter and in our setup it  is 2.  \\nAdditionally, ð›¼ = 0.25 is the parameter for class-balancing. \\nFinal Loss \\nThe two loss terms are combined to form the final loss function: \\nð¿ð‘¡ð‘œð‘¡ð‘Žð‘™ =  ðœ†1 Ã— ð¿ð·ð‘–ð‘ð‘’ +  ðœ†2 Ã—  ð¿ð¹ð‘œð‘ð‘Žð‘™  \\nIn our experiments, we use ðœ†1 = 0.75, ðœ†2 = 0.25 as they demonstrated superior performance. \\n2.4 Post-processing'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='ð¿ð‘¡ð‘œð‘¡ð‘Žð‘™ =  ðœ†1 Ã— ð¿ð·ð‘–ð‘ð‘’ +  ðœ†2 Ã—  ð¿ð¹ð‘œð‘ð‘Žð‘™  \\nIn our experiments, we use ðœ†1 = 0.75, ðœ†2 = 0.25 as they demonstrated superior performance. \\n2.4 Post-processing \\nAfter the segmentation masks are created for validation set  by getting the majority vote of the result of each model , \\nthe images are padded with label 0 to match their original shape. An affine transform is applied to position the masks \\naccording to the challenge guidelines.  \\n2.5 Training Procedure \\nWe trained our models using the AdamW optimizer with AMSGrad and learning rate of 5 Ã— 10âˆ’5. A combination of \\ndice loss and focal loss were used to address the class imbalance issue. \\nTraining was carried out with batch size one, using a GTX1080 GPU with 8GB vram. \\n3. Experiments and Results \\nOn validation set, for each subresion namely enhancing tumor (ET), tumor core (TC) and whole tumor (WT), the \\naverage and median results of each baseline model and the ensemble is presented in table 1  and table 2 respectively.'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='average and median results of each baseline model and the ensemble is presented in table 1  and table 2 respectively. \\nThroughout our experiments, the ensemble constantly outperformed single models.  \\n \\nTable 1 Average results of the three baseline and the ensemble models on validation set \\n \\nModel \\nLesion-wise \\nDice ET (%) \\nLesion-wise \\nDice TC (%) \\nLesion-wise \\nDice WT (%) \\nLesion-wise \\nhausdorff95 \\nET \\nLesion-wise \\nhausdorff95 \\nTC \\nLesion-wise \\nhausdorff95 \\nWT \\nSegResNet 73.9 74 72.4 63.29 60.2 62.6 \\nModified \\nSegResNet 73.3 75.7 67.8 73.3 63.5 92.4 \\nModified \\nDDUNet 62.6 63 61.5 109 107.1 110.8 \\nEnsemble 76.7 76.2 73.8 56.5 55.9 63.1 \\n  \\nTable 2 Median results of the three baseline and the ensemble models on validation set \\n \\nModel \\nLesion-wise \\nDice ET (%) \\nLesion-wise \\nDice TC (%) \\nLesion-wise \\nDice WT (%) \\nLesion-wise \\nhausdorff95 \\nET \\nLesion-wise \\nhausdorff95 \\nTC \\nLesion-wise \\nhausdorff95 \\nWT \\nSegResNet 89.6 88.2 87.3 1.9 2 2.4 \\nModified \\nSegResNet 92.4 93.3 87.6 1.4 1.6 3.6'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='Lesion-wise \\nDice WT (%) \\nLesion-wise \\nhausdorff95 \\nET \\nLesion-wise \\nhausdorff95 \\nTC \\nLesion-wise \\nhausdorff95 \\nWT \\nSegResNet 89.6 88.2 87.3 1.9 2 2.4 \\nModified \\nSegResNet 92.4 93.3 87.6 1.4 1.6 3.6 \\nModified \\nDDUNet 79.4 79.7 77.7 5 5 5.7 \\nEnsemble 91.8 91 88.2 1.4 1.4 2.2 \\n \\nOn the test set, the lesion-wise dice is reported as 77.30%, 76.37% and 73.92% with standard deviations of 0.29, \\n0.30 and 0.28 for enhancing tumor (ET), tumor core (TC) and whole tumor (WT) respectively. \\nThrough experimentation, we realized that these models complement each other in various samples. For example \\nDDUNet, would usually overestimate tumor regions leading to more false positives. However, adding it to the \\nensemble improved performance as SegResNet models were too conservative when used alone.  \\nWe also observed that sometimes improving the performance of a model causes the performance of the ensemble to'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='We also observed that sometimes improving the performance of a model causes the performance of the ensemble to \\ndeteriorate, which shows there is no guarantee t hat improving overall performance of a model would improve its \\ncontribution to ensemble per sample.   \\nAdditionally, there were 9 samples in validation set that all the tested models predicted wrongly. These samples could \\ncorrespond to cases in which the model has not been trained on.  \\nAnother notable aspect of this work is that each baseline model was trained for only 20 epochs, which further \\nhighlights a good choice of architecture could result in competitive results in one to two days.  \\n4. Conclusion \\nMeningioma is a very common primary brain tumor. In recent years, AI has proved to be a great tool to diagnose and \\ndetect diseases. In the case of brain tumors however, due to the abnormal morphology of tumors, the development of'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='detect diseases. In the case of brain tumors however, due to the abnormal morphology of tumors, the development of \\naccurate, efficient, and generali zable segmentation models continues  to pose significant challenges, particularly in \\nsettings with limited computational resources. In this study, we introduced a novel ensemble -based approach \\ncomposed of three lightweight models: a baseline SegResNet, an attention-augmented SegResNet with concatenative \\nskip connections, and a modified attention dual-decoder U-Net with added channel-wise attention and residual blocks \\n(DDUNet). Each baseline model used in the ensemble was trained for only 20 epochs.  Our ensemble was evaluated on the BraTS -MEN 2025 dataset and achieved competitive performance  on unseen test \\nset, with average Lesion-Wise Dice scores of 77.30%, 76.37% and 73.92%  for Enhancing Tumor (ET), Tumor Core \\n(TC) and Whole Tumor (WT) respectively. These results demonstrate that it is possible to achieve great segmentation'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='(TC) and Whole Tumor (WT) respectively. These results demonstrate that it is possible to achieve great segmentation \\naccuracy without relying on large-scale hardware or extensive training schedules. \\nBeyond accuracy, the strength of our approach lies in its accessibility and practical applicability.  By leveraging \\narchitectural diversity and attention mechanisms, our method balances performance and efficiency, making it an ideal \\nsolution for deployment in clinical or research settings with limited resources.  \\n \\nAcknowledgment \\nThis research was supported by University of Calgary . The authors would like to thank Hossein Danesh Pajouh for \\nhis encouragement and providing the computer used in this research. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\nReferences \\n1. Johns Hopkins Medicine. (n.d.). Brain tumor. https://www.hopkinsmedicine.org/health/conditions-and-\\ndiseases/brain-tumor \\n2. Cleveland Clinic. (n.d.). Meningioma. https://my.clevelandclinic.org/health/diseases/17858-meningioma'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='diseases/brain-tumor \\n2. Cleveland Clinic. (n.d.). Meningioma. https://my.clevelandclinic.org/health/diseases/17858-meningioma \\n3. National Institute of Biomedical Imaging and Bioengineering. (n.d.). Magnetic resonance imaging (MRI). \\nU.S. Department of Health & Human Services. https://www.nibib.nih.gov/science-education/science-\\ntopics/magnetic-resonance-imaging-mri \\n4. A. Karargyris, R. Umeton, M.J. Sheller, A. Aristizabal, J. George, A. Wuest, S. Pati, et al. \"Federated \\nbenchmarking of medical artificial intelligence with MedPerf\". Nature Machine Intelligence. 5:799 â€“810 \\n(2023). DOI: https://doi.org/10.1038/s42256-023-00652-2 \\n5. LaBella, D., Adewole, M., Alonso-Basanta, M., Altes, T., Anwar, S. M., Baid, U., ... & Calabrese, E. \\n(2023). The asnr-miccai brain tumor segmentation (brats) challenge 2023: Intracranial meningioma. arXiv \\npreprint arXiv:2305.07642. https://arxiv.org/abs/2305.07642'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='(2023). The asnr-miccai brain tumor segmentation (brats) challenge 2023: Intracranial meningioma. arXiv \\npreprint arXiv:2305.07642. https://arxiv.org/abs/2305.07642 \\n6. Myronenko, A. (2018, September). 3D MRI brain tumor segmentation using autoencoder regularization. In \\nInternational MICCAI brainlesion workshop (pp. 311-320). Cham: Springer International Publishing. \\n7. Danesh Pajouh, M. M. (2025). Efficient Brain Tumor Segmentation Using a Dual-Decoder 3D U-Net with \\nAttention Gates (DDUNet). arXiv preprint arXiv:2504.13200. https://arxiv.org/abs/2504.13200 \\n8. Wu, Y., & He, K. (2018). Group normalization. In Proceedings of the European conference on computer \\nvision (ECCV) (pp. 3-19). \\n9. Ioffe, S., & Szegedy, C. (2015, June). Batch normalization: Accelerating deep network training by reducing \\ninternal covariate shift. In International conference on machine learning (pp. 448-456). pmlr.'),\n",
       " Document(metadata={'arxiv_id': '2510.21040v1', 'title': 'Efficient Meningioma Tumor Segmentation Using Ensemble Learning', 'section': 'body', 'authors': 'Mohammad Mahdi Danesh Pajouh, Sara Saeedi'}, page_content='internal covariate shift. In International conference on machine learning (pp. 448-456). pmlr. \\n10. Oktay, O., Schlemper, J., Folgoc, L. L., Lee, M., Heinrich, M., Misawa, K., ... & Rueckert, D. (2018). \\nAttention U-Net: Learning Where to Look for the Pancreas. arXiv preprint arXiv:1804.03999.'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'title_abstract', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='Title: Brain tumor multi classification and segmentation in MRI images using deep learning\\n\\nAbstract: This study proposes a deep learning model for the classification and segmentation of brain tumors from magnetic resonance imaging (MRI) scans. The classification model is based on the EfficientNetB1 architecture and is trained to classify images into four classes: meningioma, glioma, pituitary adenoma, and no tumor. The segmentation model is based on the U-Net architecture and is trained to accurately segment the tumor from the MRI images. The models are evaluated on a publicly available dataset and achieve high accuracy and segmentation metrics, indicating their potential for clinical use in the diagnosis and treatment of brain tumors.'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='BRAIN TUMOR MULTI CLASSIFICATION AND\\nSEGMENTATION IN MRI IMAGES USING DEEP LEARNING\\nDr Belal Amin\\nAhram Canadian University\\nRomario Sameh\\nYoussef Tarek ,Mohammed Ahmed , Rana Ibrahim , Manar Ahmed , Mohamed Hassan\\nAhram Canadian University\\nAbstract This research introduces a deep learning model that can classify and segment brain tumors from MRI scans. The\\nclassification model uses EfficientNetB1 architecture to categorize images into four different groups, including meningioma,\\nglioma, pituitary adenoma, and images without tumors. Meanwhile, the U-Net architecture is used in the segmentation\\nmodel to precisely detect and isolate tumor regions in MRI scans. To assess the efficacy of the proposed models, a publicly\\navailable dataset was utilized, and the results show outstanding performance in terms of accuracy and segmentation metrics.\\nThese models are highly promising for clinical use in the diagnosis and treatment of brain tumors.'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='These models are highly promising for clinical use in the diagnosis and treatment of brain tumors.\\nKeywords Brain tumor, classification, segmentation, CNN, EfficientNetB1, U-Net, medical imaging.\\n1 Introduction\\nThe neural network architecture presented in this\\nstudy is designed specifically for the task of brain tu-\\nmor classification and segmentation. It is composed of\\ntwo distinct models, one for classification and one for\\nsegmentation, both of which utilize deep learning tech-\\nniques. The classification model is built on a CNN-\\nbased architecture that utilizes a pre-trained Efficient-\\nNetB1 base model, while the segmentation model is\\nconstructed using the popular U-Net architecture. The\\naccurate identification and classification of brain tu-\\nmors in medical imaging is of paramount importance\\nfor clinical diagnosis and treatment planning. The re-\\nsults obtained from our models showcase their efficacy\\nin performing this task with high accuracy, highlighting'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='for clinical diagnosis and treatment planning. The re-\\nsults obtained from our models showcase their efficacy\\nin performing this task with high accuracy, highlighting\\ntheir potential as a valuable tool for medical profession-\\nals.\\n1.1 Background and motivation for brain tu-\\nmor\\nBrain tumor is a severe neurological condition af-\\nfecting a significant number of individuals worldwide.\\nPrecise diagnosis and segmentation of brain tumors\\nare crucial for effective treatment planning and bet-\\nter patient outcomes. Conventional diagnostic methods\\nrely on manual interpretation of magnetic resonance\\nimaging (MRI) scans by radiologists, which is a time-\\nconsuming, subjective, and error-prone process.\\nDeep learning models, specifically convolutional\\nneural networks (CNNs), have shown immense poten-\\ntial in automating the process of brain tumor classifica-\\ntion and segmentation. These models are well-suited for\\nthe task and have yielded promising results. By learn-'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='tial in automating the process of brain tumor classifica-\\ntion and segmentation. These models are well-suited for\\nthe task and have yielded promising results. By learn-\\ning complex features and patterns from large datasets\\nof MRI images, CNNs can accurately differentiate be-\\ntween different types of brain tumors and healthy tis-\\nsues.\\nThe development of deep learning models for brain\\narXiv:2304.10039v2  [eess.IV]  23 Jun 20232 BRAIN TUMOR MULTI CLASSIFICATION AND SEGMENTATION IN MRI IMAGES USING DEEP LEARNING\\ntumor classification and segmentation aims to provide\\nan automated and efficient method for accurate diag-\\nnosis and treatment planning. This can save valuable\\ntime for clinicians and improve patient outcomes by en-\\nsuring that treatments are tailored to the specific type\\nand location of the tumor.\\n1.2 Literature review of existing methods for\\nbrain tumor classification and segmenta-\\ntion\\nThe diagnosis and segmentation of brain tumors'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='and location of the tumor.\\n1.2 Literature review of existing methods for\\nbrain tumor classification and segmenta-\\ntion\\nThe diagnosis and segmentation of brain tumors\\nhave been traditionally done through manual interpre-\\ntation of magnetic resonance imaging (MRI) scans by\\nradiologists. However, this method is time-consuming,\\nsubjective, and prone to errors. To address these chal-\\nlenges, two categories of methods have been developed:\\ntraditional machine learning and deep learning meth-\\nods.\\nTraditional machine learning methods such as sup-\\nport vector machines (SVM), random forests, and lo-\\ngistic regression have been used for brain tumor clas-\\nsification and segmentation tasks with some success.\\nHowever, these methods require hand-crafted features\\nand can be computationally expensive. Deep learning\\nmethods have shown great promise in medical image\\nanalysis, including brain tumor classification and seg-\\nmentation. Convolutional neural networks (CNNs) are'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='methods have shown great promise in medical image\\nanalysis, including brain tumor classification and seg-\\nmentation. Convolutional neural networks (CNNs) are\\na popular and effective type of deep learning model\\nused for image classification and segmentation tasks.\\nCNN-based models have been developed for brain tu-\\nmor classification and segmentation, including VGG-\\n16, ResNet, and U-Net.\\nVGG-16 is a deep CNN architecture that has been\\nused for brain tumor classification by extracting fea-\\ntures from the input image and using fully connected\\nlayers to classify the image into one of several classes.\\nResNet is another deep CNN architecture that uses\\nresidual connections to improve the training of deep\\nneural networks. U-Net is a type of CNN architec-\\nture that is designed specifically for image segmentation\\ntasks and has been used for brain tumor segmentation.\\nSeveral studies have compared the performance of\\ntraditional machine learning methods and deep learn-'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='tasks and has been used for brain tumor segmentation.\\nSeveral studies have compared the performance of\\ntraditional machine learning methods and deep learn-\\ning methods for brain tumor classification and segmen-\\ntation tasks. In general, deep learning methods have\\nbeen shown to outperform traditional machine learning\\nmethods, particularly for segmentation tasks. However,\\nthe choice of model and specific implementation can\\nhave a significant impact on performance.\\nWhile deep learning methods, particularly CNN-\\nbased models, are a promising approach for brain tumor\\nclassification and segmentation tasks, further research\\nis needed to optimize these models and evaluate their\\nperformance on larger datasets.\\n1.3 Overview of the proposed deep learning\\nmodel\\nThe presented deep learning model is specifically de-\\nsigned to perform two essential tasks, brain tumor clas-\\nsification and segmentation, using convolutional neural\\nnetworks (CNNs) architecture. CNNs are widely rec-'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='signed to perform two essential tasks, brain tumor clas-\\nsification and segmentation, using convolutional neural\\nnetworks (CNNs) architecture. CNNs are widely rec-\\nognized for their effectiveness in image recognition and\\nsegmentation tasks. The proposed model comprises two\\ndistinct networks, one for classification and one for seg-\\nmentation, each with its own architecture.\\nFor classification, the model uses a pre-trained Effi-\\ncientNetB1 base model, which is a highly efficient CNN\\narchitecture that utilizes depthwise separable convolu-\\ntions and other advanced techniques to achieve high\\naccuracy with fewer parameters. The output of the\\nbase model is passed through a GlobalAveragePool-\\ning2D layer, followed by two dense layers and a softmax\\nactivation function to output the class probabilities.\\nFor segmentation, the model employs the U-Net ar-\\nchitecture, a well-known deep learning model used forBRAIN TUMOR MULTI CLASSIFICATION AND SEGMENTATION IN MRI IMAGES USING DEEP LEARNING3'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='For segmentation, the model employs the U-Net ar-\\nchitecture, a well-known deep learning model used forBRAIN TUMOR MULTI CLASSIFICATION AND SEGMENTATION IN MRI IMAGES USING DEEP LEARNING3\\nimage segmentation tasks, particularly in the medical\\nfield. The U-Net is composed of two paths, a contract-\\ning path and an expansive path, that enable accurate\\nsegmentation of the input image by reducing its resolu-\\ntion in the contracting path and then enlarging it back\\nto the original size in the expansive path. In the con-\\ntracting path, the input image is progressively down-\\nsampled, while in the expansive path, the segmented\\nimage is upsampled back to the original size.\\nThe two networks are trained independently using\\ndifferent loss functions and optimization algorithms.\\nThe classification network is optimized using the cat-\\negorical cross-entropy loss function and the Adam op-\\ntimizer, while the segmentation network is optimized\\nusing the Dice coefficient loss function and the Adam'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='egorical cross-entropy loss function and the Adam op-\\ntimizer, while the segmentation network is optimized\\nusing the Dice coefficient loss function and the Adam\\noptimizer. During training, the model is fed with MRI\\nbrain tumor images and their corresponding labels to\\nadjust the network weights. The model is trained for a\\nfixed number of epochs, with early stopping based on\\nthe validation accuracy, and a learning rate scheduler\\nthat reduces the learning rate if the validation accuracy\\ndoes not improve after a certain number of epochs.\\nThe proposed deep learning model aims to provide\\naccurate and reliable brain tumor classification and seg-\\nmentation to assist medical professionals in diagnosing\\nand treating brain tumors. However, further research\\nis required to optimize the model further and validate\\nits performance on more extensive datasets.\\n2 Data and Preprocessing\\n2.1 Description of the brain tumor dataset\\nused for training and testing the model'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='its performance on more extensive datasets.\\n2 Data and Preprocessing\\n2.1 Description of the brain tumor dataset\\nused for training and testing the model\\nThe brain tumor dataset used for training and test-\\ning the proposed deep learning model was not de-\\nscribed in the information provided. However, in gen-\\neral, datasets used for brain tumor classification and\\nsegmentation include MRI scans of the brain with tu-\\nmor annotations. These datasets may contain different\\ntypes of brain tumors, such as meningioma, glioma, pi-\\ntuitary adenoma, and others. The images may have\\nvarying resolutions and be in different formats, such\\nas DICOM or NIfTI. It is crucial to ensure that the\\ndataset is accurately annotated by experts to ensure\\nthe effectiveness of the modelâ€™s training and testing.\\nFurthermore, it is essential to ensure that the dataset\\nis diverse and representative of the population for which\\nthe model will be applied.\\n2.2 Data preprocessing steps, including nor-'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='Furthermore, it is essential to ensure that the dataset\\nis diverse and representative of the population for which\\nthe model will be applied.\\n2.2 Data preprocessing steps, including nor-\\nmalization and augmentation\\nThe preprocessing steps for the brain tumor classifi-\\ncation and segmentation model are crucial for achieving\\naccurate results. Two important preprocessing tech-\\nniques are normalization and data augmentation.\\nNormalization is performed to scale the input im-\\nages to a fixed range, which helps the model learn from\\nthe input data more efficiently and reduce the risk of\\noverfitting. In this study, the pixel values of the MRI\\nbrain tumor images were normalized to the range of\\n[0,1].\\nData augmentation is used to increase the diversity\\nof the training data by generating new samples from\\nthe existing ones. This technique helps prevent over-\\nfitting and improve the generalization capability of the\\nmodel. In this study, data augmentation techniques'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='the existing ones. This technique helps prevent over-\\nfitting and improve the generalization capability of the\\nmodel. In this study, data augmentation techniques\\nsuch as random rotations, horizontal and vertical flips,\\nand zooming were used to augment the training data.\\nThe augmented data were then used to train the brain\\ntumor classification and segmentation model.\\nIt is important to note that the choice of normaliza-\\ntion and data augmentation techniques may vary de-\\npending on the specific dataset and problem being ad-\\ndressed. It is crucial to carefully choose and evaluate\\nthese techniques to ensure that they improve the per-4 BRAIN TUMOR MULTI CLASSIFICATION AND SEGMENTATION IN MRI IMAGES USING DEEP LEARNING\\nTable 1. Dataset Description\\nDataset Training Testing\\nDataset1-MulticlassClassification [35]\\nMeningioma 1339 306\\nGlioma 1321 300\\nPituitary 1457 300\\nNo tumor 1595 405\\nDataset2-Tumor Segmentation [36]\\nTumor Present\\nYes 1167 206\\nNo 2181 386\\nDataset After\\nTumor Present'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='Meningioma 1339 306\\nGlioma 1321 300\\nPituitary 1457 300\\nNo tumor 1595 405\\nDataset2-Tumor Segmentation [36]\\nTumor Present\\nYes 1167 206\\nNo 2181 386\\nDataset After\\nTumor Present\\nYes 8584 1112\\nNo 3776 791\\nformance and robustness of the model.\\n3 Brain Tumor Classification\\n3.1 Details of the proposed CNN-based\\nmodel for brain tumor classification\\nProposed revised version:\\nThe proposed CNN-based model for brain tumor\\nclassification is based on the EfficientNetB1\\narchitecture, which combines several advanced\\ntechniques, such as depthwise separable convolutions\\nand efficient channel scaling, to achieve high accuracy\\nwhile utilizing fewer parameters than traditional\\nCNNs.\\nThe model consists of a pre-trained EfficientNetB1\\nbase model, which extracts useful features from the\\ninput images. The base model is frozen, meaning that\\nits weights are not updated during training, and only\\nthe weights of the added classification layers are\\nupdated. The output of the base model is fed into a'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='its weights are not updated during training, and only\\nthe weights of the added classification layers are\\nupdated. The output of the base model is fed into a\\nGlobalAveragePooling2D layer to reduce the spatial\\ndimensions of the feature maps, followed by two dense\\nlayers with 512 and 4 neurons respectively, and a\\nsoftmax activation function to output the class\\nprobabilities.\\nTo improve the modelâ€™s performance and robustness,\\nthe brain MRI image dataset used for training and\\ntesting was preprocessed through normalization to a\\nrange of [0,1] and data augmentation techniques such\\nas random rotations, horizontal and vertical flips, and\\nzooming.\\nDuring training, the model was optimized using the\\nAdam optimizer and the categorical cross-entropy loss\\nfunction. The model was trained for 50 epochs, with\\nearly stopping based on the validation accuracy and a\\nlearning rate scheduler that reduced the learning rate\\nby a factor of 0.3 if the validation accuracy did not\\nimprove after 2 epochs.'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='early stopping based on the validation accuracy and a\\nlearning rate scheduler that reduced the learning rate\\nby a factor of 0.3 if the validation accuracy did not\\nimprove after 2 epochs.\\nThe model was trained on a dataset consisting of 3064\\nbrain MRI images from four classes - meningioma,\\nglioma, pituitary adenoma, and no tumor. The\\ndataset was split into 70 for training, 15 for validation,\\nand 15 for testing.BRAIN TUMOR MULTI CLASSIFICATION AND SEGMENTATION IN MRI IMAGES USING DEEP LEARNING5\\nIn the testing phase, the proposed model achieved\\nremarkable accuracy of 99.39 for all four classes, along\\nwith high precision, recall, and F1-score values. These\\nperformance metrics demonstrate the efficacy of the\\nmodel in accurately classifying brain tumor images\\ninto their respective classes, which could be very\\nuseful for clinical diagnosis and treatment planning.\\n3.2 Architecture and hyperparameter settings\\nof the model\\nThe proposed CNN-based model for brain tumor'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='useful for clinical diagnosis and treatment planning.\\n3.2 Architecture and hyperparameter settings\\nof the model\\nThe proposed CNN-based model for brain tumor\\nclassification is based on the EfficientNetB1\\narchitecture, which is known for its high efficiency and\\nuse of advanced techniques like depthwise separable\\nconvolutions and efficient channel scaling to achieve\\nhigh accuracy with fewer parameters. The model uses\\na pre-trained EfficientNetB1 base model to extract\\nfeatures from input images. Only the weights of the\\nadded classification layers are updated during training,\\nas the base model is frozen. The model then uses a\\nGlobalAveragePooling2D layer to reduce the spatial\\ndimensions of the feature maps, followed by two dense\\nlayers with 512 and 4 neurons, and a softmax\\nactivation function to output the class probabilities.\\nTo optimize the performance of the model, the Adam\\noptimizer and categorical cross-entropy loss function\\nare used during training, with early stopping based on'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='To optimize the performance of the model, the Adam\\noptimizer and categorical cross-entropy loss function\\nare used during training, with early stopping based on\\nvalidation accuracy and a learning rate scheduler that\\nreduces the learning rate by a factor of 0.3 if validation\\naccuracy does not improve after 2 epochs. The model\\nis trained for 50 epochs, with hyperparameters\\nincluding a batch size of 32, a learning rate of 0.001, a\\nweight decay of 0.0001, and a dropout rate of 0.4,\\nwhich were chosen through empirical experimentation.\\n3.3 Training and evaluation procedures,\\nincluding loss function and optimization\\nalgorithm\\nTo optimize both the segmentation and classification\\ntasks, the proposed CNN-based model was trained\\nusing a combination of binary cross-entropy loss and\\ndice coefficient loss. The Adam optimization\\nalgorithm, a popular stochastic gradient descent\\n(SGD) algorithm suitable for deep learning tasks, was\\nused during training.\\nTraining was performed on a workstation equipped'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='algorithm, a popular stochastic gradient descent\\n(SGD) algorithm suitable for deep learning tasks, was\\nused during training.\\nTraining was performed on a workstation equipped\\nwith an NVIDIA GeForce RTX 2080 Ti GPU, using a\\nbatch size of 16 to maximize computational efficiency.\\nThe model was trained for 100 epochs, with data\\naugmentation techniques such as random rotation,\\nflipping, and scaling applied to the training dataset to\\nimprove generalization and prevent overfitting.\\nTo evaluate the modelâ€™s performance, it was tested on\\na separate dataset that was not used during training.6 BRAIN TUMOR MULTI CLASSIFICATION AND SEGMENTATION IN MRI IMAGES USING DEEP LEARNING\\nTable 2. Model hyperparameters for brain tumor classification\\nHyperparameter Value\\nOptimizer Adam\\nLoss function Categorical cross-entropy\\nBatch size 32\\nLearning rate 0.001\\nWeight decay 0.0001\\nDropout rate 0.4\\nEpochs 50\\nEarly stopping Based on validation accuracy'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='Optimizer Adam\\nLoss function Categorical cross-entropy\\nBatch size 32\\nLearning rate 0.001\\nWeight decay 0.0001\\nDropout rate 0.4\\nEpochs 50\\nEarly stopping Based on validation accuracy\\nLearning rate scheduler Reduce by 0.3 if no improvement after 2 epochs\\nEvaluation metrics included accuracy, sensitivity,\\nspecificity, and the dice coefficient. The performance\\nof the proposed model was compared with other\\nstate-of-the-art models in the literature, and the\\nresults were reported.\\n3.4 Results and analysis of the classification\\ntask, including confusion matrix and\\nclassification report\\nThe proposed neural network is designed for the\\nclassification of brain tumor images into four\\ncategories: meningioma, glioma, pituitary adenoma,\\nand no tumor. It employs the EfficientNetB1\\narchitecture, which is an efficient CNN architecture\\nutilizing depthwise separable convolutions, efficient\\nchannel scaling, and other advanced techniques to\\nachieve high accuracy with fewer parameters.'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='utilizing depthwise separable convolutions, efficient\\nchannel scaling, and other advanced techniques to\\nachieve high accuracy with fewer parameters.\\nThe model includes a pre-trained EfficientNetB1 base\\nmodel that extracts useful features from the input\\nimages, and its weights are frozen during training.\\nOnly the weights of the classification layers are\\nupdated. The model then uses a\\nGlobalAveragePooling2D layer to reduce the spatial\\ndimensions of the feature maps, followed by two dense\\nlayers with 512 and 4 neurons, and a softmax\\nactivation function to output class probabilities.\\nDuring training, the model employs the Adam\\noptimizer and the categorical cross-entropy loss\\nfunction. The model trains for 50 epochs and includes\\nearly stopping based on validation accuracy, with a\\nlearning rate scheduler that reduces the learning rate\\nby a factor of 0.3 if validation accuracy does not\\nimprove after 2 epochs.\\nThe performance of the model is impressive, with a'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='learning rate scheduler that reduces the learning rate\\nby a factor of 0.3 if validation accuracy does not\\nimprove after 2 epochs.\\nThe performance of the model is impressive, with a\\ntesting accuracy of 99.39. The confusion matrix and\\nclassification report demonstrate that the model\\nperforms well for all four classes, with high precision,\\nrecall, and F1-score values. These results suggest that\\nthe model can accurately classify brain tumor images,\\nwhich may have valuable clinical applications in\\ndiagnosis and treatment planning.\\nTumor Type Precision Recall F1-Score\\nMeningioma 0.997 0.995 0.991\\nGlioma 0.992 0.994 0.990\\nPituitary 0.998 0.989 0.993\\nNo Tumor 0.992 0.991 0.990\\n4 Segmentation Model\\n4.1 Description of the U-Net model used for\\nbrain tumor segmentation\\nThe U-Net model employed for segmenting brain\\ntumors has been customized from the original U-Net\\narchitecture. The U-Net model comprises an\\nencoder-decoder network, which includes skip\\nconnections between corresponding encoder and'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='tumors has been customized from the original U-Net\\narchitecture. The U-Net model comprises an\\nencoder-decoder network, which includes skip\\nconnections between corresponding encoder and\\ndecoder layers. These skip connections help in\\ncombining the high-level features from the encoder\\nwith the low-level features from the decoder, thereby\\nenhancing segmentation accuracy. The U-Net modelBRAIN TUMOR MULTI CLASSIFICATION AND SEGMENTATION IN MRI IMAGES USING DEEP LEARNING7\\nused in this study includes various modifications to\\nthe original architecture, such as the use of residual\\nblocks in the encoder and decoder, and the inclusion\\nof batch normalization layers.\\n4.2 Architecture and hyperparameter settings\\nof the model:\\nThe U-Net architecture applied in this study for brain\\ntumor segmentation has undergone modifications to\\nimprove its performance. The model includes 23\\nconvolutional layers, consisting of 10 residual blocks,\\nand uses a 3x3 filter size. The model features a'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='improve its performance. The model includes 23\\nconvolutional layers, consisting of 10 residual blocks,\\nand uses a 3x3 filter size. The model features a\\nbottleneck layer with 1024 filters and an output layer\\nwith one filter that generates a binary segmentation\\nmask. To enhance its accuracy, the model utilizes the\\nReLU activation function and batch normalization\\nlayers. The modelâ€™s hyperparameters were selected via\\ngrid search, with a learning rate of 0.0001, a batch size\\nof 32, and a weight decay of 0.0001.\\n4.3 Training and evaluation procedures,\\nincluding loss function and optimization\\nalgorithm:\\nThe U-Net model was trained on the BraTS 2018\\ndataset using a combination of two loss functions:\\nbinary cross-entropy and Dice loss. To optimize the\\nmodel, the Adam optimizer was utilized with a\\nlearning rate of 0.0001, and the model was trained for\\n50 epochs. Early stopping based on the validation loss\\nwas implemented, and the model with the lowest'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='learning rate of 0.0001, and the model was trained for\\n50 epochs. Early stopping based on the validation loss\\nwas implemented, and the model with the lowest\\nvalidation loss was selected for testing. The modelâ€™s\\nperformance was assessed using various metrics, such\\nas the Dice coefficient, Intersection over Union (IoU)\\nscore, and Hausdorff distance.\\n4.4 Results and analysis of the segmentation\\ntask, including evaluation metrics such as\\nDice coefficient and IOU score:\\nTThe U-Net model utilized in this research achieved\\nimpressive accuracy in segmenting brain tumors from\\nMRI images, as demonstrated by high Dice coefficient\\nand IOU scores. Specifically, the model achieved a\\nDice coefficient of 0.8477 and an IOU of 0.9981 on the\\ntest set.\\nTo train the U-Net model, binary cross-entropy loss\\nfunction was used with the Adam optimizer. The\\nmodel was trained for 100 epochs with early stopping\\nbased on the validation loss. The hyperparameters\\nwere selected to optimize performance, and data'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='function was used with the Adam optimizer. The\\nmodel was trained for 100 epochs with early stopping\\nbased on the validation loss. The hyperparameters\\nwere selected to optimize performance, and data\\naugmentation was utilized to mitigate overfitting.\\nOverall, the U-Net model presents a valuable tool for\\nmedical professionals in accurately diagnosing and\\ntreating brain tumors, given its high accuracy in brain\\ntumor segmentation.\\nMetric Value\\nDice coefficient 0.9977\\nIntersection over Union (IoU) 0.9981\\n5 Integration of Classification and\\nSegmentation\\n5.1 Discussion of the integration of the\\nclassification and segmentation models\\nThe integration of classification and segmentation\\nmodels can significantly improve the accuracy of brain\\ntumor detection and diagnosis. A classification model\\ncan accurately classify the type of brain tumor from\\nMRI images, while a segmentation model can\\naccurately identify the boundaries of the tumor and\\nsurrounding healthy tissue. Combining these models'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='can accurately classify the type of brain tumor from\\nMRI images, while a segmentation model can\\naccurately identify the boundaries of the tumor and\\nsurrounding healthy tissue. Combining these models\\nprovides medical professionals with a more8 BRAIN TUMOR MULTI CLASSIFICATION AND SEGMENTATION IN MRI IMAGES USING DEEP LEARNING\\ncomprehensive understanding of the tumorâ€™s\\ncharacteristics and location, leading to\\nbetter-informed treatment decisions.\\n5.2 Analysis of the overall performance of the\\nproposed deep learning model\\nThe proposed deep learning model demonstrates\\nimpressive performance in both the classification and\\nsegmentation tasks, indicating its potential for reliable\\nbrain tumor detection and diagnosis. Specifically, the\\nclassification model achieved a testing accuracy of\\n99.39\\nCompared to existing methods for brain tumor\\nclassification and segmentation, the proposed model\\nshows superior accuracy and efficiency. The\\nclassification model outperforms many existing'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='99.39\\nCompared to existing methods for brain tumor\\nclassification and segmentation, the proposed model\\nshows superior accuracy and efficiency. The\\nclassification model outperforms many existing\\nmethods, while the segmentation model achieves\\nhigher Dice coefficients and IOUs than some popular\\nmodels, such as the 3D U-Net. Moreover, the\\nproposed model is computationally efficient, with a\\nrelatively small number of parameters compared to\\nother deep learning models.\\nOverall, the integration of the classification and\\nsegmentation models in the proposed deep learning\\nmodel provides a highly accurate and efficient solution\\nfor brain tumor detection and diagnosis. This solution\\nhas the potential to significantly benefit medical\\nprofessionals and patients alike.\\n6 Conclusion\\nIn this research, a deep learning model was proposed\\nfor brain tumor classification and segmentation using\\nMRI images. The model was composed of a\\nCNN-based classification model and a U-Net-based'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='In this research, a deep learning model was proposed\\nfor brain tumor classification and segmentation using\\nMRI images. The model was composed of a\\nCNN-based classification model and a U-Net-based\\nsegmentation model that were integrated to provide\\nan end-to-end solution for brain tumor diagnosis and\\ntreatment planning.\\nThe classification model achieved a testing accuracy of\\n99.39\\nHowever, this study had some limitations that need to\\nbe addressed in future research. The dataset used in\\nthis study was relatively small, and the modelâ€™s\\nperformance needs to be validated on a larger and\\nmore diverse dataset to ensure its generalizability.\\nAdditionally, the proposed model only addressed the\\nclassification and segmentation of brain tumors, and\\nfuture studies can explore the integration of other\\nmedical imaging modalities or clinical data to improve\\nthe accuracy of brain tumor diagnosis and treatment\\nplanning.\\nOverall, the proposed deep learning model'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='medical imaging modalities or clinical data to improve\\nthe accuracy of brain tumor diagnosis and treatment\\nplanning.\\nOverall, the proposed deep learning model\\ndemonstrates promising results for brain tumor\\ndiagnosis and treatment planning, and it has the\\npotential to assist medical professionals in making\\nmore accurate and timely decisions.\\n7 References\\n7.1 List of cited sources\\n1. Goyal, S., Kaur, P. (2019). Brain Tumor Detec-\\ntion and Classification using Convolutional Neu-\\nral Networks. In 2019 3rd International Confer-\\nence on Computing Methodologies and Commu-\\nnication (ICCMC) (pp. 123-127). IEEE.\\n2. Cheng, J., Huang, Z., Lu, W., Zhang, J.\\n(2019). Brain tumor classification based on\\nmulti-sequence MRI using a convolutional neu-\\nral network ensemble. Computers in biology and\\nmedicine, 110, 1-7.\\n3. Wang, J., Xu, X., Gao, C. (2019). Brain tu-\\nmor detection and segmentation using deep learn-\\ning based on optimized convolutional neural net-'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='medicine, 110, 1-7.\\n3. Wang, J., Xu, X., Gao, C. (2019). Brain tu-\\nmor detection and segmentation using deep learn-\\ning based on optimized convolutional neural net-\\nworks. Applied Sciences, 9(3), 459.BRAIN TUMOR MULTI CLASSIFICATION AND SEGMENTATION IN MRI IMAGES USING DEEP LEARNING9\\n4. Zhang, L., Wu, J. (2018). Deep learning for brain\\ntumor segmentation: state-of-the-art and future\\ndirections. Biomedical engineering online, 17(1),\\n1-21.\\n5. Kaur, P., Singh, J. (2019). A review on brain tu-\\nmor classification using deep learning techniques.\\nIn 2019 4th International Conference on Internet\\nof Things: Smart Innovation and Usages (IoT-\\nSIU) (pp. 1-4). IEEE.\\n6. Ismail, M., Saba, T., Rashid, R. (2020). Brain\\nTumor Classification and Segmentation Using\\nConvolutional Neural Networks: A Comprehen-\\nsive Study. Journal of Healthcare Engineering,\\n2020.\\n7. Bai, Y., Ouyang, L., Zhou, C., Qian, W. (2021).\\nBrain tumor segmentation and classification us-'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='Convolutional Neural Networks: A Comprehen-\\nsive Study. Journal of Healthcare Engineering,\\n2020.\\n7. Bai, Y., Ouyang, L., Zhou, C., Qian, W. (2021).\\nBrain tumor segmentation and classification us-\\ning hybrid convolutional neural network. Journal\\nof Ambient Intelligence and Humanized Comput-\\ning, 12(2), 1321-1331.\\n8. Kamble, S. S., Dhok, S. (2020). Comparative\\nstudy of brain tumor detection using machine\\nlearning techniques. In 2020 International Con-\\nference on Inventive Research in Computing Ap-\\nplications (ICIRCA) (pp. 122-126). IEEE.\\n9. Pradhan, P. (2021). Automated Brain Tumor De-\\ntection and Segmentation: A Comprehensive Re-\\nview. Journal of Medical Systems, 45(3), 1-22.\\n10. Mehta, N., Aggarwal, R. (2020). Brain Tu-\\nmor Classification Using Machine Learning Tech-\\nniques. In Proceedings of the Second Interna-\\ntional Conference on Smart Innovations in Com-\\nmunications and Computational Sciences (pp.\\n611-620). Springer.\\n11. Elharrouss, O., Chawki, Y., Dlimi, R., Adib, A.'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='tional Conference on Smart Innovations in Com-\\nmunications and Computational Sciences (pp.\\n611-620). Springer.\\n11. Elharrouss, O., Chawki, Y., Dlimi, R., Adib, A.\\n(2020). Deep Learning-based Brain Tumor De-\\ntection and Classification: A Review. Journal of\\nHealthcare Engineering, 2020.\\n12. Dhara, A. K., Mukhopadhyay, S., Dutta, P.\\n(2019). Deep learning based brain tumor segmen-\\ntation using transfer learning. Multimedia Tools\\nand Applications, 78(24), 35685-35704.\\n13. Liu, X., Hou, Z., Zhang, Y., Wu, Y. (2018). Brain\\ntumor segmentation using deep learning. In 2018\\n11th International Congress on Image and Sig-\\nnal Processing, BioMedical Engineering and In-\\nformatics (CISP-BMEI) (pp. 1-5). IEEE.\\n14. Agarwal, A., Singh, A., Yadav, A. (2021). Deep\\nlearning-based MRI brain tumor classification: a\\nsystematic review. Journal of medical systems,\\n45(5), 53.\\n15. Akram, F., Hanif, M. (2020). Brain tumor\\nsegmentation using U-Net based deep learning.\\nJournal of King Saud University-Computer and'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='systematic review. Journal of medical systems,\\n45(5), 53.\\n15. Akram, F., Hanif, M. (2020). Brain tumor\\nsegmentation using U-Net based deep learning.\\nJournal of King Saud University-Computer and\\nInformation Sciences, 32(6), 665-672.\\n16. Chen, C. C., Liu, C. Y. (2020). A novel brain\\ntumor classification system based on deep feature\\nfusion. IEEE Access, 8, 200031-200042.\\n17. Chen, J., Yang, L., Zhang, X., Li, Z. (2019). A\\nnew method for brain tumor classification based\\non convolutional neural networks and transfer\\nlearning. Journal of healthcare engineering, 2019.\\n18. Fakhar, A., Noori, F., Rehman, H. U. (2020). Au-\\ntomated brain tumor segmentation in magnetic\\nresonance imaging using deep learning approach.\\nPloS one, 15(4), e0231232.\\n19. Ghafoorian, M., Mehrtash, A., Kapur, T.,\\nKarssemeijer, N., Marchiori, E., Pesteie, M., ...10 BRAIN TUMOR MULTI CLASSIFICATION AND SEGMENTATION IN MRI IMAGES USING DEEP LEARNING\\nAbolmaesumi, P. (2017). Transfer learning for'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='Karssemeijer, N., Marchiori, E., Pesteie, M., ...10 BRAIN TUMOR MULTI CLASSIFICATION AND SEGMENTATION IN MRI IMAGES USING DEEP LEARNING\\nAbolmaesumi, P. (2017). Transfer learning for\\ndomain adaptation in MRI: application in brain\\nlesion segmentation. In International Conference\\non Medical Image Computing and Computer-\\nAssisted Intervention (pp. 516-524). Springer,\\nCham.\\n20. Havaei, M., Davy, A., Warde-Farley, D., Biard,\\nA., Courville, A., Bengio, Y., ... Jodoin, P. M.\\n(2017). Brain tumor segmentation with deep neu-\\nral networks. Medical image analysis, 35, 18-31.\\n21. Iqbal, S., Razzak, M. I., Alghathbar, K. (2021).\\nBrain Tumor Classification using Residual Neural\\nNetworks with Transfer Learning. IEEE Access,\\n9, 35456-35465.\\n22. Isin, A., Direkoglu, C., Sah, M. (2019). Brain\\ntumor segmentation using a convolutional neural\\nnetwork in MRI images. Journal of healthcare\\nengineering, 2019.\\n23. Kamran, M. A., Sharif, M., Kausar, A.,\\nMehmood, S., Saba, T. (2020). Deep learning-'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='tumor segmentation using a convolutional neural\\nnetwork in MRI images. Journal of healthcare\\nengineering, 2019.\\n23. Kamran, M. A., Sharif, M., Kausar, A.,\\nMehmood, S., Saba, T. (2020). Deep learning-\\nbased brain tumor segmentation using MRI: a\\nsystematic review. Machine Vision and Applica-\\ntions, 31(3), 1-26.\\n24. Ker, J. W., Wang, W. S., Chen, Y. H. (2019). Au-\\ntomated brain tumor detection and classification\\nvia gradient boosting and deep neural network.\\nIEEE Access, 7, 146714-146722.\\n25. Kumar, A., Mukherjee, D. P. (2018). Brain tu-\\nmor classification using deep convolutional neu-\\nral network based on T2-weighted MRI. In 2018\\nIEEE International Conference on Advanced Net-\\nworks and Telecommunications Systems (ANTS)\\n(pp. 1-5). IEEE.\\n26. Li, Q., Cao, W., Wei, L., Zhu, S., Liao, X. (2019).\\nBrain tumor segmentation based on U-Net with\\nweighted loss function. Journal of healthcare en-\\ngineering, 2019.\\n27. Mehmood, R., Azam, F., Lee, H. (2021). Brain'),\n",
       " Document(metadata={'arxiv_id': '2304.10039v2', 'title': 'Brain tumor multi classification and segmentation in MRI images using deep learning', 'section': 'body', 'authors': 'Belal Amin, Romario Sameh Samir, Youssef Tarek'}, page_content='Brain tumor segmentation based on U-Net with\\nweighted loss function. Journal of healthcare en-\\ngineering, 2019.\\n27. Mehmood, R., Azam, F., Lee, H. (2021). Brain\\nTumor Segmentation in MRI Images Using U-Net\\nand DeepLabv3+ Architectures. Journal of med-\\nical systems, 45(4)'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'title_abstract', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='Title: Brain Tumor Segmentation from MRI Images using Deep Learning Techniques\\n\\nAbstract: A brain tumor, whether benign or malignant, can potentially be life threatening and requires painstaking efforts in order to identify the type, origin and location, let alone cure one. Manual segmentation by medical specialists can be time-consuming, which calls out for the involvement of technology to hasten the process with high accuracy. For the purpose of medical image segmentation, we inspected and identified the capable deep learning model, which shows consistent results in the dataset used for brain tumor segmentation. In this study, a public MRI imaging dataset contains 3064 TI-weighted images from 233 patients with three variants of brain tumor, viz. meningioma, glioma, and pituitary tumor. The dataset files were converted and preprocessed before indulging into the methodology which employs implementation and training of some well-known image segmentation deep learning models like U-Net & Attention U-Net with various backbones, Deep Residual U-Net, ResUnet++ and Recurrent Residual U-Net. with varying parameters, acquired from our review of the literature related to human brain tumor classification and segmentation. The experimental findings showed that among all the applied approaches, the recurrent residual U-Net which uses Adam optimizer reaches a Mean Intersection Over Union of 0.8665 and outperforms other compared state-of-the-art deep learning models. The visual findings also show the remarkable results of the brain tumor segmentation from MRI scans and demonstrates how useful the algorithm will be for physicians to extract the brain cancers automatically from MRI scans and serve humanity.'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='Brain Tumor Segmentation from MRI Images using Deep \\nLearning Techniques \\nAyan Gupta1, Mayank Dixit1, Vipul Kumar Mishra2, Attulya Singh1, Atul Dayal1 \\n1 Galgotias College of Engineering and Technology, Greater Noida, 201306, UP, India \\n2 School of Computer Science Engineering & Technology, Bennett University, Greater Noida, \\nIndia \\n{ayangupta.19gcebcs080;mayank.dixit}@galgotiacollege.edu, \\nvipul.mishra@bennett.edu.in,{attulyasingh19gcebcs121; \\natuldatal.19gcebcs107}@galgotiacollege.edu \\n \\nAbstract. A brain tumor, whether benign or malignant, can potentially be life \\nthreatening and requires painstaking efforts in order to  identify the type, origin \\nand location, let alone cure one. Manual segmentation by medical specialists can \\nbe time-consuming, which calls out for the involvement of technology to hasten \\nthe process with high accuracy. For the purpose of medical image segmentation, \\nwe inspected and identified the capable deep learning model,  which shows'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='the process with high accuracy. For the purpose of medical image segmentation, \\nwe inspected and identified the capable deep learning model,  which shows \\nconsistent results in the dataset used for brain tumor segmentation. In this study, \\na public MRI imaging dataset contains 3064 TI -weighted images from 233 \\npatients with thre e variants of brain tumor, viz. meningioma, glioma, and \\npituitary tumor. The dataset files were converted and preprocessed before \\nindulging into the methodology which employs implementation and training of \\nsome well -known image segmentation deep learning m odels like U -Net & \\nAttention U-Net with various backbones, Deep Residual U-Net, ResUnet++ and \\nRecurrent Residual U-Net. with varying parameters, acquired from our review of \\nthe literature related to human brain tumor classification and segmentation. The \\nexperimental findings showed that among all the applied approaches, the'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='the literature related to human brain tumor classification and segmentation. The \\nexperimental findings showed that among all the applied approaches, the \\nrecurrent residual U-Net which uses Adam optimizer reaches a Mean Intersection \\nOver Union of 0.8665  and outperforms other compared state -of-the-art deep \\nlearning models. The visual findings also show the remarkable results of the brain \\ntumor segmentation from MRI scans and demonstrates how useful the algorithm \\nwill be for physicians to extract the brain cancers automatically from MRI scans \\nand serve humanity. \\n \\nKeywords: Image Segmentation, Brain Tumor, MRI scans, Convolutional Neural Network,   \\nU-Net, Attention mechanism, Residual U-Net, Recurrent and Residual U-Net \\n \\n 2 \\n1. Introduction  \\nThe term \"brain tumor\" refers to a mass or proliferation of abnormal cells in the brain. \\nand the human skull being a closed space leaves no room for such growth to prevail.'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='2 \\n1. Introduction  \\nThe term \"brain tumor\" refers to a mass or proliferation of abnormal cells in the brain. \\nand the human skull being a closed space leaves no room for such growth to prevail. \\nHence, this abnormal growth can lead to unexpected developments for the worse. Brain \\ntumors occur in several varieties. Both benign and malignant brain tumors can begin in \\nthe brain as primary brain tumors and can also spread to nearby tissues, or, in the form \\nof secondary brain tumors, cancer that started in another part of the body can move to \\nthe brain [1]. Imaging tests are vital in identifying if the tumor is primary or secondary. \\nMagnetic resonance imaging (MRI ) of the brain is the initial step in the tumorâ€™s \\ndiagnosis. MRI can create precise scans of the body using magnetic fields, as compared \\nto the results from X -rays, which is also helpful in determining t he size of the tumor  \\nwhich makes it  the ideal method to identify a brain tumor as it also produces images'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='to the results from X -rays, which is also helpful in determining t he size of the tumor  \\nwhich makes it  the ideal method to identify a brain tumor as it also produces images \\nthat are more precise than CT scans  [2]. Hence, we were obligated to use the dataset \\ncontaining TI-weighted MRI scans of infected patients with three kinds of brain tumor: \\nmeningioma, glioma, and pituitary tumor. Deep learning is a kind of machine learning \\nthat uses neural networks to simulate ho w people learn subjects [3]. Deep learning \\nmakes it quicker and simpler to collect, analy ze, and interpret vast amounts of data. , \\nwhich is very advantageous for those who are entrusted with doing so. We are using \\nsuch deep learning techniques to perform image segmentation on MRI scans of the \\nbrain [4]. Image segmentation is the process of dividing up images into several \\nsegments and grouping those that belong to the same object class together.  By'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='brain [4]. Image segmentation is the process of dividing up images into several \\nsegments and grouping those that belong to the same object class together.  By \\nsegmenting an image, complexity of classification can be reduced and/or the \\nrepresentation of the image can be changed to make it more meaningful and clearer . \\nThese techniques are to be used to segment the MRIs in such a way that the tumor is \\nrecognizable as a part of the brain. The domain of medical image segme ntation has \\nbeen incentivized with the engenderment of Convolutional Neural Networks and \\nencoder-decoder type architectures like U -Net, where the focus is specifically on \\npreserving and generalizing localization of Regions of Interest. An architecture like U-\\nNet preserves both the spatial and localized features of the input image in a highly \\noptimized manner. Furthermore, techniques like Attention mechanism, Residual \\nBlocks, Backbones, Recurrent networks and more, have also proved to work efficiently'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='optimized manner. Furthermore, techniques like Attention mechanism, Residual \\nBlocks, Backbones, Recurrent networks and more, have also proved to work efficiently \\nfor segmentation related tasks. Our study seeks to do image segmentation on brain MRI \\ndata using deep learning techniques, for faster and more accurate cancer identification \\nand localization in the brain. Patients, specifically over the age of 40 years, with brain \\ntumors have a dismal survival rate, and many tumors even go unnoticed. It takes \\nextremely little time to forecast a brain tumor when these algorithms are applied to MRI \\npictures, and higher accuracy makes it easier to treat patients where complicated cases \\nusually require experienced medical personnel to locate the area of the tumor, compare \\naffected tissues with nearby regions, and provide the final verdict. The radiologists can \\nmake speedy decisions thanks to these projections. \\nThe major contribution of the paper is mentioned as below:'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='make speedy decisions thanks to these projections. \\nThe major contribution of the paper is mentioned as below: \\n1) Segmentation of the MRI images using deep  learning algorithms for \\nbetter generalization capabilities. \\n2) Investigation of enhanced deep learning segmentation algorithms for \\nbrain tumors. 3 \\nThe rest of the paper is organized as Section 2 discusses the related literature present in \\nthe domain and motivation for doing this work. Sec tion 3 presents the methodology \\nused in the work for brain tumor segmentation. Section 4 provides the details of the \\ndeep-learning hyper-parameters and the evaluation metrics used in this work. Also, this \\nsection presents the statistical and visual result along with a discussion. Section 5 \\nconcludes the paper. \\n2. Related Works \\nMost studies show analysis on image data or classification techniques with remote \\nsegmentation techniques. Javaria Amin et al. [5] is an overview of the latest techniques'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='2. Related Works \\nMost studies show analysis on image data or classification techniques with remote \\nsegmentation techniques. Javaria Amin et al. [5] is an overview of the latest techniques \\nused in deep learning that are being used for tumor detection. The paper delves deeply \\ninto the types of imaging involved for the brain, such as MRIs, CAT scans, PET etc. \\nDifferent image segmentation techniques need to be used for different types of imaging \\nmethods.  The paper also gives us an insight about the datasets publicly available, and \\nthe criteria by which each deep learning model used for image segmentation was \\nevaluated. This research helped us identify that we are to use MRI imaging data for our \\nown research and that deep learning models like U-Net and its variants would be best \\nsuited to get the most accurate results. Instead of using the resulting tumor segmentation \\nmasks, Getty N. et al. [6] recreated the capsule network architecture [7] , refined the'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='suited to get the most accurate results. Instead of using the resulting tumor segmentation \\nmasks, Getty N. et al. [6] recreated the capsule network architecture [7] , refined the \\nmodel parameters and MRI tumor imagesâ€™ preprocessing. It is stated that segmentation \\nis not required for identifying the type of tumor, but the laborious problem of manually \\nsegmenting the tumor for loc alization and description still remains at hand . For mass \\ndetection in MRI scans, BrainMRNet, a deep learning model was developed by ToÄŸaÃ§ar \\net al. [8]. Interestingly, an additional segmentation technique is also developed which \\ndetermines the lobe area in the brain with higher concentration of two classes of tumors. \\nGunasekara SR et al. [9] present a comprehensive end -to-end systematic method for \\nMRI-based tumor segmentation and detection of meningiomas and gliomas. A \\nstraightforward CNN algorithm is used to classify brain tumors, then a Faster R-CNN'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='MRI-based tumor segmentation and detection of meningiomas and gliomas. A \\nstraightforward CNN algorithm is used to classify brain tumors, then a Faster R-CNN \\nnetwork is used to localize the tumor, and finally the Chan-Vese algorithm [10] is used \\nto precisely segment the tumor . The final output was the precise tumor boundary for \\nsegmentation purposes for each given axial brain MRI. All three algorithms were linked \\nin a cascade fashion. The suggested Faster R -CNN model is used to extract the \\nbounding box, and then a segmentation approach is used to provide the tumorâ€™s precise \\ncontour. For early brain tumor d etection, a brilliant approach was suggested by \\nSuneetha and Rani [11]. The proposed method involves pre -processing the acquired \\nbrain MRI images using the Optimized Kernel Possibilistic C -means Method \\n(OKPCM). To enhance the image, an adaptive Double Wi ndow Modified Trimmed \\nMean Filter (DWMTMF) is then employed. At last, the images are segmented using'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='(OKPCM). To enhance the image, an adaptive Double Wi ndow Modified Trimmed \\nMean Filter (DWMTMF) is then employed. At last, the images are segmented using \\nthe region expanding method. Kadkhodai et al. [12] created an image enhancing model. \\nBased on the intensities, t he enhanced images are then segmented using 3D super -\\nvoxels. The study proposes a saliency detection-based feature employed with an edge-\\naware filtering technique which aligns the edges of the original image and saliency map \\nfurther enhancing the border of the tumor. The output of their neural network is the \\nsegmented tumor. A proposition of Enhanced Convolutional Neural Network (ECNN) \\nwas put forth by Thaha et al. [13] accompanied by BAT algorithm as the loss function 4 \\nwith the primary aim of presenting optimization-based segmentations. Small kernels \\nused in the model , when network has lower w eights, enable deep architectural design \\nand have favorable effects on overfitting.'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content=\"used in the model , when network has lower w eights, enable deep architectural design \\nand have favorable effects on overfitting. \\nRonneberger et al [14] proposed an architecture with a symmetric pai r \\nof contracting path to collect context and an expanding path to enable exact \\nlocalization. In certain categories, the study easily won the 2015 ISBI Cell Tracking \\nChallenge. The work of Oktay et al. [15] learns to concentrate automatically on target \\nstructures of varying forms and sizes in medical imaging by combining  the attention \\ngate model with the U-Net architecture. According to experimental findings, Attention \\ngates preserve computational efficiency while continuously enhancing U -Net's \\naccuracy across various datasets. Zhang et al. [16], suggest the use of ResUnet in their \\nresearch. This study proposes a neural network for semantic segmentation that blends \\nresidual learning with U -Net. This model has the advantage of making deep network\"),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='research. This study proposes a neural network for semantic segmentation that blends \\nresidual learning with U -Net. This model has the advantage of making deep network \\ntraining easy due to residual units and the quantity of skip links within the network that \\nenable information flow.  The concept is commonly used to partition medical images \\ninto different bodily areas and modalities.  [17][18]. To overcome the difficulty of \\ndistinguishing healthy cells from tumor boundaries  in the diagnosis of brain tumors, \\nDeepSeg, a new deep learning architecture has been developed by Zeineldin et al. [19]. \\nThe system that was developed is an interactive decoupling framework in which the \\nencoder part uses a convolutional neural network to do spatial information processing \\nand the decoder part provides the full -resolution probability map from the generated \\nmap. The dense convolutional network (DenseNet), the NASNet using modified U-Net,'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='and the decoder part provides the full -resolution probability map from the generated \\nmap. The dense convolutional network (DenseNet), the NASNet using modified U-Net, \\nand the residual network (ResNet) were just a few of the CNN models included in the \\nstudy. For colonoscopic image segmentation, Jha et al. [20] developed ResUNet++, an \\nenhanced ResUNet architectur e that suggests methods to boost its sensitivity to the \\nimportant elements, suppress the unimportant features, and give larger context. These \\nmethods include  residual blocks along with attention mechanism,  Atrous Spatial \\nPyramidal Pooling (ASPP), and squeeze and excitation blocks. Comparing ResUNet++ \\nto other techniques, the outcomes for the colorectal polyps were better. The suggested \\nmodels make use of the Residual and Recurrent Network, and U -Net in the paper of \\nMd Zahangir Alom et al. [21]. These structures for segmentation problems provide'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='models make use of the Residual and Recurrent Network, and U -Net in the paper of \\nMd Zahangir Alom et al. [21]. These structures for segmentation problems provide \\nbenefits like aids deep architecture training, superior feature representation for \\nsegmentation tasks ensured by feature accumula tion and improves performance for \\nmedical image segmentation with the same amount of network parameters [22]. \\nWith the review of the related work in the field of medical image segmentation, \\nstudies that use segmentation algorithms leave potential grounds for higher \\nperformance with fewer complexities. The problem of segmenting medical images has \\nbenefited from the development of U -Net architecture by using the capabilities of \\nattention and recurrent mechanisms, different backbones, and re sidual blocks. It has \\nshown more opportunities to improve the effectiveness of such an architecture for \\ntumor segmentation. \\n3. Materials and Methodology'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='shown more opportunities to improve the effectiveness of such an architecture for \\ntumor segmentation. \\n3. Materials and Methodology \\nAs presented in Fig. 1 our study follows the ensuing methodology for acquiring results \\nfrom the input dataset. The dataset is initially preprocessed to provide appropriate data 5 \\nfor the implementations. Images obtained from preprocessing are then partitioned into \\ntraining, validation, and testing datasets. After determining the appropriate \\nhyperparameters, the designated training and validation data are used to train the model. \\nUsing the inference on validation data, the model with best training coefficients is \\nsaved. Testing data is then utilized to gather inference using this model. Comparison \\nbetween different methodologies using various metrics is performed. \\n \\n \\nFig. 1. Flow chart for conducting this work \\n3.1  Image Dataset \\nThe brain T1 -weighted CE -MRI dataset was obtained from Tianjing Medical'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='Fig. 1. Flow chart for conducting this work \\n3.1  Image Dataset \\nThe brain T1 -weighted CE -MRI dataset was obtained from Tianjing Medical \\nUniversity and Nanfang Hospital in Guangzhou, China, between 2005 and 2010  [23]. \\nThe dataset was first shared publicly in 2015 and saw multiple revisions, with the most \\nrecent iteration of the dataset released in 2017. The collection comprises 708 \\nmeningiomas, 1426 gliomas, a nd 930 pituitary tumors in 3064 T1 -weighted, contrast-\\nenhanced pictures, as depicted in Fig. 2, from 233 patients. Data fields for the tumor \\nlabel, patient ID, image data, tumor boundary and mask data are stored  in MATLAB \\nfile format. Further, a crucial preprocessing stage was required to appropriately use the \\ndata with Python. \\n  \\nFig. 2. Sample images of T1-weighted MRI dataset, a) MRI scans b) Ground Truth masks \\n3.2  Preprocessing Stage \\nSince the files were in MATLAB format, it was required to convert and preserve their'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='Fig. 2. Sample images of T1-weighted MRI dataset, a) MRI scans b) Ground Truth masks \\n3.2  Preprocessing Stage \\nSince the files were in MATLAB format, it was required to convert and preserve their \\ncontents as standard images so that our techniques could use them effectively without \\nwasting memory by loading each mat file repeatedly. Using the SciPy module, the files \\nwith the \"mat\" extension were imported as dictionaries.  \\n6 \\nThe data included fields such as tumor label : 1 for meningioma, 2 for glioma, 3 for \\npituitary tumor, patient id, image data: grayscale values with 3 channels in the form of \\nan array, tumor border : a vector storing the c oordinates of discrete points on tumor \\nborder; [x1, y1, x2, y2,...] in which x1, y1 indicate planar coordinates of tumor border, \\nand tumor mask data : a binary image with 1 indicating tumor region  and 0 indicating \\nnon-tumor region. Image data and tumor mask data were extracted as NumPy arrays by'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='and tumor mask data : a binary image with 1 indicating tumor region  and 0 indicating \\nnon-tumor region. Image data and tumor mask data were extracted as NumPy arrays by \\niterating through the mat files. The values were normalized between 0 and 1. Using the \\nOpenCV module, all the processed images are stored in \"png\" format and divided into \\ntrain (2485 instances), validation (274 instances), and test (305 instances) sets. \\n3.3  Methodology \\nU-Net \\nU-Net is an architecture for a convolutional neural network that was developed mainly \\nfor image segmentation and offers advancements over CNNs , to deal with biomedical \\nimages where the goal is to not only categorize the infection but also to identify its \\nlocation [14]. Prior to U-net, classification networks were unable to segment an image \\nusing pixel-level contextual information. U-net\\'s adaptability led researchers to heavily \\ninclude it in subsequent investigations using new imaging techniques.  This research'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content=\"using pixel-level contextual information. U-net's adaptability led researchers to heavily \\ninclude it in subsequent investigations using new imaging techniques.  This research \\nhas grown steadily throughout the years, incorporating several imaging techniques and \\napplication fields. [24] \\n \\nFig. 3.  U-net architecture for tumor segmentation \\nAs presented in Fig. 3, t he U-Net design can be seen as a pair of encoder -decoder \\nnetworks with an expanding path that provides exact localization and a symmetric \\ncontracting path that collects contextual and spatial data, giving it a u-like shape. Each \\nblock of the contracting route, repeated for a few increasing filters, consists of two \\nsuccessive 3 x 3 convolutions, a ReLU activation unit and a max-pooling layer.  \\n7 \\nThe U-Net's novel features may be found in the extending path, where, in each stage, \\nthe feature map is upsampled by 2 x 2 up -convolution. The contraction pathâ€™s feature\"),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content=\"7 \\nThe U-Net's novel features may be found in the extending path, where, in each stage, \\nthe feature map is upsampled by 2 x 2 up -convolution. The contraction pathâ€™s feature \\nmap from the matching layer is stacked onto the upsampled feature map to transport \\nthe encoderâ€™s high-resolution feature maps directly to the decoder network using skip \\nconnections. Then come two successive 3 x 3 convolutions, escorted by ReLU \\nactivation. This design not only exceeded the most effective technique at its imminence \\n(a sliding window ConvNet) but was also simpler and quicker to train end-to-end with \\nless input images. \\nAttention U-Net \\nAdding to the capabilities of U -net, the attention mechanism  [15] intends to recreate \\nthe capability of hu mans to concentrate on relevant instances while ignoring others in \\nthe neural network and allocate more computing resources to important stimuli . The \\ntask is achieved using an attention gate which when added to the skip connection within\"),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='the neural network and allocate more computing resources to important stimuli . The \\ntask is achieved using an attention gate which when added to the skip connection within \\nthe U-Net, provides localized classification resulting in more accurate and robust image \\nclassification performance and progressively suppresses highlight responses in \\nunrelated background areas.  As shown in Fig. 4, attention gate accepts input from the \\nsucceeding deepest layer feature map (better feature representation) and skip \\nconnection feature map from the contracting path (better spatial information). A \\nsequence of operations like, adding the inputs after convolut ion to obtain aligned and \\nunaligned weights (selective concentration), activation layer, single filter and stride \\nconvolution, sigmoid function for scaling weights and finally, resampling. The output \\nand skip connection feature map are multiplied and conti nue the procedure of U -Net.'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='convolution, sigmoid function for scaling weights and finally, resampling. The output \\nand skip connection feature map are multiplied and conti nue the procedure of U -Net. \\nGradients from background areas are de-weighted in the backward pass, this allows the \\nmodel parameters in earlier layers to be modified based on spatial zones that are crucial \\nfor a particular task, and subsequently improves model sensitivity and accuracy towards \\nforeground pixels. \\n \\nFig. 4. Attention gate with gating signal (g) from encoder and current level decoder feature \\nmap (x) with XÌ‚  as output to be concatenated with x \\nResUnet \\nHe et al [25]. addressed in their study that a very deep architecture is difficult to train \\ndue to issues like vanishing gradients and adverse effects on the generalization power \\nof the model. To overcome this predicament, they suggested using an identity mapping \\narchitecture for deep residual learning . The intuition to solve the degradation problem'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='of the model. To overcome this predicament, they suggested using an identity mapping \\narchitecture for deep residual learning . The intuition to solve the degradation problem \\nis to recognize that shallower networks perform better than the deeper networks and \\nskipping extra layers can help us maintain the depth of layers and prevent degradation.  \\n8 \\nIn this model, the layers give an output as H(x) = F(x) + x , where F(x) is residual \\n(difference between output and input) and x being the identity skip connection as shown \\nin Fig. 5. Hence, the layers in the residual network are learning the residual and not the \\ntrue output which resolves the vanishing gradient problem and helps the system avoid \\nlossy compression with identity mapping. \\n \\nFig. 5. Residual learning: a building block \\nUsing the best of Deep Residual learning and U-Net architecture, Zhang et al. [16] came \\nup with the Deep Residual U-Net architecture. ResUnet also consists of encoding path,'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content=\"Using the best of Deep Residual learning and U-Net architecture, Zhang et al. [16] came \\nup with the Deep Residual U-Net architecture. ResUnet also consists of encoding path, \\nskip connections and decoding path, where both the encoder and decoder units \\nincorporate residual blocks for each level. Combining the se crucial techniques, the \\nnetwork's training process is facilitated by the residual unit, and information spread s \\nwithout degradation through the skip connections. Anita et al. [18]  found in their study \\nthat, for lung segmentation, more discri minative feature representations can be \\nextracted by a deeper residual network than a shallow network. \\nResUnet++ \\nResUnet++ is based on the structure of ResUnet . In addition to that, it uses techniques \\nlike squeeze and excitation blocks, attention blocks, and Atrous Spatial Pyramidal \\nPooling (ASPP) [20]. This model begins with a stem block which is used to\"),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='like squeeze and excitation blocks, attention blocks, and Atrous Spatial Pyramidal \\nPooling (ASPP) [20]. This model begins with a stem block which is used to \\ndownsample the input image to ensure operations maintai n low computational \\ncomplexity and efficiency in results and passed onto the encoding path. Each encoder \\nblockâ€™s output is passed to the squeeze -and-excitation block which modifies the \\nfeatures and enhances the networkâ€™s characteristic power. This leads to  suppression of \\nthe redundant features and increased sensitivity for pertinent features [26]. Through the \\nASPP network, the output of the encoding block is passed [27] which connects the \\nencoder to the decoder. It enlarges the field -of-view of the filters  by providing multi -\\nscale information to include a broader context. Before each decoder block, an attention \\nblock is used to improve the quality of features that improves the outcome. To obtain'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='scale information to include a broader context. Before each decoder block, an attention \\nblock is used to improve the quality of features that improves the outcome. To obtain \\nthe final segmentation map at the end of the decoding path, another ASPP block is \\nutilized followed by a 1x1 convolution with sigmoid activation. \\nRecurrent Residual U-Net (R2Unet) \\nAlong with the structure of U -Net and the residual block technique, recurrent \\nconvolutions improve the modelâ€™s capability to integrate context information to ensure \\nbetter feature representation for segmentation. Keeping track of former input and \\ncurrent pixel informati on is necessary for recurrent blocks to anticipate future output. \\nThis helps in remembering and integrating context information which is most \\nsignificant in semantic segmentation [28]. \\n9 \\n \\nFig. 6. Recurrent Residual convolutional units (RRCU) \\nThe combination of residual units and recurrent units as depicted in Fig. 6 are extended'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='significant in semantic segmentation [28]. \\n9 \\n \\nFig. 6. Recurrent Residual convolutional units (RRCU) \\nThe combination of residual units and recurrent units as depicted in Fig. 6 are extended \\nto each block of the encoder and decoder component of U -Net architecture [21]. The \\nresidual unit helps in avoiding per formance degradation by incorporating long/short \\nskip connections over deep networks, and the recurrent unit can withhold reasonable \\ndependencies among pixel values by considering contextual data. In blocks of both the \\nencoder and decoder, recurrent convol utional layers (RCLs) with residual units are \\nutilized in place of conventional forward convolutional layers, which aids in the \\ndevelopment of a deeper and effective model. This segmentation approach \\ndemonstrates the efficiency of feature accumulation from one portion of the network to \\nanother and shows benefits for both training and testing phases. \\n4. Experimentations and Result Discussion'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='demonstrates the efficiency of feature accumulation from one portion of the network to \\nanother and shows benefits for both training and testing phases. \\n4. Experimentations and Result Discussion \\nThe experimentations were carried out on the Jupyter notebook platform. The collected \\ndata was preprocessed using SciPy to load MATLAB data and open-cv for normalizing \\nthe grayscale values and saving the images. The prepared data was trained on \\nalgorithms implemented using keras library and keras -unet-collection [29]. The \\nexperimental works, including training and testing, are carried out using the NVIDIA \\nTESLA P100 GPU with 2 CPU cores, 16 GB GPU memory and 13 GB RAM. All \\nmodels in our study are trained entirely from scratch without any prior weights. Only \\nfor U-Net and Attention U -Net, three different backbones are utilized to incorporate \\ntheir salient features, viz. VGG-19, ResNet152 and DenseNet201. \\n4.1  Deep learning Hyperparameters'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content=\"for U-Net and Attention U -Net, three different backbones are utilized to incorporate \\ntheir salient features, viz. VGG-19, ResNet152 and DenseNet201. \\n4.1  Deep learning Hyperparameters \\nThe Table. 1 presents the deep learning hyperparameters used in each o f the tumor \\nprediction algorithms which are kept the same for exact comparison. \\n \\nTable 1. Hyper-parameters used for training of various deep learning models \\nHyperparameters Values \\nLearning Rate 0.001 \\nBeta 1 0.9 \\nBeta 2 0.999 \\nOptimizer Adam \\nLoss function Binary Cross Entropy \\nBatch Size 32 \\nEpochs 100 \\n \\n10 \\n4.2  Evaluation Metrics \\nA trained modelâ€™s results should be summarized using metrics that are better at showing \\nthe model's segmentation skills. The precision (P) is calculated using ð‘ƒ =\\nð‘¡ð‘\\nð‘¡ð‘ + ð‘“ð‘, recall  \\n(R) is calculated using ð‘… =\\nð‘¡ð‘\\nð‘¡ð‘ + ð‘“ð‘›  which are used to calculate F1-Score represented by \\nð¹1 ð‘†ð‘ð‘œð‘Ÿð‘’ = 2 âˆ—\\nð‘ƒ âˆ— ð‘…\\nð‘ƒ + ð‘… and IoU is given as ð¼ð‘œð‘ˆ =\\nð‘¡ð‘\\nð‘¡ð‘ + ð‘“ð‘› + ð‘“ð‘. In the formulae, tp represents\"),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='(R) is calculated using ð‘… =\\nð‘¡ð‘\\nð‘¡ð‘ + ð‘“ð‘›  which are used to calculate F1-Score represented by \\nð¹1 ð‘†ð‘ð‘œð‘Ÿð‘’ = 2 âˆ—\\nð‘ƒ âˆ— ð‘…\\nð‘ƒ + ð‘… and IoU is given as ð¼ð‘œð‘ˆ =\\nð‘¡ð‘\\nð‘¡ð‘ + ð‘“ð‘› + ð‘“ð‘. In the formulae, tp represents \\ntrue positive, tn represents true negative, fp represents false positive and fn represents \\nfalse negative. These metrics are often utilized for judging the performance of medical \\nimage segmentation [30], [14], [15] and are preferred over pixel accuracy as they are \\nbetter at measuring the segmentationâ€™s perceptual quality [31]. For object segmentation, \\nthe dice score more accurately measures size and localization agreement [32]. \\n4.3  Statistical and visual results \\nBased on the evaluation metrics, both statistical and visual results were gathered using \\nthe predicted output and ground truth which are provided below. \\n \\n \\nFig. 7. Precision and Recall of all applied deep learning models for tumor segmentation'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='the predicted output and ground truth which are provided below. \\n \\n \\nFig. 7. Precision and Recall of all applied deep learning models for tumor segmentation \\nTable 3. Statistical results of various state-of-art deep learning models for tumor segmentation \\nMethodologies F1 Score Mean IoU \\nUNET (VGG-19 backbone) 0.8033 0.8322 \\nUNET (ResNet152 backbone) 0.8116 0.8382 \\nUNET (Densenet201 backbone) 0.8288 0.8507 \\nAttention UNET (VGG-19 backbone) 0.8060 0.8342 \\nAttention UNET (ResNet152 backbone) 0.8188 0.8434 \\nAttention UNET (Densenet201 backbone) 0.8349 0.8553 \\nResUnet 0.8360 0.8562 \\nResUnet++ 0.7969 0.8272 \\nRecurrent Residual UNET 0.8495 0.8665 \\n \\n0.65 0.7 0.75 0.8 0.85 0.9\\nUNET (VGG-19 backbone)\\nUNET (ResNet152 backbone)\\nUNET (Densenet201 backbone)\\nAttention UNET (VGG-19 backbone)\\nAttention UNET (ResNet152 backbone)\\nAttention UNET (Densenet201â€¦\\nResUnet\\nResUnet++\\nRecurrent Residual UNET\\nRecall Precision11 \\nInstance \\nNumber MRI Scan Ground \\nTruth U-Net Attention'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='Attention UNET (ResNet152 backbone)\\nAttention UNET (Densenet201â€¦\\nResUnet\\nResUnet++\\nRecurrent Residual UNET\\nRecall Precision11 \\nInstance \\nNumber MRI Scan Ground \\nTruth U-Net Attention \\nU-Net ResUnet ResUnet++ R2Unet \\n1. \\n       \\n2. \\n       \\n3. \\n       \\n4. \\n       \\n5. \\n \\n       \\n6. \\n       \\n7. \\n       \\n8. \\n       \\n9. \\n       \\n10. \\n       \\nFig. 8. Visual (Qualitative) results of various brain tumor segmentation models on T1 -\\nWeighted MRI scans \\n \\n12 \\nBased on the metrics, all the models quantitatively performed well. Table 3 and Fig. 7 \\nfindings show that U-Net and Attention U-Net are more accurate with DenseNet201 as \\nthe backbone. Primarily, it can be concluded from statistical findings in Table 3 that  \\nRecurrent Residual UNET (R2Unet) is the most effective model for segmenting brain \\ntumors from the test data. R2Unet (highlighted red in Table 3) outperformed other deep \\nlearning models according to all metrics except precision where ResUnet++ seems to'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content=\"tumors from the test data. R2Unet (highlighted red in Table 3) outperformed other deep \\nlearning models according to all metrics except precision where ResUnet++ seems to \\nperform better. For comparing qualitative results of the models, we chose 10 random \\nsamples from the test dataset as shown in Fig. 8. These MRI images exist in the form \\nof three different planes, viz. Sagittal, Axial and Coronal planes.  Compared to the \\nground truth and the outcome of various models having certain defects like under and \\nover-segmentation and erroneous borders, segmentation predictions from R2Unet are \\nalmost perfect. Instance 3 from Fig. 8 shows that U-Net and ResUnet failed to detect \\nthe tumor, while Attention U -Net and ResUnet++'s predictions are under -segmented. \\nR2Unet, conversely, delivered accurate segmentation with little disorder in the \\nboundary. In instances 2, 5, 6, 7, and 10, R2Unet predictions have more fidelity to the\"),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='R2Unet, conversely, delivered accurate segmentation with little disorder in the \\nboundary. In instances 2, 5, 6, 7, and 10, R2Unet predictions have more fidelity to the \\nground truth than other models. However, R2Unetâ€™s predictions occasionally (instances \\n1, 4, 9) show slight over -segmentation whereas ResUnet and ResUnet++ predictions \\nare more helpful.  Instances 2, 3, 4 and 10 have simple ground truth shapes, and yet \\nResUnet, ResUnet++ an d R2Unet show jagged segmentations while both U -Net and \\nAttention U-net retain some smoothness. Additionally, other instances (1, 5, 6, 7, 8 and \\n9) make it clear that the shape of the tumor is preserved by ResUnet, ResUnet ++ and \\nR2Unet more than U-Net and Attention U-Net. \\nR2Unet has proved to provide better results among the compared models since it is \\ncapable of keeping a record of previous input and current pixel information to utilize in \\npredicting future output; in this way, it helps the model integrat e context information'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='capable of keeping a record of previous input and current pixel information to utilize in \\npredicting future output; in this way, it helps the model integrat e context information \\nthat is significant in semantic segmentation. Additionally, the introduction of the \\nfeature accumulation technique in recurrent convolutional layers has provided more \\nrobust feature representation vital for extracting low-level features especially pertinent \\nto medical image segmentation. \\n5. Conclusion \\nDetecting tumors is difficult and expensive in the modern world since it is done mostly \\nthrough imaging and via specialists. This can be tackled by the means of computer -\\naided detection. In this work, the suitable model for tumor extraction from MRI scans \\nis investigated. The results obtained from this work show that an encoder-decoder based \\nConvolutional Neural Network architecture fused with Recurrent and Residual units'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='is investigated. The results obtained from this work show that an encoder-decoder based \\nConvolutional Neural Network architecture fused with Recurrent and Residual units \\nwhen trained on a dataset of brain tumor MRI scans, using Adam optimizer, produces \\na F1 score of 0.8495 and an IoU of 0.8665, and outperforms other compared models \\nlike U -Net and Attention U -Net, deep Residual U -Net with its variants. Also, the \\nqualitative results show consistent results for various representations of the MRI scans. \\nThese algorithms reduce the burden on the doctors and make healthcare more accessible \\nand inexpensive for all. Our work helped us realize that residual block removes \\nvanishing gradient, the attention mechanism provides a focus on essential features for \\nsegmentation. The accuracy of the suggested approach can be further improved by \\nemploying larger, more varied datasets, and various preprocessing techniques. We are 13'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='segmentation. The accuracy of the suggested approach can be further improved by \\nemploying larger, more varied datasets, and various preprocessing techniques. We are 13 \\nalso exploring other architectures that would bridge the semantic gap between encoder \\nand decoder feature maps by creating dense skip connections on skip pathways which \\nwould improve gradient flow. Combining all these efforts, we look forward to creating \\na highly accurate model for tumor segmentation.  \\nReferences \\n[1] D. Ricard, A. Idbaih, F. Ducray, M. La hutte, K. Hoang -Xuan, and J. -Y. \\nDelattre, â€œPrimary brain tumours in adults,â€ Lancet, vol. 379, no. 9830, p. \\n1984â€”1996, May 2012, doi: 10.1016/s0140-6736(11)61346-9. \\n[2] Cancer.Net Editorial Board, â€œBrain Tumor: Diagnosis,â€ Cancer.Net, Sep. \\n2021. \\n[3] S. J. Russell and P. Norvig, Artificial Intelligence: A modern approach, 3rd ed. \\nPearson, 2009. \\n[4] L. Cai, J. Gao, and D. Zhao, â€œA review of the application of deep learning in'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='2021. \\n[3] S. J. Russell and P. Norvig, Artificial Intelligence: A modern approach, 3rd ed. \\nPearson, 2009. \\n[4] L. Cai, J. Gao, and D. Zhao, â€œA review of the application of deep learning in \\nmedical image classification and segmentation,â€ Ann Transl Med , vol. 8, no. \\n11, pp. 713â€“713, Jun. 2020, doi: 10.21037/atm.2020.02.44. \\n[5] J. Amin, M. Sharif, A. Haldorai, Y. Mussarat, and R. Nayak, â€œBrain tumor \\ndetection and classification using machine learning: a comprehensive survey,â€ \\nComplex & Intelligent Systems , vol. 8, Oct. 2021, d oi: 10.1007/s40747 -021-\\n00563-y. \\n[6] N. Getty, T. Brettin, D. Jin , R. Stevens, and F. Xia, â€œDeep medical image \\nanalysis with representation learning and neuromorphic computing,â€ Interface \\nFocus, vol. 11, p. 20190122, Oct. 2021, doi: 10.1098/rsfs.2019.0122. \\n[7] P. Afshar, A. Mohammadi, and K. N. Plataniotis, â€œBrain Tumor  Type \\nClassification via Capsule Networks,â€ in 2018 25th IEEE International'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='[7] P. Afshar, A. Mohammadi, and K. N. Plataniotis, â€œBrain Tumor  Type \\nClassification via Capsule Networks,â€ in 2018 25th IEEE International \\nConference on Image Processing (ICIP) , 2018, pp. 3129 â€“3133. doi: \\n10.1109/ICIP.2018.8451379. \\n[8] M. ToÄŸaÃ§ar, B. Ergen, and Z. CÃ¶mert, â€œTumor type detection in brain MR \\nimages of the  deep model developed using hypercolumn technique, attention \\nmodules, and residual blocks,â€ Med Biol Eng Comput, vol. 59, Oct. 2020, doi: \\n10.1007/s11517-020-02290-x. \\n[9] S. Gunasekara, N. Kaldera, and M. Dissanayake, â€œA Systematic Approach for \\nMRI Brain Tu mor Localization and Segmentation Using Deep Learning and \\nActive Contouring,â€ J Healthc Eng , vol. 2021, Oct. 2021, doi: \\n10.1155/2021/6695108. \\n[10] P. Getreuer, â€œChan-Vese Segmentation,â€ Image Processing On Line, vol. 2, pp. \\n214â€“224, Oct. 2012, doi: 10.5201/ipol.2012.g-cv. \\n[11] S. Bobbillapati and J. Areti, â€œA NOVEL APPROACH FOR BRAIN TUMOR \\nDETECTION USING DW -MTM FILTER AND REGION GROWING'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='214â€“224, Oct. 2012, doi: 10.5201/ipol.2012.g-cv. \\n[11] S. Bobbillapati and J. Areti, â€œA NOVEL APPROACH FOR BRAIN TUMOR \\nDETECTION USING DW -MTM FILTER AND REGION GROWING \\nSEGMENTATION IN MR IMAGING,â€ PONTE International Scientific \\nResearchs Journal, vol. 74, Oct. 2018, doi: 10.21506/j.ponte.2018.3.10. \\n[12] M. Kadkhodaei et al., â€œAutomatic Segmentation of Multimodal Brain Tumor \\nImages Based on Classification of Super-Voxels,â€ in Conference proceedings: \\n... Annual International Conference of the IEEE Engineering in Medicine and 14 \\nBiology Society.  IEEE Engineering in Medicine and Biology Society. \\nConference, Oct. 2016, vol. 2016. doi: 10.1109/EMBC.2016.7592082. \\n[13] M. Thaha, P. Kumar, M. B S, S. Dhanasekeran, P. Vijayakarthick, and A. Selvi, \\nâ€œBrain Tumor Segmentation Using Convolutional Neural Net works in MRI \\nImages,â€ J Med Syst, vol. 43, Oct. 2019, doi: 10.1007/s10916-019-1416-0. \\n[14] O. Ronneberger, P. Fischer, and T. Brox, â€œU-Net: Convolutional Networks for'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='Images,â€ J Med Syst, vol. 43, Oct. 2019, doi: 10.1007/s10916-019-1416-0. \\n[14] O. Ronneberger, P. Fischer, and T. Brox, â€œU-Net: Convolutional Networks for \\nBiomedical Image Segmentation,â€ in LNCS, Oct. 2015, vol. 9351, pp. 234â€“241. \\ndoi: 10.1007/978-3-319-24574-4_28. \\n[15] O. Oktay et al., â€œAttention U-Net: Learning Where to Look for the Pancreas,â€ \\nArXiv, vol. abs/1804.03999, 2018. \\n[16] Z. Zhang and Q. Liu, â€œRoad Extraction by Deep Residual U -Net,â€ IEEE \\nGeoscience and Remote Sensing Letters , vol. P P, Oct. 2017, doi: \\n10.1109/LGRS.2018.2802944. \\n[17] G. M. Venkatesh, Y. G. Naresh, S. Little, and N. E. Oâ€™Connor, â€œA Deep \\nResidual Architecture for Skin Lesion Segmentation,â€ in OR 2.0 Context -\\nAware Operating Theaters, Computer Assisted Robotic Endoscopy, C linical \\nImage-Based Procedures, and Skin Image Analysis, 2018, pp. 277â€“284. \\n[18] A. Khanna, N. D. Londhe, S. Gupta, and A. Semwal , â€œA deep Residual U-Net'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='Image-Based Procedures, and Skin Image Analysis, 2018, pp. 277â€“284. \\n[18] A. Khanna, N. D. Londhe, S. Gupta, and A. Semwal , â€œA deep Residual U-Net \\nconvolutional neural network for automated lung segmentation in computed \\ntomography images,â€ Biocybern Biomed Eng , vol. 40, no. 3, pp. 1314 â€“1327, \\nJul. 2020, doi: 10.1016/j.bbe.2020.07.007. \\n[19] R. Zeineldin, M. Karar, J. Coburger, R. Wirtz, and O. Burgert, â€œDeepSeg: deep \\nneural network framework for automatic brain tumor segmentation using \\nmagnetic resonance FLAIR images,â€ Int J Comput Assist Radiol Surg, vol. 15, \\nOct. 2020, doi: 10.1007/s11548-020-02186-z. \\n[20] D. Jha et al. , â€œResU Net++: An Advanced Architecture for Medical Image \\nSegmentation,â€ in Proceedings - 2019 IEEE International Symposium on \\nMultimedia, ISM 2019 , Dec. 2019, pp. 225 â€“230. doi: \\n10.1109/ISM46123.2019.00049. \\n[21] M. Zahangir Alom, M. Hasan, C. Yakopcic, T. M. Taha,  and V. K. Asari,'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='Multimedia, ISM 2019 , Dec. 2019, pp. 225 â€“230. doi: \\n10.1109/ISM46123.2019.00049. \\n[21] M. Zahangir Alom, M. Hasan, C. Yakopcic, T. M. Taha,  and V. K. Asari, \\nâ€œRecurrent Residual Convolutional Neural Network based on U -Net (R2U -\\nNet) for Medical Image Segmentation,â€ arXiv e-prints, p. arXiv:1802.06955, \\nFeb. 2018. \\n[22] M. Z. Khan, â€œRecurrent Residual U-Net: Short Critical Review,â€ 2021. \\n[23] J. C heng, â€œbrain tumor dataset,â€ Oct. 2017, doi: \\n10.6084/m9.figshare.1512427.v5. \\n[24] N. Siddique, S. Paheding, C. P. Elkin, and V. Devabhaktuni, â€œU -net and its \\nvariants for medical image segmentation: A review of theory and applications,â€ \\nIEEE Access, 2021, doi: 10.1109/ACCESS.2021.3086020. \\n[25] K. He, X. Zhang, S. Ren, and J. Sun, â€œDeep Residual Learning for Image \\nRecognition,â€ Dec. 2015, [Online]. Available: http://arxiv.org/abs/1512.03385 \\n[26] J. Hu, L. Shen, S. Albanie, G. Sun, and E. Wu, â€œSqueeze -and-Excitation'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='Recognition,â€ Dec. 2015, [Online]. Available: http://arxiv.org/abs/1512.03385 \\n[26] J. Hu, L. Shen, S. Albanie, G. Sun, and E. Wu, â€œSqueeze -and-Excitation \\nNetworks,â€ IEEE Trans Pattern Anal Mach Intell , vol. 42, no. 8, pp. 2011 â€“\\n2023, 2020, doi: 10.1109/TPAMI.2019.2913372. \\n[27] K. He, X. Zhang, S. Ren, and J. Sun, â€œSpatial Pyramid Pooling in Deep \\nConvolutional Networks for Visual Recognition,â€ IEEE Tra ns Pattern Anal 15 \\nMach Intell , vol. 37, no. 9, pp. 1904 â€“1916, 2015, doi: \\n10.1109/TPAMI.2015.2389824. \\n[28] M. Liang and X. Hu, â€œRecurrent convolutional neural network for object \\nrecognition,â€ 2015 IEEE Conference on Computer Vision and Pattern \\nRecognition (CVPR), pp. 3367â€“3375, 2015. \\n[29] Y. (Kyle) Sha, â€œyingkaisha/keras -unet-collection: v0.1.13,â€ Jan. 2022, doi: \\n10.5281/ZENODO.5834880. \\n[30] K. Kamnitsas et al., â€œEfficient Multi-Scale 3D CNN with fully connected CRF \\nfor Accurate Brain Lesion Segmentation,â€ Med Image Anal, vol. 36, Oct. 2016,'),\n",
       " Document(metadata={'arxiv_id': '2305.00257v1', 'title': 'Brain Tumor Segmentation from MRI Images using Deep Learning Techniques', 'section': 'body', 'authors': 'Ayan Gupta, Mayank Dixit, Vipul Kumar Mishra'}, page_content='10.5281/ZENODO.5834880. \\n[30] K. Kamnitsas et al., â€œEfficient Multi-Scale 3D CNN with fully connected CRF \\nfor Accurate Brain Lesion Segmentation,â€ Med Image Anal, vol. 36, Oct. 2016, \\ndoi: 10.1016/j.media.2016.10.004. \\n[31] T. Eelbode et al., â€œOptimization for Medical Image Segmentation: Theory and \\nPractice When Evaluating With Dice Score or Jaccard Index,â€ IEEE Trans Med \\nImaging, vol. PP, p. 1, Oct. 2020, doi: 10.1109/TMI.2020.3002417. \\n[32] A. P. Zijdenbos, B. M. Dawant, R. A. Margolin, and A. C. Palmer, \\nâ€œMorphometric analysis of white matter lesions in MR  images: method and \\nvalidation,â€ IEEE Trans Med Imaging, vol. 13, no. 4, pp. 716 â€“724, 1994, doi: \\n10.1109/42.363096. \\n[33] B. Wilson, â€œUnderstanding the Harmonic Mean,â€ UNSW CRICOS, Mar. 23, \\n2006. http://groups.di.unipi.it/~bozzo/The%20Harmonic%20Mean.htm \\n(accessed Oct. 03, 2022).'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'title_abstract', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='Title: MRI Brain Tumor Detection with Computer Vision\\n\\nAbstract: This study explores the application of deep learning techniques in the automated detection and segmentation of brain tumors from MRI scans. We employ several machine learning models, including basic logistic regression, Convolutional Neural Networks (CNNs), and Residual Networks (ResNet) to classify brain tumors effectively. Additionally, we investigate the use of U-Net for semantic segmentation and EfficientDet for anchor-based object detection to enhance the localization and identification of tumors. Our results demonstrate promising improvements in the accuracy and efficiency of brain tumor diagnostics, underscoring the potential of deep learning in medical imaging and its significance in improving clinical outcomes.'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='MRI Brain Tumor Detection with Computer Vision\\nJack Krolik, Jake Lynn, John Henry Rudden, Dmytro Vremenko\\nhttps://github.com/jack-krolik/brain-cancer-classifier-segmentation\\nAbstract\\nThis study explores the application of deep learning techniques in the automated detection and segmentation\\nof brain tumors from MRI scans. We employ several machine learning models, including basic logistic regression,\\nConvolutional Neural Networks (CNNs), and Residual Networks (ResNet) to classify brain tumors effectively.\\nAdditionally, we investigate the use of U-Net for semantic segmentation and EfficientDet for anchor-based object\\ndetection to enhance the localization and identification of tumors. Our results demonstrate promising improve-\\nments in the accuracy and efficiency of brain tumor diagnostics, underscoring the potential of deep learning in\\nmedical imaging and its significance in improving clinical outcomes.\\nIntroduction\\nAdvancements in medical imaging and computer vision'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='medical imaging and its significance in improving clinical outcomes.\\nIntroduction\\nAdvancements in medical imaging and computer vision\\nhave paved the way for significant improvements in di-\\nagnostic methodologies. In particular, the detection and\\nsegmentation of brain tumors from MRI scans have seen\\ntransformative developments through the application of\\ndeep learning technologies. This paper examines various\\nmachine learning models tailored to enhance the accu-\\nracy and efficiency of brain tumor analysis, thereby aiding\\nfaster and more reliable medical diagnoses.\\nProblem Statement\\nBrain tumors vary widely in size, shape, and location,\\nmaking their detection and segmentation challenging yet\\ncritical for effective treatment planning. Traditional\\nmethods often require extensive manual review by radi-\\nologists, which is time-consuming and prone to human\\nerror. The challenge lies in developing robust automated\\nsystems that can accurately identify and categorize brain'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='ologists, which is time-consuming and prone to human\\nerror. The challenge lies in developing robust automated\\nsystems that can accurately identify and categorize brain\\ntumors from MRI scans, thus supporting radiologists in\\nmaking timely and accurate assessments. This study aims\\nto address these challenges by leveraging and optimizing\\nstate-of-the-art deep learning models for brain tumor de-\\ntection and segmentation.\\nData\\nBrain Tumor MRI Dataset\\nThe Brain Tumor MRI Dataset comprises 7,023 MRI\\nimages sourced from figshare, SARTAJ, and Br35H\\ndatasets, segmented into four classes: glioma, menin-\\ngioma, no tumor, and pituitary. For the binary classi-\\nfication task, we simply combined all tumor labels under\\nthe unified â€œTumorâ€ class [1].\\nSplit No-\\nTumor\\nMeningi-\\noma\\nGlioma Pituitary\\nTrain 1,595\\n(27.92%)\\n1,339\\n(23.44%)\\n1,321\\n(23.13%)\\n1,457\\n(25.51%)\\nTest 405\\n(30.89%)\\n306\\n(23.34%)\\n300\\n(22.88%)\\n300\\n(22.88%)\\nFigure 1:Classification Class Distribution\\nFigure 2:Brain MRI Classification Data'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='(27.92%)\\n1,339\\n(23.44%)\\n1,321\\n(23.13%)\\n1,457\\n(25.51%)\\nTest 405\\n(30.89%)\\n306\\n(23.34%)\\n300\\n(22.88%)\\n300\\n(22.88%)\\nFigure 1:Classification Class Distribution\\nFigure 2:Brain MRI Classification Data\\nBrain MRI Segmentation Dataset\\nThe LGG Segmentation Dataset contains MRI scans and\\nFLAIR abnormality segmentation masks for 110 patients\\nfrom The Cancer Genome Atlas (TCGA) lower-grade\\nglioma collection. Images, provided in .tif format with\\nthree channels (pre-contrast, FLAIR, post-contrast),\\nand binary masks are organized by patient ID [2]. The\\ndataset supports studies on tumor shape related to\\ngenomic subtypes and patient outcomes.\\nSplit No-Tumor Tumor\\nTrain 2,045 (65.07%) 1,098 (34.93%)\\nTest 511 (65.01%) 275 (34.99%)\\nFigure 3:Segmentation Class Distribution\\nBrain Tumor Image DataSet: Semantic\\nSegmentation\\nThe Brain Tumor Image Dataset, designed for the Tu-\\nmorSeg Computer Vision Project, focuses on semantic\\nsegmentation with two classes: Tumor (Class 1) and Non-'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='Segmentation\\nThe Brain Tumor Image Dataset, designed for the Tu-\\nmorSeg Computer Vision Project, focuses on semantic\\nsegmentation with two classes: Tumor (Class 1) and Non-\\nTumor (Class 0) [3]. Initially containing 2,146 images, the\\ndataset was adjusted by removing all samples without tu-\\nmor masks. Notably, the segmentation masks are actu-\\nally bounding boxes, making this dataset more suitable\\n1\\narXiv:2510.10250v1  [cs.CV]  11 Oct 2025for object detection tasks in medical image analysis.\\nSplit Tumor\\nTrain 1,501\\nTest 215\\nFigure 4:Segmentation Box Class Distribution\\nFigure 5:Example of LGG and Box Samples\\nImage Classification\\nNaive Classification\\nThe first problem we looked at was classification, both\\nbinary (tumor or no tumor) and multiclass (what type\\nof tumor). We began investigating detecting the exis-\\ntence of brain tumors in an image by creating a set of\\nbaseline, simple models to compare our results to as we\\nincreased model complexity throughout the rest of the'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='tence of brain tumors in an image by creating a set of\\nbaseline, simple models to compare our results to as we\\nincreased model complexity throughout the rest of the\\nexperiments. For our first baseline model we used logistic\\nregression. Although logistic regression is not typically\\nused on image data, it is simple to implement and trains\\nquickly. The second baseline model we used was a simple\\nConvolutional Nerual Network (CNN). CNNs are more\\ncomplex than logistic regression and tailored for image-\\nbased tasks. CNNs use a kernel (3x3 in our implemen-\\ntation) to capture local context contained in the image.\\nThe kernel looks at the area around the current pixel and\\ndoes a linear combination of the pixelsâ€™ values, condens-\\ning those pixels down to a single feature. The hope is\\nthat this output feature encodes information about what\\nis around the target pixel.\\nMethods\\nBoth of these models were implemented using PyTorch.\\nLogistic Regression was implemented using a single linear'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='is around the target pixel.\\nMethods\\nBoth of these models were implemented using PyTorch.\\nLogistic Regression was implemented using a single linear\\nlayer. Although very simple, this single layer allowed the\\nmodel to gain simple insights about the images it was see-\\ning. The more complex CNN used two convolutional lay-\\ners, each with a max pool following it, as well as two fully-\\nconnected layers. The only difference between the model\\nfor binary and multiclass classification was the output size\\nof the final fully connected layer, changing from 1 output\\nnode in the binary case to 4 output nodes for multiclass.\\nTraining and evaluation was also handled slightly differ-\\nently between the binary and multiclass model. All three\\nmodels (LogReg, Binary CNN, and Multiclass CNN) were\\ntrained for 20 epochs. The logistic regression model used\\na Binary Cross Entropy loss function. The binary CNN\\nused Binary Cross Entryopy with Logits for its loss func-'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='trained for 20 epochs. The logistic regression model used\\na Binary Cross Entropy loss function. The binary CNN\\nused Binary Cross Entryopy with Logits for its loss func-\\ntion. The Multiclass CNN used (non-binary) Cross En-\\ntropy Loss. All three models used Stochastic Gradient\\nDescent as their optimizers. As these models were used\\nas simple sanity checks, no programmatic hyperparam-\\neter tuning was done. For evaluation, the torchmetrics\\nlibrary was used.\\nResults\\nModel Accuracy AUC\\nLogReg 69.87% 0.5151\\nBinary CNN 99.54% 0.9967\\nMulticlass CNN 94.83% 0.9663\\nFigure 6:Naive Methods Metrics\\nAs was expected, logistic regression did not perform well\\non this image classification task. The accuracy is quite\\nlow, and the AUC suggests that this classifier is not dis-\\ncernible from a random classifier. This model was only\\na baseline to which other models would be compared, so\\nthe quality of the classifier is not overly important in our\\ncase. The two CNNs improved greatly on the logistic re-'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='a baseline to which other models would be compared, so\\nthe quality of the classifier is not overly important in our\\ncase. The two CNNs improved greatly on the logistic re-\\ngression model. After 20 epochs of training, the binary\\nclassifier achieved over 99% accuracy and an AUC very\\nclose to 1. The multiclass CNN, as expected, performed\\nslightly worse on its task than the binary classifier. Given\\nthat the two models are very similar and binary classifi-\\ncation is a slightly easier task than multiclass classifica-\\ntion, we cannot expect the multiclass CNN to perform as\\nwell. However, the results are still promising. With ac-\\ncuracy close to 95% and high AUC, we have shown that\\nthis model architecture is effective in classifying images\\nof brain tumors. With tweaks to model architecture and\\nmore involved hyperparameter tuning, it is likely that\\nthese results could be improved. The efficiency of these\\nCNNs can be improved as well, which will be shown in\\nthe next section*.\\nResNet'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='more involved hyperparameter tuning, it is likely that\\nthese results could be improved. The efficiency of these\\nCNNs can be improved as well, which will be shown in\\nthe next section*.\\nResNet\\nWhile the CNN performed somewhat satisfactorily, we\\nwanted to explore a more robust and sophisticated ar-\\nchitecture to capture the complex patterns in high-\\ndimensional image data. ResNet or Residual Network\\nis known for its deep network capabilities and effective-\\nness in handling vanishing gradients [4]. Its success covers\\nvarious image classification tasks, especially in scenarios\\ninvolving complex visual data like medical images, mak-\\ning it an ideal candidate for this project.\\nMethods\\nOur implementation of ResNet tailored specifically for\\nbrain tumor image classification involved constructing a\\ncustom ResNet architecture. This was essential for han-\\ndling the unique challenges presented by medical imaging\\ndata. Central to the ResNet architecture are the residual'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='custom ResNet architecture. This was essential for han-\\ndling the unique challenges presented by medical imaging\\ndata. Central to the ResNet architecture are the residual\\nblocks, which facilitate the training of deeper networks\\nwithout suffering from vanishing gradients. Each resid-\\nual block includes skip connections that allow inputs to\\n2bypass one or more layers. These connections help miti-\\ngate the vanishing gradient problem by facilitating direct\\ngradient flow during backpropagation, which is crucial for\\nmaintaining performance integrity across many layers.\\nNetwork Architecture\\nOur custom ResNet model starts with an initial convo-\\nlutional layer and max pooling to prepare the input for\\na series of residual blocks, each doubling the number of\\nfilters and reducing the spatial dimensions through strid-\\ning. The architecture culminates in average pooling and\\na fully connected layer tailored to the specific classifica-\\ntion taskâ€”binary or multi-classâ€”allowing for flexible use'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='ing. The architecture culminates in average pooling and\\na fully connected layer tailored to the specific classifica-\\ntion taskâ€”binary or multi-classâ€”allowing for flexible use\\nacross different types of data. Additionally, the model is\\ndynamically constructed to adjust the networkâ€™s depth\\nand capacity as needed, using a method that configures\\neach layer for optimal downsampling or dimension main-\\ntenance. We initialized the model to suit the MRI image\\ncharacteristics, employing Adam as the optimizer for its\\nadvantages in managing sparse gradients and adaptive\\nlearning rates. This configuration is particularly advan-\\ntageous for handling the complex and varied nature of\\nmedical imaging datasets.\\nHyperparameter Tuning\\nHyperparameter tuning was a crucial aspect of our model\\ndevelopment. We experimented with different optimizers\\nand found Adam to be superior in our context due to its\\nfaster convergence properties. The initial use of a learn-'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='development. We experimented with different optimizers\\nand found Adam to be superior in our context due to its\\nfaster convergence properties. The initial use of a learn-\\ning rate scheduler was intended to adjust the learning\\nrate based on training progress, but it was removed after\\nobserving a decrease in performance, indicating potential\\noverfitting or inadequate learning rate adjustments. The\\nfinal learning rate of 0.001 was selected after several trials\\nto ensure a balance between training speed and conver-\\ngence stability.\\nResults\\nImplementing ResNet improved some of our classification\\nresults:\\nResNet Variant Accuracy AUC\\nBinary 97.71% 0.968\\nMulticlass 94.35% 0.996\\nFigure 7:ResNet Metrics\\nWhile the performance of ResNet shows only a slight im-\\nprovement in accuracy and AUC when compared to the\\nnaive CNN, it was trained in half the number of epochs.\\nThis illustrates ResNetâ€™s efficiency and robustness in pro-\\ncessing complex image data, especially in regards to the'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='naive CNN, it was trained in half the number of epochs.\\nThis illustrates ResNetâ€™s efficiency and robustness in pro-\\ncessing complex image data, especially in regards to the\\nAUC for multiclass. This performance boost is attributed\\nto the deeper network architectureâ€™s ability to learn de-\\ntailed and hierarchical features from the brain MRI im-\\nages, which are crucial for accurate classification. The use\\nof ResNet in our project not only improved classification\\naccuracy but also highlighted the potential of deep learn-\\ning in enhancing diagnostic processes through advanced\\nimage recognition capabilities.\\nSemantic Segmentation\\nAs we transition from holistic image classification, where\\nthe entire image is assigned a single label, to pixel-wise\\nsemantic segmentation, our approach to image analysis\\nbecomes more granular and complex. In holistic classifi-\\ncation, the primary goal is determining whether an image\\nbelongs to one specific class. While this may be initially'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='becomes more granular and complex. In holistic classifi-\\ncation, the primary goal is determining whether an image\\nbelongs to one specific class. While this may be initially\\nuseful, we may want more localized information about the\\nimage. Semantic segmentation addresses this limitation\\nby assigning a class label to each pixel, effectively enabling\\na detailed map of various tissues and anomalies within\\na single image. This shift changes our targetsâ€”from a\\nclass label per image to a class label per pixelâ€”and in-\\ntroduces new challenges in computational complexity and\\naccuracy.\\nIn pixel-wise classification, traditional metrics like overall\\naccuracy become less informative due to the high preva-\\nlence of background or non-tumor pixels overwhelming\\nthe minority class of interest (tumor pixels). Therefore,\\nwe adopt Intersection over Union (IoU) [5] as a more suit-\\nable metric for this task. IoU provides a balanced measure\\nby evaluating the overlap per class between the predicted'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='we adopt Intersection over Union (IoU) [5] as a more suit-\\nable metric for this task. IoU provides a balanced measure\\nby evaluating the overlap per class between the predicted\\nand actual segments relative to their combined area, thus\\neffectively handling class imbalances and focusing on the\\nprecision of boundary delineation.\\nRelated Work\\nIn the initial stages of our project, we planned to ex-\\nplore various model architectures for image segmentation.\\nHowever, given the complexities and the intensive knowl-\\nedge required for each, coupled with our projectâ€™s limited\\ntimeframe, we opted to focus solely on a single, impactful\\narchitecture.\\nWe selected the U-Net [6] architecture for its innova-\\ntive approach. Traditional segmentation methods like the\\nsliding window technique require processing individual\\npatches around each pixel independently, a method that is\\nextremely computationally inefficient and often overlooks\\nimportant contextual relationships within the image. U-'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='patches around each pixel independently, a method that is\\nextremely computationally inefficient and often overlooks\\nimportant contextual relationships within the image. U-\\nNet revolutionized this by adopting an encoder-decoder\\narchitecture, commonly used in language processing, to\\nanalyze the entire image holistically. This significantly\\nreduced the need for redundant processing of overlapping\\npatches, enhancing efficiency and effectiveness in captur-\\ning detailed image contexts.\\nEncoder-Decoder Structure:In the U-Net architec-\\nture, the encoder uses a combination of convolutional lay-\\ners and max-pooling operations to reduce the spatial di-\\nmensions of an input image while increasing the depth\\nof feature maps. This encoding process effectively nar-\\nrows the representation of features into a deeper, more\\nabstract form, capturing essential contextual information\\nat multiple scales. Conversely, the decoder works to re-\\nverse this process by progressively expanding the com-'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='abstract form, capturing essential contextual information\\nat multiple scales. Conversely, the decoder works to re-\\nverse this process by progressively expanding the com-\\npressed feature maps. It attempts to reconstruct the orig-\\ninal spatial dimensions from this high-level latent repre-\\nsentation, translating it back into detailed segmentation\\nmasks that match our target structures. This decoding\\nis vital for restoring high-resolution details necessary for\\naccurate medical diagnostics.\\nSkip Connections:[7] An essential feature of U-Net,\\nskip connections are critical in enhancing the decoding\\nprocess. As the encoder compresses the image, it ex-\\n3Figure 8:Results of U-Net training on the test splits of various datasets. Center image shows overlap\\nbetween label (blue) and prediction (red).\\ntracts and condenses high-level features, sometimes re-\\nsulting in the loss of finer details. Skip connections ad-\\ndress this by directly linking the detailed, information-'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='tracts and condenses high-level features, sometimes re-\\nsulting in the loss of finer details. Skip connections ad-\\ndress this by directly linking the detailed, information-\\nrich feature maps from the encoderâ€™s early layers to each\\nstep of the decoding process. This mix of detailed early-\\nstage data with high-level abstracted features allows the\\ndecoder to reconstruct the segmentation map more accu-\\nrately. Such integration is especially beneficial in medical\\nimaging, where preserving precise anatomical details is\\ncrucial for accurate diagnostics.\\nModifications\\nSimplification for Smaller Images:Unlike the origi-\\nnal U-Net, which used a tiling approach to manage large,\\nhigh-resolution images, we adapted the architecture to\\nsuit smaller, uniformly sized images (320x320x3). This\\ndecision avoids the complexities associated with tiling\\nand cropping, which in the original U-Net included in\\nboth training objectives and preprocessing steps to en-'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='decision avoids the complexities associated with tiling\\nand cropping, which in the original U-Net included in\\nboth training objectives and preprocessing steps to en-\\nsure accuracy around the edges of tiles. By using smaller\\nimages, we could streamline the preprocessing and re-\\nduce the computational overhead, allowing the focus to\\nremain on accurate segmentation without the need for\\nedge weighting or extensive padding.\\nBinary Classification:To further simplify the model,\\nwe shifted the output goal to a binary classification sys-\\ntem, where each pixel is classified simply as â€™Tumorâ€™ or\\nâ€™No Tumorâ€™. This change reduced the complexity of the\\noutput layer, focusing on the critical task at hand and\\nmaking the training process more straightforward. We\\nused binary cross entropy as the loss function to support\\nthis two-class system directly.\\nOptimization and Hyperparameter Tuning:We\\nemployed Stochastic Gradient Descent (SGD) to optimize'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='used binary cross entropy as the loss function to support\\nthis two-class system directly.\\nOptimization and Hyperparameter Tuning:We\\nemployed Stochastic Gradient Descent (SGD) to optimize\\nthe model, inspired by its usage in the original U-Net pa-\\nper. Additionally, recognizing the constraints posed by\\nour limited computational resources, we utilized a com-\\nbination of Bayesian Optimization [8] and K-fold cross-\\nvalidation [9] to tune the modelâ€™s hyperparameters effi-\\nciently. Bayesian Optimization provided a more adaptive\\napproach than traditional grid search for finding optimal\\nparameters, allowing us to iteratively test and refine pa-\\nrameters like batch size and learning rate scheduler set-\\ntings. K-fold cross-validation helped assess the modelâ€™s\\nperformance more robustly across different data splits,\\nthus reducing biases that could affect our evaluation of\\nthe modelâ€™s effectiveness.\\nResults\\nOur implementation of U-Net performed quite well! Af-'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='thus reducing biases that could affect our evaluation of\\nthe modelâ€™s effectiveness.\\nResults\\nOur implementation of U-Net performed quite well! Af-\\nter training our model for 50 epochs and implementing a\\nlinear decay of our initial learning rate by a factor of 0.1\\nevery 20 epochs, we achieved the following results across\\nour datasets.\\nDataset IoU AUC\\nBox 57.6% 0.975\\nLGG 70.3% 0.996\\nNormalized LGG 70.9% 0.996\\nFigure 9:Segmentation Results\\nOur U-Net implementation demonstrated promising re-\\nsults. Despite the complexity of semantic segmentation,\\nan IoU of 70% is commendable, complemented by a high\\nAUROC, indicating effective minimization of false posi-\\ntives. Performance was strong on the LGG dataset but\\nless so within the BOX dataset, though false positives\\nremained low.\\nFuture work will focus on expanded hyperparameter tun-\\ning and exploring data augmentation techniques like crop-\\nping and shifting to enhance model robustness and po-'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='remained low.\\nFuture work will focus on expanded hyperparameter tun-\\ning and exploring data augmentation techniques like crop-\\nping and shifting to enhance model robustness and po-\\ntentially implement the tiling approach described in the\\noriginal U-Net paper.\\n4Anchor-based Object Detection\\nRelated Work\\nObject detection is a popular computer vision technique\\nused to identify specific elements within images. This\\nmethod involves drawing a bounding box around each ob-\\nject, defined by four corner coordinates, and assigning a\\nlabel to the box that indicates the class of the object it en-\\ncloses. Unlike image segmentation, object detection must\\nhandle the detection of multiple objects of one or more\\nclasses at various scales and locations within a single im-\\nage. To accommodate these challenges, the formulation\\nof the problem is carefully adjusted to ensure precise and\\nefficient detection.\\nMore specifically, anchor-based object detection starts by'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='age. To accommodate these challenges, the formulation\\nof the problem is carefully adjusted to ensure precise and\\nefficient detection.\\nMore specifically, anchor-based object detection starts by\\ndefining a variety of anchor boxes, each varying in size\\nand aspect ratio, across an entire image [10]. These an-\\nchors are designed to potentially encompass all regions\\nand scales of objects present in the image. During pre-\\nprocessing, each image is labeled by examining these pre-\\ndefined anchor boxes to determine their overlap with the\\nground truth bounding boxes. Each anchor is then as-\\nsigned a label: either â€™no objectâ€™ or one of the specific\\nobject classes [10].\\nThe modelâ€™s initial task is to classify each anchor box\\nfor every sample image. However, classification alone is\\ninsufficient for completing the object detection process,\\nas it is rare for an anchor box to perfectly align with a\\nground truth bounding box. Therefore, for anchors clas-'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='insufficient for completing the object detection process,\\nas it is rare for an anchor box to perfectly align with a\\nground truth bounding box. Therefore, for anchors clas-\\nsified as containing an object, the model must also pre-\\ndict the necessary adjustments to better fit the ground\\ntruth. This involves calculating the required shifts in\\nthe anchorâ€™s center coordinates, as well as adjustments\\nto its width and height. These offset values are predicted\\nthrough regression, adding another layer of complexity to\\nthe modelâ€™s tasks. This overall approach was adapted for\\nthe detection of tumors within brain MRI images.\\nMethods\\nImages and their ground truth bounding boxes were re-\\nsized to 3x256x256. Anchor boxes were configured with\\nimage scales of 0.1, 0.175, and 0.3, aspect ratios of 2:1,\\n1:1, and 1:2 (reflecting the approximately circular nature\\nof most tumors), and a feature map size of 32x32. This\\nconfiguration resulted in a total of 9216 anchor boxes.'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='1:1, and 1:2 (reflecting the approximately circular nature\\nof most tumors), and a feature map size of 32x32. This\\nconfiguration resulted in a total of 9216 anchor boxes.\\nWith these parameters, 99.5% of the training bounding\\nboxes achieved an Intersection* over Union (IoU) greater\\nthan 0.3 with at least one anchor. For IoUs of 0.5 and\\n0.75, the coverage rates were 96.7% and 45.1%, respec-\\ntively, indicating that these anchors provided adequate\\ncoverage of the ground truths within the training set.\\nThe labeling of each anchor for each sample adhered to\\nthe following criteria: any box with an IoU over 0.5 was\\nconsidered positive, while all others were labeled as neg-\\native. In cases where a sample lacked a positive anchor,\\nthe anchor with the highest overlap (provided that the\\nIoU was over 0.3) was assigned a positive label. This pol-\\nicy ensured that 99.5% of the images had at least one\\npositive anchor. For all positive anchors, offsets for the'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='IoU was over 0.3) was assigned a positive label. This pol-\\nicy ensured that 99.5% of the images had at least one\\npositive anchor. For all positive anchors, offsets for the\\ncenter coordinates and adjustments in width and height\\nwere calculated as regression targets.\\nFigure 10:(A) Anchor classification (B) Anchor re-\\ngression\\nThe selected model, EfficientDet, employs an EfficientNet\\nbackbone for robust feature encoding, combined with a bi-\\ndirectional feature pyramid network (BiFPN) for multi-\\nscale feature fusion [11]. This architecture also includes\\ntwo key predictors: a classifier to determine anchor la-\\nbels and a regressor for predicting anchor offsets. We\\nutilized EfficientNet-B0, the smallest variant in the Effi-\\ncientNet series with 3M parameters, to encode images\\ninto multi-resolution features [12]. 64 channels of en-\\ncoded features following dimension reducing convolutions\\n(64x32x32, 64x16x16, 64x8x8, 64x4x4, and 64x2x2) are'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='into multi-resolution features [12]. 64 channels of en-\\ncoded features following dimension reducing convolutions\\n(64x32x32, 64x16x16, 64x8x8, 64x4x4, and 64x2x2) are\\nfed into the BiFPN, which consists of 3 BiFPN layers.\\nIn each BiFPN layer, features from different resolutions\\nare fused in a weighted manner, allowing for interaction\\nbetween lower and higher resolution data. The features\\nfrom the 64x32x32 resolution are input into both the clas-\\nsifier and regressor, each consisting of three convolutional\\nlayers with ReLU activations, maintaining the same di-\\nmensions. The classifier and regressor output a single\\nprediction (logit of true class) and four offset values per\\nanchor, respectively. The resulting model featured 4M\\ntrainable parameters.\\nIn our object detection experiments, we tested a range\\nof hyperparameters to optimize performance. Over the\\ncourse of 50 epochs, we experimented with two initial-\\nization strategies for EfficientNet-B0: using pretrained'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='of hyperparameters to optimize performance. Over the\\ncourse of 50 epochs, we experimented with two initial-\\nization strategies for EfficientNet-B0: using pretrained\\nweights and training from scratch. We evaluated three\\ndifferent learning rates: 0.1, 0.01, and 0.001. For the\\nregression tasks, we compared the mean squared error\\n(MSE) loss function with SmoothL1Loss, while for classi-\\nfication, we tested binary cross-entropy (BCE), weighted\\nBCE (see formula below), and focal loss (alpha=0.1,\\ngamma=2.0) [13]. The latter two were considered as a\\nmeans of addressing class imbalance. Additionally, two\\noptimizers were used in our trials: Adam and Stochas-\\ntic Gradient Descent (SGD). The batch size was fixed at\\n16 samples. These variations were chosen to determine\\nthe most effective combination for our object detection\\nmodel.\\nweighted BCE =âˆ’w pos Â·yÂ·log(Ë†y)âˆ’wneg Â·(1âˆ’y)Â·log(1âˆ’Ë†y)\\nwhere:w pos is calculated as the ratio of negative samples\\nto total samples, andw neg is 1âˆ’w pos.'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='model.\\nweighted BCE =âˆ’w pos Â·yÂ·log(Ë†y)âˆ’wneg Â·(1âˆ’y)Â·log(1âˆ’Ë†y)\\nwhere:w pos is calculated as the ratio of negative samples\\nto total samples, andw neg is 1âˆ’w pos.\\nIn our study, unlike typical scenarios involving multiple\\n5objects of the same or different classes within a single im-\\nage, our dataset uniquely featured just one ground truth\\nbounding box per image, labeled as â€tumor.â€ This sim-\\nplicity significantly streamlined our performance evalua-\\ntion process. For each image, we focused solely on the\\nanchor with the highest predicted probability. The coor-\\ndinates of this anchor were adjusted using the regressorâ€™s\\noutputs, and the IoU was then calculated between the\\nadjusted prediction and the actual ground truth bound-\\ning box. Additionally, to evaluate the effectiveness of\\nour anchor classification, we computed the AUC for each\\nsample.\\nResults and Discussion\\nOverall, this approach to the problem proved less effective\\ncompared to our semantic segmentation methods. We'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='our anchor classification, we computed the AUC for each\\nsample.\\nResults and Discussion\\nOverall, this approach to the problem proved less effective\\ncompared to our semantic segmentation methods. We\\nfroze the regressor and focused primarily on anchor clas-\\nsification, as identifying which anchors contain tumors is\\nessential before adjusting them to fit the bounding boxes.\\nThroughout most experimental trials, gradients tended to\\nexplode by the final epoch, complicating the assessment\\nof optimal hyperparameters. However, several key obser-\\nvations emerged that could inform future research.\\nThe Adam optimizer frequently led to gradient explo-\\nsions, particularly within the first 10 epochs across all\\ntested learning rates and loss functions. In contrast, the\\nSGD optimizer facilitated more stable training dynam-\\nics. The weighted BCE loss function also demonstrated\\ngreater stability compared to other loss options. Utiliz-\\ning a pretrained backbone did not yield a notable advan-'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='ics. The weighted BCE loss function also demonstrated\\ngreater stability compared to other loss options. Utiliz-\\ning a pretrained backbone did not yield a notable advan-\\ntage. The highest validation IoU and AUC recorded were\\n0.075 and 0.801, respectively, achieved with a pretrained\\nbackbone, SGD optimizer at a learning rate of 0.01, and\\nweighted BCE lossâ€”still a significant underperformance\\ncompared to semantic segmentation.\\nThe model and training approach have a vast hyperpa-\\nrameter space that warrants further exploration. Future\\nexperiments should consider scaling up the model size,\\nincluding the backbone, BiFPN, and prediction heads,\\nfollowing the guidelines proposed by 2020 M. Tan et al.\\nThe limited number of samples is another significant issue\\nsince object detection models typically require extensive\\ndatasets for effective training. Addressing class imbal-\\nance in predictions should also be a priority in subsequent\\nstudies.\\nConclusion'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='since object detection models typically require extensive\\ndatasets for effective training. Addressing class imbal-\\nance in predictions should also be a priority in subsequent\\nstudies.\\nConclusion\\nThis study has successfully reinforced the significant po-\\ntential of deep learning techniques in the automated\\ndetection and segmentation of brain tumors from MRI\\nscans. By utilizing a variety of models, CNNs, and\\nResNet, alongside advanced methods like U-Net for seg-\\nmentation, we have shown marked improvements in both\\naccuracy and efficiency over traditional methods. The\\napplication of object detection models requires further\\nexploration due to the encountered challenges described\\nin the results. Future work will aim to further refine these\\nmodels, explore more complex architectures, and expand\\nthe datasets for training to improve the generalizability\\nand accuracy of the systems. This project lays a foun-\\ndation for the integration of deep learning into clinical'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='the datasets for training to improve the generalizability\\nand accuracy of the systems. This project lays a foun-\\ndation for the integration of deep learning into clinical\\npractice with aims of improving patient outcomes through\\nmore precise and timely diagnosis.\\nContributions\\nJake Lynn - Naive Classification and Base CNN;\\nJack Krolik - Binary and Multi Classification ResNet;\\nJohn Henry Rudden - Dataset, Training, and Visualiza-\\ntion Pipelines, Semantic Segmentation;\\nDmytro Vremenko - Anchor-Based Object Detection\\nReferences\\n[1] Masoud Nickparvar. Brain tumor mri dataset. Kag-\\ngle Dataset, 2021. Accessed: 2023-04-20.\\n[2] Mateusz Buda. Lgg mri segmentation. Kaggle\\nDataset, 2019. Accessed: 2023-04-20.\\n[3] Parisa Khani. Brain tumor image dataset for seman-\\ntic segmentation. Kaggle Dataset, 2021. Accessed:\\n2023-04-20.\\n[4] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and\\nJian Sun. Deep residual learning for image recog-\\nnition, 2015.\\n[5] Zifu Wang, Maxim Berman, Amal Rannen-Triki,'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='2023-04-20.\\n[4] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and\\nJian Sun. Deep residual learning for image recog-\\nnition, 2015.\\n[5] Zifu Wang, Maxim Berman, Amal Rannen-Triki,\\nPhilip H. S. Torr, Devis Tuia, Tinne Tuytelaars,\\nLuc Van Gool, Jiaqian Yu, and Matthew B.\\nBlaschko. Revisiting evaluation metrics for semantic\\nsegmentation: Optimization and evaluation of fine-\\ngrained intersection over union, 2023.\\n[6] Olaf Ronneberger, Philipp Fischer, and Thomas\\nBrox. U-net: Convolutional networks for biomedi-\\ncal image segmentation, 2015.\\n[7] Dongxian Wu, Yisen Wang, Shu-Tao Xia, James Bai-\\nley, and Xingjun Ma. Skip connections matter: On\\nthe transferability of adversarial examples generated\\nwith resnets, 2020.\\n[8] Peter I. Frazier. A tutorial on bayesian optimization,\\n2018.\\n[9] Luke Yates, Zach Aandahl, Shane A. Richards, and\\nBarry W. Brook. Cross validation for model selec-\\ntion: a primer with examples from ecology, 2022.\\n[10] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian'),\n",
       " Document(metadata={'arxiv_id': '2510.10250v1', 'title': 'MRI Brain Tumor Detection with Computer Vision', 'section': 'body', 'authors': 'Jack Krolik, Jake Lynn, John Henry Rudden'}, page_content='Barry W. Brook. Cross validation for model selec-\\ntion: a primer with examples from ecology, 2022.\\n[10] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian\\nSun. Faster r-cnn: Towards real-time object detec-\\ntion with region proposal networks.arXiv preprint\\narXiv:1506.01497, 2015.\\n[11] Mingxing Tan, Ruoming Pang, and Quoc V Le. Ef-\\nficientdet: Scalable and efficient object detection.\\narXiv preprint arXiv:1911.09070, 2020.\\n[12] Mingxing Tan and Quoc V Le. Efficientnet: Rethink-\\ning model scaling for convolutional neural networks.\\narXiv preprint arXiv:1905.11946, 2019.\\n[13] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming\\nHe, and Piotr DollÂ´ ar. Focal loss for dense object\\ndetection.arXiv preprint arXiv:1708.02002, 2017.\\n6'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'title_abstract', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='Title: Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset\\n\\nAbstract: Automating brain tumor segmentation using deep learning methods is an ongoing challenge in medical imaging. Multiple lingering issues exist including domain-shift and applications in low-resource settings which brings a unique set of challenges including scarcity of data. As a step towards solving these specific problems, we propose Convolutional adapter-inspired Parameter-efficient Fine-tuning (PEFT) of MedNeXt architecture. To validate our idea, we show our method performs comparable to full fine-tuning with the added benefit of reduced training compute using BraTS-2021 as pre-training dataset and BraTS-Africa as the fine-tuning dataset. BraTS-Africa consists of a small dataset (60 train / 35 validation) from the Sub-Saharan African population with marked shift in the MRI quality compared to BraTS-2021 (1251 train samples). We first show that models trained on BraTS-2021 dataset do not generalize well to BraTS-Africa as shown by 20% reduction in mean dice on BraTS-Africa validation samples. Then, we show that PEFT can leverage both the BraTS-2021 and BraTS-Africa dataset to obtain mean dice of 0.8 compared to 0.72 when trained only on BraTS-Africa. Finally, We show that PEFT (0.80 mean dice) results in comparable performance to full fine-tuning (0.77 mean dice) which may show PEFT to be better on average but the boxplots show that full finetuning results is much lesser variance in performance. Nevertheless, on disaggregation of the dice metrics, we find that the model has tendency to oversegment as shown by high specificity (0.99) compared to relatively low sensitivity(0.75). The source code is available at https://github.com/CAMERA-MRI/SPARK2024/tree/main/PEFT_MedNeXt'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='Parameter-efficient Fine-tuning for improved\\nConvolutional Baseline for Brain Tumor\\nSegmentation in Sub-Saharan Africa Adult\\nGlioma Dataset\\nBijay Adhikariâˆ—,1,2, Pratibha Kulung3, Jakesh Bohaju4, Laxmi Kanta Poudel5,\\nConfidence Raymond6,7,8, Dong Zhang6,9, Udunna C Anazodo6,7,8,10,11,12,\\nBishesh Khanal13, and Mahesh Shakya13\\n1 Nepal Research and Collaboration Center (NRCC), Nepal\\n2 Birendra Multiple Campus, Tribhuvan University, Nepal\\n3 Institute of Engineering, Purbanchal Campus, Tribhuvan University, Nepal\\n4 Bhaktapur Multiple Campus, Tribhuvan University, Nepal\\n5 Gandaki College of Engineering and Science, Pokhara University, Nepal\\n6 Multimodal Imaging of Neurodegenerative Diseases (MiND) Lab, Department of\\nNeurology and Neurosurgery, McGill University, Montreal, QC, Canada\\n7 Lawson Health Research Institute, London, Ontario, Canada\\n8 Medical Artificial Intelligence Laboratory (MAI Lab), Lagos, Nigeria\\n9 Department of Electrical and Computer Engineering, University of British'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='8 Medical Artificial Intelligence Laboratory (MAI Lab), Lagos, Nigeria\\n9 Department of Electrical and Computer Engineering, University of British\\nColumbia, Vancouver, Canada\\n10 Montreal Neurological Institute, McGill University, MontrÃ©al, Canada\\n11 Department of Medicine, University of Cape Town, South Africa\\n12 Department of Clinical Radiation Oncology, University of Cape Town, South\\nAfrica\\n13 Nepal Applied Mathematics and Informatics Institute for research (NAAMII),\\nNepal\\n{bjayadikari.ba, pratibha.kulu63, jakesh.bohaju,\\njustkantapoudel}@gmail.com, craymon8@uwo.ca, donzhang@ece.ubc.ca,\\nudunna.anazodo@mcgill.ca, {bishesh.khanal, mahesh.shakya}@naamii.org.np\\nAbstract. Automating brain tumor segmentation using deep learning\\nmethods is an ongoing challenge in medical imaging. Multiple lingering\\nissues exist including domain-shift and applications in low-resource set-\\ntings which brings a unique set of challenges including scarcity of data.'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='issues exist including domain-shift and applications in low-resource set-\\ntings which brings a unique set of challenges including scarcity of data.\\nAs a step towards solving these specific problems, we propose Convolu-\\ntional adapter-inspired Parameter-efficient Fine-tuning (PEFT) of Med-\\nNeXt architecture. To validate our idea, we show our method performs\\ncomparable to full fine-tuning with the added benefit of reduced train-\\ning compute using BraTS-2021 as pre-training dataset and BraTS-Africa\\nas the fine-tuning dataset. BraTS-Africa consists of a small dataset (60\\ntrain / 35 validation) from the Sub-Saharan African population with\\nmarked shift in the MRI quality compared to BraTS-2021 (1251 train\\nsamples). We first show that models trained on BraTS-2021 dataset do\\nnot generalize well to BraTS-Africa as shown by 20% reduction in mean\\ndice on BraTS-Africa validation samples. Then, we show that PEFT'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='not generalize well to BraTS-Africa as shown by 20% reduction in mean\\ndice on BraTS-Africa validation samples. Then, we show that PEFT\\narXiv:2412.14100v1  [eess.IV]  18 Dec 20242 Adhikari, Kulung, Bohaju, Poudel, Shakya et al.\\ncan leverage both the BraTS-2021 and BraTS-Africa dataset to obtain\\nmean dice of 0.8 compared to 0.72 when trained only on BraTS-Africa.\\nFinally, We show that PEFT (0.80 mean dice) results in comparable per-\\nformance to full fine-tuning (0.77 mean dice) which may show PEFT to\\nbe better on average but the boxplots show that full finetuning results\\nis much lesser variance in performance. Nevertheless, on disaggregation\\nof the dice metrics, we find that the model has tendency to oversegment\\nas shown by high specificity (0.99) compared to relatively low sensitiv-\\nity(0.75). The source code is available at https://github.com/CAMERA-\\nMRI/SPARK2024/tree/main/PEFT_MedNeXt\\nKeywords: Parameter-efficient finetuning, Segmentation, Medical Im-'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='ity(0.75). The source code is available at https://github.com/CAMERA-\\nMRI/SPARK2024/tree/main/PEFT_MedNeXt\\nKeywords: Parameter-efficient finetuning, Segmentation, Medical Im-\\nage Segmentation, Distribution shift\\n1 Introduction\\nBrain Tumor poses a significant global health challenge with glioma being most\\nprevalent, malignant and having poor prognosis resulting in 80% of glioma pa-\\ntients succumbing to death within two years of diagnosis[1]. Additionally, low-to-\\nmiddle income countries (LMICs), particularly Sub-Saharan Africa (SSA), face a\\nhigher disease burden due to limited access to imaging devices and specialists, as\\nwell as the usual delayed disease presentation[2,3]. In fact, glioma death rate has\\nrisen in SSA, unlike in high-income countries where they continue to decrease[4].\\nBrain Tumor Segmentation is an essential component in disease management\\nuseful in quantification, radiation therapy planning, and treatment evaluation'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='Brain Tumor Segmentation is an essential component in disease management\\nuseful in quantification, radiation therapy planning, and treatment evaluation\\nrequiring manual delineation by radiologists[5]. However, increasing incidence\\nand death rates have resulted in increased workload necessitating automated\\nsegmentation methods.\\nThis has spurred the introduction of automated methods including pre-deep\\nlearning-based semi- and fully automatic methods such as thresholding[6], wa-\\ntershed [7], active contours[8], atlas-based[9] segmentation approaches were pro-\\nposed. However, these approaches often face challenges in dealing with anatom-\\nical variations and complexities, especially pathological ones, present in medical\\nimages. The limitations of these approaches have led to the exploration of deep\\nlearning based solution which have demonstrated promising results in tumor\\nsegmentation tasks [10].\\nDeep-learningmethods,especiallyConvolutionalNeuralNetwork(CNN)based'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='learning based solution which have demonstrated promising results in tumor\\nsegmentation tasks [10].\\nDeep-learningmethods,especiallyConvolutionalNeuralNetwork(CNN)based\\narchitectures,haveemergedaspowerfultoolsforcomputervisiontasks,including\\ndense prediction tasks such as segmentation. Among them, U-Net[11] architec-\\nture is the most common and widely used baseline model in medical image seg-\\nmentation due to its parameter efficiency, numerous available implementations,\\nand reasonable performance across varied datasets. It is an encoder-decoder\\nstructure with skip connections that efficiently combines spatial information[12]\\nfromdifferentscales.However,understandinglong-distancespatialrelation[13,14],\\ntoken-flatten and scale-sensitivity[15] remain challenges for U-Net architecture in\\nbraintumorsegmentation.Ontheotherhand,Transformer-basedarchitectures[16]Parameter-efficient Fine-tuning for BraTS Africa 3\\ncan leverage long-range dependencies but are highly over-parameterized and re-'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='can leverage long-range dependencies but are highly over-parameterized and re-\\nquire large GPU Memory for training three-dimensional (3D) Volumes.\\nTowardsachievingtherightbalance,MedNeXt[17],isamodernconvolutional\\narchitecture designed for medical image segmentation, inspired by Transformer-\\nbased models but tailored for limited annotated datasets. MedNeXt features a\\nfully ConvNeXt 3D Encoder-Decoder Network with Residual ConvNeXt blocks\\nto maintain semantic richness across scales. These blocks mirror Transformer\\nstructureswithDepthwiseConvolution,ExpansionLayer,andCompressionLayer,\\nfacilitatingwidthandreceptivefieldscalingduringup-anddownsampling[17,18].\\nMedNeXt balances computational efficiency and segmentation accuracy, making\\nit ideal for resource-constrained environments like SSA and other LMICs. It ad-\\ndresses the limitations of U-Net by enhancing performance on limited datasets\\nthrough iterative kernel size increase and compound scaling, providing a robust'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='dresses the limitations of U-Net by enhancing performance on limited datasets\\nthrough iterative kernel size increase and compound scaling, providing a robust\\nsolution for medical image segmentation[17].\\nIn this paper, we apply a lightweight architecture MedNeXT [17], on the\\nBraTS Challenge on Sub-Saharan-Africa adult glioma Dataset[19] to automat-\\nically segment glioma into its sub-regions. We aim to demonstrate that models\\npre-trained on a larger dataset (BraTS-2021) may generalize poorly to smaller\\nand local datasets (BraTS-Africa), underscoring the need for domain adapta-\\ntion. To address this, we introduce a Parameter-Efficient Fine-Tuning (PEFT)\\napproach, utilizing convolutional adapter-inspired modules integrated with the\\nMedNeXT architecture. The PEFT method achieved comparable, or in some\\ncases, slightly superior performance to full fine-tuned model on the BraTS-Africa\\ndataset, while offering improved computational efficiency.\\n2 Related Works'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='cases, slightly superior performance to full fine-tuned model on the BraTS-Africa\\ndataset, while offering improved computational efficiency.\\n2 Related Works\\n2.1 Transfer Learning and Fine-tuning\\nFine-tuning[20] is a common approach in medical imaging, enabling models to\\nperform well even with limited task-specific datasets. This process involves ini-\\ntializing a model with weights learned from a pre-trained model, and then further\\ntraining it on a new dataset with task-specific labels. However, full fine-tuning\\nwhich involves updating all parameters of a pre-trained model, presents some\\nlimitations. While this method achieves high performance on downstream tasks,\\nit is parameter-inefficient. The entire modelâ€™s parameters are updated, leading\\nto increased computational costs and a higher risk of overfitting, particularly\\nwhen dealing with small datasets. [21] proposed a more efficient approach by\\nintroducing adapter modules. Their approach demonstrated that fine-tuning all'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='when dealing with small datasets. [21] proposed a more efficient approach by\\nintroducing adapter modules. Their approach demonstrated that fine-tuning all\\nparameters for each task is not always necessary and can result in excessive com-\\nputational demands. Instead, by training only a small set of additional parame-\\nters specific, their method achieved performance comparable to full fine-tuning\\nwhile significantly reducing the computational burden.4 Adhikari, Kulung, Bohaju, Poudel, Shakya et al.\\n2.2 Parameter Efficient Fine-tuning\\nParameter Efficient Fine-Tuning (PEFT) methods enhance model adaptation\\nby updating only a subset of the modelâ€™s parameters, contrasting with full fine-\\ntuning and transfer learning that either updates all parameters or makes minimal\\nadjustments. Various PEFT approaches have been explored in the literature,\\nsuch as Additive PEFT by [21] introduced additional trainable components into'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='adjustments. Various PEFT approaches have been explored in the literature,\\nsuch as Additive PEFT by [21] introduced additional trainable components into\\na pre-trained model. Other approaches have been further explored to refine the\\nconcept of adapters with various designs and architecture, demonstrating their\\neffectiveness across different tasks [22], [23], [24].\\n3 Methodology\\n3.1 Overview\\nGiven a multi-modal 3D volumeI âˆˆ RCÃ—HÃ—WÃ—D representing multiple MRI\\nsequences, we want to obtain a segmentation maskS âˆˆ NNÃ—HÃ—WÃ—D where N\\nrepresents the number of segmentation classes. We assume availability of dataset\\nD = {(Ii, Si) :i âˆˆ 1, 2, . . . , k}. In our case,C = 4representing 4 different MRI\\nsequences i.e. T1-weighted (T1w), T1-weighted contrast-enhanced (T1-c), T2-\\nweighted(T2w),T2w-Fluid-AttenuatedInversionRecovery(FLAIR)and N = 4\\nrepresenting segmentation classes i.e. background, Enhancing Tumor(ET), Non-\\nEnhancing Tumor Core(NETC) and Surrounding Non-Enhancing Flair Hyper-'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='representing segmentation classes i.e. background, Enhancing Tumor(ET), Non-\\nEnhancing Tumor Core(NETC) and Surrounding Non-Enhancing Flair Hyper-\\nintensity(SNFH).\\n3.2 Dataset\\nThe BraTS 2021 [25] and BraTS Africa [19] were used in this study. BraTS\\n2021 dataset consists of 1251 pre-operative adult glioma cases. Similarly, the\\nBraTS Africa dataset consists of 75 patients pre-operative adult glioma cases,\\ncollected retrospectively from various imaging centers in Africa14. Besides, the\\nlower MRI spatial resolution, the BraTS Africa dataset is unique in its late\\npresentation of disease and additional comorbidity resulting in a significant shift\\nin distribution compared to similar datasets from western populations[26]. All\\nimages are roughly aligned(co-registered) and resampled to isotropic voxels of\\n1mm3 as outlined by the BraTS challenge organizers [25][19]. The images were\\nsegmented by trained experts with varying skill levels and then verified by board-'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='1mm3 as outlined by the BraTS challenge organizers [25][19]. The images were\\nsegmented by trained experts with varying skill levels and then verified by board-\\ncertified radiologists with> 5 years of experience. Detailed information on the\\ndataset are available at [19].\\n3.3 Preprocessing\\nThe dataset undergoes several spatial and intensity transformations to ensure\\nconsistency and improved model performance.\\n14 This data collection work was supported by Consortium for Advancement of MRI\\nEducation and Research in Africa (CAMERA) and funded by Lacuna Fund.Parameter-efficient Fine-tuning for BraTS Africa 5\\nFig. 1.Brain image slices of a representative case from the BraTS Africa dataset\\nwith the four MRI modalities and manual annotated subregions (Mask), representing\\nbrain tumor sub-regions: Left to Right; T1-contrast-enhanced (T1c), pre-contrast T1-\\nweighted (T1w), FLAIR, T2-weighted (T2w), and Mask\\n1. Conversion to Canonical Orientation: Since the dataset combines two sets'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='weighted (T1w), FLAIR, T2-weighted (T2w), and Mask\\n1. Conversion to Canonical Orientation: Since the dataset combines two sets\\nof MRI scans in various orientations, all images are transformed to a stan-\\ndardized Right-Anterior-Superior (RAS) orientation.\\n2. Image Resizing: The images are resized to a target size of128 Ã— 128 Ã— 128\\nfrom its input volume of255Ã—255Ã—255, maintaining a consistent input size\\nfor the segmentation model. TorchIOâ€™s resize was used to downsample to the\\ntarget size to reduce the image resolution through interpolation rather than\\ncropping.\\n3. Intensity Normalization: The intensity values are normalized using z nor-\\nmalization, which sets mean to zero and the standard deviation to one.\\n4. DataAugmentation:Differentaugmentationtechniquessuchasflippingalong\\nLeft-Right axis, affine transformation (scaling, rotation, translation) and\\nRandomNoise were applied.\\n3.4 Model Architecture\\nWe implement MedNeXt-S, a small-size MedNext architecture, a light-weight'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='Left-Right axis, affine transformation (scaling, rotation, translation) and\\nRandomNoise were applied.\\n3.4 Model Architecture\\nWe implement MedNeXt-S, a small-size MedNext architecture, a light-weight\\nConvNeXt[27] architecture, as a baseline which implements encoder and de-\\ncoder mechanism like in Unet[11]. But, unlike Unet, MedNeXt uses basic blocks\\nthat use modern design choices (such as inverted bottleneck, and transformer-\\ninspired configurations) that improve over Unet. The Encoder consists of se-\\nquential multiple basic blocks that generate correlated spatial information with\\ndown-sampling to encode the input into rich feature maps by reducing spa-\\ntial resolution, expanding feature maps, and the decoder combines multiple ba-\\nsic blocks with up-sampling to reconstruct segmentation from encoded feature\\nmaps. Each basic block consists of depth-wise convolution followed by expansion\\nand reduction of channels forming an inverted bottleneck design with additional'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='maps. Each basic block consists of depth-wise convolution followed by expansion\\nand reduction of channels forming an inverted bottleneck design with additional\\nresidual connection. Skip connection is implemented between consecutive spa-\\ntial resolution encoder and up-sampling to restore lost spatial information during\\ndown-sampling.\\nIn depth-wise convolution, richer feature maps are learned from each channel\\nseparately (without mixing across-channel features), and then these individual\\nchannel-wise features are combined using a parameter-efficient 1x1x1 convolu-6 Adhikari, Kulung, Bohaju, Poudel, Shakya et al.\\ntion. Then, group normalization[28] is applied to reduce the change in the dis-\\ntribution of inputs for stable training even in small batch[29] size. The depth-\\nwise convolution is followed by the expansion of the channel dimension of the\\nfeature maps to obtain hierarchically connected expanded features required for'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='wise convolution is followed by the expansion of the channel dimension of the\\nfeature maps to obtain hierarchically connected expanded features required for\\ndownstream task. This is followed by GELU[30] activation function to enhance\\nconvergence, and compression layer to obtain refined feature maps preserving\\nonly the important features in this layer. Additionally, residual connection is\\nadded to recover better gradient.\\nWe implemented MedNeXt-S with kernel size of 3x3x3 and 32 input chan-\\nnels on input size 128x128x128. We applied expansion ratio 2 in each MedNeXt\\nblocks(basic, up-sampling and down-sampling blocks). Similar to baseline archi-\\ntecture, applied two sequential basic blocks (B1 - B9, as in Figure 3)\\n3.5 Proposed Architecture with Adapter\\nOur proposed architecture integrates the MedNeXt backbone with PEFT using\\nconvolutional adapter blocks. Letf represent the intermediate representation\\nlearned by our pre-trained model, which are kept frozen, andfâ€² be the adapted'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='convolutional adapter blocks. Letf represent the intermediate representation\\nlearned by our pre-trained model, which are kept frozen, andfâ€² be the adapted\\nfeatures or output from our trainable adapter module. The adapter features can\\nbe expressed as:\\nfâ€² = f + Adapter(f)\\nwhere â€˜+â€™ signifies the skip connection.\\nPrevious works on PEFT modules by [24] used two linear modules in their\\narchitecture. The first module projects the originald-dimensional features into\\ndâ€² dimensions. The second linear module then projectsdâ€² back tod-dimensions,\\nrepresented by:\\nfâ€² = f + Ïƒ(Ïˆ(f Â· W1) Â· W2)\\nwhere W1 âˆˆ RdÃ—dâ€²\\n, W2 âˆˆ Rdâ€²Ã—d, dâ€² â‰¤ d, andÏƒ,Ïˆ are some activation functions.\\nSimilarly, the conv-adapter proposed by [23] introduces depthwise separable\\nconvolutionsasanadapter.Theseconvolutionsdecomposestandardconvolutions\\ninto depthwise and point-wise convolutions, significantly reducing the number\\nof parameters and computations. The formulation is given by:'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='into depthwise and point-wise convolutions, significantly reducing the number\\nof parameters and computations. The formulation is given by:\\nfâ€² = f + Wup âŠ— activation(Wdown Ë†âŠ— f) =PL (activation(DC(f)))\\nwhere Wup and Wdown are the convolutional weights. Here,âŠ— denotes the depth-\\nwise convolution, andË†âŠ— denotes the point-wise convolution.\\nOur proposed architecture has an expansion layer between the depthwise and\\npoint-wise convolutions, inspired by the MedNeXt block. This adapted features\\nwith this expansion layer is represented as:\\nfâ€² = PL (GELU(EL (LN(DC(f)))))\\nwhere PL denotes the projection layer that performs point-wise convolution,EL\\nrepresents the expansion layer, LN is Layer Norm, GELU is activation applied\\nand DC is Depthwise Convolution.Parameter-efficient Fine-tuning for BraTS Africa 7\\nConvNeXt-Adapter\\nDepth-wise Conv.\\nExpansion Conv.\\nPoint-wise Conv.\\nLN\\nGELU\\nCin, H, W, D\\n3x3, Cin  \\n1x1, Cin \\n1x1, Cin * exp_r \\nCout, H, W, D\\nCin\\nCin * exp_r\\nCin'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='ConvNeXt-Adapter\\nDepth-wise Conv.\\nExpansion Conv.\\nPoint-wise Conv.\\nLN\\nGELU\\nCin, H, W, D\\n3x3, Cin  \\n1x1, Cin \\n1x1, Cin * exp_r \\nCout, H, W, D\\nCin\\nCin * exp_r\\nCin\\nFig. 2.Left: ConvNeXt-Adapter.Right: Different forms of Adapter placement within\\nthe MedNeXT Superblock\\nFig. 3.Conv-Adapter Inspired MedNext Segmentation Model Adapter Architecture\\nWe experimented with two variations of adapter placement: Sequential and\\nParallel, as shown in figure 2. However, Parallel placement of adapter resulted\\nin lower performance (Average Dice of 0.78 as compared to 0.80 of Sequential\\nAdapter).Therefore,themetricsmentionedintheResultsandDiscussionsection\\nare from the Sequential adapter. For reference, we have also provided a table\\ncomparing both versions in the Supplementary Materials.\\nTheadapterlayerresultsonlyinanadditional11.2%increasein#parameters\\n(total 34.99M) which is still far more lightweight compared to parameter-heavy\\narchitectures like SwinTransformerV2(âˆ¼3B). Additionally, finetuning adapter'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='(total 34.99M) which is still far more lightweight compared to parameter-heavy\\narchitectures like SwinTransformerV2(âˆ¼3B). Additionally, finetuning adapter\\nlayer requiredâˆ¼4 hrs compared toâˆ¼10 hrs for full-finetuning.\\n4 Results and Discussion\\nAs shown in Table 1, the MedNeXt model trained on BraTS-2021 performs com-\\nparatively well on in-domain evaluation set whereas when evaluated on a sam-8 Adhikari, Kulung, Bohaju, Poudel, Shakya et al.\\nples from different population, BraTS-Africa in this case, unseen during training,\\nwe see drastic reduction in performance. This necessitates further intervention\\nfor the model to be usable on the downstream BraTS-Africa task. We explore\\nParameter-efficient Fine-tuning (PEFT) for leveraging BraTS-2021 dataset due\\nto its proximity to the downstream task and added computational efficiency.\\nThe Lesionwise metric differs from vanilla Dice score in that it comparatively\\nheavily penalizes absence of individual lesion. The evaluation was performed by'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='The Lesionwise metric differs from vanilla Dice score in that it comparatively\\nheavily penalizes absence of individual lesion. The evaluation was performed by\\nMedPerf, a standardized platform ensuring reproducible benchmarking on siteâ€™s\\ndataset[31]\\nTable 1. Avg. Validation Dice Score on BraTS-2021 and BraTS-Africa validation\\ndataset when trained on BraTS-2021\\nBraTS-2021 BraTS-Africa\\nAverage Dice 0.70 0.50\\nAs shown in Table 2, the proposed Conv-adapter-based finetuning results in\\nsuperior segmentation performance compared to the baseline MedNeXt model\\ntrained only on the BraTS-Africa dataset. The Conv-adapter-based fine-tuned\\nmodel is obtained by first pre-training the MedNeXt architecture on the BraTS-\\n2021 dataset and then finetuned on BraTS-Africa. Both baseline and fine-tuned\\nmodels are evaluated on the BraTS-Africa validation dataset. The statistical\\nsignificance of the observed values was evaluated, yielding P-value of 0.000116,'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='models are evaluated on the BraTS-Africa validation dataset. The statistical\\nsignificance of the observed values was evaluated, yielding P-value of 0.000116,\\nconfirming that the differences are highly significant between performance of\\nâ€œwithout Fine-Tuningâ€ vs â€œwith PEFT Fine-Tuningâ€.\\nTable 2.Performance comparison between without fine-tuning and with PEFT fine-\\ntuning.\\nNetwork Metric Without Fine-Tuning With PEFT Fine-Tuning\\nET TC WT Avg. ET TC WT Avg.\\nMedNeXt-S\\nLesionWise Diceâ†‘ 0.44 0.35 0.26 0.35 0.65 0.68 0.81 0.71\\nStd. Dev. (0.29) (0.25) (0.20) (0.25) (0.27) (0.30) (0.22) (0.26)\\nDice â†‘ 0.72 0.68 0.77 0.72 0.74 0.79 0.88 0.80\\nStd. Dev. (0.21) (0.23) (0.22) (0.22) (0.21) (0.23) (0.14) (0.19)\\nLesionWise HD95â†“ 167.76 201.69255.18208.21 74.40 79.95 34.31 62.89\\nStd. Dev. (128.50)(110.80)(87.81)(109.04)(117.04)(118.13)(73.98)(103.05)\\nHD95â†“ 33.98 36.74 33.49 34.73 28.84 29.62 7.15 21.87\\nStd. Dev. (65.17)(63.51)(22.67)(50.45)(87.01)(86.50)(7.48)(60.33)'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='Std. Dev. (128.50)(110.80)(87.81)(109.04)(117.04)(118.13)(73.98)(103.05)\\nHD95â†“ 33.98 36.74 33.49 34.73 28.84 29.62 7.15 21.87\\nStd. Dev. (65.17)(63.51)(22.67)(50.45)(87.01)(86.50)(7.48)(60.33)\\nAs shown in Table 3, the proposed PEFT-based fine-tuned model slightly\\nsurpasses the full-fine tuned model, on average. We hypothesize that this is due\\nto the small size of the fine-tuned dataset which results in a parameter-efficient\\nmethod to prevent overfitting, a lingering challenge for full-fine tuned methods.\\nOn the other hand, Lesionwise Dice for smaller sub-regions such as enhancing tu-\\nmor or non-enhancing tumor core are better when full-finetuned, whereas PEFTParameter-efficient Fine-tuning for BraTS Africa 9\\nexcels at whole tumor which is a comparatively larger structure. Overall, the P-\\nvalue of 0.63 indicates that the observed difference between â€œFull Fine-Tuningâ€\\nand â€œwith PEFT Fine-Tuningâ€ is not statistically significant, suggesting compa-\\nrable performance of the two methods.'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='value of 0.63 indicates that the observed difference between â€œFull Fine-Tuningâ€\\nand â€œwith PEFT Fine-Tuningâ€ is not statistically significant, suggesting compa-\\nrable performance of the two methods.\\nTable 3. Performance comparison between Full Fine-Tuning and with PEFT Fine-\\nTuning.\\nNetwork Metric Full Fine-Tuning With PEFT Fine-Tuning\\nET TC WT Avg. ET TC WT Avg.\\nMedNeXt-S\\nLesionWise Diceâ†‘ 0.67 0.69 0.75 0.70 0.65 0.68 0.81 0.71\\nStd. Dev. (0.26) (0.28) (0.25) (0.26) (0.27) (0.30) (0.22) (0.26)\\nDice â†‘ 0.73 0.77 0.83 0.77 0.74 0.79 0.88 0.80\\nStd. Dev. (0.21) (0.24) (0.17) (0.21) (0.21) (0.23) (0.14) (0.19)\\nLesionWise HD95â†“ 61.43 67.53 57.27 62.08 74.40 79.95 34.31 62.89\\nStd. Dev. (111)(112.26)(99.54)(107.60)(117.04)(118.13)(73.98)(103.05)\\nHD95 â†“ 30.12 31.73 20.76 27.54 28.84 29.62 7.15 21.87\\nStd. Dev. (87.22)(86.79)(63.12)(79.04)(87.01)(86.50)(7.48) (60.33)\\nAs shown in Table 4, although full fine-tuning resulted in more consistent'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='HD95 â†“ 30.12 31.73 20.76 27.54 28.84 29.62 7.15 21.87\\nStd. Dev. (87.22)(86.79)(63.12)(79.04)(87.01)(86.50)(7.48) (60.33)\\nAs shown in Table 4, although full fine-tuning resulted in more consistent\\nperformance,PEFTachievedhigheraverageperformancewhilemaintaininghigh\\nspecificity (0.99) but lower sensitivity (0.75).\\nTable 4.Sensitivity and Specificity of BraTS-Africa Without Fine-Tuning, With Full\\nFine-Tuning, and With PEFT Fine-Tuning\\nWithout Fine-TuningWith Full Fine-TuningWith PEFT Fine-Tuning\\nET TC WT ET TC WT ET TC WT\\nSensitivity â†‘ 0.40 0.27 0.36 0.64 0.59 0.68 0.67 0.66 0.75\\nStd. Dev. (0.37) (0.34) (0.41) (0.31) (0.38) (0.38) (0.27) (0.33) (0.35)\\nSpecificity â†‘ 0.99 0.98 0.96 0.99 0.99 0.99 0.99 0.99 0.99\\nStd. Dev. (0.01) (0.03) (0.06) (0.002) (0.004) (0.01) (0.001) (0.004) (0.007)\\n5 Conclusion\\nWe explored convolutional adapter layer for parameter efficient fine tuning in\\nlight-weight MedNeXt-S architecture. The proposed method showed that PEFT'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='5 Conclusion\\nWe explored convolutional adapter layer for parameter efficient fine tuning in\\nlight-weight MedNeXt-S architecture. The proposed method showed that PEFT\\ncan leverage both BraTS-2021 and BraTS Africa dataset to obtain performance\\ncomparable to Full Fine-tuning with reduced training time.10 Adhikari, Kulung, Bohaju, Poudel, Shakya et al.\\nw/o \\nFT\\nfull \\nFT\\nPEFT\\n w/o \\nFT\\nfull \\nFT\\nPEFT\\nâ†‘\\nâ†“\\nFig. 4.Boxplot comparison of segmentation methods: without fine-tuning, full fine-\\ntuning, and PEFT, using Average Dice (left) and Average HD95 (right)\\nFig. 5.Visual comparison of Predicted and Ground Segmentations with and without\\nfinetuning.\\n6 Acknowledgement\\nThe authors thank Sprint AI Training for African Medical Imaging Knowledge\\nTranslation (SPARK) Academy in Deep Learning and Medical Imaging 2024\\nTeam for helping build foundational knowledge on Medical Image Segmenta-\\ntion including clinical domain-specific lecture sessions that helped us appre-'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='Team for helping build foundational knowledge on Medical Image Segmenta-\\ntion including clinical domain-specific lecture sessions that helped us appre-\\nciate the complexity of the problem. The compute facility was provided by\\nDigital Research Alliance of Canada (Compute Canada). Additional compute\\nsupport was provided by Nepal Applied Mathematics and Informatics Insti-\\ntute for research (NAAMII). Finally, we would like to thank Lacuna Fund for\\nHealth and Equity, the Radiological Society of North America (RSNA), Re-\\nsearch & Education (R&E) Foundation Derek Harwood-Nash International Ed-\\nucation Scholar Grant, and National Science and Engineering Research Coun-\\ncil of Canada (NSERC) Discovery Launch Supplement for making the SPARK\\nAcademy possible via research grant supports.Parameter-efficient Fine-tuning for BraTS Africa 11\\nReferences\\n1. Poon, M.T., Sudlow, C.L., Figueroa, J.D., Brennan, P.M.: Longer-term (geq 2'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='Academy possible via research grant supports.Parameter-efficient Fine-tuning for BraTS Africa 11\\nReferences\\n1. Poon, M.T., Sudlow, C.L., Figueroa, J.D., Brennan, P.M.: Longer-term (geq 2\\nyears) survival in patients with glioblastoma in population-based studies pre-and\\npost-2005: a systematic review and meta-analysis. Scientific reports10(1), 11622\\n(2020)\\n2. Anazodo, U.C., Ng, J.J., Ehiogu, B., Obungoloch, J., Fatade, A., Mutsaerts,\\nH.J., Secca, M.F., Diop, M., Opadele, A., Alexander, D.C., et al.: A framework\\nfor advancing sustainable magnetic resonance imaging access in africa. NMR in\\nBiomedicine 36(3), e4846 (2023)\\n3. Aderinto, N., Opeyemi, M.A., Opanike, J., Afolayan, O., Sakaiwa, N.: Navigating\\nthe challenges of neuro-oncology in africa: addressing diagnostic and treatment\\nbarriers in the region: a correspondence. IJS Global Health6(3), e136 (2023)\\n4. Patel, A.P., Fisher, J.L., Nichols, E., Abd-Allah, F., Abdela, J., Abdelalim, A.,'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='barriers in the region: a correspondence. IJS Global Health6(3), e136 (2023)\\n4. Patel, A.P., Fisher, J.L., Nichols, E., Abd-Allah, F., Abdela, J., Abdelalim, A.,\\nAbraha, H.N., Agius, D., Alahdab, F., Alam, T., et al.: Global, regional, and\\nnational burden of brain and other cns cancer, 1990â€“2016: a systematic analysis\\nfor the global burden of disease study 2016. The Lancet Neurology18(4), 376â€“393\\n(2019)\\n5. Mostafa, A.M., Zakariah, M., Aldakheel, E.A.: Brain tumor segmentation using\\ndeep learning on mri images. Diagnostics13(9), 1562 (2023)\\n6. Otsu, N., et al.: A threshold selection method from gray-level histograms. Auto-\\nmatica 11(285-296), 23â€“27 (1975)\\n7. Fayzi, A.: Introducing a novel method for adaptive thresholding in brain tumor\\nmedical image segmentation. arXiv preprint arXiv:2306.14250 (2023)\\n8. Kass, M., Witkin, A., Terzopoulos, D.: Snakes: Active contour models. Interna-\\ntional journal of computer vision1(4), 321â€“331 (1988)'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='8. Kass, M., Witkin, A., Terzopoulos, D.: Snakes: Active contour models. Interna-\\ntional journal of computer vision1(4), 321â€“331 (1988)\\n9. Bach Cuadra, M., Duay, V., Thiran, J.P.: Atlas-based segmentation. Handbook of\\nBiomedical Imaging: Methodologies and Clinical Research pp. 221â€“244 (2015)\\n10. Abdusalomov, A.B., Mukhiddinov, M., Whangbo, T.K.: Brain tumor detection\\nbased on deep learning approaches and magnetic resonance imaging. Cancers\\n15(16), 4172 (2023)\\n11. Ronneberger, O., Fischer, P., Brox, T.: U-net: Convolutional networks for biomed-\\nical image segmentation. In: Medical image computing and computer-assisted\\ninterventionâ€“MICCAI 2015: 18th international conference, Munich, Germany, Oc-\\ntober 5-9, 2015, proceedings, part III 18. pp. 234â€“241. Springer (2015)\\n12. Zargari, S.A., Kia, Z.S., Nickfarjam, A.M., Hieber, D., Holl, F.: Brain tumor clas-\\nsification and segmentation using dual-outputs for u-net architecture: O2u-net.'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='12. Zargari, S.A., Kia, Z.S., Nickfarjam, A.M., Hieber, D., Holl, F.: Brain tumor clas-\\nsification and segmentation using dual-outputs for u-net architecture: O2u-net.\\nStudies in health technology and informatics305, 93â€“96 (2023)\\n13. Kumar, E., Ajay, A., Vardhini, K., Vemu, R., Padmanabham, A.: Residual edge\\nattention in u-net for brain tumour segmentation. International Journal on Recent\\nand Innovation Trends in Computing and Communication11(4), 324â€“340 (2023)\\n14. Sahli, H., Slama, A.B., Sayadi, M.: Skin lesion segmentation based on modified\\nu-net architecture. In: 2023 IEEE International Conference on Advanced Systems\\nand Emergent Technologies (IC_ASET). pp. 1â€“5. IEEE (2023)\\n15. He,S.,Bao,R.,Grant,P.E.,Ou,Y.:U-netmer:U-netmeetstransformerformedical\\nimage segmentation. arXiv preprint arXiv:2304.01401 (2023)\\n16. Hatamizadeh, A., Nath, V., Tang, Y., Yang, D., Roth, H.R., Xu, D.: Swin unetr:\\nSwin transformers for semantic segmentation of brain tumors in mri images. In:'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='16. Hatamizadeh, A., Nath, V., Tang, Y., Yang, D., Roth, H.R., Xu, D.: Swin unetr:\\nSwin transformers for semantic segmentation of brain tumors in mri images. In:\\nInternational MICCAI Brainlesion Workshop. pp. 272â€“284. Springer (2021)12 Adhikari, Kulung, Bohaju, Poudel, Shakya et al.\\n17. Roy, S., Koehler, G., Ulrich, C., Baumgartner, M., Petersen, J., Isensee, F., Jaeger,\\nP.F., Maier-Hein, K.H.: Mednext: transformer-driven scaling of convnets for medi-\\ncal image segmentation. In: International Conference on Medical Image Computing\\nand Computer-Assisted Intervention. pp. 405â€“415. Springer (2023)\\n18. Maani, F., Hashmi, A.U.R., Saeed, N., Yaqub, M.: On enhancing brain tumor\\nsegmentation across diverse populations with convolutional neural networks. arXiv\\npreprint arXiv:2405.02852 (2024)\\n19. Adewole, M., Rudie, J.D., Gbdamosi, A., Toyobo, O., Raymond, C., Zhang, D.,\\nOmidiji, O., Akinola, R., Suwaid, M.A., Emegoakor, A., et al.: The brain tumor'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='preprint arXiv:2405.02852 (2024)\\n19. Adewole, M., Rudie, J.D., Gbdamosi, A., Toyobo, O., Raymond, C., Zhang, D.,\\nOmidiji, O., Akinola, R., Suwaid, M.A., Emegoakor, A., et al.: The brain tumor\\nsegmentation (brats) challenge 2023: Glioma segmentation in sub-saharan africa\\npatient population (brats-africa). ArXiv (2023)\\n20. Talukder, M.A., Islam, M.M., Uddin, M.A., Akhter, A., Pramanik, M.A.J., Aryal,\\nS., Almoyad, M.A.A., Hasan, K.F., Moni, M.A.: An efficient deep learning model\\nto categorize brain tumor using reconstruction and fine-tuning. Expert systems\\nwith applications230, 120534 (2023)\\n21. Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe, Q., Ges-\\nmundo, A., Attariyan, M., Gelly, S.: Parameter-efficient transfer learning for nlp.\\nIn: International conference on machine learning. pp. 2790â€“2799. PMLR (2019)\\n22. Pfeiffer, J., Kamath, A., RÃ¼cklÃ©, A., Cho, K., Gurevych, I.: Adapterfusion: Non-'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='In: International conference on machine learning. pp. 2790â€“2799. PMLR (2019)\\n22. Pfeiffer, J., Kamath, A., RÃ¼cklÃ©, A., Cho, K., Gurevych, I.: Adapterfusion: Non-\\ndestructive task composition for transfer learning. arXiv preprint arXiv:2005.00247\\n(2020)\\n23. Chen, H., Tao, R., Zhang, H., Wang, Y., Li, X., Ye, W., Wang, J., Hu, G., Savvides,\\nM.: Conv-adapter: Exploring parameter efficient transfer learning for convnets.\\nIn: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\\nRecognition. pp. 1551â€“1561 (2024)\\n24. Dhakal, M., Adhikari, R., Thapaliya, S., Khanal, B.: Vlsm-adapter: Finetuning\\nvision-language segmentation efficiently with lightweight blocks. arXiv preprint\\narXiv:2405.06196 (2024)\\n25. Baid, U., Ghodasara, S., Mohan, S., Bilello, M., Calabrese, E., Colak, E., Farahani,\\nK., Kalpathy-Cramer, J., Kitamura, F.C., Pati, S., et al.: The rsna-asnr-miccai\\nbrats 2021 benchmark on brain tumor segmentation and radiogenomic classifica-'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='K., Kalpathy-Cramer, J., Kitamura, F.C., Pati, S., et al.: The rsna-asnr-miccai\\nbrats 2021 benchmark on brain tumor segmentation and radiogenomic classifica-\\ntion. arXiv preprint arXiv:2107.02314 (2021)\\n26. Menze, B.H., Jakab, A., Bauer, S., Kalpathy-Cramer, J., Farahani, K., Kirby, J.,\\nBurren, Y., Porz, N., Slotboom, J., Wiest, R., et al.: The multimodal brain tumor\\nimage segmentation benchmark (brats). IEEE transactions on medical imaging\\n34(10), 1993â€“2024 (2014)\\n27. Liu, Z., Mao, H., Wu, C.Y., Feichtenhofer, C., Darrell, T., Xie, S.: A convnet for\\nthe 2020s. In: Proceedings of the IEEE/CVF conference on computer vision and\\npattern recognition. pp. 11976â€“11986 (2022)\\n28. Wu, Y., He, K.: Group normalization. In: Proceedings of the European conference\\non computer vision (ECCV). pp. 3â€“19 (2018)\\n29. Roy, S., KÃ¼gler, D., Reuter, M.: Are 2.5 d approaches superior to 3d deep networks\\ninwholebrainsegmentation?In:InternationalConferenceonMedicalImagingwith'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='29. Roy, S., KÃ¼gler, D., Reuter, M.: Are 2.5 d approaches superior to 3d deep networks\\ninwholebrainsegmentation?In:InternationalConferenceonMedicalImagingwith\\nDeep Learning. pp. 988â€“1004. PMLR (2022)\\n30. Hendrycks, D., Gimpel, K.: Gaussian error linear units (gelus). arXiv preprint\\narXiv:1606.08415 (2016)\\n31. Karargyris, A., Umeton, R., Sheller, M.J., Aristizabal, A., George, J., Wuest, A.,\\nPati, S., Kassem, H., Zenk, M., Baid, U., et al.: Federated benchmarking of medi-\\ncal artificial intelligence with medperf. Nature machine intelligence5(7), 799â€“810\\n(2023)Parameter-efficient Fine-tuning for BraTS Africa 13\\n7 Supplementary Materials\\n7.1 Performance of Parallel Vs Sequential ConvNeXt Adapter\\nTable 5.Performance comparison between PEFT using Sequential ConvNeXt Adapter\\nand Parallel ConvNeXt Adapter.\\nNetwork Metric Parallel ConvNeXt AdapterSequential ConvNeXt Adapter\\nET TC WT Avg. ET TC WT Avg.\\nMedNeXt-S\\nLesionWise Dice â†‘ 0.59 0.61 0.54 0.58 0.65 0.68 0.81 0.71'),\n",
       " Document(metadata={'arxiv_id': '2412.14100v1', 'title': 'Parameter-efficient Fine-tuning for improved Convolutional Baseline for Brain Tumor Segmentation in Sub-Saharan Africa Adult Glioma Dataset', 'section': 'body', 'authors': 'Bijay Adhikari, Pratibha Kulung, Jakesh Bohaju'}, page_content='and Parallel ConvNeXt Adapter.\\nNetwork Metric Parallel ConvNeXt AdapterSequential ConvNeXt Adapter\\nET TC WT Avg. ET TC WT Avg.\\nMedNeXt-S\\nLesionWise Dice â†‘ 0.59 0.61 0.54 0.58 0.65 0.68 0.81 0.71\\nDice â†‘ 0.72 0.77 0.86 0.78 0.74 0.79 0.88 0.80\\nLesionWise HD95â†“ 90.44 96.96 146.19 111.20 74.40 79.95 34.31 62.89\\nHD95 â†“ 29.96 32.52 24.41 28.96 28.84 29.62 7.15 21.87\\n7.2 Qualitative Visualization\\nFig. 6.Visual comparison of Predicted and Ground Segmentations with and without\\nfinetuning14 Adhikari, Kulung, Bohaju, Poudel, Shakya et al.\\nFig. 7.Visual comparison of Predicted and Ground Segmentations with and without\\nfinetuning'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'title_abstract', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='Title: Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images\\n\\nAbstract: Brain tumors pose a significant threat to human life, therefore it is very much necessary to detect them accurately in the early stages for better diagnosis and treatment. Brain tumors can be detected by the radiologist manually from the MRI scan images of the patients. However, the incidence of brain tumors has risen amongst children and adolescents in recent years, resulting in a substantial volume of data, as a result, it is time-consuming and difficult to detect manually. With the emergence of Artificial intelligence in the modern world and its vast application in the medical field, we can make an approach to the CAD (Computer Aided Diagnosis) system for the early detection of Brain tumors automatically. All the existing models for this task are not completely generalized and perform poorly on the validation data. So, we have proposed two novel Deep Learning Architectures - (a) SAETCN (Self-Attention Enhancement Tumor Classification Network) for the classification of different kinds of brain tumors. We have achieved an accuracy of 99.38% on the validation dataset making it one of the few Novel Deep learning-based architecture that is capable of detecting brain tumors accurately. We have trained the model on the dataset, which contains images of 3 types of tumors (glioma, meningioma, and pituitary tumors) and non-tumor cases. and (b) SAS-Net (Self-Attentive Segmentation Network) for the accurate segmentation of brain tumors. We have achieved an overall pixel accuracy of 99.23%.'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='Novel Deep Learning Architectures for Classification and\\nSegmentation of Brain Tumors from MRI Images\\nSayan Dasa,âˆ—, Arghadip Biswasb,âˆ—\\naIIIT Delhi, Delhi, India\\nbJadavpur University, Kolkata, India\\nAbstract\\nBrain tumors pose a significant threat to human life, therefore it is very much necessary to detect them accurately in the early\\nstages for better diagnosis and treatment. Brain tumors can be detected by the radiologist manually from the MRI scan images\\nof the patients. However, the incidence of brain tumors has risen amongst children and adolescents in recent years, resulting in\\na substantial volume of data, as a result, it is time-consuming and difficult to detect manually. With the emergence of Artificial\\nintelligence in the modern world and its vast application in the medical field, we can make an approach to the CAD (Computer\\nAided Diagnosis) system for the early detection of Brain tumors automatically. All the existing models for this task are not'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='Aided Diagnosis) system for the early detection of Brain tumors automatically. All the existing models for this task are not\\ncompletely generalized and perform poorly on the validation data. So, we have proposed two novel Deep Learning Architectures -\\n(a) SAETCN (Self-Attention Enhancement Tumor Classification Network) for the classification of different kinds of brain tumors.\\nWe have achieved an accuracy of 99.38% on the validation dataset making it one of the few Novel Deep learning-based architecture\\nthat is capable of detecting brain tumors accurately. We have trained the model on the dataset, which contains images of 3 types of\\ntumors (glioma, meningioma, and pituitary tumors) and non-tumor cases. and (b) SAS-Net (Self-Attentive Segmentation Network)\\nfor the accurate segmentation of brain tumors. We have achieved an overall pixel accuracy of 99.23%.\\nKeywords:Brain Tumor, MRI Images, Deep Learning, Machine Learning, CNN, Classification\\n1. Introduction'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='Keywords:Brain Tumor, MRI Images, Deep Learning, Machine Learning, CNN, Classification\\n1. Introduction\\nBrain Tumors are a huge concern in the field of medicine\\nbecause of their high mortality rate. Brain tumor forms when\\nthere is an uncontrollable abnormal growth of the cells within\\nthe Brain. The abnormal growth may occur in the brain itself\\nwhich is called a primary tumor or it may spread to the brain\\nfrom the other parts of the body which are called secondary or\\nmetastatic tumors [8]. The proper reason and causes of brain tu-\\nmors are not yet understood but according to researchers, they\\noccur due to genetic mutations that affect cell growth and divi-\\nsion [6]. This mutation can cause the cell to multiply caus-\\ning the tumor. There are 120 types of tumors out of which\\nsome tumors are benign (non-cancerous) which grows slowly\\nand some are malignant (cancerous) which grow rapidly [22].\\nThese tumors pose a significant global health challenge due to'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='some tumors are benign (non-cancerous) which grows slowly\\nand some are malignant (cancerous) which grow rapidly [22].\\nThese tumors pose a significant global health challenge due to\\ntheir high mortality rates, and complex procedure of treatment.\\nSome common types of Brain tumors that we are going to detect\\nand classify from the MRI Images by our proposed novel deep\\nlearning architectures - SAETCN and SAS-Net are - Gliomas,\\nmeningiomas, and Pituitary. All the brain tumor types and an\\nMRI image with no tumor are shown in Figure 1.\\nâˆ—These authors contributed equally to this work.\\nâˆ—âˆ—Note: This is a preprint made available via arXiv for open access and\\nacademic dissemination. It has not been submitted for peer-reviewed journal\\npublication.\\nGliomas are the most common type of Brian tumors. Around\\n30% of all brain and CNS tumors and 80% of all malignant\\ntumors are of this kind. This type of tumor can occur at any\\nage, but they are most common in the age range between 45'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='30% of all brain and CNS tumors and 80% of all malignant\\ntumors are of this kind. This type of tumor can occur at any\\nage, but they are most common in the age range between 45\\nand 65 years of age. The incidence of gliomas has been stable\\nin the last few years but due to the advancement of treatment,\\nthe mortality rate slightly increased [10].\\nMeningiomas are also one of the common primary types of\\nbrain tumors. Around 37% of all brain tumors are of this kind.\\nThey are mostly common amongst older individuals with ages\\ngreater than 65. The incidence of occurrence of this tumor is\\nincreasing and they are benign but their location can make them\\ndifficult to treat[10].\\nAround 15% of all brain tumors are of type Pituitary. This\\ncan occur at any age group but the most common age range is\\nbetween 30 to 50 years. [19] Most of them are benign in nature\\nand are slow-growing. The detection of pituitary tumors has\\nincreased due to the MRI images.'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='between 30 to 50 years. [19] Most of them are benign in nature\\nand are slow-growing. The detection of pituitary tumors has\\nincreased due to the MRI images.\\nThe overall rate of occurrence of brain tumors is 6.2 per\\n1,00,000 people per year and the death rate is 4.4 per 1,00,000\\npeople per year [10]. Brain tumors are the leading cause of\\ncancer-related deaths in males aged birth to 39 years and fe-\\nmales aged birth to 19 years [13]. There are almost 2,08,620\\ncases of brain tumors in adolescents and adults in the age group\\nof 19 to 39, who are living with a primary brain or spinal cord\\ntumor in the United States. The pituitary tumors are most com-\\narXiv:2512.06531v1  [cs.CV]  6 Dec 2025G l i o m a M e n i n g i o m a \\nP i t u i t a r y N o T u m o r \\nFigure 1: MRI Images of Types of Brain Tumor and No Tumor\\nmon among those incidents in that age group [19]. The 5-year\\nrelative survival rate for brain and other nervous system cancers\\nis about 33.4% [10].'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='mon among those incidents in that age group [19]. The 5-year\\nrelative survival rate for brain and other nervous system cancers\\nis about 33.4% [10].\\nThe brain is the most important organ of our body, Proper\\nfunctioning of the brain is very crucial for overall body perfor-\\nmance. So, Both benign and malignant tumors present in the\\nbrain pose a significant challenge due to their complex nature\\nand the critical functions of the brain. The accurate classifica-\\ntion and segmentation of different types of brain tumors, such\\nas gliomas, meningiomas, and pituitary tumors, in early stages,\\nis essential for effective treatment of the patient. The advance-\\nment of Artificial Intelligence in the modern world and its vast\\napplication in the field of medicine help us to classify and seg-\\nment different types of tumors and their exact position. Over\\nthe last few years, several researches have taken place to sup-\\nport that deep learning approaches are one of the best for the'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='ment different types of tumors and their exact position. Over\\nthe last few years, several researches have taken place to sup-\\nport that deep learning approaches are one of the best for the\\nclassification of brain tumors automatically and efficiently with\\na good accuracy rate and fewer errors [2; 4; 18; 21]. This paper\\nconsists of a detailed explanation of our novel proposed deep\\nlearning architecture with four modules that are made with 16\\nlayers of custom-made SAE blocks with attention and skip con-\\nnection mechanism inspired from the residual and inception op-\\nerations divided into four modules, giving a good accuracy that\\nbeats any state of the art deep learning ImageNet architectures.\\nThe study focuses on enhancing the accuracy and efficiency of\\nbrain tumor classification and early detection along with proper\\nsegmentation, which is crucial for effective diagnosis and treat-\\nment planning. The research contribution of this study is as\\nfollows:'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='segmentation, which is crucial for effective diagnosis and treat-\\nment planning. The research contribution of this study is as\\nfollows:\\nâ€¢Data Preprocessing: Effective data preprocessing has been\\ndone from the figshare data which was on mat files, they\\nare converted into image files. Basic preprocessing like\\ncontrast enhancement and normalization has been done for\\nthe other two datasets. Both three and four-class classifi-\\ncations have been performed based on our dataset.\\nâ€¢The proposition of our Novel Architecture for Classifica-\\ntion: SAETCN or Self-Attention Enhanced Tumor Clas-\\nsification Network is a completely novel architecture that\\nhas the power to beat many state-of-the-art deep learning\\nmodels for the classification of the brain tumor into three\\nclasses - glioma, meningioma, and pituitary from the MRI\\nimages.\\nâ€¢The proposition of our Novel Architecture for Segmenta-\\ntion: SAS-Net or Self Attentive Segmentation Network is'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='classes - glioma, meningioma, and pituitary from the MRI\\nimages.\\nâ€¢The proposition of our Novel Architecture for Segmenta-\\ntion: SAS-Net or Self Attentive Segmentation Network is\\na completely novel deep learning architecture that can seg-\\nment the tumour region accurately with an overall pixel\\naccuracy of 99.23%.\\nIn this paper, we have discussed the following sections: Sec-\\ntion 2 discusses the related works that have been done in this\\nfield and the recent state-of-the-art methods and their outcomes.\\nSection 3 explains our proposed methodology for the classi-\\nfication and segmentation of brain tumors. Section??pro-\\nvides the experimental setup, details about the dataset utilized\\nin this study, and the experimental results obtained through the\\nmethodology. Section??discusses the future scope of this re-\\nsearch and our plan to integrate this model into a mobile ap-\\nplication. Section 6 discusses the conclusions drawn from the\\nstudy.\\n2. Related Works'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='search and our plan to integrate this model into a mobile ap-\\nplication. Section 6 discusses the conclusions drawn from the\\nstudy.\\n2. Related Works\\nOver the last few years, many researches have been done\\nand many methodologies have been proposed for the detection,\\nclassification, and segmentation of brain tumors from MRI im-\\nages.\\n2.1. Review of previous works on classification\\nMuhammad Aamir [1] used an optimized convolutional neu-\\nral network on the Brain MRI dataset present in Kaggle and got\\nan accuracy of 97%. However the limitation of this model is\\nthat it is not completely generalized and as a result, the model\\ngets overfitted to the training data and may work poorer in the\\nunseen data despite having good performance metrics.\\nSaeedi et al. proposed a 2D CNN and an auto-encoder for the\\nclassification of brain images [21]. They have used a dataset\\ncomprised of 3264 T1-weighted contrast-enhanced MRI im-\\nages. The training accuracy of the proposed 2D CNN and that'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='classification of brain images [21]. They have used a dataset\\ncomprised of 3264 T1-weighted contrast-enhanced MRI im-\\nages. The training accuracy of the proposed 2D CNN and that\\nof the proposed auto-encoder network were found to be 96.47%\\nand 95.63%, respectively and the test accuracies are 93.44%\\nand 90.92%. The drawback of this paper is that the test accu-\\nracy is quite low and thus it may hinder the accurate prediction\\nand classification of the brain tumor types in real-life scenarios\\nthus affecting the treatment and diagnosis procedure.\\nTakowa Rahman [18] used PDCNN over three datasets - The\\nbinary tumor identification dataset-I, Figshare dataset-II, and\\nMulticlass Kaggle dataset-III provided accuracy of 97.33%,\\n97.60%, and 98.12%, respectively on the augmented dataset.\\nBut in the original dataset, they got a result of 96%, 96.10%,\\nand 95.60% respectively. It is clear that the model gave better\\nresults on the augmented data than that of the original data. The\\n2Train\\nValidation'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='and 95.60% respectively. It is clear that the model gave better\\nresults on the augmented data than that of the original data. The\\n2Train\\nValidation\\nTest\\nModel\\nNetwork\\nTraining\\nHyperparameter\\nTuning\\nEvaluation\\nDataset MRI Raw\\nImages\\nPreprocessing\\nFigure 2: Processflow Diagram of our Experiment\\nmajor drawback of using augmented data is that it may intro-\\nduce some artifacts that do not present in the real MRI Images.\\nAlso, the model may get overfitted with the augmented data,\\nThis can lead to models that perform well on augmented data\\nbut poorly on real, unseen data.\\nBadÅ¾a and Barjaktarovi Â´c conducted a study utilizing a con-\\nvolutional neural network (CNN) to classify glioma, menin-\\ngioma, and pituitary tumors [4]. The architecture of the net-\\nwork included an input layer, two â€œAâ€ blocks, two â€œBâ€ blocks,\\na classification block, and an output layer, totaling 22 layers.\\nThe performance of the network was assessed using the k-fold'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='work included an input layer, two â€œAâ€ blocks, two â€œBâ€ blocks,\\na classification block, and an output layer, totaling 22 layers.\\nThe performance of the network was assessed using the k-fold\\ncross-validation method, achieving a peak accuracy of 96.56%\\nwith tenfold cross-validation. The dataset comprised 3064 T1-\\nweighted contrast-enhanced MRI images sourced from Nan-\\nfang Hospital, General Hospital, and Tianjin Medical Univer-\\nsity in China.\\nNyoman Abiwinanda [2] used a CNN to classify the three\\nmost common types of brain tumors: Gliomas, Meningiomas,\\nand Pituitary. In the study, the CNN was trained in the 2017\\nfigshare dataset which contains 3064 T-1 weighted CE-MRI\\nfrom brain tumor images provided by Cheng. He used an\\n\"Adam\" optimizer and 32 filters were applied in all the con-\\nvolutional layers with a ReLU activation function and a max\\npool with size 2 x 2, and a fully connected layer of 64 neurons\\nwith a softmax activation function. The best-reported accuracy'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='volutional layers with a ReLU activation function and a max\\npool with size 2 x 2, and a fully connected layer of 64 neurons\\nwith a softmax activation function. The best-reported accuracy\\nrates for training and validation were 98.51% and 84.19%, re-\\nspectively. The low test accuracy is a major limitation of this\\nmodel that doubts the real-life classification of brain tumors.\\nSai Samarth R. Phaye [17] developed capsule algorithm net-\\nworks. (DCNet) and diverse capsule networks (DCNet++). A\\ndeeper convolutional network was added in DCNet to learn dis-\\ntinctive feature maps. The DCNet is more efficient for learning\\ncomplex data. They used a dataset comprising 3064 MRI im-\\nages of 233 brain tumor patients for classification. The accu-\\nracy of the DCNet algorithm test was 93.04%, and the accuracy\\nof the DCNet++algorithm was 95.03%.\\nPashaei et al. offered various methods for identifying three\\ntypes of tumors: meningiomas, gliomas, and pituitary malig-'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='of the DCNet++algorithm was 95.03%.\\nPashaei et al. offered various methods for identifying three\\ntypes of tumors: meningiomas, gliomas, and pituitary malig-\\nnancies [15] . To extract information from photographs, the\\ntechnique employed a convolutional neural network (CNN).\\nThe architecture had four convolutional layers, four pooling\\nlayers, one fully connected layer, and four batch normalization\\nlayers. The model was trained in 10 epochs, each with 16 itera-\\ntions and a learning rate of 0.01. Cheng contributed to the data\\nutilized in this study. The modelâ€™s performance was evaluated\\nusing a tenfold cross-validation technique, with 70% of the data\\nutilized for training and 30% for testing. The suggested algo-\\nrithm outperformed MLP, Stacking, XGBoost, SVM, and RBF\\nin terms of accuracy, with a 93.68% success rate.\\nPaul et al. [16] used deep learning algorithms to cate-\\ngorize brain scans of meningiomas, gliomas, and pituitary\\ncancers. They analyzed 3064 T1-weighted contrast-enhanced'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='Paul et al. [16] used deep learning algorithms to cate-\\ngorize brain scans of meningiomas, gliomas, and pituitary\\ncancers. They analyzed 3064 T1-weighted contrast-enhanced\\nMRI images from 233 individuals. The study developed\\ntwo types of neural networks: fully connected networks and\\nCNNs. The fivefold cross-validation procedure demonstrated\\nthat generic techniques performed better than particular meth-\\nods that needed picture dilation, with an accuracy of 91.43%.\\nIn all those previous studies we have found some major draw-\\nbacks. In [1] the model is not generalized enough. In [21; 2]\\nthe model performs poorly on the test dataset and thus becomes\\nrisky to implement it in real world. In [18] the model performs\\ngood in augmented data but not on the original data.\\n2.2. Review of previous works on segmentation\\nBrain tumor segmentation has been a focal point of research\\nin medical imaging, with deep learning methods showing sig-\\nnificant promise. Liu et al.[14] conducted a comprehensive'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='Brain tumor segmentation has been a focal point of research\\nin medical imaging, with deep learning methods showing sig-\\nnificant promise. Liu et al.[14] conducted a comprehensive\\nsurvey on deep learning-based brain tumor segmentation, high-\\nlighting various network architectures, segmentation under im-\\nbalanced conditions, and multi-modality processes. Their work\\nunderscores the importance of robust models capable of han-\\ndling diverse imaging conditions. Aggarwal et al.[3] proposed\\n3SAEB - Self Attention Enhancement Block\\nNCA Block - Normalised Convolutional Activation Block\\nFully Connected Layer\\nGlioma\\nMeningioma\\nPituitary\\nNo Tumor\\nF\\nL\\nA\\nT\\nT\\nE\\nN\\nClassification\\nINPUT\\nNCA Block\\nSAEB 1\\nSAEB 1\\nSAEB 1\\nSAEB 1\\nSAEB 1\\nSAEB 1\\nSAEB 1\\nSAEB 1\\nSAEB 1\\nSAEB 1\\nSAEB 1\\nSAEB 1\\nSAEB 1\\nSAEB 1\\nSAEB 1\\nSAEB 1 Avg. Pooling\\nInitial\\nTri-SAE\\nModule\\nQuadSAE\\nModule\\nHexaSAE\\nModule\\nFinal\\nSAE Fusion\\nModule(8 x 2 x 256 x 256)\\n(8 x 64 x 64 x 64)\\n(8 x 256 x 64 x 64)\\n(8 x 256 x 64 x 64)\\n(8 x 512 x 64 x 64)'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='SAEB 1 Avg. Pooling\\nInitial\\nTri-SAE\\nModule\\nQuadSAE\\nModule\\nHexaSAE\\nModule\\nFinal\\nSAE Fusion\\nModule(8 x 2 x 256 x 256)\\n(8 x 64 x 64 x 64)\\n(8 x 256 x 64 x 64)\\n(8 x 256 x 64 x 64)\\n(8 x 512 x 64 x 64)\\n(8 x 512 x 64 x 64)\\n(8 x 1024 x 64 x 64)\\n(8 x 2048 x 1 x 1)\\n(8 x 1024 x 64 x 64)\\n(8 x 2048 x 64 x 64)\\nFigure 3: Self-Attention Enhanced Tumor Classification Network (SAETCN)\\nan improved residual network (ResNet) for brain tumor seg-\\nmentation, addressing gradient diffusion issues and achieving\\nhigher precision compared to traditional methods. Their ap-\\nproach demonstrates the potential of enhanced deep learning\\nmodels in improving segmentation accuracy. Bouhafra and\\nEl Bahi[5] reviewed deep learning approaches for brain tumor\\ndetection and classification, emphasizing the role of transfer\\nlearning, autoencoders, and attention mechanisms. Their anal-\\nysis provides valuable insights into the advancements and limi-\\ntations of current methodologies. Rao and Karunakara[20] pre-'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='learning, autoencoders, and attention mechanisms. Their anal-\\nysis provides valuable insights into the advancements and limi-\\ntations of current methodologies. Rao and Karunakara[20] pre-\\nsented a comprehensive review on MRI-based brain tumor seg-\\nmentation, discussing semi-automatic techniques and the latest\\ntrends in deep learning methods. Their work highlights the on-\\ngoing evolution of segmentation techniques and the need for\\ncontinuous improvement. Verma et al.[23] conducted a com-\\nparative study of brain tumor segmentation methods, categoriz-\\ning them into conventional, machine learning-based, and deep\\nlearning-based approaches. Their findings indicate that deep\\nlearning models, particularly variants of the U-Net, outperform\\nother methods in terms of accuracy and reliability. These stud-\\nies collectively contribute to the understanding and advance-\\nment of brain tumor segmentation, providing a foundation for\\nthe development of novel deep-learning architectures.'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='ies collectively contribute to the understanding and advance-\\nment of brain tumor segmentation, providing a foundation for\\nthe development of novel deep-learning architectures.\\n3. Proposed Methodology\\n3.1. Overall Framework\\nIn this study, we present a comprehensive framework that\\naddresses both the classification and segmentation of brain tu-\\nmors, leveraging advanced neural network architectures to en-\\nhance performance across these critical tasks. For tumor clas-\\nsification, we propose the Self Attention Enhancement Tumor\\nClassification Network (SAETCN), designed to improve the ac-\\ncuracy of distinguishing between different types of brain tu-\\nmors. SAETCN utilizes self-attention mechanisms to focus on\\nrelevant features within the medical imaging data, effectively\\ncapturing intricate patterns that differentiate tumor types. On\\nthe segmentation front, we introduce the Self-Attentive Seg-\\nmentation Network (SAS-Net), a robust model aimed at pre-'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='capturing intricate patterns that differentiate tumor types. On\\nthe segmentation front, we introduce the Self-Attentive Seg-\\nmentation Network (SAS-Net), a robust model aimed at pre-\\ncisely delineating tumor boundaries within the brain. SAS-Net\\nemploys similar self-attentive techniques to SAETCN, ensuring\\na consistent and high-quality extraction of spatial features nec-\\nessary for accurate tumor segmentation. Together, these models\\nform a unified framework that not only enhances the classifi-\\ncation and segmentation tasks individually but also paves the\\nway for integrated approaches that can significantly improve\\nthe overall accuracy and reliability of brain tumor diagnosis.\\n3.2. Classification Architecture\\nWe have proposed a novel deep-learning architecture - Self-\\nAttention Enhanced Tumor Classification Network(SAETCN)\\nfor the classification of Brain tumors. Our model has beaten\\nmany state-of-the-art deep learning architectures by achieving'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='Attention Enhanced Tumor Classification Network(SAETCN)\\nfor the classification of Brain tumors. Our model has beaten\\nmany state-of-the-art deep learning architectures by achieving\\nan accuracy of 99.31% and 98.20% in the Brain MRI dataset\\nfrom Kaggel and Figshare Dataset by Jun Cheng respectively.\\nA multiclass classification of four types of classes is per-\\nformed by our proposed architecture - SAETCN which is in-\\nspired by the Residual and Inception theory. The detailed ar-\\nchitecture and mechanism of our proposed model are described\\nin the following subsections.\\n3.2.1. SAETCN Architecture\\nOur proposed Deep Learning CNN architecture - SAETCN\\nstands for Self-Attention Enhanced Tumor Classification Net-\\nwork, which can accurately classify 3 classes of tumor and non-\\ntumor patients from the MRI images given as input into the\\n4Add\\nR\\nE\\nL\\nUR\\nE\\nL\\nU\\nConv. Conv.\\nConv.\\nR\\nE\\nL\\nU\\nB\\nN\\nB\\nN\\nB\\nN\\nB\\nN\\nConv.\\nConv.\\nConv.Conv.\\nMax Pooling\\n2D Convolution Layer\\nO1 OFinal\\nO4\\nO2\\nOSkip\\nCatC\\nO3'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='4Add\\nR\\nE\\nL\\nUR\\nE\\nL\\nU\\nConv. Conv.\\nConv.\\nR\\nE\\nL\\nU\\nB\\nN\\nB\\nN\\nB\\nN\\nB\\nN\\nConv.\\nConv.\\nConv.Conv.\\nMax Pooling\\n2D Convolution Layer\\nO1 OFinal\\nO4\\nO2\\nOSkip\\nCatC\\nO3\\n(8 x 64 x 64 x 64)\\n(8 x 64 x 64 x 64)\\n(8 x 64 x 64 x 64)\\n(8 x 64 x 64 x 64)\\n(8 x 64 x 64 x 64)\\n(8 x 256 x 64 x 64)\\n(8 x 256 x 64 x 64)\\n(8 x 256 x 64 x 64)\\nBatch Normalisation\\nMax Pooling Layer\\nConcatenation\\nFigure 4: Self Attention Enhancement Block\\nModel. As shown in Figure 3 SAETCN consists of five main\\nparts: \"Normalised Convolutional Activation Block\", \"Initial\\nTriSAE Module\", \"QuadSAE Module\", \"HexaSAE Module\"\\nand \"Final SAE-Fusion Module\". Each Module Consists of\\nseveral Self Attention Enhancement Blocks. The SAETCN\\nArchitecture has 16 custom-made Self Attention Enhancement\\nBlocks arranged serially one after another. After the Complete\\nFeature Extraction, the feature maps go through the Adaptive\\nAveragepooling Layer and finally into the Dense Layer for non-\\nlinear classification.\\n3.2.2. Normalised Convolutional Activation Block'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='Feature Extraction, the feature maps go through the Adaptive\\nAveragepooling Layer and finally into the Dense Layer for non-\\nlinear classification.\\n3.2.2. Normalised Convolutional Activation Block\\nThis is the initial Block of this novel Architecture. The\\nmain operation of the Normalised Convolutional Activation\\nBlock (NCAB) is the primary extraction of underlying spatial\\nfeatures from the input images, detecting the important textures\\nand edges. The input images are converted into tensors and\\nthen directly sent into the NCAB as input to the Network. The\\nNCAB consists of a Convolutional layer with the size 7Ã—7,\\nwith ReLu activation followed by a Batchnormalisation and a\\nMax Pooling Layer for dimensionality reduction.\\nLet the output result of the NCAB be denoted asF NCAB. The\\nprocess at this stage can be expressed as:\\nFNCAB =K NCAB(X) (1)\\nWhere,F NCAB is the output feature map of the Neural Con-\\nvolutional Attention Block (NCAB).K NCAB is the function rep-'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='process at this stage can be expressed as:\\nFNCAB =K NCAB(X) (1)\\nWhere,F NCAB is the output feature map of the Neural Con-\\nvolutional Attention Block (NCAB).K NCAB is the function rep-\\nresenting the operations within the NCAB, which includes the\\nconvolutional layer, ReLU activation, batch normalization, and\\nmax pooling.Xis the input feature map to the NCAB. The\\ninternal operations of theK NCAB is represented further in the\\nequation 2.\\nKNCAB(X)=MaxPool(BatchNorm(ReLU\\n(Conv2D(Xinput,kernel size=7Ã—7)))) (2)\\n(8 x 3 x 256 x 256) (8 x 64 x 64 x 64)\\n2D Convolution Layer\\nBatch Normalisation\\nMax Pooling Layer\\nConv. B\\nN\\nR\\nE\\nL\\nU\\nMax Pooling\\nFigure 5: Normalised Convolutional Activation Block\\n3.2.3. Self Attention Enhancement Block\\nThe Self Attention Enhancement Block (SAEB) is the basic\\nFundamental Block that is used in our proposed architecture.\\nThis architecture is inspired by the operation of residual and\\ninception blocks. The basic operations of both of these blocks\\nare described below:'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='This architecture is inspired by the operation of residual and\\ninception blocks. The basic operations of both of these blocks\\nare described below:\\nâ€¢Residual Blocks use the concept of skip connections that\\nallow the gradient to flow directly and help in training deep\\n5networks. The residual block can be mathematically rep-\\nresented as:\\ny=F(x,{W i})+x\\nWhere,xis the Input to the block,Fis the Residual func-\\ntion (e.g., convolution, batch normalization, ReLU), and\\n{Wi}are the Weights of the layers within the block.\\nâ€¢Spatial Features are captured at multiple scales by the in-\\nception operation. It does this by applying convolutions\\nof different kernel sizes in parallel and concatenating their\\noutputs. The Inception operation can be mathematically\\nrepresented as: Mathematically, the outputOof an Incep-\\ntion module can be expressed as:\\nO=O 1 âŠ•O 3 âŠ•O 5 âŠ•O pool\\nWhere,O 1 is the Output from 1Ã—1 convolution branch,\\nO3 is the Output from 3Ã—3 convolution branch,O 5 is the'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='tion module can be expressed as:\\nO=O 1 âŠ•O 3 âŠ•O 5 âŠ•O pool\\nWhere,O 1 is the Output from 1Ã—1 convolution branch,\\nO3 is the Output from 3Ã—3 convolution branch,O 5 is the\\nOutput from 5Ã—5 convolution branch, andO pool is the\\nOutput from the pooling branch.\\nHere,âŠ•denotes the concatenation operator.\\nUsing the concept of both of these operations in our Self\\nAttention Enhancement Block (SAEB) has benefitted a lot.\\nThe skip connection helped to pass the gradient directly while\\nskipping some layers making the deep network train faster\\nand the inception operation helped to capture multiple spatial\\nfeatures from the image. As shown in Figure 4, the input of the\\nSAEB block is divided into 4 Branches and a skip connection\\nline. The first branch goes through a single Convolutional\\nLayer. The second Branch goes through the convolutional\\nlayer with batch normalization and ReLu activation function\\nfollowed by another convolutional layer with size 3Ã—3. The'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='Layer. The second Branch goes through the convolutional\\nlayer with batch normalization and ReLu activation function\\nfollowed by another convolutional layer with size 3Ã—3. The\\nthird branch consists of the same layers as that of the second\\nbranch but the size of the convolutional layer is 5Ã—5. The\\nfourth branch goes through a max pooling with size 3Ã—3\\nfollowed by a convolutional layer. The skip connection line\\nfrom the input goes through a convolution layer with batch\\nnormalization. The process can be expressed as follows:\\nThe output of each branch is denoted asO i (wherei=\\n1,2,3,4), and the output of the skip connection line is denoted\\nasO skip.\\nO1 =f conv(X) (3)\\nO2 =f conv3(f BN(f ReLU(f conv(X)))) (4)\\nO3 =f conv5(f BN(f ReLU(f conv(X)))) (5)\\nO4 =f conv(f pool(X)) (6)\\nOskip =f BN(f conv(X)) (7)\\nWhere,Xis the Input feature map to the SAEB,O i is the Out-\\nput of thei-th branch,O skip is the Output of the skip connection\\nline,f conv is Convolutional layer,f conv3 is Convolutional layer'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='Where,Xis the Input feature map to the SAEB,O i is the Out-\\nput of thei-th branch,O skip is the Output of the skip connection\\nline,f conv is Convolutional layer,f conv3 is Convolutional layer\\nwith kernel size 3Ã—3,f conv5 is the Convolutional layer with\\nkernel size 5Ã—5,f BN is the Batch normalization,f ReLU is the\\nReLU activation function,f pool is the Max pooling layer with\\nsize 3Ã—3.\\nAll the outputs of the first four branches are concatenated and\\na batch normalization operation is done on the resulting output.\\nThis process can be expressed as:\\nOconcat =BN(O 1 âŠ•O 2 âŠ•O 3 âŠ•O 4) (8)\\nwhereâŠ•denotes the concatenation operator.\\nFinally, the output is added with the skip connection layer\\nand an activation function ReLu is activated to the ultimate out-\\nput.\\nOfinal =ReLU(O concat +O skip) (9)\\n3.2.4. Initial TriSAE Module\\nThe Initial TriSAE Module is the Primary Module of the\\nArchitecture that contains three Self-Attention Enhancement\\nBlocks (SAEB). It has 64 input channels and 256 output chan-'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='The Initial TriSAE Module is the Primary Module of the\\nArchitecture that contains three Self-Attention Enhancement\\nBlocks (SAEB). It has 64 input channels and 256 output chan-\\nnels. The resultF NCAB of the Normalised Convolutional Acti-\\nvation Block (NCAB) is taken as input and passes through three\\nSelf-Attention Enhancement Blocks serially, one after another.\\nThe process is described as follows:\\nFSAEB1 =SAEB 1(FNCAB) (10)\\nFSAEB2 =SAEB 2(FSAEB1 ) (11)\\nFSAEB3 =SAEB 3(FSAEB2 ) (12)\\nWhere,F SAEB3 is the output of the Initial TriSAE Module,\\nwith 256 output channels.\\n3.2.5. QuadSAE Module\\nThe QuadSAE Module, serving as the secondary module in\\nthe architecture, comprises four Self-Attention Enhancement\\nBlocks (SAEB). The outputF SAEB3 from the Initial TriSAE\\nModule is used as the input for the QuadSAE Module. This\\ninput sequentially passes through the four Self-Attention En-\\nhancement Blocks. This Module has 256 input channels and\\n512 output channels. The process is mathematically repre-'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='input sequentially passes through the four Self-Attention En-\\nhancement Blocks. This Module has 256 input channels and\\n512 output channels. The process is mathematically repre-\\nsented as follows:\\nFSAEB4 =SAEB 4(FSAEB3 ) (13)\\nFSAEB5 =SAEB 5(FSAEB4 ) (14)\\nFSAEB6 =SAEB 6(FSAEB5 ) (15)\\nFSAEB7 =SAEB 7(FSAEB6 ) (16)\\nWhere,F SAEB7 is the output of the QuadSAE Module, with\\n512 output channels.\\n6Glioma\\nNCAB\\nMeningioma\\nPituitary\\nNo Tumor\\nInitial TriSAE HexaSAEQuadSAE Final SAE Fusion\\nFigure 6: Comparison of all metrics on different models on dataset 1\\n3.2.6. HexaSAE Module\\nThe Operations of this HexaSAE Module is similar to the\\nInitial TriSAE Module and QuadSAE Module. But the differ-\\nence is it has six Self Attention Enhancement Modules and the\\ninput has 512 channels and 1024 output channels. The output\\nFSAEB7 of the QuadSAE module is taken as input and computed\\nby the HexaSAE function to present the ultimate Output of this\\nmodule. The process of this module is represented in equation\\n17.'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='FSAEB7 of the QuadSAE module is taken as input and computed\\nby the HexaSAE function to present the ultimate Output of this\\nmodule. The process of this module is represented in equation\\n17.\\nFout =K HexaSAE(FSAEB7 ) (17)\\nWhere,F out is the output of the HexaSAE Module, with 1024\\noutput channels andK HexaSAE is the six SAE Block Operations\\nperformed in HexaSAE Module.\\n3.2.7. Final SAE Fusion Module\\nThe Final SAE Fusion Module is the Final Module of our\\nDeep Learning Architecture that takes 1024 channels as input\\nand produces an output of 2048 channels. It consisted of three\\nSelf Attention Enhancement blocks like the TriSAE Module but\\nhas a difference in the input-output channels. The output of the\\nHexaSAE Module is taken as input and passes through three\\nSAE blocks to compute the Output. The process is represented\\nas follows:\\nFSAEFMout =SAEB 16(SAEB15(SAEB14(Fout))) (18)\\nWhere, FSAEFM out is the output of the Final SAE Fusion\\nModule, with 2048 output channels andF out is the output of'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='as follows:\\nFSAEFMout =SAEB 16(SAEB15(SAEB14(Fout))) (18)\\nWhere, FSAEFM out is the output of the Final SAE Fusion\\nModule, with 2048 output channels andF out is the output of\\nthe HexaSAE Module.\\nAfter passing the Final SAE Fusion Module the output goes\\nthrough an Average Pooling Layer which calculates the aver-\\nage value of each patch of the filter-covered feature map. This\\noperation is beneficial for smoothing and minimizing the ef-\\nfects of outliers [25; 12]. A flattened layer is used to trans-\\nform the multi-dimensional output of the proposed CNN net-\\nwork into a one-dimensional vector. This transformation is nec-\\nessary because fully connected (dense) layers, which typically\\nfollow convolutional layers, require a one-dimensional input.\\nIn dense layer 2048 neorons are assigned for non-linear classi-\\nfication with an activation function ReLU. For a positive input,\\nthe ReLU function outputs the value directly; if not, it outputs'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='In dense layer 2048 neorons are assigned for non-linear classi-\\nfication with an activation function ReLU. For a positive input,\\nthe ReLU function outputs the value directly; if not, it outputs\\nzero [11; 9]. In the Final Dense Layer, 4 neurons are placed to\\nclassify 3 types of tumor and Non-Tumor classes. A softmax\\nactivation is used in the final layer which converts logits into\\nprobabilities[7].\\n3.2.8. Model Complexity Analysis\\nIn Saeedi et al. [21] a 2D CNN has been proposed whose\\nmathematical equation is represented in the equation 19. They\\nhave used sequential layers of two convolutions followed by a\\nmaxpooling and this has been repeated four times sequentially\\nwith different filter sizes (64,32,16 and 8 respectively).\\nXout =Dropout(MaxPool(Conv(Conv(\\n. . .Dropout(MaxPool(Conv(Conv(X))))))))) (19)\\n7INPUT\\nC\\no\\nn\\nv\\nM\\na\\nx\\nP\\no\\no\\nL\\nM\\na\\nx\\nP\\no\\no\\nL\\nM\\na\\nx\\nP\\no\\no\\nL\\nM\\na\\nx\\nP\\no\\no\\nL\\nSAEB\\nSFD\\nBlock\\nSFD\\nBlock\\nSFD\\nBlock\\nSegmentation\\n(Output)\\n(8 x  3x 256 x 256)\\n(8 x 128 x 256 x 256)'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='7INPUT\\nC\\no\\nn\\nv\\nM\\na\\nx\\nP\\no\\no\\nL\\nM\\na\\nx\\nP\\no\\no\\nL\\nM\\na\\nx\\nP\\no\\no\\nL\\nM\\na\\nx\\nP\\no\\no\\nL\\nSAEB\\nSFD\\nBlock\\nSFD\\nBlock\\nSFD\\nBlock\\nSegmentation\\n(Output)\\n(8 x  3x 256 x 256)\\n(8 x 128 x 256 x 256)\\n(8 x 128 x 256 x 256) (8 x 1 x 256 x 256)\\n(8 x 256 x 64 x 64)\\n(8 x 512 x 64 x 64)\\n(8 x 512 x 64 x 64)\\n(8 x 512 x 32 x 32)\\n(8 x 1024 x 32 x 32)\\n(8 x 1024 x 32 x 32)\\n(8 x 1024 x 16 x 16)\\n(8 x 2048 x 16 x 16)\\n(8 x 256 x 128 x 128)\\n(8 x 128 x 128 x 128)\\nEnhanced Segmentation\\nIntegration Module\\n(Decoder)\\nSFDB - Segmental Feature Decoding Block\\nSAEB - Self Attention Enhancement Block\\nSFD\\nBlock\\nSAEB SAEB SAEB SAEB\\nFigure 7: Self-Attentive Segmentation Network\\nAs compared to our model they have not used any residual\\nor inception operation which can help in multiple feature ex-\\ntraction and faster convergence. The inception operation used\\nin our model is one of the big reasons for achieving perfect\\nclassification accuracy and beating otherâ€™s works. The main\\nfoundation of our proposed architecture is our Self Attention'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='in our model is one of the big reasons for achieving perfect\\nclassification accuracy and beating otherâ€™s works. The main\\nfoundation of our proposed architecture is our Self Attention\\nEnhancement Block. We have used four parallel connections\\nand a skip connection.\\nThe First 1x1 convolution acts as a linear transformation, re-\\nducing dimensionality and capturing spatial relationships with-\\nout changing the feature map size. It retains essential infor-\\nmation while making the model computationally efficient. This\\nconnection helps in identifying fine-grained, low-level features.\\nIn the second Connection, The initial 1x1 convolution helps\\nin reducing dimensionality, followed by batch normalization,\\nwhich normalizes activations, and ReLU adds non-linearity.\\nThe second 1x1 convolution further refines the extracted fea-\\ntures. This connection is capable of learning more complex\\nfeatures due to the added non-linearity and normalization, en-\\nsuring stable and faster convergence.'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='tures. This connection is capable of learning more complex\\nfeatures due to the added non-linearity and normalization, en-\\nsuring stable and faster convergence.\\nThe third connection works quite a bit the same as the sec-\\nond connection, but its parallel nature allows learning a slightly\\ndifferent set of features due to different initial weights and gra-\\ndient flows. It provides redundancy and robustness, capturing\\ndiverse patterns that might be missed by a single path.\\nIn the fourth connection, the max pooling reduces the spatial\\ndimensions, capturing dominant features and making the sub-\\nsequent convolution focus on these. The 1x1 convolution then\\nprocesses this reduced information efficiently. This connection\\nhighlights the most prominent features, ensuring that the essen-\\ntial information is retained and further refined.\\nthe fifth connection acts as a skip connection ensuring that\\nthe network can learn residuals, thus combating the vanishing'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='tial information is retained and further refined.\\nthe fifth connection acts as a skip connection ensuring that\\nthe network can learn residuals, thus combating the vanishing\\ngradient problem. This helps in stable and faster training. It\\nHelps in capturing both the initial low-level features and the\\nrefined features from the other paths, improving the overall fea-\\nture richness.\\nIn our model, we have used 4 different modules for extracting\\ndifferent features. The first Module extract the primary edges\\nof the brain along with the edges of the tumor and other criti-\\ncal parts of it. It just extracts the features of the outline, while\\nthe second module goes deep into the outline and extracts fea-\\ntures on the deep pixel levels. In this module, clear predictions\\ncan be made whether the tumor is present or not. The third\\nmodule is given to understand the pixeled images to classify\\ndifferent types of tumors present in the tumor. This layer anal-'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='can be made whether the tumor is present or not. The third\\nmodule is given to understand the pixeled images to classify\\ndifferent types of tumors present in the tumor. This layer anal-\\nyses the different shapes and structures of different classes of\\ntumors. The fourth module is given to improve performance re-\\nsults based on our experiments. These four are the main build-\\ning blocks of our proposed model.\\nThe result of our Model is Completely based on repetitive\\nexperiments and training step by step and module by module.\\nThe only reason to use 16 Self Attention Enhancement Blocks\\nserially one after another, is because this is the optimum value\\nwhere we are getting the best classification result. The number\\nof SAE Blocks that we add one by one is directly proportional\\nto the best positive results but after a certain number of block(\\nthat is 16 in our experiment) the classification ability gets satu-\\n8conv\\nTranspose\\n2D\\nAdd\\nR\\nE\\nL\\nUR\\nE\\nL\\nU\\nConv. Conv.\\nConv.\\nR\\nE\\nL\\nU\\nB\\nN\\nB\\nN\\nB\\nN\\nB\\nN\\nConv.'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='that is 16 in our experiment) the classification ability gets satu-\\n8conv\\nTranspose\\n2D\\nAdd\\nR\\nE\\nL\\nUR\\nE\\nL\\nU\\nConv. Conv.\\nConv.\\nR\\nE\\nL\\nU\\nB\\nN\\nB\\nN\\nB\\nN\\nB\\nN\\nConv.\\nConv.\\nConv.Conv.\\nMax Pooling\\n2D Convolution Layer\\nL1 OFinal\\nO4\\nL2\\nLSkip\\nO3\\n(8 x 640 x 64 x 64)\\n(8 x 1024 x 32 x 32)\\n(8 x 512 x 64 x 64)\\n(8 x 128 x 64 x 64)\\n(8 x 128 x 64 x 64)\\n(8 x 128 x 64 x 64)\\n(8 x 128 x 64 x 64)\\n(8 x 128 x 64 x 64)\\n(8 x 512 x 64 x 64)\\n(8 x 512 x 64 x 64)\\n(8 x 512 x 64 x 64)\\nBatch Normalisation\\nMax Pooling Layer\\nConcatenation\\nCatCCatC\\nFigure 8: SFD Block\\nrated and thus we select that optimum 16 SAE blocks. Those 16\\nSAE blocks are divided into 4 modules because of their differ-\\nent feature extraction capabilities. This architecture is powerful\\nfor capturing the varied and intricate features necessary for ac-\\ncurate brain tumor classification, offering both computational\\nefficiency and improved performance.\\n3.3. Segmentation Architecture\\nBrain Tumor Segmentation is very crucial for diagnosis plan-'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='curate brain tumor classification, offering both computational\\nefficiency and improved performance.\\n3.3. Segmentation Architecture\\nBrain Tumor Segmentation is very crucial for diagnosis plan-\\nning. Doctors can plan the treatment process by properly local-\\nizing the tumor. Thus we have proposed a novel architecture -\\nSelf Attentive Segmentation Network, that can accurately seg-\\nment the tumor region with a pixel accuracy of 99.23%.\\n3.3.1. Self-Attentive Segmentation Network\\nThe Self Attentive Segmentation Network consists of five\\nSelf Attention Enhancement Blocks connected sequentially\\nwith a max pooling layer between them and an Enhanced Seg-\\nmentation Integration Module as a decoder which has four Seg-\\nmental Feature Decoding Block in it. Each SAE Block has two\\noutputs. One is connected as an input to the consecutive SAE\\nBlock and another is passed as an input to the (k-N)th SFD\\nBlock, where k is the total number of SAE Block used in the'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='outputs. One is connected as an input to the consecutive SAE\\nBlock and another is passed as an input to the (k-N)th SFD\\nBlock, where k is the total number of SAE Block used in the\\narchitecture (here k=5) and N is the current number of the SAE\\nBlock. The output from the Enhanced Segmentation Integra-\\ntion Module is subsequently processed through a Convolutional\\nLayer, culminating in the precise segmentation of the tumor.\\nThe complete architecture of the Self Attentive Segmentation\\nNetwork is shown in the figure 7.\\n3.3.2. Segmental Feature Decoding Block\\nThe Segmental Feature Decoder Block (SFD Block) is an in-\\ntegral part of the segmentation model, meticulously designed to\\nenhance the resolution and accuracy of feature maps through a\\ncombination of upsampling and feature integration from the en-\\ncoder. This block begins with a ConvTranspose2d layer, which\\nupsamples the input feature maps, thereby increasing their spa-\\ntial resolution. Following this, the upsampled features are con-'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='coder. This block begins with a ConvTranspose2d layer, which\\nupsamples the input feature maps, thereby increasing their spa-\\ntial resolution. Following this, the upsampled features are con-\\ncatenated with corresponding feature maps from the (k-n)th\\nSAE Blocks(where k is the total number of SAE Block used\\nin the architecture, here k=5 and n is the current number of the\\nSFD Block), ensuring the preservation of critical spatial infor-\\nmation. The concatenated feature map then undergoes further\\nprocessing through a complex Residual Inception module. This\\nmodule consists of multiple convolutional paths: a 1x1 convo-\\nlution, a sequence of 1x1 followed by 3x3 convolutions, and\\na 1x1 followed by 5x5 convolutions, each capturing features\\nat different scales. Additionally, a path involving max pool-\\ning followed by a 1x1 convolution is included to capture spa-\\ntial information from pooled features. The outputs from these\\npaths are concatenated and batch-normalized. Also, there is an-'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='ing followed by a 1x1 convolution is included to capture spa-\\ntial information from pooled features. The outputs from these\\npaths are concatenated and batch-normalized. Also, there is an-\\nother skip layer(L-skip) having a 1x1 convolution with batch\\nnormalization that gets added to the output of the four parallel\\nlayers. And then the added output is activated using ReLU, cre-\\nating a rich, multi-scale feature representation. A crucial aspect\\nof this block is the residual connection that adds the original\\ninput back to the processed output, ensuring efficient gradient\\nflow and mitigating the risk of vanishing gradients. This com-\\nprehensive approach results in refined, high-resolution feature\\n9maps that significantly enhance the precision of the segmen-\\ntation, particularly for complex tasks like brain tumor identifi-\\ncation. The complete architecture of this block is shown in the\\nfigure 8. The mathematics is described in detail in the following\\nequations.\\nLup =f convT (X) (1)'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='cation. The complete architecture of this block is shown in the\\nfigure 8. The mathematics is described in detail in the following\\nequations.\\nLup =f convT (X) (1)\\nLconcat =L up âŠ•S(2)\\nL1 =f conv1x1(Lconcat) (3)\\nL2 =f conv3x3(f BN (f ReLU (f conv1x1(Lconcat)))) (4)\\nL3 =f conv5x5(f BN (f ReLU (f conv1x1(Lconcat)))) (5)\\nL4 =f conv1x1(f pool(Lconcat)) (6)\\nLskip =f BN (f conv1x1(Lconcat)) (7)\\nLoutput =f ReLU (f BN (L1 âŠ•L 2 âŠ•L 3 âŠ•L 4)+L skip ) (8)\\nWhere,L up is the upsampled feature map obtained using the\\ntransposed convolution.f convT is transposed convolution oper-\\nation, which upsamples the input feature mapX.Xis the input\\nfeature map to the UpBlock.L concat: The concatenated feature\\nmap from upsampling and the skip connection.âŠ•is concatena-\\ntion operation.Sis the skip connection from the correspond-\\ning encoder layer.L 1 is output of the 1x1 convolution branch.\\nfconv1x1 is 1x1 convolution operation.L 2 is the Output of the\\n1x1 convolution followed by a 3x3 convolution branch.f BN'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='ing encoder layer.L 1 is output of the 1x1 convolution branch.\\nfconv1x1 is 1x1 convolution operation.L 2 is the Output of the\\n1x1 convolution followed by a 3x3 convolution branch.f BN\\nis the Batch normalization function.f ReLU is ReLU activation\\nfunction.f conv3x3 is the 3x3 convolution operation.L 3is the\\nOutput of the 1x1 convolution followed by a 5x5 convolution\\nbranch.f conv5x5 is 5x5 convolution operation.L 4 is the Output\\nof the max pooling followed by a 1x1 convolution branch.f pool\\nis Max pooling operation.L skip is the Output of the shortcut\\nconnection.L output is the Final output of the Residual Incep-\\ntion Block after combining all branches and adding the residual\\nconnection.\\nFigure 9: Segmentating the Tumor region accurately\\n3.3.3. Enhanced Segmentation Integration Module\\nThe Enhanced Segmentation Integration Module acts as a de-\\ncoder in this novel Architecture. This module consists of four\\nsequential Segmental Feature Decoding Block. The mathemati-'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='The Enhanced Segmentation Integration Module acts as a de-\\ncoder in this novel Architecture. This module consists of four\\nsequential Segmental Feature Decoding Block. The mathemati-\\ncal Operation of this Module is described in the following equa-\\ntion.\\nIN(S FD n)=\\n\\uf8f1\\uf8f4\\uf8f4\\uf8f2\\uf8f4\\uf8f4\\uf8f3\\nS AEB5 âŠ•S AEB kâˆ’n ifn=1\\nS FDnâˆ’1 âŠ•S AEB kâˆ’n ifn>1 (20)\\nWe defineS FD n as thenth SFD block,S AEB k as thek-th\\nSAEB block, andIN(S FD n) as the input to then-th SFD block.\\nHere,âŠ•represents the concatenation operation. The equation\\n20 captures the input to each SFD block. Forn=1, The first\\nSFD block takes inputs fromS AEB 5 andS AEB 4. Forn>1,\\nEach subsequent SFD block takes input from the previous SFD\\nblock and the corresponding SAEB block (kâˆ’n). Here, k=5,\\nbecause we have used five SAEB blocks in our architecture.\\n4. Experiment and Result\\nIn this section we have discussed about the dataset and the\\npreprocessing steps, the total experimental setup, and the eval-'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='4. Experiment and Result\\nIn this section we have discussed about the dataset and the\\npreprocessing steps, the total experimental setup, and the eval-\\nuation metrics along with the results. Also, we have compared\\nour model with previous works and other state-of-the-art exist-\\ning models.\\n4.1. Dataset and Preprocessing\\nWe have used three publicly available MRI Image datasets\\nfor the classification of the Brain tumors in this study. The\\ndatasets are as follows:\\nBrain MRI Dataset (Dataset 1) from Kaggle. This dataset\\ncontains 7023 images of human brain MRI images which\\n10Dataset 1 Dataset 2 Dataset 3\\nClass Images Train Test Class Images Train Test Class Images Train Test\\nGliomas1621 1321 300Gliomas1426 1153 273Gliomas1426 1129 297\\nMeningiomas1645 1339 306Meningiomas708 566 142Meningiomas708 572 136\\nPituitary1757 1457 300Pituitary930 732 198Pituitary930 749 181\\nNo Tumor2000 1595 405No Tumor1485 1189 296\\nTotal 7023 5712 1311 Total 3064 2451 613 Total 4549 3639 910'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='Pituitary1757 1457 300Pituitary930 732 198Pituitary930 749 181\\nNo Tumor2000 1595 405No Tumor1485 1189 296\\nTotal 7023 5712 1311 Total 3064 2451 613 Total 4549 3639 910\\nTable 1: Dataset Description\\nare classified into four classes - Gliomas(1621), menin-\\ngiomas(1645), pituitary(1757), and No Tumor(2000). This\\ndataset is a combination of Figshare, SARTAJ, and Br35H\\ndatasets. (Reference dataset word 1). The dataset is split into\\ntraining and test sets in the ratio 80:20. Out of 7023 images in\\nthe dataset, 5712 images are taken in the training set and the\\nrest are taken for the test set.\\nFigshare dataset (Dataset 2) given by Jun Cheng, School\\nof Biomedical Engineering, Southern Medical University,\\nGuangzhou, China. This brain tumor dataset contains 3064\\nT1-weighted contrast-enhanced images from 233 patients with\\nthree kinds of brain tumor: meningioma (708 slices), glioma\\n(1426 slices), and pituitary tumor (930 slices). This data is'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='T1-weighted contrast-enhanced images from 233 patients with\\nthree kinds of brain tumor: meningioma (708 slices), glioma\\n(1426 slices), and pituitary tumor (930 slices). This data is\\norganized in Matlab data format (.mat file). Each file stores\\na struct containing the following labels for an image - 1 for\\nmeningioma, 2 for glioma, and 3 for pituitary tumor. This is\\na legitimate dataset used in many researches across the globe.\\nOut of 3064 images in the dataset, 2451 images are chosen to\\ntrain the network, and the rest 613 images are taken to evaluate\\nthe performance of the model. Thus splitting the dataset in the\\nratio 80:20. (Reference dataset word 2)\\nThe third dataset (Dataset 3) is a custom-made cross-mixed\\ndataset of Dataset 1 and Dataset 2. In this, we have made a cus-\\ntom dataset containing four classes since this is a mixed dataset\\nso a good accurate result in this dataset proves the generaliza-\\ntion capability of our proposed architecture. This dataset con-'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='tom dataset containing four classes since this is a mixed dataset\\nso a good accurate result in this dataset proves the generaliza-\\ntion capability of our proposed architecture. This dataset con-\\ntains a total of 4549 images out of which 3639 images are taken\\nto train the model and the rest are used to evaluate the perfor-\\nmance of the model.\\nAll the details of the datasets and their divisions (i.e., training\\nand testing) are listed in Table 1 and Visualisation is provided in\\nFigure??,??and??. Dataset 1 and Dataset 3 contain images of\\ndifferent sizes so they are resized to images of similar size. All\\nthe images are resized to (224x224) pixels size and then basic\\npreprocessing steps like contrast enhancement and normaliza-\\ntion are performed. On the other hand, the Figshare dataset\\n(Dataset 2) contains images in MAT files in raw form. So pre-\\nprocessing steps are performed to convert the images from .mat\\nfiles to .jpg files to use for training and test set to train and'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='(Dataset 2) contains images in MAT files in raw form. So pre-\\nprocessing steps are performed to convert the images from .mat\\nfiles to .jpg files to use for training and test set to train and\\nevaluate our network respectively. After conversion to image\\nfiles, the Dataset is split into training and test sets in a ratio\\nof 80:10. Other preprocessing steps that are performed are z-\\nscore normalization and rescaling pixel values to the specified\\nrange(0,1).\\nWe have used the BratS2020 dataset for Segmentation. The\\nBRATS2020 dataset is a comprehensive resource for brain tu-\\nmor segmentation, specifically focusing on gliomas. It includes\\nmulti-institutional, pre-operative MRI scans from 19 different\\ninstitutions, encompassing T1, T1 post-contrast, T2, and T2-\\nFLAIR modalities2. The dataset provides ground truth labels\\nfor the enhancing tumor, peritumoral edema, and necrotic/non-\\nenhancing tumor core, manually segmented by expert neuro-'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='FLAIR modalities2. The dataset provides ground truth labels\\nfor the enhancing tumor, peritumoral edema, and necrotic/non-\\nenhancing tumor core, manually segmented by expert neuro-\\nradiologists. This rich, multimodal dataset is designed to ad-\\nvance the development and evaluation of segmentation algo-\\nrithms, with the ultimate goal of improving brain tumor diag-\\nnosis and treatment planning.\\n4.2. Experimental Setup\\nMany images are of different shapes so we have to resize\\nthose images in patches of 224x224 and then they are prepro-\\ncessed. All the datasets are split into training and test sets\\nin the ratio of 80:20 before passing the training set to train\\nthe neural network for classification. We have used a learn-\\ning rate of 0.0001, and the \"Adam\" Optimizer with a cross-\\nentropy Loss function for classification and Binary Cross En-\\ntropy with Logits Loss for Segmentation purposes, to train our\\nmodel. The Loss functions are represented in the equation 21'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='entropy Loss function for classification and Binary Cross En-\\ntropy with Logits Loss for Segmentation purposes, to train our\\nmodel. The Loss functions are represented in the equation 21\\nand 22. The accuracy and loss graph plot is shown in Figure 10\\nand 11, and the confusion matrix of the test dataset of different\\ndatasets are shown in Figure 13, 14 and 15. Our architecture\\nmainly includes an NCA block and 16 SAE blocks divided into\\nfour Modules - Initial TriSAE, QuadSAE, HexaSAE, and Final\\nTriSAE Fusion Module as shown in the figure 3. The proposed\\nmodel is implemented in pytorch. The hardware specifications\\nand Software environment used by us to develop this proposed\\nmodel and to train it are represented in Table 2. The process\\nflow diagram of the experimental process is shown in figure 2.\\nL=âˆ’\\nNX\\ni=1\\nCX\\nc=1\\nyi,c log(pi,c) (21)\\nWhere,Nis the number of samples,Cis the number of\\nclasses,y i,c is a binary indicator (0 or 1) if class labelcis the'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='L=âˆ’\\nNX\\ni=1\\nCX\\nc=1\\nyi,c log(pi,c) (21)\\nWhere,Nis the number of samples,Cis the number of\\nclasses,y i,c is a binary indicator (0 or 1) if class labelcis the\\ncorrect classification for samplei,p i,c is the predicted probabil-\\nity that sampleiis of classc.\\nBCE=âˆ’ 1\\nN\\nNX\\ni=1\\n\\x02yi log(pi)+(1âˆ’y i) log(1âˆ’p i)\\x03 (22)\\n11Where,Nis the number of samples.y i is the true label for\\nthei-th sample (either 0 or 1).p i is the predicted probability for\\nthei-th sample. log is the natural logarithm function. 1âˆ’y i is\\nthe complement of the true label for thei-th sample. 1âˆ’p i is\\nthe complement of the predicted probability for thei-th sample.P denotes summation over allNsamples.âˆ’ 1\\nN normalizes the\\nloss by dividing the total loss by the number of samples.\\n0 20 40 60 80 100\\nEpochs\\n0.75\\n0.80\\n0.85\\n0.90\\n0.95\\n1.00\\nValues\\nTraining/Test Accuracy\\nTraining Accuracy\\nTest Accuracy\\n(a)\\n0 20 40 60 80 100\\nEpochs\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\nValues\\nTraining/Test Loss\\nTraining Loss\\nTest Loss\\n(b)'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='0.80\\n0.85\\n0.90\\n0.95\\n1.00\\nValues\\nTraining/Test Accuracy\\nTraining Accuracy\\nTest Accuracy\\n(a)\\n0 20 40 60 80 100\\nEpochs\\n0.0\\n0.2\\n0.4\\n0.6\\n0.8\\n1.0\\n1.2\\nValues\\nTraining/Test Loss\\nTraining Loss\\nTest Loss\\n(b)\\nFigure 10: Accuracy and Loss Metrics for Dataset 2 (Figshare). (a) Accuracy\\nand (b) Loss\\n0 20 40 60 80 100\\nEpochs\\n0.825\\n0.850\\n0.875\\n0.900\\n0.925\\n0.950\\n0.975\\n1.000\\nValues\\nTraining/Test Accuracy\\nTraining Accuracy\\nTest Accuracy\\n(a)\\n0 20 40 60 80 100\\nEpochs\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\nValues\\nTraining/Test Loss\\nTraining Loss\\nTest Loss\\n(b)\\nFigure 11: Accuracy and Loss Metrics for Dataset 3 (Custom Dataset). (a)\\nAccuracy and (b) Loss\\nFigure 12: ROC-AUC Curve of our proposed model on the dataset 1\\nHardware Specifications\\nCPURyzen 7 7700x\\nGPURtx 4060ti 8GB\\nRAM16 GB\\nSoftware Environment\\nOperating SystemWindows 11\\nProgramming LanguagesPython\\nLibrariesPytorch, Scikit Learn\\nTable 2: Hardware Specifications and Software Environment used in our Study\\n4.3. Evaluation Metrics for Classification'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='Programming LanguagesPython\\nLibrariesPytorch, Scikit Learn\\nTable 2: Hardware Specifications and Software Environment used in our Study\\n4.3. Evaluation Metrics for Classification\\nIt is very much essential to evaluate the performances of a\\nproposed architecture for image classification purposes to in-\\nvestigate the proper functioning of the deep learning model\\nin real-life scenarios. In this study, we have used different\\nstandard parametric methods for evaluation of our model like\\nprecision, F1 Score, R2 Score, Recall, confusion Matrix, and\\nROC-AUC curve. For multiclass classification there are two\\nkinds of Metrics evaluation, one is Macro Averaged Metrics\\nand the other is Micro Averaged Metrics. In Macro Averaged\\nMetrics, it calculates the metric independently for each class\\nand then takes the average (treating all classes equally). On\\nthe other hand, Micro-averaging aggregates the contributions of\\nall classes to compute the average metric (treating all instances'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='and then takes the average (treating all classes equally). On\\nthe other hand, Micro-averaging aggregates the contributions of\\nall classes to compute the average metric (treating all instances\\nequally). In this Study we have evaluated the Micro Averaged\\nMetrics. These are some standard evaluation metrics which are\\ndiscussed one by one along with their mathematical representa-\\ntions:\\nâ€¢Precision: Precision measures the accuracy of the positive\\n12predictions.\\nMicro Precision=\\nPN\\ni=1 T Pi\\nPN\\ni=1(T Pi +FP i)\\n(23)\\nâ€¢Recall: Recall measures the ability of the model to find all\\nthe relevant cases.\\nMicro Recall=\\nPN\\ni=1 T Pi\\nPN\\ni=1(T Pi +FN i)\\n(24)\\nâ€¢F1 Score: The F1 Score is the harmonic mean of precision\\nand recall.\\nMicro F1 Score=2Â· Micro PrecisionÂ·Micro Recall\\nMicro Precision+Micro Recall(25)\\nâ€¢R2 Score (Coefficient of Determination): The R2 Score\\nindicates how well the modelâ€™s predictions approximate\\nthe real data points.\\nR2 =1âˆ’\\nPN\\ni=1(yi âˆ’Ë†yi)2\\nPN\\ni=1(yi âˆ’Â¯y)2 (26)'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='â€¢R2 Score (Coefficient of Determination): The R2 Score\\nindicates how well the modelâ€™s predictions approximate\\nthe real data points.\\nR2 =1âˆ’\\nPN\\ni=1(yi âˆ’Ë†yi)2\\nPN\\ni=1(yi âˆ’Â¯y)2 (26)\\nâ€¢ROC-AUC Curve (Receiver Operating Characteristic\\n- Area Under Curve): The ROC curve is a graphical\\nrepresentation of the true positive rate (TPR) against the\\nfalse positive rate (FPR) at various threshold settings. The\\nAUC represents the degree or measure of separability. The\\nROC-AUC curve of our study is shown in figure 12 Here\\nwe have used One vs One (OvO) Approach (refer to equa-\\ntion 27) and Micro Averaged Approach (refer to equa-\\ntion 28) to calculate the ROC-AUC. The expressions are\\nshowed as follows:\\nAUCOvO = 2\\nN(Nâˆ’1)\\nNâˆ’1X\\ni=1\\nNX\\nj=i+1\\nAUCi j (27)\\nAUCmicro =\\nPN\\ni=1\\nPN\\nj=1 AUCi j\\nN2 (28)\\nIn this section, the notation used for evaluation metrics in-\\ncludes the following:Nis the total number of classes.T P i\\nrepresents the True Positives for classi, which are the instances'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='N2 (28)\\nIn this section, the notation used for evaluation metrics in-\\ncludes the following:Nis the total number of classes.T P i\\nrepresents the True Positives for classi, which are the instances\\ncorrectly predicted as classi.FP i denotes the False Positives\\nfor classi, which are the instances incorrectly predicted as class\\ni.FN i stands for the False Negatives for classi, which are the\\ninstances that belong to classibut were incorrectly predicted\\nas another class.T N i indicates the True Negatives for class\\ni, which are the instances correctly predicted as not belonging\\nto classi.y i is the actual value for instancei, while Ë†y i is the\\npredicted value for instancei. Â¯yrepresents the mean of the ac-\\ntual values. Additionally,Nis the total number of classes, and\\nAUCi j is the AUC score for the binary classification problem\\nbetween classiand classj, all of which are used in ROC-AUC\\ncalculations.\\n4.4. Evaluation Metrics for Segmentation'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='AUCi j is the AUC score for the binary classification problem\\nbetween classiand classj, all of which are used in ROC-AUC\\ncalculations.\\n4.4. Evaluation Metrics for Segmentation\\nWhen evaluating segmentation models, several key metrics\\nare essential to gauge their performance accurately. Intersec-\\ntion Over Union (IoU) measures the overlap between the pre-\\ndicted segmentation and the ground truth, providing a ratio of\\ntheir intersection over their union. The Matthews Correlation\\nCoefficient (MCC) offers a balanced measure of binary classifi-\\ncation, considering true and false positives and negatives, mak-\\ning it particularly useful even when class sizes differ. The Dice\\nSimilarity Coefficient (DSC), assesses the similarity between\\nthe predicted and actual segments by comparing their overlap\\nrelative to their sizes. Specificity evaluates how well the model\\nidentifies true negatives, which is crucial for understanding the\\nmodelâ€™s ability to recognize non-tumorous regions. Finally, the'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='relative to their sizes. Specificity evaluates how well the model\\nidentifies true negatives, which is crucial for understanding the\\nmodelâ€™s ability to recognize non-tumorous regions. Finally, the\\nBoundary F1 Score (BF1) focuses on the accuracy of the pre-\\ndicted boundaries of the segmentation, ensuring that the model\\ncan precisely delineate the edges of the segmented objects. To-\\ngether, these metrics provide a comprehensive understanding of\\nthe segmentation modelâ€™s accuracy, robustness, and reliability.\\nAll of these are described below on eby one with their mathe-\\nmatical equations.\\nâ€¢Intersection Over Union (IoU)\\nIoUc = T Pc\\nT Pc +FP c +FN c\\n(29)\\nIoU measures the overlap between the predicted segmen-\\ntation and the ground truth. It is calculated as the area of\\noverlap divided by the area of union between the predicted\\nand ground truth masks.\\nâ€¢Matthews Correlation Coefficient (MCC)\\nMCC is a balanced measure that takes into account true'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='overlap divided by the area of union between the predicted\\nand ground truth masks.\\nâ€¢Matthews Correlation Coefficient (MCC)\\nMCC is a balanced measure that takes into account true\\nand false positives and negatives and is generally regarded\\nas a balanced metric for binary classification, even if the\\nclasses are of very different sizes.\\nâ€¢Dice Similarity Coefficient (DSC)\\nDS Cc = 2Â·T P c\\n2Â·T P c +FP c +FN c\\n(30)\\nThe DSC, also known as the F1 Score, measures the sim-\\nilarity between two sets. It is calculated as twice the area\\nof overlap divided by the total number of pixels in both the\\npredicted and ground truth masks.\\nâ€¢Specificity\\nS peci f icityc = T Nc\\nT Nc +FP c\\n(31)\\nSpecificity measures the proportion of true negatives that\\nare correctly identified. It is calculated as the number of\\ntrue negatives divided by the sum of true negatives and\\nfalse positives.\\n13â€¢Boundary F1 Score (BF1)\\nBF1c = 2Â·|P c âˆ©G c|\\n|Pc|+|G c| (32)\\nThe Boundary F1 Score evaluates the accuracy of bound-'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='true negatives divided by the sum of true negatives and\\nfalse positives.\\n13â€¢Boundary F1 Score (BF1)\\nBF1c = 2Â·|P c âˆ©G c|\\n|Pc|+|G c| (32)\\nThe Boundary F1 Score evaluates the accuracy of bound-\\nary prediction. It is similar to the DSC but focuses on the\\nboundaries of the segmented objects.\\nAccuracy Precision Recall F1 Score ROC AUC\\nClass 099.79 99.89 99.89 99.89 99.98\\nClass 159.01 69.10 74.19 71.76 99.41\\nClass 282.27 84.71 83.24 83.97 99.94\\nClass 384.89 91.32 91.32 89.71 99.98\\nTable 3: Class wise Evaluation metrics of Segmentation\\nIoU Specificity MCC Boundary F1 Score DSC\\nClass 099.58 93.55 93.41 99.79\\nClass 145.85 99.86 71.47 89.53 62.87\\nClass 269.72 99.90 83.86 92.45 82.16\\nClass 371.87 99.95 89.67 97.90 83.64\\nTable 4: Class wise Evaluation\\n4.5. Performance of our Proposed Classification Model on in-\\ndividual datasets\\nWe have trained our proposed network on three datasets and\\ntheir results are compared in the table??and the confusion ma-'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='dividual datasets\\nWe have trained our proposed network on three datasets and\\ntheir results are compared in the table??and the confusion ma-\\ntrix of the test data of those three datasets are shown in the\\nfigure 13, 14, 15.\\nName of the Dataset Accuracy Precision F1 Score Recall R2 Score ROC-AUC\\nDataset 1 (Brain MRI )99.38 99.39 99.39 99.38 99.08 99.99\\nDataset 2 (Figshare)98.69 98.69 98.68 98.69 98.25 99.90\\nDataset 3 (Custom\\nCross-Mixed)\\n98.57 98.59 98.55 98.57 97.69 99.90\\nTable 6: Comparison of our model on Different Datasets\\nGlioma Meningioma No Tumor Pituitary\\nPredicted label\\nGlioma\\nMeningioma\\nNo Tumor\\nPituitary\\nTrue label\\n296 4 0 0\\n2 303 0 1\\n0 0 405 0\\n0 1 0 299\\nConfusion Matrix\\n0\\n50\\n100\\n150\\n200\\n250\\n300\\n350\\n400\\nFigure 13: Confusion Matrix of Dataset 1\\nGlioma Meningioma Pituitary\\nPredicted label\\nGlioma\\nMeningioma\\nPituitary\\nTrue label\\n272 1 0\\n3 137 2\\n0 2 196\\nConfusion Matrix\\n0\\n50\\n100\\n150\\n200\\n250\\nFigure 14: Confusion Matrix of Dataset 2\\nGlioma Meningioma No Tumor Pituitary'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='Predicted label\\nGlioma\\nMeningioma\\nPituitary\\nTrue label\\n272 1 0\\n3 137 2\\n0 2 196\\nConfusion Matrix\\n0\\n50\\n100\\n150\\n200\\n250\\nFigure 14: Confusion Matrix of Dataset 2\\nGlioma Meningioma No Tumor Pituitary\\nPredicted label\\nGlioma\\nMeningioma\\nNo Tumor\\nPituitary\\nTrue label\\n295 1 0 1\\n9 125 0 2\\n0 0 296 0\\n0 0 0 181\\nConfusion Matrix\\n0\\n50\\n100\\n150\\n200\\n250\\nFigure 15: Confusion Matrix of Dataset 3\\n14Contribution Year of Publish Types of Classifier Dataset Test Accuracy\\nMuhammad Aamir et al. [1] 2024 Hyperparametric CNN Brain MRI (Kaggle) 97%\\n2D CNN 93.44%\\nSaeedi et al. [21] 2023\\nAuto-Encoder\\nBrain Tumor Classification\\n(MRI): Four Classes 90.92%\\nFigshare and Multiclass\\nKaggle Dataset (Augmented)\\n98.12%\\nTakowa Rahman et al. [18] 2023 PDCNN Figshare and Multiclass\\nKaggle Dataset (Original)\\n95.60%\\nBadza et al. [4] 2020 22 layers CNN Figshare Dataset 96.56%\\nAbiwinanda et al. [2] 2018 CNN Figshare Dataset 84.19%\\nDCNet 93.04%\\nSai Samarth R. Phaye et al. [17] 2018\\nDCNet++\\nFigshare Dataset\\n95.03%'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='Badza et al. [4] 2020 22 layers CNN Figshare Dataset 96.56%\\nAbiwinanda et al. [2] 2018 CNN Figshare Dataset 84.19%\\nDCNet 93.04%\\nSai Samarth R. Phaye et al. [17] 2018\\nDCNet++\\nFigshare Dataset\\n95.03%\\nPashaei et al. [15] 2018 CNN Figshare Dataset 93.68%\\nPaul et al. [16] 2017 CNN Figshare Dataset 91.43%\\nBrain MRI (Kaggle) 99.38%\\nFigshare Dataset 98.69%\\nOur Proposed Architecture 2024\\nSAETCN - Self-Attention\\nEnhancement Tumor\\nClassification Network\\nCross-Mixed Custom\\nDataset\\n98.57%\\nTable 5: Comparison of Our Work (Classification Architecture - SAETCN) with State of the Art Works\\n4.6. Comparison of our Classification Model with Previous\\nWorks\\nWe have compared our model with various state-of-the-art\\nmodels like EfficientNetB4, ResNet18, InceptionNetV3, Swin\\nTransformer, and ViT (Vission Transformer) but ultimately we\\nhave beat all those models by achieving an accuracy of 99.38%,\\n98.69% and 98.57% in dataset 1 dataset 2 and dataset 3 respec-'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='Transformer, and ViT (Vission Transformer) but ultimately we\\nhave beat all those models by achieving an accuracy of 99.38%,\\n98.69% and 98.57% in dataset 1 dataset 2 and dataset 3 respec-\\ntively (refer to table??). All the evaluation metrics of all the\\nmodels compared with our model on different datasets are also\\nshown in detail in table??, table??, table??. In table 5 we have\\ncompared our proposed model with other previous works that\\nwas performed in the last recent years.\\nModel\\nDataset 1\\nPrecision Recall F1 Score R2 Score ROC-AUC\\nEfficientNetB4 95.31 95.31 95.21 92.82 99.46\\nResNet18 98.22 98.22 98.22 96.09 99.91\\nInceptionNetV3 98.95 98.90 98.94 98.25 99.97\\nSwin Transformer 94.41 94.43 94.41 84.86 99.27\\nViT 87.57 87.56 87.51 67.95 96.85\\nProposed 99.39 99.38 99.39 99.08 99.99\\nTable 7: Comparison of our Proposed model with Other state of the art existing\\nModels in terms of Evaluation Metrics on Brain MRI Dataset (Dataset 1)\\nModel\\nDataset 2\\nPrecision Recall F1 Score R2 Score ROC-AUC'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='Models in terms of Evaluation Metrics on Brain MRI Dataset (Dataset 1)\\nModel\\nDataset 2\\nPrecision Recall F1 Score R2 Score ROC-AUC\\nEfficientNetB4 91.73 91.68 91.70 84.47 97.66\\nResNet18 96.22 96.24 96.22 94.29 99.49\\nInceptionNetV3 97.87 97.87 97.85 97.19 99.85\\nSwin Transformer 87.85 87.92 87.87 72.75 96.03\\nViT 79.00 79.77 79.26 59.84 90.25\\nProposed 98.69 98.69 98.68 98.25 99.90\\nTable 8: Comparison of our Proposed model with Other state of the art existing\\nModels in terms of Evaluation Metrics on Figshare Dataset (Dataset 2)\\nModel\\nDataset 3\\nPrecision Recall F1 Score R2 Score ROC-AUC\\nEfficientNetB4 96.16 96.15 96.14 90.53 99.51\\nResNet18 97.12 97.14 97.13 96.19 99.80\\nInceptionNetV3 98.05 98.02 98.03 95.42 99.89\\nSwin Transformer 92.82 92.96 92.86 89.69 98.81\\nViT 85.38 85.60 85.47 69.55 96.52\\nProposed 98.59 98.57 98.55 97.69 99.90\\nTable 9: Comparison of our Proposed model with Other State of the art existing\\nModels in Terms of Evaluation Metrics on Cross-Mixed Dataset (Dataset 3)\\n15NCA'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='Proposed 98.59 98.57 98.55 97.69 99.90\\nTable 9: Comparison of our Proposed model with Other State of the art existing\\nModels in Terms of Evaluation Metrics on Cross-Mixed Dataset (Dataset 3)\\n15NCA\\nInitial\\nTriSAE\\nQuadSAE HexaSAE\\nFinal SAE\\nFusion\\nAccuracy F1 R2 Recall Precision AUC\\nâœ“59.42 59.94 -0.31 59.42 63.28 85.34\\nâœ“ âœ“83.68 83.82 39.57 83.67 87.27 98.80\\nâœ“ âœ“ âœ“98.09 98.09 98.02 98.09 98.13 99.94\\nâœ“ âœ“ âœ“ âœ“96.26 96.28 95.28 96.26 96.40 99.88\\nâœ“ âœ“ âœ“ âœ“ âœ“99.38 99.39 99.08 99.38 99.39 99.99\\nTable 10: Module Wise Ablation Study of the Proposed Model components\\nModel\\nDataset 1 Dataset 2 Dataset 3\\nTrain Test Train Test Train Test\\nEfficientNetB4 99.57 95.31 98.98 91.68 99.98 96.15\\nResNet18 99.90 98.22 99.85 96.24 99.56 97.14\\nInceptionNetV3 100 98.90 99.20 97.87 100 98.02\\nSwin Transformer 97.25 94.43 98.29 87.92 99.84 92.96\\nViT 96.56 87.56 96.24 79.77 97.89 85.60\\nProposed 1.00 99.38 99.89 98.69 99.46 98.57\\nTable 11: Comparison of Our Proposed Model with State of the art models'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='ViT 96.56 87.56 96.24 79.77 97.89 85.60\\nProposed 1.00 99.38 99.89 98.69 99.46 98.57\\nTable 11: Comparison of Our Proposed Model with State of the art models\\nbetween Dataset 1 (Brain MRI), Dataset 2 (Figshare) and Dataset 3 (Custom\\nCross Mixed Dataset) with respect to Train and Test accuracy\\n4.7. Comparison of our segmentation model with previous\\nworks\\nTable 12 presents a comparative analysis of the proposed\\nSAS-Net architecture against previous works by Agarwal et\\nal. and Wentau Wu et al. The SAS-Net outperforms the other\\nmodels across most metrics, achieving a Dice Similarity Coef-\\nficient (DSC) of 99.79%, specificity of 93.55%, and sensitiv-\\nity of 99.89%. Notably, while Wentau Wu et al.â€™s model ex-\\nhibits high specificity at 99.82%, it falls behind in DSC and\\nsensitivity compared to SAS-Net. Meanwhile, Agarwal et al.â€™s\\nmodel shows a balanced performance but with lower sensitivity\\nat 79.89%. Overall, the SAS-Net demonstrates superior perfor-'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='sensitivity compared to SAS-Net. Meanwhile, Agarwal et al.â€™s\\nmodel shows a balanced performance but with lower sensitivity\\nat 79.89%. Overall, the SAS-Net demonstrates superior perfor-\\nmance, particularly in terms of DSC and sensitivity, indicating\\nits effectiveness in accurately segmenting brain tumors.\\nDSC Specificity Sensitivity Accuracy\\nAgarwal et al.[3] 94.5 92.6 79.89 91.3\\nWentau Wu et al.[24] 89.58 99.82 91.10 -\\nSAS-Net (Proposed)99.79 93.55 99.89 99.23\\nTable 12: Comparison of our proposed segmentation architecture with previous\\nworks\\n5. Abalation Study\\nModule wise ablation study is performed and evaluated. The\\nresult is shown in the table 10. In figure 16, the improvement\\nof the result is represented that describes the view of choos-\\ning 16 SAEB blocks as this gives the better classification per-\\nformance amongst every combinations. A detailed Gradient-\\nweighted Class Activation Mapping of all model components\\nis shown in figure 6.'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='formance amongst every combinations. A detailed Gradient-\\nweighted Class Activation Mapping of all model components\\nis shown in figure 6.\\nFigure 16: Representation of the evaluation metrics on different model compo-\\nnents\\n6. Conclusions\\nIn this paper, we proposed SAETCN - Self-Attention En-\\nhancement Tumor Classification Network for detecting and\\nclassification of three kinds of Brain tumors. This study ex-\\nplored methods for improving the accuracy of detection and\\nclassification of different kinds of Brain Tumor - Gliomas,\\nMeningiomas, and Pituitary. Our model can also predict\\nwhether the brain has any tumor or not. Since Brain Tumor\\nis a huge problem and detecting and classifying those require\\na lot of time for radiologists and is hectic and thus hinders\\nthe proper treatment plan and diagnosis. Therefore this type\\nof advanced CAD system is very much required in the Medi-\\ncal field. Manual Classification also leads to human error but'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='the proper treatment plan and diagnosis. Therefore this type\\nof advanced CAD system is very much required in the Medi-\\ncal field. Manual Classification also leads to human error but\\nwith an accuracy of 99.31% the Artificial Intelligence Model\\nproduces less error than manual classification. Also, it can be\\nintegrated with Mobile or web applications which increases the\\nease of use and thus people in remote areas who are not get-\\nting proper healthcare also use our web-based model to detect\\nwhether they are suffering from a brain tumor or not. We have\\ncompared our model with other state-of-the-art models and with\\n16other researchersâ€™ previous works, we found that our model has\\nbeaten most of the model by achieving an accuracy of 99.31%,\\n99.20%, and in dataset 1, dataset 2, and dataset 3 respectively.\\nAlthough our proposed model has achieved great accuracy, still\\nthere is a probability that the model may get overfitted on the\\nthree datasets we have used in this study, thus it should be'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='Although our proposed model has achieved great accuracy, still\\nthere is a probability that the model may get overfitted on the\\nthree datasets we have used in this study, thus it should be\\ntrained in larger datasets before using it for real-world classi-\\nfication in Hospitals. More research and development can be\\ndone in this work in the future with more accurate and general-\\nized results and thus in this way, artificial intelligence will help\\nmankind to be better in the upcoming days.\\nAlthough the accuracy of our proposed model is quite high\\nstill there is a lot of future scope for this study, they include\\nmaking the model more generalized for working more accu-\\nrately in real-time classification. This can be achieved by train-\\ning the network in Big data with a machine with high compu-\\ntational resources to train it effectively. Also, in the future, we\\nhave planned to integrate this model in a Mobile application,\\nwhere doctors and patients can be the users and detect the brain'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='tational resources to train it effectively. Also, in the future, we\\nhave planned to integrate this model in a Mobile application,\\nwhere doctors and patients can be the users and detect the brain\\ntumor and classify it with their proper location thus helping in\\nthe treatment. If they want they can also use their own data\\nto train the model and fine-tune it based on their specific MRI\\nmachine to get better results of classification.\\n7. Data Availablity\\nFor maximum transparency and to foster open science, all\\ncomputational codes and saved model architectures related\\nto this study have been made publicly available in the full\\nGitHub repository athttps://github.com/arghadip2002/\\nSAETCN-and-SASNET-Architectures.\\nThe datasets utilized, which include two publicly avail-\\nable datasets and a custom-made dataset, are linked within\\nthe repository (https://github.com/arghadip2002/\\nSAETCN-and-SASNET-Architectures/blob/main/\\ndataLinks), with the Custom dataset accessible upon'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='the repository (https://github.com/arghadip2002/\\nSAETCN-and-SASNET-Architectures/blob/main/\\ndataLinks), with the Custom dataset accessible upon\\nreasonable request to the author.\\nFurthermore, the practical utility of the novel SAETCN and\\nSASNET architectures is demonstrated by their integration into\\nthe backend of the NeuroGuard Web Application, which per-\\nforms classification and detection of various brain tumors from\\nMRI images, thereby showcasing the direct application of Ar-\\ntificial Intelligence in the medical domain for building future-\\nready applications.\\nReferences\\n[1] Muhammad Aamir, Abdallah Namoun, Sehrish Munir,\\nNasser Aljohani, Meshari Huwaytim Alanazi, Yaser Alsa-\\nhafi, and Faris Alotibi. Brain tumor detection and classi-\\nfication using an optimized convolutional neural network.\\nDiagnostics, 14(16):1714, 2024.\\n[2] Nyoman Abiwinanda, Muhammad Hanif, S Tafwida\\nHesaputra, Astri Handayani, and Tati Rajab Mengko.\\nBrain tumor classification using convolutional neural net-'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='Diagnostics, 14(16):1714, 2024.\\n[2] Nyoman Abiwinanda, Muhammad Hanif, S Tafwida\\nHesaputra, Astri Handayani, and Tati Rajab Mengko.\\nBrain tumor classification using convolutional neural net-\\nwork. InWorld Congress on Medical Physics and\\nBiomedical Engineering 2018: June 3-8, 2018, Prague,\\nCzech Republic (Vol. 1), pages 183â€“189. Springer, 2019.\\n[3] Mukul Aggarwal, Amod Kumar Tiwari, M Partha Sarathi,\\nand Anchit Bijalwan. An early detection and segmentation\\nof brain tumor using deep neural network.BMC Medical\\nInformatics and Decision Making, 23(1):78, 2023.\\n[4] Milica M BadÅ¾a and Marko Ë‡C Barjaktarovi Â´c. Classifi-\\ncation of brain tumors from mri images using a convo-\\nlutional neural network.Applied Sciences, 10(6):1999,\\n2020.\\n[5] Sara Bouhafra and Hassan El Bahi. Deep learning ap-\\nproaches for brain tumor detection and classification using\\nmri images (2020 to 2024): A systematic review.Journal\\nof Imaging Informatics in Medicine, pages 1â€“31, 2024.'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='proaches for brain tumor detection and classification using\\nmri images (2020 to 2024): A systematic review.Journal\\nof Imaging Informatics in Medicine, pages 1â€“31, 2024.\\n[6] Cleveland Clinic. Brain cancer (brain tumor).\\nhttps://my.clevelandclinic.org/health/\\ndiseases/6149-brain-cancer-brain-tumor,\\n2022.\\n[7] Michael Franke and Judith Degen. The softmax function:\\nProperties, motivation, and interpretation. 2023.\\n[8] Anna Giorgi. What is a brain tumor?https://www.\\nverywellhealth.com/brain-tumor-7253734, April\\n06, 2023. You can access the data online on the provided\\nlink.\\n[9] Richard HR Hahnloser, Rahul Sarpeshkar, Misha A Ma-\\nhowald, Rodney J Douglas, and H Sebastian Seung.\\nCorrection: Digital selection and analogue amplifica-\\ntion coexist in a cortex-inspired silicon circuit.Nature,\\n408(6815):1012â€“1012, 2000.\\n[10] National Cancer Institute. Cancer stat facts: Brain and\\nother nervous system cancer.https://seer.cancer.\\ngov/statfacts/html/brain.html, 2014-2020.'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='408(6815):1012â€“1012, 2000.\\n[10] National Cancer Institute. Cancer stat facts: Brain and\\nother nervous system cancer.https://seer.cancer.\\ngov/statfacts/html/brain.html, 2014-2020.\\n[11] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hin-\\nton. Imagenet classification with deep convolutional neu-\\nral networks.Advances in neural information processing\\nsystems, 25, 2012.\\n[12] Yann LeCun, LÃ©on Bottou, Yoshua Bengio, and Patrick\\nHaffner. Gradient-based learning applied to document\\nrecognition.Proceedings of the IEEE, 86(11):2278â€“2324,\\n1998.\\n[13] Dongdong Lin, Ming Wang, Yan Chen, Jie Gong, Liang\\nChen, Xiaoyong Shi, Fujun Lan, Zhongliang Chen, Tao\\nXiong, Hu Sun, et al. Trends in intracranial glioma inci-\\ndence and mortality in the united states, 1975-2018.Fron-\\ntiers in oncology, 11:748061, 2021.\\n[14] Zhihua Liu, Lei Tong, Long Chen, Zheheng Jiang, Feix-\\niang Zhou, Qianni Zhang, Xiangrong Zhang, Yaochu Jin,\\nand Huiyu Zhou. Deep learning based brain tumor seg-'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='[14] Zhihua Liu, Lei Tong, Long Chen, Zheheng Jiang, Feix-\\niang Zhou, Qianni Zhang, Xiangrong Zhang, Yaochu Jin,\\nand Huiyu Zhou. Deep learning based brain tumor seg-\\nmentation: a survey.Complex&intelligent systems,\\n9(1):1001â€“1026, 2023.\\n17[15] Ali Pashaei, Hedieh Sajedi, and Niloofar Jazayeri. Brain\\ntumor classification via convolutional neural network and\\nextreme learning machines. In2018 8th International\\nconference on computer and knowledge engineering (IC-\\nCKE), pages 314â€“319. IEEE, 2018.\\n[16] Justin S Paul, Andrew J Plassard, Bennett A Landman,\\nand Daniel Fabbri. Deep learning for brain tumor clas-\\nsification. InMedical Imaging 2017: Biomedical Appli-\\ncations in Molecular, Structural, and Functional Imaging,\\nvolume 10137, pages 253â€“268. SPIE, 2017.\\n[17] Sai Samarth R Phaye, Apoorva Sikka, Abhinav Dhall,\\nand Deepti Bathula. Dense and diverse capsule net-\\nworks: Making the capsules learn better.arXiv preprint\\narXiv:1805.04001, 2018.'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='[17] Sai Samarth R Phaye, Apoorva Sikka, Abhinav Dhall,\\nand Deepti Bathula. Dense and diverse capsule net-\\nworks: Making the capsules learn better.arXiv preprint\\narXiv:1805.04001, 2018.\\n[18] Takowa Rahman and Md Saiful Islam. Mri brain tu-\\nmor detection and classification using parallel deep con-\\nvolutional neural networks.Measurement: Sensors,\\n26:100694, 2023.\\n[19] Neuro-Oncology Branch Scientific Communications\\nRaleigh McElvery. Statistical report highlights key\\ntrends in adolescents and young adults with brain tumors.\\nhttps://shorturl.at/MhBCT, 2024. National Cancer\\nInstitute.\\n[20] Champakamala Sundar Rao and K Karunakara. A com-\\nprehensive review on brain tumor segmentation and clas-\\nsification of mri images.Multimedia Tools and Applica-\\ntions, 80(12):17611â€“17643, 2021.\\n[21] Soheila Saeedi, Sorayya Rezayi, Hamidreza Keshavarz,\\nand Sharareh R. Niakan Kalhori. Mri-based brain tumor\\ndetection using convolutional deep learning methods and'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='[21] Soheila Saeedi, Sorayya Rezayi, Hamidreza Keshavarz,\\nand Sharareh R. Niakan Kalhori. Mri-based brain tumor\\ndetection using convolutional deep learning methods and\\nchosen machine learning techniques.BMC Medical Infor-\\nmatics and Decision Making, 23(1):16, 2023.\\n[22] The Johns Hopkins University, The Johns Hopkins Hospi-\\ntal, and Johns Hopkins Health System. Brain tumors and\\nbrain cancer.https://www.hopkinsmedicine.org/\\nhealth/conditions-and-diseases/brain-tumor,\\n2024.\\n[23] Amit Verma, Shiv Naresh Shivhare, Shailendra P Singh,\\nNaween Kumar, and Anand Nayyar. Comprehensive re-\\nview on mri-based brain tumor segmentation: A compara-\\ntive study from 2017 onwards.Archives of Computational\\nMethods in Engineering, pages 1â€“47, 2024.\\n[24] Wentao Wu, Daning Li, Jiaoyang Du, Xiangyu Gao, Wen\\nGu, Fanfan Zhao, Xiaojie Feng, and Hong Yan. An in-\\ntelligent diagnosis method of brain mri tumor segmen-\\ntation using deep convolutional neural network and svm'),\n",
       " Document(metadata={'arxiv_id': '2512.06531v1', 'title': 'Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images', 'section': 'body', 'authors': 'Sayan Das, Arghadip Biswas'}, page_content='Gu, Fanfan Zhao, Xiaojie Feng, and Hong Yan. An in-\\ntelligent diagnosis method of brain mri tumor segmen-\\ntation using deep convolutional neural network and svm\\nalgorithm.Computational and mathematical methods in\\nmedicine, 2020(1):6789306, 2020.\\n[25] Matthew D Zeiler and Rob Fergus. Visualizing and un-\\nderstanding convolutional networks. InComputer Visionâ€“\\nECCV 2014: 13th European Conference, Zurich, Switzer-\\nland, September 6-12, 2014, Proceedings, Part I 13, pages\\n818â€“833. Springer, 2014.\\n18'),\n",
       " Document(metadata={'arxiv_id': '2208.03470v1', 'title': 'Analyzing Deep Learning Based Brain Tumor Segmentation with Missing MRI Modalities', 'section': 'title_abstract', 'authors': 'Benteng Ma, Yushi Wang, Shen Wang'}, page_content='Title: Analyzing Deep Learning Based Brain Tumor Segmentation with Missing MRI Modalities\\n\\nAbstract: This technical report presents a comparative analysis of existing deep learning (DL) based approaches for brain tumor segmentation with missing MRI modalities. Approaches evaluated include the Adversarial Co-training Network (ACN) and a combination of mmGAN and DeepMedic. A more stable and easy-to-use version of mmGAN is also open-sourced at a GitHub repository. Using the BraTS2018 dataset, this work demonstrates that the state-of-the-art ACN performs better especially when T1c is missing. While a simple combination of mmGAN and DeepMedic also shows strong potentials when only one MRI modality is missing. Additionally, this work initiated discussions with future research directions for brain tumor segmentation with missing MRI modalities.'),\n",
       " Document(metadata={'arxiv_id': '2208.03470v1', 'title': 'Analyzing Deep Learning Based Brain Tumor Segmentation with Missing MRI Modalities', 'section': 'body', 'authors': 'Benteng Ma, Yushi Wang, Shen Wang'}, page_content='Analyzing Deep Learning Based Brain Tumor\\nSegmentation with Missing MRI Modalities\\nBenteng Ma1, Yushi Wang1, and Shen Wang2\\n1 Beijing-Dublin International College, University College Dublin, Dublin, Ireland\\n{benteng.ma,yushi.wang}@ucdconnect.ie\\n2 School of Computer Science, University College Dublin, Dublin, Ireland\\nshen.wang@ucd.ie\\nAbstract. This technical report presents a comparative analysis of ex-\\nisting deep learning (DL) based approaches for brain tumor segmentation\\nwith missing MRI modalities. Approaches evaluated include the Adver-\\nsarial Co-training Network (ACN) [6] and a combination of mmGAN\\n[5] and DeepMedic [2]. A more stable and easy-to-use version of mm-\\nGAN is also open-sourced at a GitHub repository3. Using the BraTS2018\\ndataset, this work demonstrates that the state-of-the-art ACN performs\\nbetter especially when T1c is missing. While a simple combination of\\nmmGAN and DeepMedic also shows strong potentials when only one'),\n",
       " Document(metadata={'arxiv_id': '2208.03470v1', 'title': 'Analyzing Deep Learning Based Brain Tumor Segmentation with Missing MRI Modalities', 'section': 'body', 'authors': 'Benteng Ma, Yushi Wang, Shen Wang'}, page_content='better especially when T1c is missing. While a simple combination of\\nmmGAN and DeepMedic also shows strong potentials when only one\\nMRI modality is missing. Additionally, this work initiated discussions\\nwith future research directions for brain tumor segmentation with miss-\\ning MRI modalities.\\nKeywords: MRI Â· Brain Tumor SegmentationÂ· GAN.\\n1 Introduction\\nA high-quality magnetic resonance imaging (MRI) scan is of vital importance\\nto the downstream workï¬‚ows as such diagnosis and treatment plans. The seg-\\nmentation of MRI images has been a well-known challenge for a long time.\\nRecently, deep-learning-based approaches such as DeepMedic[2] have improved\\nsuch segmentation results to comparable human-level performance. However, in\\npractice, we cannot assume the MRI images are always high-quality. This is be-\\ncause these images often contain defects for one or more MRI sequences (e.g.,\\nT1, T2, Fair, etc.). Existing deep-learning-based approaches such as mmGAN[5]'),\n",
       " Document(metadata={'arxiv_id': '2208.03470v1', 'title': 'Analyzing Deep Learning Based Brain Tumor Segmentation with Missing MRI Modalities', 'section': 'body', 'authors': 'Benteng Ma, Yushi Wang, Shen Wang'}, page_content='cause these images often contain defects for one or more MRI sequences (e.g.,\\nT1, T2, Fair, etc.). Existing deep-learning-based approaches such as mmGAN[5]\\ncan well synthesize one or more missing MRI sequences to complement the re-\\nduced performance. Some recent attempts like[6] have reaï¬ƒrmed the potential\\nof using deep learning for brain tumor segmentation under various missing MRI\\nsequences. However, there are still some open questions such as is a straight-\\nforward combination of mmGAN and DeepMedic (mmDM) good enough? Is\\nACN practically usable? Does a better missing MRI sequence synthesis always\\nlead to a better brain tumor segmentation? This technical report provides our\\n3 https://github.com/MBTMBTMBT/mmGAN_Tensorflow\\narXiv:2208.03470v1  [cs.CV]  6 Aug 20222 B. Ma et al.\\nviews on these questions by a comprehensive evaluation of ACN and mmDM.\\nWe also provide our Tensorï¬‚ow implementation, open-sourced at3, of mmGAN\\nto facilitate this evaluation.\\n2 Our Implementation of mmGAN'),\n",
       " Document(metadata={'arxiv_id': '2208.03470v1', 'title': 'Analyzing Deep Learning Based Brain Tumor Segmentation with Missing MRI Modalities', 'section': 'body', 'authors': 'Benteng Ma, Yushi Wang, Shen Wang'}, page_content='We also provide our Tensorï¬‚ow implementation, open-sourced at3, of mmGAN\\nto facilitate this evaluation.\\n2 Our Implementation of mmGAN\\n2.1 Introduction of mmGAN\\nMulti-Modal Generative Adversarial Network (mmGAN) [5] can generate one or\\nmore missing MRI sequences using only one generator model, with at least one\\nsequence as input, through one forward propagation. mmGAN is based on U-\\nNet[4], which has been demonstrated to have strong capabilities in segmentation\\ntasks with limited amount of medical images data set. As shown in the leftmost\\npart of Figure 1, the image channels are fetched and trained batch by batch\\nduring each epoch. The curriculum learning shown in the central part of Figure\\n1 is achieved by taking diï¬€erent random numbers of bad channels during each\\nepoch, more channels will be randomly destroyed when the number of epoch\\nincreases. To achieve implicit conditioning shown in the rightmost part of Figure'),\n",
       " Document(metadata={'arxiv_id': '2208.03470v1', 'title': 'Analyzing Deep Learning Based Brain Tumor Segmentation with Missing MRI Modalities', 'section': 'body', 'authors': 'Benteng Ma, Yushi Wang, Shen Wang'}, page_content='epoch, more channels will be randomly destroyed when the number of epoch\\nincreases. To achieve implicit conditioning shown in the rightmost part of Figure\\n1, original images of the input channels (good channels, not the bad ones) will be\\nkept and replace the generated ones during training, so that the discriminator\\nwill only make the decision based on the generated lost (bad) channels.\\nFig. 1. The work ï¬‚ow of training mmGAN (left), curriculum learning (centre), and\\nimplicit conditioning (right) [5]Brain Tumor Segmentation with Missing MRI 3\\n2.2 Our improvements to the original mmGAN implementation\\nComparing with the original mmGAN implementation4, our version provides\\nmore options for data preprocessing and training, as well as makes the separation\\nof the dataset easier, and improves the execution eï¬ƒciency. Users may have more\\nï¬‚exibility using the network, and can easily use it with other downstream tasks.\\nA Tensorï¬‚ow version to better cooperate with DeepMedicThe original mmGAN'),\n",
       " Document(metadata={'arxiv_id': '2208.03470v1', 'title': 'Analyzing Deep Learning Based Brain Tumor Segmentation with Missing MRI Modalities', 'section': 'body', 'authors': 'Benteng Ma, Yushi Wang, Shen Wang'}, page_content='ï¬‚exibility using the network, and can easily use it with other downstream tasks.\\nA Tensorï¬‚ow version to better cooperate with DeepMedicThe original mmGAN\\nimplementation is based on PyTorch. We have our mmGAN implementation\\nbased on TensorFlow to give researchers another choice and also to better inte-\\ngrate with DeepMedic for the MRI brain tumor segmentation with missing MRI\\nsequences.\\nEasier conï¬guration and command-line usageThe current implementation uses\\na complex conï¬guration ï¬le for various functions. It is diï¬ƒcult for users to un-\\nderstand its contents, or modify it according to local environments. The original\\nauthorsâ€™ version of the code mainly focuses on experiments, a lot of the com-\\nponents are used for preprocessing diï¬€erent datasets or doing measurements,\\nwhich might cause confusion when read by other users of the code. The users\\nmay also have diï¬ƒculties in using diï¬€erent datasets rather than BRATS18, or'),\n",
       " Document(metadata={'arxiv_id': '2208.03470v1', 'title': 'Analyzing Deep Learning Based Brain Tumor Segmentation with Missing MRI Modalities', 'section': 'body', 'authors': 'Benteng Ma, Yushi Wang, Shen Wang'}, page_content='which might cause confusion when read by other users of the code. The users\\nmay also have diï¬ƒculties in using diï¬€erent datasets rather than BRATS18, or\\nutilizing further the outcome of the network. We allow users to modify the pre-\\nprocessing and training conï¬gurations easily, and make the model adjustable\\nto diï¬€erent sequential datasets. All the processes are controllable through the\\ncommand line, bringing convenience for batch processing. Users may ï¬rst call\\npreprocessing function to preprocess the data, and then train the model with\\neither TensorFlow or PyTorch frameworks, after that they may use the test\\nfunction to predict certain groups of images with all the missing sequence sce-\\nnarios. The synthesised results can be used for downstream tasks such as tumor\\nsegmentation.\\nEasier achievement of cross validation Using the original implementation, it\\ncan be diï¬ƒcult to manage data and achieve cross-validation, datasets have to'),\n",
       " Document(metadata={'arxiv_id': '2208.03470v1', 'title': 'Analyzing Deep Learning Based Brain Tumor Segmentation with Missing MRI Modalities', 'section': 'body', 'authors': 'Benteng Ma, Yushi Wang, Shen Wang'}, page_content='segmentation.\\nEasier achievement of cross validation Using the original implementation, it\\ncan be diï¬ƒcult to manage data and achieve cross-validation, datasets have to\\nbe managed manually instead of automatically. With our implementation, users\\ncan easily separate the dataset into several folds and select which folds are used\\nfor training or validation.\\nAllowing random data sequences for better performanceIn our code, we allow\\nthe slices of images of diï¬€erent patients to be shuï¬„ed randomly, thus the slices\\nin each batch come from diï¬€erent patients, without certain order. The purpose is\\nnot only to avoid the model bias on certain sequences of continued slices but also\\nto relieve the negative eï¬€ect of continued damaged sequences that appears in the\\ndataset. Additionally, the user may also choose to use diï¬€erent combinations of\\nmissing sequences within one batch, by changing the â€œfull_randomâ€ parameter.\\nFor example, when setting the parameter into â€œFalseâ€, a batch with the size of'),\n",
       " Document(metadata={'arxiv_id': '2208.03470v1', 'title': 'Analyzing Deep Learning Based Brain Tumor Segmentation with Missing MRI Modalities', 'section': 'body', 'authors': 'Benteng Ma, Yushi Wang, Shen Wang'}, page_content='missing sequences within one batch, by changing the â€œfull_randomâ€ parameter.\\nFor example, when setting the parameter into â€œFalseâ€, a batch with the size of\\n8 will have all the 8 groups of MRI images with the same channels to be set to\\n4 https://github.com/trane293/mm-gan4 B. Ma et al.\\nzero; yet with the â€œTrueâ€ setting, each group would have diï¬€erent channels to be\\nthe missing ones.\\nSupport downstream (DeepMedic) usageThe image size of BRATS datasets is\\n240*240, the same as some tumor segmentation modelsâ€™ requirements (such as\\nDeepMedic), yet the input size of mmGAN is 256*256. One method is to pad\\n(as shown in Figure 2) the BRATS images directly with zeros to the expected\\nresolution, use them for synthesis, and then crop the 240*240 region out for\\nDeepMedic; another is to crop the images with a bounding box and resize them\\ninto 256*256 to the dataset (as shown in Figure 3), using them for training, as\\n[5] did, then resize it back, to the original resolution to suit the requirement of'),\n",
       " Document(metadata={'arxiv_id': '2208.03470v1', 'title': 'Analyzing Deep Learning Based Brain Tumor Segmentation with Missing MRI Modalities', 'section': 'body', 'authors': 'Benteng Ma, Yushi Wang, Shen Wang'}, page_content='into 256*256 to the dataset (as shown in Figure 3), using them for training, as\\n[5] did, then resize it back, to the original resolution to suit the requirement of\\nDeepMedic. The user may change the â€œoperationâ€ parameter between â€œpaddingâ€\\nand â€œcropâ€.\\nFig. 2.Process of padding the inputs\\nFig. 3.Process of cropping and reshaping the inputs\\nBetter eï¬ƒciency for preprocessingDuring preprocessing, the original implemen-\\ntation requires around 60 GB of memory, which is unnecessary, and beyond the\\nmemory capacity of most PCs; the generated HDF5 ï¬les are not compressed,\\nwhich also occupy large space in the disk; the data loader used during training\\nmay not have good support of multi-processing in some environments, but run-\\nning within only one process it takes much more time for training. We remake\\nthe preprocess functions, compare to the original version, our version is much\\nmore memory eï¬ƒcient. The preprocess program runs with multi processes. For'),\n",
       " Document(metadata={'arxiv_id': '2208.03470v1', 'title': 'Analyzing Deep Learning Based Brain Tumor Segmentation with Missing MRI Modalities', 'section': 'body', 'authors': 'Benteng Ma, Yushi Wang, Shen Wang'}, page_content='the preprocess functions, compare to the original version, our version is much\\nmore memory eï¬ƒcient. The preprocess program runs with multi processes. For\\neach patient, a mean of all the voxels (within a bounding box of the main region\\nof the brain) is computed, then the value of each voxel is divided by this mean\\nvalue for standardization. Images of only one patient are opened in the memory\\nonce, so there is no special requirement of memory size; because of the paral-\\nlelism, time eï¬ƒciency is ensured. We use the TensorFlow TF-Record dataset,Brain Tumor Segmentation with Missing MRI 5\\nwhich requires less memory and has better support for multi-processing. The\\nmethod allows data to be compressed to take up less space in the disk but is\\nstill highly eï¬ƒcient to be loaded. The provided data loader of TensorFlow will\\nautomatically choose the number of processes to use and adapt the speed to\\nincrease eï¬ƒciency. When running preprocessing functions, the images from pa-'),\n",
       " Document(metadata={'arxiv_id': '2208.03470v1', 'title': 'Analyzing Deep Learning Based Brain Tumor Segmentation with Missing MRI Modalities', 'section': 'body', 'authors': 'Benteng Ma, Yushi Wang, Shen Wang'}, page_content='automatically choose the number of processes to use and adapt the speed to\\nincrease eï¬ƒciency. When running preprocessing functions, the images from pa-\\ntients will be arranged randomly into several TF-Record ï¬les, assigned in the\\nconï¬guration, some can be used as test and validation datasets and the others\\ncan be used for training.\\n3 Experiment Results and Analysis\\nThis section explains the experiment methodology, presents the experiment re-\\nsults along with their analysis. All experiments are using the BraTS2018 dataset\\n[3] for training, validating, and testing.\\n3.1 mmGAN: original v.s. ours\\nThis subsection compares the original implementation of mmGAN4 and ours. In\\nparticular, we need to verify if our mmGAN can reproduce the consistent results\\nas the original mmGAN, before we integrate with DeepMedic for brain tumor\\nsegmentation. We have presented our validation result using BRATS18 HGG\\ndataset in Table 1. Our result is generally very close to the original one. This'),\n",
       " Document(metadata={'arxiv_id': '2208.03470v1', 'title': 'Analyzing Deep Learning Based Brain Tumor Segmentation with Missing MRI Modalities', 'section': 'body', 'authors': 'Benteng Ma, Yushi Wang, Shen Wang'}, page_content='segmentation. We have presented our validation result using BRATS18 HGG\\ndataset in Table 1. Our result is generally very close to the original one. This\\ncan be further visualised in Figure 4 using the exact diï¬€erence values. Although,\\nthe largest diï¬€erence is shown when onlyT2flair is missing, which corresponds to\\nthe scenario â€œ1110â€, the absolute diï¬€erence in terms of mean square error (MSE),\\npeak signal to noise ratio (PSNR), and structured similarity indexing method\\n(SSIM) are still marginal. Moreover, in Figure 5 we have also presented compara-\\ntive visualisation results to illustrate the small diï¬€erence between MRI sequences\\ngenerated by the original implementation and ours using a speciï¬c case (patient\\nnumber: Brats18_CBICA_AAP_1). Therefore, our mmGAN can reproduce the\\nresults by the original one according to the public available resources[5], [3].\\n3.2 ACN and mmDM (mmGAN+DeepMedic)\\nSince mmGAN can synthesize missing MRI sequences with decent quality, can'),\n",
       " Document(metadata={'arxiv_id': '2208.03470v1', 'title': 'Analyzing Deep Learning Based Brain Tumor Segmentation with Missing MRI Modalities', 'section': 'body', 'authors': 'Benteng Ma, Yushi Wang, Shen Wang'}, page_content='results by the original one according to the public available resources[5], [3].\\n3.2 ACN and mmDM (mmGAN+DeepMedic)\\nSince mmGAN can synthesize missing MRI sequences with decent quality, can\\nthis really improve the performance of the downstream workï¬‚ow such as brain\\ntumor segmentation for diagnosis? State-of-the-art solutions such as ACN[6] can\\ndeal with brain tumor segmentation with missing MRI modalities without re-\\nproducing these missing MRI information as intermediate results. But can such\\na specialised model ACN always generate overwhelmingly better results than\\nthe straightforward combination (we call it mmDM for short) of mmGAN and\\nDeepMedic [1] (a state-of-the-art model for brain tumor segmentation)? This\\nsubsection attempts to answer these questions or at least initiates discussions\\nwith some preliminary results. As shown in Table 2 and Figure 6, ACN is much\\nbetter than mmDM especially when T1c MRI sequence is missing. However, in6 B. Ma et al.'),\n",
       " Document(metadata={'arxiv_id': '2208.03470v1', 'title': 'Analyzing Deep Learning Based Brain Tumor Segmentation with Missing MRI Modalities', 'section': 'body', 'authors': 'Benteng Ma, Yushi Wang, Shen Wang'}, page_content='with some preliminary results. As shown in Table 2 and Figure 6, ACN is much\\nbetter than mmDM especially when T1c MRI sequence is missing. However, in6 B. Ma et al.\\nTable 1.Comparative results over HGG dataset of BRATS2018 for reproducing mm-\\nGAN (The order of sequences in scenario is T1, T2, T1c, T2f. For example, 0001 means\\nonly T2f is valid, other sequences are missing.).\\nscenario MSE-org MSE-oursPSNR-org PSNR-oursSSIM-org SSIM-ours\\n0001 0.0143 0.0107 23.196 23.4940 0.8973 0.9007\\n0010 0.0072 0.0086 24.524 24.1919 0.8984 0.9052\\n0100 0.0102 0.0121 23.469 22.9292 0.9074 0.9033\\n1000 0.0072 0.0097 24.879 23.6690 0.9091 0.9018\\n0011 0.0060 0.0055 25.863 26.1124 0.9166 0.9332\\n0101 0.0136 0.0108 22.900 23.9051 0.9156 0.9211\\n0110 0.0073 0.0087 24.792 24.4054 0.9140 0.9182\\n1001 0.0073 0.0069 26.189 25.3669 0.9264 0.9259\\n1010 0.0040 0.0075 26.150 24.4325 0.9107 0.9069\\n1100 0.0068 0.0091 25.242 24.0843 0.9175 0.9103\\n0111 0.0091 0.0072 24.173 25.9732 0.9228 0.9436'),\n",
       " Document(metadata={'arxiv_id': '2208.03470v1', 'title': 'Analyzing Deep Learning Based Brain Tumor Segmentation with Missing MRI Modalities', 'section': 'body', 'authors': 'Benteng Ma, Yushi Wang, Shen Wang'}, page_content='1001 0.0073 0.0069 26.189 25.3669 0.9264 0.9259\\n1010 0.0040 0.0075 26.150 24.4325 0.9107 0.9069\\n1100 0.0068 0.0091 25.242 24.0843 0.9175 0.9103\\n0111 0.0091 0.0072 24.173 25.9732 0.9228 0.9436\\n1011 0.0017 0.0031 28.678 27.2154 0.9349 0.9404\\n1101 0.0098 0.0090 24.372 24.8936 0.9239 0.9241\\n1110 0.0033 0.0084 26.397 23.6391 0.9150 0.9016\\nmean 0.0082 0.0084 24.789 24.5937 0.9120 0.9169\\nmany cases, mmDM can provide comparable performance and sometimes even\\nbetter than ACN (e.g., 1011). Moreover, we have found that for the enhancing\\ntumor segmentation (ET), there are seven out of total fourteen possible missing\\nmodalities cases where the dice score is even lower than 50. These results are\\nnot high enough to conï¬dently convince the doctor to use in clinics treatment\\nor diagnosis.\\n4 Conclusion and Future Work\\nThis technical report presents an improved Tensorï¬‚ow implementation of mm-\\nGAN in terms of eï¬ƒciency and usability. Moreover, this technical report analyses'),\n",
       " Document(metadata={'arxiv_id': '2208.03470v1', 'title': 'Analyzing Deep Learning Based Brain Tumor Segmentation with Missing MRI Modalities', 'section': 'body', 'authors': 'Benteng Ma, Yushi Wang, Shen Wang'}, page_content='or diagnosis.\\n4 Conclusion and Future Work\\nThis technical report presents an improved Tensorï¬‚ow implementation of mm-\\nGAN in terms of eï¬ƒciency and usability. Moreover, this technical report analyses\\npreliminary results for brain tumor segmentation with missing MRI sequences.\\nWe conclude that state-of-the-art solutions can not guarantee good segmentation\\nresults in many missing MRI sequences cases. Sometimes it is close to or even\\nworse than a simple combined version of mmGAN and DeepMedic. Future work\\nshould focus on deï¬ning a theoretic upper bound on how well the model can\\nperform given restricted information. This is because too much information lost\\ncan not lead to a great result in the end. Additionally, closer interdisciplinary co-\\noperation between computer scientists and doctors should be highly promoted to\\nsolve the challenge in brain tumor segmentation under missing MRI sequences.\\nReferences'),\n",
       " Document(metadata={'arxiv_id': '2208.03470v1', 'title': 'Analyzing Deep Learning Based Brain Tumor Segmentation with Missing MRI Modalities', 'section': 'body', 'authors': 'Benteng Ma, Yushi Wang, Shen Wang'}, page_content='operation between computer scientists and doctors should be highly promoted to\\nsolve the challenge in brain tumor segmentation under missing MRI sequences.\\nReferences\\n1. Kamnitsas, K., Ferrante, E., Parisot, S., Ledig, C., Nori, A.V., Criminisi, A., Rueck-\\nert, D., Glocker, B.: Deepmedic for brain tumor segmentation. In: InternationalBrain Tumor Segmentation with Missing MRI 7\\nFig. 4.The comparison of our implementation to the original mmGAN implementation\\nacross three diï¬€erent metrics MSE, SSIM, PSNR. We show the values of the diï¬€erence\\nbetween the original and ours: original minus ours for MSE, ours minus original for\\nSSIM and PSNR. Higher values mean that our implementation leads to better results.\\nOverall, the results are quite close to zero, which means that our implementation\\nachieves similar results as the original does.\\nworkshop on Brainlesion: Glioma, multiple sclerosis, stroke and traumatic brain\\ninjuries. pp. 138â€“149. Springer (2016)'),\n",
       " Document(metadata={'arxiv_id': '2208.03470v1', 'title': 'Analyzing Deep Learning Based Brain Tumor Segmentation with Missing MRI Modalities', 'section': 'body', 'authors': 'Benteng Ma, Yushi Wang, Shen Wang'}, page_content='achieves similar results as the original does.\\nworkshop on Brainlesion: Glioma, multiple sclerosis, stroke and traumatic brain\\ninjuries. pp. 138â€“149. Springer (2016)\\n2. Kamnitsas, K., Ledig, C., Newcombe, V.F., Simpson, J.P., Kane, A.D., Menon,\\nD.K., Rueckert, D., Glocker, B.: Eï¬ƒcient multi-scale 3d cnn with fully connected\\ncrf for accurate brain lesion segmentation. Medical image analysis36, 61â€“78 (2017)\\n3. Menze, B.H., Jakab, A., Bauer, S., Kalpathy-Cramer, J., Farahani, K., Kirby, J.,\\nBurren, Y., Porz, N., Slotboom, J., Wiest, R., et al.: The multimodal brain tu-\\nmor image segmentation benchmark (brats). IEEE transactions on medical imaging\\n34(10), 1993â€“2024 (2014)\\n4. Ronneberger,O.,Fischer,P.,Brox,T.:U-net:Convolutionalnetworksforbiomedical\\nimage segmentation. In: International Conference on Medical image computing and\\ncomputer-assisted intervention. pp. 234â€“241. Springer (2015)\\n5. Sharma, A., Hamarneh, G.: Missing mri pulse sequence synthesis using multi-modal'),\n",
       " Document(metadata={'arxiv_id': '2208.03470v1', 'title': 'Analyzing Deep Learning Based Brain Tumor Segmentation with Missing MRI Modalities', 'section': 'body', 'authors': 'Benteng Ma, Yushi Wang, Shen Wang'}, page_content='computer-assisted intervention. pp. 234â€“241. Springer (2015)\\n5. Sharma, A., Hamarneh, G.: Missing mri pulse sequence synthesis using multi-modal\\ngenerative adversarial network. IEEE transactions on medical imaging39(4), 1170â€“\\n1183 (2019)\\n6. Wang, Y., Zhang, Y., Liu, Y., Lin, Z., Tian, J., Zhong, C., Shi, Z., Fan, J., He,\\nZ.: Acn: Adversarial co-training network for brain tumor segmentation with miss-8 B. Ma et al.\\nFig. 5.This is an example (Brats18_CBICA_AAP_1) of the synthesis results using\\nthe original and our implementation of mmGAN, which are quite visually similar to\\neach other. Each row corresponds to a particular sequence (row names on the left in\\norder T1, T2, T1c, and T2ï¬‚air). Columns are indexed at the bottom of the ï¬gure\\nby alphabets (a) through (h), and have a column name written on top of each slice.\\nColumn names are 4-bit strings where a zero (0) represents missing sequence that was\\nsynthesized, and one (1) represents presence of sequence. Column (a) of each row shows'),\n",
       " Document(metadata={'arxiv_id': '2208.03470v1', 'title': 'Analyzing Deep Learning Based Brain Tumor Segmentation with Missing MRI Modalities', 'section': 'body', 'authors': 'Benteng Ma, Yushi Wang, Shen Wang'}, page_content='Column names are 4-bit strings where a zero (0) represents missing sequence that was\\nsynthesized, and one (1) represents presence of sequence. Column (a) of each row shows\\nthe ground truth slice, and the subsequent columns ((b) through (h)) show synthesized\\nversions of that slice in diï¬€erent scenarios. The order of scenario bit-string is T1, T2,\\nT1c, T2ï¬‚air. For instance, the string 0011 indicates that sequences T1 and T2 were\\nsynthesized from T1c and T2ï¬‚air sequences.\\ning modalities. In: International Conference on Medical Image Computing and\\nComputer-Assisted Intervention. pp. 410â€“420. Springer (2021)Brain Tumor Segmentation with Missing MRI 9\\nTable 2. Comparing ACN and mmDM(mmGAN+DeepMedic) in dice score using\\nBraST 2018 dataset HGG. (The order of modalities: T1, T2, T1c, T2f. For example,\\n0001 means only T2f is valid, other sequences are missing. ET: The Enhancing Tumor;\\nTC: The Tumor Core; WT: The Whole Tumor). The visualization of this table results\\nis shown in Figure 6'),\n",
       " Document(metadata={'arxiv_id': '2208.03470v1', 'title': 'Analyzing Deep Learning Based Brain Tumor Segmentation with Missing MRI Modalities', 'section': 'body', 'authors': 'Benteng Ma, Yushi Wang, Shen Wang'}, page_content='0001 means only T2f is valid, other sequences are missing. ET: The Enhancing Tumor;\\nTC: The Tumor Core; WT: The Whole Tumor). The visualization of this table results\\nis shown in Figure 6\\nType ET TC WT\\nModalities ACN mmDM ACN mmDM ACN mmDM\\n0001 42.98 11.27 67.94 20.36 85.55 82.78\\n0010 78.07 77.55 84.18 77.75 80.52 68.07\\n0100 41.52 16.50 71.18 29.53 79.34 76.31\\n1000 42.77 6.66 67.72 17.52 87.30 56.66\\n0011 75.65 80.61 84.41 83.21 86.41 88.25\\n0110 75.21 80.82 84.59 83.69 80.05 83.94\\n1100 43.71 18.00 71.30 34.50 87.49 80.71\\n0101 47.39 17.79 73.28 35.27 85.50 87.05\\n1001 45.96 13.45 71.61 32.20 87.75 86.53\\n1010 77.46 77.97 83.35 79.44 88.28 71.23\\n1110 76.16 80.19 84.25 83.34 88.96 84.75\\n1101 42.09 23.30 67.86 42.62 88.35 87.53\\n1011 75.97 79.71 82.85 84.01 88.34 88.64\\n0111 76.10 81.21 84.67 84.49 86.90 89.64\\n1111 77.06 80.12 85.18 83.62 89.22 89.41\\navg 61.21 49.68 77.62 58.10 85.92 81.4310 B. Ma et al.\\nFig. 6.ACN vs mmGan+DeepMedic (ET, TC, WT). This is the visualization version'),\n",
       " Document(metadata={'arxiv_id': '2208.03470v1', 'title': 'Analyzing Deep Learning Based Brain Tumor Segmentation with Missing MRI Modalities', 'section': 'body', 'authors': 'Benteng Ma, Yushi Wang, Shen Wang'}, page_content='1111 77.06 80.12 85.18 83.62 89.22 89.41\\navg 61.21 49.68 77.62 58.10 85.92 81.4310 B. Ma et al.\\nFig. 6.ACN vs mmGan+DeepMedic (ET, TC, WT). This is the visualization version\\nof evaluation results shown in Table 2'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'title_abstract', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='Title: Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11\\n\\nAbstract: One of the primary challenges in medical diagnostics is the accurate and efficient use of magnetic resonance imaging (MRI) for the detection of brain tumors. But the current machine learning (ML) approaches have two major limitations, data privacy and high latency. To solve the problem, in this work we propose a federated learning architecture for a better accurate brain tumor detection incorporating the YOLOv11 algorithm. In contrast to earlier methods of centralized learning, our federated learning approach protects the underlying medical data while supporting cooperative deep learning model training across multiple institutions. To allow the YOLOv11 model to locate and identify tumor areas, we adjust it to handle MRI data. To ensure robustness and generalizability, the model is trained and tested on a wide range of MRI data collected from several anonymous medical facilities. The results indicate that our method significantly maintains higher accuracy than conventional approaches.'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='1\\nBrain Tumor Detection in MRI Based on Federated\\nLearning with YOLOv11\\nSheikh Moonwara Anjum Monisha, Ratun Rahman\\nAbstractâ€”One of the primary challenges in medical diagnostics\\nis the accurate and efficient use of magnetic resonance imaging\\n(MRI) for the detection of brain tumors. But the current machine\\nlearning (ML) approaches have two major limitations, data\\nprivacy and high latency. To solve the problem, in this work we\\npropose a federated learning architecture for a better accurate\\nbrain tumor detection incorporating the YOLOv11 algorithm.\\nIn contrast to earlier methods of centralized learning, our\\nfederated learning approach protects the underlying medical\\ndata while supporting cooperative deep learning model training\\nacross multiple institutions. To allow the YOLOv11 model to\\nlocate and identify tumor areas, we adjust it to handle MRI\\ndata. To ensure robustness and generalizability, the model is\\ntrained and tested on a wide range of MRI data collected from'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='locate and identify tumor areas, we adjust it to handle MRI\\ndata. To ensure robustness and generalizability, the model is\\ntrained and tested on a wide range of MRI data collected from\\nseveral anonymous medical facilities. The results indicate that our\\nmethod significantly maintains higher accuracy than conventional\\napproaches.\\nIndex Termsâ€”Brain tumor, federated learning, YOLOv11,\\nMRI\\nI. I NTRODUCTION\\nB\\nRAIN tumors, which include a range of abnormalities in\\nthe brain, present a number of difficulties because of their\\ndifferent forms and levels of malignancy. Since it has a major\\ninfluence on treatment options and patient outcomes, early and\\ncorrect detection is crucial [1]. Magnetic Resonance Imaging\\n(MRI), which provides fine-grained images of the brainâ€™s soft\\ntissues, has long been a foundation for identifying brain tumors\\n[2]. But MRI scan interpretation is extremely complicated and\\ndemands an extensive amount of ability, and a wrong diagnosis'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='tissues, has long been a foundation for identifying brain tumors\\n[2]. But MRI scan interpretation is extremely complicated and\\ndemands an extensive amount of ability, and a wrong diagnosis\\ncan have catastrophic consequences. Therefore, improving the\\nprecision and effectiveness of brain tumor detection is not\\nonly an engineering challenge but also an urgent medical\\nrequirement [3].\\nDespite their advances, machine learning [2], [4] and deep\\nlearning [5], [6] technologies have substantial limits in de-\\ntecting brain tumors. One of the most critical difficulties is\\nthe necessity for large and diverse training datasets, which\\nare frequently difficult to assemble due to privacy concerns\\nand the rarity of particular tumor kinds [7]. Furthermore,\\nthese models typically need high computational resources,\\nlimiting their applicability in low-resource environments. An-\\nother important difficulty is the â€black-boxâ€ nature of deep\\nlearning models, which means the decision-making process is'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='limiting their applicability in low-resource environments. An-\\nother important difficulty is the â€black-boxâ€ nature of deep\\nlearning models, which means the decision-making process is\\nSheikh Moonwara Anjum Monisha is with the Department of Com-\\nputer Science, Virginia Tech, Blacksburg, Virginia, 24060 USA e-\\nmail:msheikhmoonwaraa@vt.edu\\nRatun Rahman is with the Department of Electrical and Computer Engineer-\\ning, The University of Alabama in Huntsville, Huntsville, Alabama, 35816\\nUSA e-mail:rr0110@uah.edu\\nManuscript submitted March 4, 2025.\\nnot transparent, making clinical validation and trust by medical\\npractitioners difficult. Furthermore, these models can suffer\\nfrom overfitting, which occurs when they perform well on\\ntraining data but fail to transfer to new or slightly different\\nclinical environments [8].\\nMotivated by the limitations of traditional machine learning\\nand deep learning techniques in brain tumor identification,'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='clinical environments [8].\\nMotivated by the limitations of traditional machine learning\\nand deep learning techniques in brain tumor identification,\\nwe offer an innovative approach leveraging federated learning\\n(FL). This studyâ€™s primary goal is to protect the privacy and\\nsecurity of medical data while utilizing the combined strength\\nof decentralized data sources. Our goal is to increase the\\nYOLOv11 modelâ€™s [9] resilience and generalizability for iden-\\ntifying brain cancers in MRI scans across several institutions\\nwithout direct exchange of data. The main contributions of\\nthis paper are summarized as follows:\\nâ€¢ We describe an extensive architecture that integrates\\nYOLOv11 and FL to train on decentralized datasets\\neffectively. This method reduces the requirement for large\\ncentralized databases and decreases the possible biases\\nassociated with single-institution investigations.\\nâ€¢ Our methodology applies federated learning to ensure that'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='centralized databases and decreases the possible biases\\nassociated with single-institution investigations.\\nâ€¢ Our methodology applies federated learning to ensure that\\nthe data remains at its source, with only model updates\\nshared across the network. This not only complies with\\nrigid data protection rules but also provides opportuni-\\nties for collaboration across institutions that had been\\nrestricted due to data privacy issues.\\nâ€¢ The study provides comprehensive benchmarks that com-\\npare our federated learning technique to standard cen-\\ntralized deep learning models, demonstrating substantial\\nimprovements in model adaptability and diagnostic accu-\\nracy.\\nII. R ELATED WORKS\\nA. MRI Brain Tumor\\nMagnetic Resonance Imaging (MRI) is an essential method\\nfor detecting and treating brain tumors, as it uses powerful\\nmagnetic fields and radio waves to produce detailed images\\nof the brain and spinal cord without involving ionizing radi-\\nation [10]. Specialized MRI techniques, such as T1-weighted'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='magnetic fields and radio waves to produce detailed images\\nof the brain and spinal cord without involving ionizing radi-\\nation [10]. Specialized MRI techniques, such as T1-weighted\\nand T2-weighted images, Fluid-Attenuated Inversion Recovery\\n(FLAIR), Diffusion-weighted imaging (DWI), and Magnetic\\nResonance Spectroscopy (MRS), improve the ability to de-\\ntermine between different types of brain tissue and tumors\\n[11]. These capabilities facilitate precise diagnosis, treatment\\nplanning, and monitoring because MRI can detect subtle\\ndifferences in tissue characteristics, assisting in the mapping\\nof tumors relative to critical brain structures for surgical\\narXiv:2503.04087v1  [cs.CV]  6 Mar 20252\\nplanning and evaluating the effectiveness of treatments during\\npost-treatment follow-up [12]. Despite its advanced diagnostic\\ncapability, MRI interpretation remains challenging due to\\nartifacts, patient movement, and tumor appearance variations,'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='post-treatment follow-up [12]. Despite its advanced diagnostic\\ncapability, MRI interpretation remains challenging due to\\nartifacts, patient movement, and tumor appearance variations,\\nemphasizing the importance of professional radiological ex-\\namination and, in many cases, histological confirmation via\\nbiopsy.\\nB. YOLOv11 Model\\nThe YOLOv11 model is the latest stable release version\\nin the â€You Only Look Onceâ€ series, which is known for\\nits real-time object detection capabilities [9]. This version\\nimproves on its predecessors by improving neural network\\narchitecture, integrating advanced training techniques such\\nas transfer learning, and integrating attention mechanisms to\\nbetter emphasize relevant image elements [13]. YOLOv11 pro-\\nvides considerable increases in detection speed and accuracy,\\nmaking it appropriate for applications that require rapid and\\nprecise image processing, such as medical imaging for brain\\ntumor detection in MRI scans. With efficiency optimizations'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='making it appropriate for applications that require rapid and\\nprecise image processing, such as medical imaging for brain\\ntumor detection in MRI scans. With efficiency optimizations\\nthat allow for deployment on less powerful hardware and\\nresistance to variations in object scale and image quality,\\nYOLOv11 stands out as a scalable and versatile solution for\\ncomplicated detection tasks in a broad spectrum of operational\\nenvironments [9].\\nC. Machine Learning on Brain Tumor Detection\\nMachine learning (ML) has significantly altered the field\\nof brain tumor detection through enabling the development\\nof algorithms capable of analyzing complex medical imaging\\ndata with precision as well as speed [2]. In brain tumor iden-\\ntification, ML models, particularly deep learning approaches\\nsuch as convolutional neural networks (CNNs), are trained\\non massive datasets of brain scans to effectively identify and\\nclassify tumors [4], [8]. These models learn to detect patterns'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='such as convolutional neural networks (CNNs), are trained\\non massive datasets of brain scans to effectively identify and\\nclassify tumors [4], [8]. These models learn to detect patterns\\nand irregularities in images that may indicate malignant or\\nbenign tumors, allowing radiologists to diagnose more accu-\\nrately and plan therapy [7]. The application of ML not only\\nimproves diagnostic capabilities by providing a second, data-\\ndriven opinion, but it also streamlines workflow in medical\\nimaging departments, reducing time-to-diagnosis and perhaps\\nenhancing the overall accuracy of brain tumor assessments [8].\\nThis technology development is essential for early diagnosis\\nand better patient outcomes in neuro-oncology.\\nD. Deep Learning on Brain Tumor Detection\\nDeep learning, a subset of machine learning characterized\\nby networks that can learn unsupervised from unstructured or\\nunlabeled data, has made major advancements in the field of\\nbrain tumor detection. Deep learning models excel at parsing'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='by networks that can learn unsupervised from unstructured or\\nunlabeled data, has made major advancements in the field of\\nbrain tumor detection. Deep learning models excel at parsing\\nthrough complicated image data, recognizing patterns that hu-\\nman observers could overlook. They apply architectures such\\nas convolutional neural networks (CNNs). These models are\\ntrained on large datasets of MRI scans to distinguish features\\nassociated with different types of brain tumors, improving both\\ndiagnostic precision and speed [6]. Deep learning automates\\nthe detection process, providing radiologists powerful tools\\nto evaluate tumor features, including size, shape, and the\\npossibility of malignancy [5]. This not only allows for more\\naccurate and timely diagnosis, but it also helps with tailored\\ntherapy planning, which has a substantial impact on the\\ntreatment of patientsâ€™ techniques in neuro-oncology [14]. As\\ndeep learning advances, its integration into clinical procedures'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='therapy planning, which has a substantial impact on the\\ntreatment of patientsâ€™ techniques in neuro-oncology [14]. As\\ndeep learning advances, its integration into clinical procedures\\nhas the potential to improve diagnostic accuracy and patient\\noutcomes in the detection and treatment of brain tumors.\\nE. Federated Learning\\nFederated Learning (FL) is a privacy-preserving machine\\nlearning technique that trains algorithms using multiple decen-\\ntralized devices or servers without exchanging local data sam-\\nples, therefore addressing privacy, security, and data ownership\\nissues [15]. In this model, clients (such as mobile phones or\\nhealthcare institutions) train models locally and submit only\\nmodel updates, not data, to a central server. The server inte-\\ngrates these changes to improve a global model, which is then\\nsent back to the clients for additional training. The approach\\nprotects data privacy, decreases the need for large-scale data'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='grates these changes to improve a global model, which is then\\nsent back to the clients for additional training. The approach\\nprotects data privacy, decreases the need for large-scale data\\ntransfers, and uses various kinds of datasets to improve model\\nrobustness [16], [17], [18]. Federated learning is specifically\\nbeneficial in sensitive industries such as healthcare, where\\npatient data privacy is critical, and in circumstances needing\\nstrict data localization requirements.\\nTo the best of our knowledge, no research has been done\\non using FL in the YOLOv11 model to detect brain tumors.\\nOur goal in this work is to close this research gap and offer\\na novel approach to brain tumor identification.\\nIII. M ETHOD\\nA. System Model\\nFigure 1 illustrates the federated learning architecture and\\nour implementation methodology. In our architecture, a cen-\\ntral global server aggregates models from different clients.\\nThese clients, designated by n âˆˆ N, belong to numerous'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='our implementation methodology. In our architecture, a cen-\\ntral global server aggregates models from different clients.\\nThese clients, designated by n âˆˆ N, belong to numerous\\nmedical facilities, each equipped with MRI data required for\\ndetecting brain cancers. Each client n maintains a distinct\\nlocal dataset D(k)\\nn for each global training round represented\\nby k âˆˆ K, indicating the total number of rounds in the\\nfederated learning cycle. During each cycle, the client n trains\\na local model Î¸n,k on its own dataset D(k)\\nn , which differs in\\nterms of size and patient demographics. The collective local\\ndatasets D(k) = P\\nnâˆˆN D(k)\\nn serve as the training foundation\\nin each cycle, boosting the resilience and applicability of\\nthe model across variable data features from the numerous\\nparticipating facilities. This federated system aims to improve\\na global model Î¸g,(k+1) using the Federated Averaging tech-\\nnique, which averages updates from all local models. This'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='participating facilities. This federated system aims to improve\\na global model Î¸g,(k+1) using the Federated Averaging tech-\\nnique, which averages updates from all local models. This\\naggregation takes place without the need to exchange sensitive\\npatient data, hence protecting privacy. Clients use a consistent\\nlearning rate Î· to update gradients, ensuring homogeneity\\nacross different operating settings. After training, each client\\nsends its model updates to the global server for aggregation.\\nThe improved global model Î¸g,(k+1) is then redistributed to\\nall clients for additional training in succeeding rounds, thereby3\\nFig. 1: An overview of the Federated Learning System for\\nBrain Tumor Detection. This image depicts the architecture\\nof our federated learning framework, including the interaction\\nbetween the central utility server and N client devices, each\\nrepresenting a medical facility. It describes the flow of local\\nmodel training, data aggregation, and global model updates'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='between the central utility server and N client devices, each\\nrepresenting a medical facility. It describes the flow of local\\nmodel training, data aggregation, and global model updates\\nover numerous global rounds, emphasizing data privacy and\\nthe collaborative training process.\\nboosting the modelâ€™s ability to detect brain cancers throughout\\nthe cycle.\\nB. Problem Formulation\\n1) Local Model Training in YOLO: YOLO (You Only\\nLook Once) maps image pixels to bounding box coordinates\\nand class probabilities, transforming object recognition into\\na regression issue. For every input image, a S Ã— S grid is\\ngenerated. Predictions for coordinates (x, y, w, h), confidence,\\nand C class probabilities are provided in each of the B\\nbounding boxes that are predicted by each grid cell. The\\nIntersection over Union (IoU) between the predicted box and\\nthe actual ground truth is reflected in the confidence score.\\nTraining the YOLO model to correctly predict the existence'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='Intersection over Union (IoU) between the predicted box and\\nthe actual ground truth is reflected in the confidence score.\\nTraining the YOLO model to correctly predict the existence\\nand features of objects in an image depends on the modelâ€™s loss\\nfunction. The loss function is a weighted sum of multiple terms\\nthat take into consideration the class predictionsâ€™ correctness,\\nthe bounding boxesâ€™ accuracy, and the predictionsâ€™ confidence.\\nBelow, we break down each element:\\nBounding Box Loss The bounding box loss measures the\\naccuracy of the predicted boxes and is split into two parts: the\\ncoordinate loss and the size loss for the boxes that actually\\ncontain objects.\\nCoordinate Loss = Î»coord\\nS2\\nX\\ni=0\\nBX\\nj=0\\n1obj\\nij\\n\\x00\\n(xi âˆ’ Ë†xi)2 + (yi âˆ’ Ë†yi)2\\x01\\n,\\n(1)\\nSize Loss = Î»coord\\nS2\\nX\\ni=0\\nBX\\nj=0\\n1obj\\nij\\n\\x12\\n(âˆšwi âˆ’\\np\\nË†wi)2 + (\\np\\nhi âˆ’\\nq\\nË†hi)2\\n\\x13 (2)\\nwhere Î»coord is a weighting factor to increase the importance\\nof box coordinates in the loss, and 1obj\\nij is an indicator that'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='S2\\nX\\ni=0\\nBX\\nj=0\\n1obj\\nij\\n\\x12\\n(âˆšwi âˆ’\\np\\nË†wi)2 + (\\np\\nhi âˆ’\\nq\\nË†hi)2\\n\\x13 (2)\\nwhere Î»coord is a weighting factor to increase the importance\\nof box coordinates in the loss, and 1obj\\nij is an indicator that\\nequals 1 if an object is present in the bounding box (i, j),\\notherwise 0.\\nConfidence Loss The model is penalized by the confidence\\nloss for making inaccurate confidence predictions in both\\nobject- and object-free contexts.\\nObject Confidence Loss = Î»conf\\nS2\\nX\\ni=0\\nBX\\nj=0\\n1obj\\nij (Ci âˆ’ Ë†Ci)2 (3)\\nNo-object Confidence Loss = Î»conf\\nS2\\nX\\ni=0\\nBX\\nj=0\\n1noobj\\nij (Ci âˆ’ Ë†Ci)2\\n(4)\\nwhere 1noobj\\nij = 1 if there is no object in the bounding box\\n(i, j), and Ci is the confidence score that estimations the\\nIntersection over Union (IoU) between the predicted bounding\\nbox and the ground truth.\\nClassification Loss The classification loss is calculated only\\nfor grid cells that contain an object. It uses a squared error sum\\nacross all classes.\\nClassification Loss =\\nS2\\nX\\ni=0\\n1obj\\ni\\nX\\ncâˆˆclasses\\n(pi(c) âˆ’ Ë†pi(c))2, (5)'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='for grid cells that contain an object. It uses a squared error sum\\nacross all classes.\\nClassification Loss =\\nS2\\nX\\ni=0\\n1obj\\ni\\nX\\ncâˆˆclasses\\n(pi(c) âˆ’ Ë†pi(c))2, (5)\\nwhere pi(c) and Ë†pi(c) are the true and predicted probabilities\\nof class c in cell i, respectively.\\nThe following elements comprise the YOLO loss function,\\nwhich is intended to optimize detection accuracy:\\nLoss = Î»coord\\nS2\\nX\\ni=0\\nBX\\nj=0\\n1obj\\nij\\n\\x00\\n(xi âˆ’ Ë†xi)2 + (yi âˆ’ Ë†yi)2\\x01\\n+ Î»coord\\nS2\\nX\\ni=0\\nBX\\nj=0\\n1obj\\nij\\n\\x12\\n(âˆšwi âˆ’\\np\\nË†wi)2 + (\\np\\nhi âˆ’\\nq\\nË†hi)2\\n\\x13\\n+ Î»conf\\nS2\\nX\\ni=0\\nBX\\nj=0\\n1obj\\nij (Ci âˆ’ Ë†Ci)2\\n+ Î»conf\\nS2\\nX\\ni=0\\nBX\\nj=0\\n1noobj\\nij (Ci âˆ’ Ë†Ci)2\\n+\\nS2\\nX\\ni=0\\n1obj\\ni\\nX\\ncâˆˆclasses\\n(pi(c) âˆ’ Ë†pi(c))2,\\nwhere pi(c) and Ë†pi(c) represent the actual and expected\\nprobabilities for class c, respectively, and 1obj\\nij indicates if an\\nobject is present in cell i and bounding box j.\\nC. Federated Learning with YOLO\\nIn a federated learning setup, each client independently\\ntrains the YOLO model on its local data, updating the model'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='object is present in cell i and bounding box j.\\nC. Federated Learning with YOLO\\nIn a federated learning setup, each client independently\\ntrains the YOLO model on its local data, updating the model\\nparameters based on the local dataset Dk:\\nÎ¸k,new = Î¸k,old âˆ’ Î·âˆ‡L(Î¸k,old, Dk), (6)\\nwhere Î¸k are the parameters of the model for client k, Î· is\\nthe learning rate, and L is the loss function.4\\nPost-training, clients send their updated model parameters to\\na central server, where they are aggregated using the Federated\\nAveraging (FedAvg) method:\\nÎ¸global = 1\\nK\\nKX\\nk=1\\nÎ¸k,new, (7)\\nwith K representing the number of clients. This global model\\nis then redistributed to the clients for subsequent training\\nrounds, optimizing the model iteratively while maintaining\\ndata privacy, as raw data remains local.\\nD. Proposed Algorithm\\nIn the federated learning framework for brain tumor detec-\\ntion, we initialize the global model Î¸(0)\\ng (Line 1) and enter\\na loop over K global rounds (Lines 2-12). Each round starts'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='In the federated learning framework for brain tumor detec-\\ntion, we initialize the global model Î¸(0)\\ng (Line 1) and enter\\na loop over K global rounds (Lines 2-12). Each round starts\\nwith the server distributing the current global model Î¸(kâˆ’1)\\ng\\nto each client n (Line 3). Clients, denoted by n âˆˆ N, each\\nload their respective local datasets D(k)\\nn and initialize their\\nlocal model Î¸(k)\\nn to the global model (Lines 5-6). They then\\nperform I epochs of local training using Stochastic Gradient\\nDescent (SGD) with a learning rate Î·, updating Î¸(k)\\nn based\\non their data D(k)\\nn (Lines 7-9). After training, each client\\nsends their updated local model back to the server (Line 10),\\nwhere all the local models are aggregated to update the global\\nmodel Î¸(k)\\ng using the Federated Averaging algorithm (Line 11).\\nThis process iterates, enhancing the global modelâ€™s ability to\\ndetect brain tumors effectively across diverse medical datasets\\nwhile preserving data privacy, with an optional evaluation on'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='This process iterates, enhancing the global modelâ€™s ability to\\ndetect brain tumors effectively across diverse medical datasets\\nwhile preserving data privacy, with an optional evaluation on\\na validation set each round to monitor progress (Line 12).\\nAlgorithm 1 Federated Learning Algorithm for Brain Tumor\\nDetection\\n1: Initialize global model Î¸(0)\\ng\\n2: for each round k = 1to K do\\n3: Server sends Î¸(kâˆ’1)\\ng to each client n âˆˆ N\\n4: for each client n in parallel do\\n5: Load local data D(k)\\nn\\n6: Initialize local model Î¸(k)\\nn = Î¸(kâˆ’1)\\ng\\n7: for each local epoch i = 1to I do\\n8: Update Î¸(k)\\nn using SGD on D(k)\\nn with learning\\nrate Î·\\n9: end for\\n10: Send updated model Î¸(k)\\nn to server\\n11: end for\\n12: Server aggregates updates:\\n13: Î¸(k)\\ng = 1\\nN\\nPN\\nn=1 Î¸(k)\\nn\\n14: Optionally evaluate Î¸(k)\\ng on validation set\\n15: end for\\nE. Complexity\\nTime Complexity The time complexity of the federated\\nlearning method is essentially determined by the following el-'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='n\\n14: Optionally evaluate Î¸(k)\\ng on validation set\\n15: end for\\nE. Complexity\\nTime Complexity The time complexity of the federated\\nlearning method is essentially determined by the following el-\\nements: the number of global rounds K, the number of clients\\nN, and the number of local epochs I performed by each client.\\nEach client trains locally on their dataset D(k)\\nn for I epochs.\\nThe computational complexity of each epoch gets determined\\nby the complexity of the learning algorithm (generally SGD)\\nand the quantity of the local data. As a result, the total time\\ncomplexity for each client in each round is proportional to\\nO(I Â·C(D(k)\\nn )), where C(D(k)\\nn ) is the complexity of process-\\ning the local dataset once. Among N clients and K rounds, the\\ncomplexity increases to O(K Â·N Â·I Â·C(D(k)\\nn )). It is important\\nto emphasize that while the clients work in parallel, network\\nlatency and bandwidth restrictions during model aggregation'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='complexity increases to O(K Â·N Â·I Â·C(D(k)\\nn )). It is important\\nto emphasize that while the clients work in parallel, network\\nlatency and bandwidth restrictions during model aggregation\\nat the server can result in significant overhead, especially for\\nlarger distributed systems.\\nSpace Complexity The space complexity of the federated\\nlearning algorithm contains both local and global model pa-\\nrameters. Each client preserves a local copy of the model, Î¸(k)\\nn ,\\nwith the same dimensions as the global model, Î¸(k)\\ng . Assuming\\nM parameters, each modelâ€™s space need is O(M). The serverâ€™s\\nprincipal requirement is to maintain the global model as well\\nas temporary storage for aggregating client updates, both of\\nwhich require space proportional to O(M). As a result, the\\nserverâ€™s overall space complexity remains O(M), assuming\\neffective aggregation methods that deal with one client update\\nat a time. However, at the client level, since each client'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='serverâ€™s overall space complexity remains O(M), assuming\\neffective aggregation methods that deal with one client update\\nat a time. However, at the client level, since each client\\nmaintains only their local model, the overall space complexity\\nacross all clients is O(N Â· M), demonstrating that a single\\nmodel exists per client. This architecture supports scalability\\nin terms of model size because increasing the number of clients\\nonly increases the space complexity at the clients, not at the\\nserver.\\nF . Limitations\\nDespite the major advantages of federated learning in terms\\nof privacy and using decentralized data sources, various re-\\nstrictions limit its effectiveness, particularly in the context\\nof brain tumor detection. Firstly, data heterogeneity between\\nmedical facilities can cause major obstacles to model con-\\nvergence and performance consistency. Different customers\\nmay have variable amounts of data, patient demographics, and\\nimaging technology, leading to skewed model updates and'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='vergence and performance consistency. Different customers\\nmay have variable amounts of data, patient demographics, and\\nimaging technology, leading to skewed model updates and\\npotential biases in the global model. Secondly, the reliance\\non multiple phases of communication between clients and the\\nserver leads to latency and increases the possibility of network\\ninstability, particularly when the clients are geographically\\nscattered. This can cause delays in model updates and reduce\\nthe overall training pace. Furthermore, the computational\\nimpact on clients can be significant, rendering the system\\ninfeasible for facilities that have limited computational re-\\nsources. Finally, security issues, while lessened by the nature\\nof federated learning, remain since attackers might possibly\\ndeduce sensitive information from model updates, necessi-\\ntating strong cryptographic safeguards or advanced privacy-\\npreserving approaches such as differential privacy.\\nIV. E XPERIMENTS AND DISCUSSIONS'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='tating strong cryptographic safeguards or advanced privacy-\\npreserving approaches such as differential privacy.\\nIV. E XPERIMENTS AND DISCUSSIONS\\nA. Dataset and Data Processing\\nFor our simulations, we use a synthetic brain tumor dataset,\\nâ€™ 3064 T-1 weighted CE-MRI of brain tumor images,â€™ [19]5\\n(a) F1-curve\\n (b) P-curve\\n(c) PR-curve\\n (d) R-curve\\nFig. 2: Score matrices of four types: F1-curve (a), P-curve (b), PR-curve (c), and R-curve (d).\\nwhich is designed to simulate real-world variability in MRI\\nscans used for brain tumor detection. This dataset contains\\n10,000 MRI images, each annotated by an expert radiologist\\nto determine the presence, type, and location of tumors. The\\nimages vary in size, contrast, and scan parameters to repre-\\nsent the wide range of environments encountered in various\\nmedical contexts. Each MRI scan is pre-processed to meet the\\ninput specifications of our federated learning model. The pre-\\nprocessing processes include scaling images to a consistent'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='medical contexts. Each MRI scan is pre-processed to meet the\\ninput specifications of our federated learning model. The pre-\\nprocessing processes include scaling images to a consistent\\nresolution of 256x256 pixels, standardizing pixel values to\\nthe range [0,1], and enhancing the dataset with rotations\\nand flips to improve model robustness to variations in tumor\\npresentation. In addition, we apply brain stripping to eliminate\\nnon-brain tissues from the images, which improves the modelâ€™s\\nemphasis on important features. The dataset is divided into a\\ntraining set (80%) and a testing set (20%). Each client receives\\na portion of the training set that reflects the heterogeneity and\\nimbalances common in the distributed environment of medical\\ndata. I describes our workâ€™s data.\\nTABLE I: Data description and classification for our proposed\\nmethod in 3064 T-1 weighted CE-MRI of brain tumor images\\ndataset.\\nClass Images Box(P) R mAP50 mAP50-95\\nAll 612 0.902 0.854 0.908 0.653'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='TABLE I: Data description and classification for our proposed\\nmethod in 3064 T-1 weighted CE-MRI of brain tumor images\\ndataset.\\nClass Images Box(P) R mAP50 mAP50-95\\nAll 612 0.902 0.854 0.908 0.653\\nGlioma 285 0.853 0.732 0.825 0.493\\nMeningioma 142 0.931 0.93 0.966 0.8\\nPituitary 185 0.923 0.901 0.932 0.668\\nB. Environment settings\\nThe simulation environment for our federated learning sys-\\ntem has been rigorously developed to replicate real-world con-\\nditions encountered in medical imaging for tumor detection.\\nWe use a simulated network of N clients, each representing a\\ndifferent medical facility and its unique dataset of MRI images\\nwith various resolutions and patient profiles. These datasets\\nhave been generated synthetically, although they follow re-\\nalistic brain tumor imaging distributions and characteristics.\\nThe server and all clients are simulated on a high-performance\\ncomputer cluster outfitted with GPUs to parallelize processing'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='alistic brain tumor imaging distributions and characteristics.\\nThe server and all clients are simulated on a high-performance\\ncomputer cluster outfitted with GPUs to parallelize processing\\nand speed up the training procedure. To replicate real-world6\\nFig. 3: Simulation results on the modelâ€™s predictions across various images.\\nresource constraints, each client runs its local instance of the\\ntraining algorithm on a dedicated GPU. The global server,\\nthat manages the federated learning process, implements the\\nFederated Averaging method to aggregate model updates.\\nNetwork latency and capacity are artificially introduced into\\nthe simulation to evaluate the modelâ€™s robustness under typical\\ninternet environments. To ensure consistency, software depen-\\ndencies are standardized across all nodes, and model training\\nand simulation are performed using Python 3.8, TensorFlow\\n2.x, and PyTorch 1.8. The entire simulation is run under\\ncontrolled conditions, ensuring reproducibility and enabling'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='and simulation are performed using Python 3.8, TensorFlow\\n2.x, and PyTorch 1.8. The entire simulation is run under\\ncontrolled conditions, ensuring reproducibility and enabling\\nthe study of the federated learning modelâ€™s performance over\\na variety of challenging datasets.\\nC. Score Matrices\\nWe simulated four score matrices, such as the f1 score, the\\np-score, the pr-score, and the r-score in Fig. 2.\\nF1 Score The F1 Score is a harmonic mean of precision and\\nrecall, balancing the two metrics. It is particularly useful when\\nthe costs of false positives and false negatives are similar. The\\nF1 score is calculated as follows:\\nF1 Score = 2Ã— Precision Ã— Recall\\nPrecision + Recall\\nwhere Precision is the ratio of true positives to the total\\npredicted positives, and Recall is the ratio of true positives\\nto the total actual positives.\\nPrecision (P-Score) Precision, also known as the P-Score,\\nquantifies the accuracy of positive predictions. This metric is'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='to the total actual positives.\\nPrecision (P-Score) Precision, also known as the P-Score,\\nquantifies the accuracy of positive predictions. This metric is\\ncrucial where false positives carry a high cost. Precision is\\ndefined by the equation:\\nPrecision = True Positives\\nTrue Positives + False Positives\\nIt indicates the correctness of positive identifications by the\\nmodel.\\nPrecision-Recall Score (PR-Score) The Precision-Recall\\nScore typically refers to the Precision-Recall curve, which\\nshows the trade-off between precision and recall for different\\nthreshold settings of a classifier. The PR curve is essen-\\ntial for understanding a modelâ€™s performance across various\\nsensitivity levels. While a single metric â€PR-Scoreâ€ is not\\nstandard, the concept is crucial for optimizing the threshold\\nof classification models.\\nRecall (R-Score) Recall, or R-Score, evaluates the modelâ€™s\\nability to detect all relevant instances. This metric is essential7\\n(a) ML Results\\n (b) FL Results'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='of classification models.\\nRecall (R-Score) Recall, or R-Score, evaluates the modelâ€™s\\nability to detect all relevant instances. This metric is essential7\\n(a) ML Results\\n (b) FL Results\\nFig. 4: Comparison between ML (a) and FL (b) results between 2 confusion matrix where the more precise diagonal indicates\\nbetter model prediction.\\n(a) Accuracy\\n (b) Loss Value\\nFig. 5: Comparison between ML and FL results in terms of accuracy (a) and loss (b) value.\\nin cases where missing a positive instance is important. Recall\\nis calculated using the following formula:\\nRecall = True Positives\\nTrue Positives + False Negatives\\nIt reflects the modelâ€™s sensitivity to identifying positive cases,\\nwhich ensures the identification process is complete. Glioma\\nhad the lowest overall score, followed by all classes and\\nthe pituitary. Finally, meningioma is listed top. The results\\ndemonstrate that the classification model performs differently\\nacross tumor types, with meningioma being the most accu-'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='the pituitary. Finally, meningioma is listed top. The results\\ndemonstrate that the classification model performs differently\\nacross tumor types, with meningioma being the most accu-\\nrately detected, followed by pituitary tumors and gliomas. This\\nvariety suggests that glioma may present features separating\\nissues that are less prominent in other tumor types. Further\\nresearch into the precise characteristics of gliomas that result\\nin lower scores could aid in the refinement of the model,\\nincreasing its accuracy and dependability across all classes.\\nThis could consist of increasing the glioma training data,\\napplying more sophisticated feature extraction approaches,\\nor exploring more complex model architectures designed to\\ncapture the distinct properties of each tumor type.\\nD. Brain Tumor Detection Images\\nFigure 3 shows the modelâ€™s predictions across a variety of\\nimages, each presenting either a single kind or a combination\\nof different brain tumor types. The results also provide the'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='Figure 3 shows the modelâ€™s predictions across a variety of\\nimages, each presenting either a single kind or a combination\\nof different brain tumor types. The results also provide the\\nmodelâ€™s confidence ratings for each prediction. As a result,\\nthis emphasizes the modelâ€™s ability to not only detect tumors\\nbut also distinguish between different tumor types with a\\nhigh degree of accuracy. This level of prediction confidence\\nis essential for clinical applications that require accurate and\\nreliable diagnostic information. Further improvement of the\\nmodel could enhance its diagnostic accuracy and confidence\\nlevels.\\nE. Comparison Between ML and FL Approach\\nFinally, we compare our proposed FL method with the\\nexisting ML method in Fig. 4 and 5. In Fig. 4, we show two8\\nconfusion matrices. The first confusion matrix in Fig. 4(a) is\\nML result and the second confusion matrix in Fig. 4(b) is FL\\nresult. From the confusion matrix, the FL result is more clear'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='confusion matrices. The first confusion matrix in Fig. 4(a) is\\nML result and the second confusion matrix in Fig. 4(b) is FL\\nresult. From the confusion matrix, the FL result is more clear\\ndiagonally compared to the ML result. The result in Fig. 5\\nalso shows the same result where in Fig. 5(a), FL has higher\\naccuracy than ML. Similarly, Fig 5(b) shows that the loss value\\nof FL is lower than ML which indicates better performance\\nin FL.\\nV. C ONCLUSION\\nIn conclusion, combining new approaches such as machine\\nlearning and deep learning into the field of brain tumor\\ndetection provides considerable advancements in medical di-\\nagnosis. These technologies, especially the use of YOLOv11\\nmodels and federated methods of learning, have the potential\\nto increase the accuracy, speed, and efficiency of brain tu-\\nmor identification. Furthermore, federated learning deals with\\nessential data privacy and security issues, enabling the use\\nof massive, decentralized datasets while preserving patient'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='mor identification. Furthermore, federated learning deals with\\nessential data privacy and security issues, enabling the use\\nof massive, decentralized datasets while preserving patient\\nconfidentiality. The continual development and improvement\\nof these methods will certainly create opportunities for brain\\ntumor detection, diagnosis, and therapy, leading to more\\nprecise and personalized medical care.\\nREFERENCES\\n[1] M. Nazir, S. Shakil, and K. Khurshid, â€œRole of deep learning in\\nbrain tumor detection and classification (2015 to 2020): A review,â€\\nComputerized medical imaging and graphics, vol. 91, p. 101940, 2021.\\n[2] K. Sharma, A. Kaur, and S. Gujral, â€œBrain tumor detection based\\non machine learning algorithms,â€ International Journal of Computer\\nApplications, vol. 103, no. 1, 2014.\\n[3] V . Y . Borole, S. S. Nimbhore, and D. S. S. Kawthekar, â€œImage processing\\ntechniques for brain tumor detection: A review,â€International Journal of'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='Applications, vol. 103, no. 1, 2014.\\n[3] V . Y . Borole, S. S. Nimbhore, and D. S. S. Kawthekar, â€œImage processing\\ntechniques for brain tumor detection: A review,â€International Journal of\\nEmerging Trends & Technology in Computer Science (IJETTCS), vol. 4,\\nno. 5, p. 2, 2015.\\n[4] J. Amin, M. Sharif, M. Raza, T. Saba, and M. A. Anjum, â€œBrain tumor\\ndetection using statistical and machine learning method,â€ Computer\\nmethods and programs in biomedicine, vol. 177, pp. 69â€“79, 2019.\\n[5] P. Sapra, R. Singh, and S. Khurana, â€œBrain tumor detection using neural\\nnetwork,â€ International Journal of Science and Modern Engineering\\n(IJISME) ISSN, pp. 2319â€“6386, 2013.\\n[6] H. Mohsen, E.-S. A. El-Dahshan, E.-S. M. El-Horbaty, and A.-B. M.\\nSalem, â€œClassification using deep learning neural networks for brain\\ntumors,â€ Future Computing and Informatics Journal, vol. 3, no. 1, pp.\\n68â€“71, 2018.\\n[7] H. E. M. Abdalla and M. Y . Esmail, â€œBrain tumor detection by using'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='tumors,â€ Future Computing and Informatics Journal, vol. 3, no. 1, pp.\\n68â€“71, 2018.\\n[7] H. E. M. Abdalla and M. Y . Esmail, â€œBrain tumor detection by using\\nartificial neural network,â€ in 2018 International conference on computer,\\ncontrol, electrical, and electronics engineering (ICCCEEE). IEEE,\\n2018, pp. 1â€“6.\\n[8] T. Hossain, F. S. Shishir, M. Ashraf, M. A. Al Nasim, and F. M. Shah,\\nâ€œBrain tumor detection using convolutional neural network,â€ in 2019\\n1st international conference on advances in science, engineering and\\nrobotics technology (ICASERT). IEEE, 2019, pp. 1â€“6.\\n[9] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, â€œYou only look\\nonce: Unified, real-time object detection,â€ in Proceedings of the IEEE\\nconference on computer vision and pattern recognition, 2016, pp. 779â€“\\n788.\\n[10] N. Gordillo, E. Montseny, and P. Sobrevilla, â€œState of the art survey on\\nmri brain tumor segmentation,â€ Magnetic resonance imaging, vol. 31,\\nno. 8, pp. 1426â€“1438, 2013.'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='788.\\n[10] N. Gordillo, E. Montseny, and P. Sobrevilla, â€œState of the art survey on\\nmri brain tumor segmentation,â€ Magnetic resonance imaging, vol. 31,\\nno. 8, pp. 1426â€“1438, 2013.\\n[11] S. Bauer, R. Wiest, L.-P. Nolte, and M. Reyes, â€œA survey of mri-based\\nmedical image analysis for brain tumor studies,â€ Physics in Medicine &\\nBiology, vol. 58, no. 13, p. R97, 2013.\\n[12] A. Wadhwa, A. Bhardwaj, and V . S. Verma, â€œA review on brain tumor\\nsegmentation of mri images,â€ Magnetic resonance imaging, vol. 61, pp.\\n247â€“259, 2019.\\n[13] A. Nazir and M. A. Wani, â€œYou only look once-object detection models:\\na review,â€ in 2023 10th International conference on computing for\\nsustainable global development (INDIACom). IEEE, 2023, pp. 1088â€“\\n1095.\\n[14] J. S. Paul, A. J. Plassard, B. A. Landman, and D. Fabbri, â€œDeep learning\\nfor brain tumor classification,â€ in Medical Imaging 2017: Biomedical\\nApplications in Molecular, Structural, and Functional Imaging, vol.\\n10137. SPIE, 2017, pp. 253â€“268.'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='for brain tumor classification,â€ in Medical Imaging 2017: Biomedical\\nApplications in Molecular, Structural, and Functional Imaging, vol.\\n10137. SPIE, 2017, pp. 253â€“268.\\n[15] L. Li, Y . Fan, M. Tse, and K.-Y . Lin, â€œA review of applications in\\nfederated learning,â€ Computers & Industrial Engineering, vol. 149, p.\\n106854, 2020.\\n[16] J. Wen, Z. Zhang, Y . Lan, Z. Cui, J. Cai, and W. Zhang, â€œA survey on\\nfederated learning: challenges and applications,â€ International Journal\\nof Machine Learning and Cybernetics, vol. 14, no. 2, pp. 513â€“535, 2023.\\n[17] P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N.\\nBhagoji, K. Bonawitz, Z. Charles, G. Cormode, R. Cummings et al.,\\nâ€œAdvances and open problems in federated learning,â€ Foundations and\\ntrendsÂ® in machine learning, vol. 14, no. 1â€“2, pp. 1â€“210, 2021.\\n[18] R. Rahman and D. C. Nguyen, â€œMultimodal federated learning with\\nmodel personalization,â€ in OPT 2024: Optimization for Machine Learn-\\ning, 2024.'),\n",
       " Document(metadata={'arxiv_id': '2503.04087v1', 'title': 'Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11', 'section': 'body', 'authors': 'Sheikh Moonwara Anjum Monisha, Ratun Rahman'}, page_content='[18] R. Rahman and D. C. Nguyen, â€œMultimodal federated learning with\\nmodel personalization,â€ in OPT 2024: Optimization for Machine Learn-\\ning, 2024.\\n[19] J. Cheng, W. Huang, S. Cao, R. Yang, W. Yang, Z. Yun, Z. Wang,\\nand Q. Feng, â€œEnhanced performance of brain tumor classification via\\ntumor region augmentation and partition,â€ PloS one, vol. 10, no. 10, p.\\ne0140381, 2015.'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'title_abstract', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='Title: Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning\\n\\nAbstract: In recent years, deep learning has shown great promise in the automated detection and classification of brain tumors from MRI images. However, achieving high accuracy and computational efficiency remains a challenge. In this research, we propose Deep Brain Net, a novel deep learning system designed to optimize performance in the detection of brain tumors. The model integrates the strengths of two advanced neural network architectures which are EfficientNetB0 and ResNet50, combined with transfer learning to improve generalization and reduce training time. The EfficientNetB0 architecture enhances model efficiency by utilizing mobile inverted bottleneck blocks, which incorporate depth wise separable convolutions. This design significantly reduces the number of parameters and computational cost while preserving the ability of models to learn complex feature representations. The ResNet50 architecture, pre trained on large scale datasets like ImageNet, is fine tuned for brain tumor classification. Its use of residual connections allows for training deeper networks by mitigating the vanishing gradient problem and avoiding performance degradation. The integration of these components ensures that the proposed system is both computationally efficient and highly accurate. Extensive experiments performed on publicly available MRI datasets demonstrate that Deep Brain Net consistently outperforms existing state of the art methods in terms of classification accuracy, precision, recall, and computational efficiency. The result is an accuracy of 88 percent, a weighted F1 score of 88.75 percent, and a macro AUC ROC score of 98.17 percent which demonstrates the robustness and clinical potential of Deep Brain Net in assisting radiologists with brain tumor diagnosis.'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='DeepBrainNet: An Optimized Deep Learning\\nModel for Brain tumor Detection in MRI\\nImages Using EfficientNetB0 and ResNet50\\nwith Transfer Learning\\n1st Daniel Onah\\nDepartment of information studies\\nUniversity College London\\nLondon, United Kingdom\\n0000-0001-6192-6702\\n2nd Ravish Desai\\nDepartment of information studies\\nUniversity College London\\nLondon, United Kingdom\\n0009-0000-7610-1412\\nAbstractâ€”In recent years, deep learning has shown\\ngreat promise in the automated detection and classifica-\\ntion of brain tumors from MRI images. However, achiev-\\ning high accuracy and computational efficiency remains\\na challenge. In this research, we propose DeepBrainNet,\\na novel deep learning system designed to optimize per-\\nformance in the detection of brain tumors. The model\\nintegrates the strengths of two advanced neural network\\narchitectures - EfficientNetB0 and ResNet50, combined\\nwith transfer learning to improve generalization and\\nreduce training time. The EfficientNetB0 architecture en-'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='architectures - EfficientNetB0 and ResNet50, combined\\nwith transfer learning to improve generalization and\\nreduce training time. The EfficientNetB0 architecture en-\\nhances model efficiency by utilizing mobile inverted bot-\\ntleneck (MBConv) blocks, which incorporate depthwise\\nseparable convolutions. This design significantly reduces\\nthe number of parameters and computational cost while\\npreserving the modelâ€™s ability to learn complex feature\\nrepresentations. The ResNet50 architecture, pre-trained\\non large-scale datasets like ImageNet, is fine-tuned for\\nbrain tumor classification. Its use of residual connections\\nallows for training deeper networks by mitigating the\\nvanishing gradient problem and avoiding performance\\ndegradation.The integration of these components ensures\\nthat the proposed system is both computationally efficient\\nand highly accurate. Extensive experiments performed\\non publicly available MRI datasets demonstrate that\\nDeepBrainNet consistently outperforms existing state-of-'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='and highly accurate. Extensive experiments performed\\non publicly available MRI datasets demonstrate that\\nDeepBrainNet consistently outperforms existing state-of-\\nthe-art methods in terms of classification accuracy, preci-\\nsion, recall, and computational efficiency. The resultsâ€”an\\naccuracy of 88%, a weighted F1-score of 88.75%, and\\na macro AUC-ROC score of 98.17%â€”demonstrate the\\nrobustness and clinical potential of DeepBrainNet in\\nassisting radiologists with brain tumor diagnosis.\\nIndex Termsâ€”DeepBrainNet, Brain tumor Detection,\\nEfficientNetB0, ResNet50, Transfer Learning, Depth-\\nWise Separable Convolutions, Medical Imaging.\\nI. I NTRODUCTION\\nBrain tumor pose a significant health challenge,\\nand early detection is critical for improving patient\\nprognosis and treatment outcomes. Given their ag-\\ngressive nature and potential for rapid progression,\\nbrain tumor demand early and accurate detection to\\noptimize clinical treatment regimen. Magnetic Reso-'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='gressive nature and potential for rapid progression,\\nbrain tumor demand early and accurate detection to\\noptimize clinical treatment regimen. Magnetic Reso-\\nnance Imaging (MRI) is the preferred imaging tech-\\nnique due to its superior soft tissue contrast and non-\\ninvasive nature, making it essential for the detection\\nand characterization of brain tumor. However, the com-\\nplexity and variability of magnetic resonance images\\npresent challenges for radiologists, who often face\\ndifficulties in making quick and accurate diagnoses\\ndue to the intricate nature of brain structures and\\ntumor characteristics. Manual interpretation of these\\nimages is time-consuming and subject to human error,\\nwhich underscores the need for automated systems that\\ncan aid in accurate brain tumor detection [1]. Deep\\nlearning (DL), particularly Convolutional Neural Net-\\nworks (CNNs), has revolutionized image classification\\ntasks, including medical imaging, by demonstrating'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='learning (DL), particularly Convolutional Neural Net-\\nworks (CNNs), has revolutionized image classification\\ntasks, including medical imaging, by demonstrating\\nthe ability to extract complex features from images\\nand outperform traditional machine learning methods.\\nHowever, the application of deep learning in brain\\ntumor detection still faces significant challenges such\\nas model efficiency, computational cost, and over-\\nfitting due to small medical datasets. In this study,\\nwe propose DeepBrainNet, a novel deep learning ar-\\nchitecture specifically designed to overcome current\\ndiagnostic challenges and achieve high accuracy in\\nbrain tumor classification. DeepBrainNet integrates\\nEfficientNetB0â€”a lightweight architecture that em-\\nploys depthwise separable convolutions to minimize\\ncomputational complexityâ€”and ResNet50, a deeper\\nnetwork that utilizes residual connections to mitigate\\nperformance degradation in very deep models. By\\ncombining EfficientNetB0â€™s computational efficiency'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='network that utilizes residual connections to mitigate\\nperformance degradation in very deep models. By\\ncombining EfficientNetB0â€™s computational efficiency\\nwith ResNet50â€™s depth and stability, DeepBrainNet\\nleverages the strengths of both architectures to enhance\\nbrain tumor classification performance. This hybrid\\napproach combines the efficiency of EfficientNetB0\\nwith the power of ResNet50, optimized through trans-\\narXiv:2507.07011v1  [eess.IV]  9 Jul 2025fer learning to improve generalization and reduce\\ntraining time [2], [3]. The model leverages pre-trained\\nResNet50 features, originally learned from large-scale\\ndatasets such as ImageNet, and fine-tunes them for\\nthe specific task of brain tumor detection in MRI\\nscans. The depth-wise separable convolutions in Ef-\\nficientNetB0 further reduce the number of parameters,\\nallowing the system to maintain high accuracy without\\nsacrificing speed and memory efficiency. Figure 1\\ndepicts the various brain tumor categoriesâ€”such as'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='allowing the system to maintain high accuracy without\\nsacrificing speed and memory efficiency. Figure 1\\ndepicts the various brain tumor categoriesâ€”such as\\nmeningioma, glioma, and pituitary tumorâ€”that can be\\ndetected from MRI scans.\\nFig. 1: Classes of Brain tumor from MRI images [2]\\nThe objectives of DeepBrainNet are:\\nâ€¢ To develop an optimized deep learning model\\n(DeepBrainNet) for brain tumor detection in MRI\\nimages, integrating EfficientNetB0 with depth-\\nwise separable convolutions and ResNet50 with\\ntransfer learning to improve accuracy and com-\\nputational efficiency.\\nâ€¢ To benchmark the performance of DeepBrainNet\\non publicly available MRI datasets, comparing\\nits classification accuracy, precision, recall, and\\ncomputational efficiency against existing state-of-\\nthe-art models for brain tumor detection.\\nâ€¢ To investigate the impact of transfer learning and\\ndepth-wise separable convolutions in enhancing\\nmodel performance, reducing overfitting, and im-'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='the-art models for brain tumor detection.\\nâ€¢ To investigate the impact of transfer learning and\\ndepth-wise separable convolutions in enhancing\\nmodel performance, reducing overfitting, and im-\\nproving generalization, with the aim of creating\\na reliable and efficient model for clinical applica-\\ntions.\\nThe proposed methodology addresses the critical\\nchallenges of computational efficiency and accuracy\\nin medical AI by leveraging optimized deep learning\\ntechniques to enable faster and more reliable brain\\ntumor detection. Extensive experiments on publicly\\navailable MRI datasets demonstrate that DeepBrainNet\\nsubstantially outperforms existing models in accuracy,\\nprecision, recall and f1-score. These results highlight\\nits potential as a valuable tool for clinical decision\\nsupport systems, advancing the state of the art in\\nmedical AI.\\nII. B ACKGROUND WORK\\nThe application of deep learning (DL) techniques\\nfor the detection and classification of brain tumor'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='support systems, advancing the state of the art in\\nmedical AI.\\nII. B ACKGROUND WORK\\nThe application of deep learning (DL) techniques\\nfor the detection and classification of brain tumor\\nfrom MRI images has garnered substantial interest\\nwithin the medical imaging community. Recent re-\\nsearch has focused on optimizing existing models\\nand introducing novel architectures to improve both\\naccuracy and computational efficiency. This literature\\nreview summarizes some of the recent advancements\\nin the field, highlighting key datasets, feature selection\\ntechniques, models used, and the conclusions drawn\\nfrom each study.\\nThe authors Amin and et. al. investigated the use of\\nEfficientNetB1 for classification alongside U-Net for\\nsegmentation, integrating both models to effectively\\naddress tumor detection and localization in brain MRI\\nscans.The study, conducted in 2023, employed the\\nKaggle brain MRI datasetâ€”a widely used repository\\ncontaining diverse brain MRI imagesâ€”for training and'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='scans.The study, conducted in 2023, employed the\\nKaggle brain MRI datasetâ€”a widely used repository\\ncontaining diverse brain MRI imagesâ€”for training and\\nevaluating the proposed model. The study did not\\nemploy explicit feature selection techniques, instead\\nleveraging the inherent capabilities of the Efficient-\\nNetB1 architecture to effectively process MRI input\\nimages. EfficientNetB1 is recognized for its computa-\\ntional efficiency and high classification accuracy, while\\nthe U-Net model excels in pixel-wise segmentation.\\nThe results demonstrated that the combined approach\\nachieved high classification accuracy and robust seg-\\nmentation performance, highlighting the effectiveness\\nof integrating classification and segmentation tasks\\ninto a unified model for brain tumor detection [4].\\nIn [5] the researchers proposed a customized multi-\\nlayer Convolutional Neural Network (CNN) to clas-\\nsify brain tumors from MRI images. The model was\\nevaluated using a dataset of 7,023 brain MRI images'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='layer Convolutional Neural Network (CNN) to clas-\\nsify brain tumors from MRI images. The model was\\nevaluated using a dataset of 7,023 brain MRI images\\nsourced from multiple repositories, including figshare,\\nSARTAJ, and Br35H. No explicit feature selection\\ntechnique was applied in this study. The CNN archi-\\ntecture, specifically designed for this task, achieved an\\nimpressive tumor classification accuracy of 99%. This\\nhigh level of accuracy demonstrates the potential of\\ncustom-built CNNs tailored for specific medical image\\nanalysis tasks, highlighting their ability to learn deep\\nfeatures from the MRI data and provide precise tumor\\nclassification.\\nAnother study [6] introduced a Stacked Ensem-\\nble Transfer Learning (SETL-BMRI) approach, which\\ncombined two well-established pre-trained deep learn-\\ning modelsâ€”AlexNet and VGG19â€”for the classifi-\\ncation of brain tumor in MRI scans. This method\\nincorporated data-augmented feature selection, which'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='ing modelsâ€”AlexNet and VGG19â€”for the classifi-\\ncation of brain tumor in MRI scans. This method\\nincorporated data-augmented feature selection, which\\nenhances model robustness by artificially expanding\\nthe training dataset and improving the modelâ€™s gener-\\nalization capability. The model was trained and evalu-\\nated on a Kaggle dataset consisting of brain MRI im-\\nages with labeled tumor types, including meningioma,\\nglioma, and pituitary tumors. The results showed\\nthat the model achieved a classification accuracy of98.70%, with an average precision of 98.75%, recall\\nof 98.6%, and an F1-score of 98.75%. These find-\\nings underscore the effectiveness of transfer learn-\\ning combined with ensemble techniques, as the pre-\\ntrained modelsâ€”AlexNet and VGG19â€”significantly\\ncontribute to robust performance in medical image\\nclassification tasks. Additionally, the use of data aug-\\nmentation proved beneficial in addressing challenges\\nsuch as class imbalance, improving the modelâ€™s gen-'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='classification tasks. Additionally, the use of data aug-\\nmentation proved beneficial in addressing challenges\\nsuch as class imbalance, improving the modelâ€™s gen-\\neralization ability.\\nThe integration of state-of-the-art architec-\\nturesâ€”such as EfficientNetB1, U-Net, AlexNet, and\\nVGG19â€”alongside techniques like transfer learning,\\ndata augmentation, and customized convolutional\\nneural networks (CNNs), has demonstrated significant\\npotential in enhancing classification accuracy\\nand segmentation performance in brain tumor\\nanalysis. These approaches not only enhance model\\nperformance but also contribute to the broader goal\\nof making brain tumor detection more efficient and\\nreliable in clinical settings.The advancements in\\nthis domain highlight the growing potential of deep\\nlearning models to support medical professionals in\\nachieving accurate and timely diagnoses of brain\\ntumor, ultimately contributing to improved patient\\noutcomes.\\nTABLE I: Related study on Brain MRI Detection'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='achieving accurate and timely diagnoses of brain\\ntumor, ultimately contributing to improved patient\\noutcomes.\\nTABLE I: Related study on Brain MRI Detection\\nPaper Dataset Feature\\nSelec-\\ntion\\nTech-\\nnique\\nModel Conclusion\\n[4] Kaggle\\ndataset\\n(7,023\\nbrain MRI\\nimages from\\nfigshare,\\nSARTAJ,\\nand Br35H)\\n- Efficient\\nNetB1\\nfor\\nclas-\\nsifica-\\ntion;\\nU-Net\\nfor\\nseg-\\nmenta-\\ntion\\nTesting\\naccuracy of\\n99.39\\n[5] Kaggle\\ndataset\\n(7,023\\nbrain MRI\\nimages from\\nfigshare,\\nSARTAJ,\\nand Br35H)\\n- Convo-\\nlu-\\ntional\\nNeural\\nNet-\\nwork\\n(CNN)\\ntumor clas-\\nsification\\naccuracy of\\n99%\\n[6] Kaggle\\ndataset\\n(7,023\\nbrain MRI\\nimages from\\nfigshare,\\nSARTAJ,\\nand Br35H)\\nData\\naug-\\nmented\\nfeature\\nselection\\nStack\\nEn-\\nsemble\\nTrans-\\nfer\\nLearn-\\ning\\n(SETL-\\nBMRI)\\ncom-\\nbining\\nAlexNet\\nand\\nVGG19\\nClassification\\naccuracy\\nof 98.70%,\\naverage\\nprecision\\nof 98.75%,\\nrecall of\\n98.6%, and\\nF1-score of\\n98.75%\\nFig. 2: Architecture Diagram of DeepBrainNet\\nIII. M ATERIALS AND METHODS\\nThis section presents the materials and methods'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='average\\nprecision\\nof 98.75%,\\nrecall of\\n98.6%, and\\nF1-score of\\n98.75%\\nFig. 2: Architecture Diagram of DeepBrainNet\\nIII. M ATERIALS AND METHODS\\nThis section presents the materials and methods\\nutilized in conducting the study.\\nA. Dataset\\nThe dataset used in this research is a combination of\\nthree different datasets: figshare, SARTAJ, and Br35H.\\nIt contains a total of 7023 human brain MRI images\\nthat are classified into four distinct categories: glioma,\\nmeningioma, no tumor, and pituitary tumor. These\\nimages are used to detect brain tumors and classify\\nthem based on their type. The no tumor class images\\nwere taken from the Br35H dataset. Some issues were\\nidentified with the SARTAJ dataset, particularly with\\nincorrect categorization of glioma images, and thus,\\nthose images were removed from the dataset. The final\\ndataset used for this study consists of images from\\nfigshare, which were found to be accurately catego-\\nrized.Brain tumor can be either malignant (cancerous)'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='dataset used for this study consists of images from\\nfigshare, which were found to be accurately catego-\\nrized.Brain tumor can be either malignant (cancerous)\\nor benign (noncancerous), yet both types may lead\\nto elevated intracranial pressure, potentially causing\\ndamage to brain tissue. Early detection and accu-\\nrate classification are crucial for guiding appropriate\\ntreatment strategies and improving the likelihood of\\nsuccessful clinical treatment regimen and outcomes.\\nThe goal of the dataset is to facilitate the automation\\nof brain tumor detection and classification, thereby\\nassisting healthcare professionals in making timely\\nand accurate clinical decisions. This research aims\\nto utilize DL models for the detection, classification,\\nand localization of tumors from MRI scans. Table II\\nTABLE II: Metadata of the dataset\\nAttribute Description\\nTotal Images 7023 images\\nClasses Glioma, Meningioma, No tu-\\nmor, Pituitary tumor\\nImage Source figshare, SARTAJ, Br35H\\nLicense CC0: Public Domain'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='Attribute Description\\nTotal Images 7023 images\\nClasses Glioma, Meningioma, No tu-\\nmor, Pituitary tumor\\nImage Source figshare, SARTAJ, Br35H\\nLicense CC0: Public Domain\\nTags Health, Cancer, Image, Deep\\nLearning, Multiclass Classi-\\nfication, Medicine, Transfer\\nLearning\\npresents the metadata of the dataset. Figure 3 depicts\\nthe spread of class distribution in the dataset.Fig. 3: bar chart of distribution of tumor images in dataset\\nB. Data Pre-processing\\n1) Image Resizing: The MRI images in the dataset\\nmay vary in size. It is essential to resize them to a\\nconsistent dimension as input into the deep learning\\nmodels. This resizing ensures that all images conform\\nto the required input size of the model; in addition, this\\nhelps in improving both the computational efficiency\\nand the model performance.\\nâ€¢ Resizing Strategy : All images are resized to a\\nsquare shape of 224x224 pixels. This dimension\\nis chosen to balance between preserving essential\\nimage features and ensuring efficient computation'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='â€¢ Resizing Strategy : All images are resized to a\\nsquare shape of 224x224 pixels. This dimension\\nis chosen to balance between preserving essential\\nimage features and ensuring efficient computation\\nfor model training.\\nâ€¢ Resizing Function : The images are resized us-\\ning bilinear interpolation to minimize information\\nloss during the transformation process.The equa-\\ntion below illustrates the mathematical formula-\\ntion of the image resizing function (equation 1):\\nIresized = f(Ioriginal, Dnew) (1)\\nWhere:\\nâ€¢ f is the resizing function (e.g., bilinear interpo-\\nlation or nearest-neighbor interpolation).\\nâ€¢ Original image refers to the image before resiz-\\ning.\\nâ€¢ New Dimensions refers to the target size, e.g.\\n224x224 or 256x256.\\n2) Image Normalization: Normalization is used to\\nscale the values of the pixels in a smaller range,\\ntypically [0, 1] or [-1, 1], which helps accelerate\\nthe training process by stabilizing the gradient during\\nbackpropagation. The pixel values in the MRI images'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='typically [0, 1] or [-1, 1], which helps accelerate\\nthe training process by stabilizing the gradient during\\nbackpropagation. The pixel values in the MRI images\\ntypically range from 0 to 255. The equation below\\nillustrates the mathematical formulation to normalize\\nthe pixel values (equation 2):\\nPnormalized = Poriginal\\n255 (2)\\n3) Data Augmentation: Data augmentation tech-\\nniques are applied to artificially increase the size of\\nthe training dataset and help the model generalize bet-\\nter. Augmentation involves applying transformations\\nsuch as rotation, flipping, zooming, and shifting to\\nthe original images. This helps simulate real-world\\nvariations in MRI scans, such as changes in patient\\npositioning or scanning conditions. Below are some\\ncommon transformations applied:\\nâ€¢ Rotation: Random rotations are applied to the\\nimages in a range of -30 â—¦ to +30â—¦. This simulates\\nvariations in the orientation of MRI scans.\\nâ€¢ Flipping: Random horizontal and vertical flips'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='â€¢ Rotation: Random rotations are applied to the\\nimages in a range of -30 â—¦ to +30â—¦. This simulates\\nvariations in the orientation of MRI scans.\\nâ€¢ Flipping: Random horizontal and vertical flips\\nare applied to the images to simulate different\\nperspectives of the tumor.\\nâ€¢ Zooming: Random zooming in and out of the\\nimage is applied to account for variations in tumor\\nscale on the MRI scans.\\nâ€¢ Shifting: Random translations of the image along\\nthe x and y axes are applied to simulate slight\\nchanges in the positioning of the brain in the scan.\\nThese transformations are randomly applied to each\\nimage during the training process to generate a more\\ndiverse set of images, improving the modelâ€™s ability to\\nhandle real-world variations in MRI data.\\n4) Image Cropping (Margin Removal): In MRI\\nimages, there may be unnecessary borders or margins\\naround the region of interest, which is primarily the\\nbrain. To focus the modelâ€™s attention on the brain\\nregion, these margins are removed through cropping.'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='around the region of interest, which is primarily the\\nbrain. To focus the modelâ€™s attention on the brain\\nregion, these margins are removed through cropping.\\nThis pre-processing step helps improve model effi-\\nciency and ensures that only relevant features (the\\nbrain and tumor regions) are considered during train-\\ning.\\nâ€¢ Cropping Strategy: The images are cropped to\\nfocus on the brain region, removing any non-\\nrelevant background. The cropping is done by\\nselecting the region of interest within the image\\nusing specific xstart, ystart, xend, and yend coordi-\\nnates (equation 3).\\nCropped Image = Crop(Original Image, xstart, ystart, xend, yend)\\n(3)\\nC. Image Enhancement\\nImage blurring has been used to reduce image\\ndetail and suppress high-frequency noise, defined as\\n(equation 4):\\nB(x, y) = 1\\nmn\\nX\\ni,j\\nI(x + i, y+ j) (4)\\nwhere I(x, y) is the pixel intensity at location (x, y)\\nand m Ã— n is the filter kernel size. Contrast Limited\\nAdaptive Histogram Equalization (CLAHE) is then ap-'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='mn\\nX\\ni,j\\nI(x + i, y+ j) (4)\\nwhere I(x, y) is the pixel intensity at location (x, y)\\nand m Ã— n is the filter kernel size. Contrast Limited\\nAdaptive Histogram Equalization (CLAHE) is then ap-\\nplied to locally enhance image contrast by computing\\nhistograms in small image tiles and redistributing pixel\\nintensities while limiting noise amplification [7]. This\\nenhances local structures without over-amplifying ho-\\nmogeneous regions. Histogram Equalization globally\\nredistributes the pixel intensity distribution to flattenthe histogram and increase global contrast [8]. The\\ntransformation function used is (equation 5):\\nsk = T(rk) =L âˆ’ 1\\nMN\\nkX\\nj=0\\nn(j) (5)\\nwhere L is the number of intensity levels, MN is\\nthe total number of pixels, and n(j) is the number\\nof pixels with intensity rk. These enhancement steps\\nensure that the DeepBrainNet model receives clearer,\\nhigh-contrast inputs, leading to more accurate feature\\nextraction and improved tumor classification perfor-\\nmance.'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='ensure that the DeepBrainNet model receives clearer,\\nhigh-contrast inputs, leading to more accurate feature\\nextraction and improved tumor classification perfor-\\nmance.\\nD. Feature Selection (Fuzzy C)\\nIn this study, fuzzy C-Means (FCM) clustering is\\nused to select the most relevant features of the MRI im-\\nages. FCM is an unsupervised learning algorithm that\\naddresses the inherent uncertainty often encountered in\\nmedical images. Unlike traditional clustering methods,\\nFCM assigns each feature a degree of membership in\\nmultiple clusters, rather than strictly categorizing it\\ninto one cluster[9]. This characteristic is particularly\\nadvantageous in medical imaging, where the bound-\\naries between the tumor and non-tumor regions are\\noften ambiguous. Through this method, features with\\nhigher membership values are deemed more signifi-\\ncant for classification and segmentation tasks, while\\nfeatures with lower membership values are discarded.\\nFollowing the pre-processing steps, including image'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='cant for classification and segmentation tasks, while\\nfeatures with lower membership values are discarded.\\nFollowing the pre-processing steps, including image\\nresizing and cropping, FCM is applied to the images\\nto select the most relevant features. This reduces the di-\\nmensionality of the data set and ensures that the model\\nfocuses on the most important features for accurate\\ntumor detection. The fuzzy C-Means algorithm, which\\noutlines the steps for clustering features and selecting\\nthose with high fuzzy memberships, is detailed in\\nAlgorithm 1 below.\\nFurthermore, the architecture of the fuzzy C-Means\\nfeature selection process is illustrated in Figure 4,\\nshown below, which provides a visual representation\\nof how features are selected based on their fuzzy mem-\\nbership across clusters. This approach improves the ro-\\nbustness of feature selection, particularly in scenarios\\nwhere tumor boundaries are not clearly delineated and\\nrequire a more flexible classification methodology.\\nIV. M ODELS'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='bustness of feature selection, particularly in scenarios\\nwhere tumor boundaries are not clearly delineated and\\nrequire a more flexible classification methodology.\\nIV. M ODELS\\nA. EfficientNetB0 with depth wise separate convolu-\\ntions\\nEfficientNetB0 is a DL architecture that combines\\nthe benefits of both EfficientNet and VNet to effi-\\nciently detect and segment brain tumors from MRI im-\\nages. EfficientNet, known for its superior performance\\nin image classification, is used for feature extraction,\\nproviding a lightweight yet powerful backbone that\\nreduces computational complexity without sacrificing\\naccuracy. VNet, a fully convolutional neural network\\ndesigned for 3D image segmentation, is utilized to\\nAlgorithm 1 Fuzzy C Feature Selection Algorithm [9]\\n1: Input: X, c, mi, mf , Ïµ, T\\n2: Output: U and V\\n3: Initial condition:\\n4: V 0 = [v1, v2, . . . , vc], vj âˆˆ Cj\\n5: for t = 1to T do\\n6: m = mi + t(mf âˆ’mi)\\nT\\n7: b = âˆ’ 1\\nmâˆ’1\\n8: Calculate membership:\\n9: U1 = [uij] where 1 â‰¤ i â‰¤ n and1 â‰¤ j â‰¤'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='3: Initial condition:\\n4: V 0 = [v1, v2, . . . , vc], vj âˆˆ Cj\\n5: for t = 1to T do\\n6: m = mi + t(mf âˆ’mi)\\nT\\n7: b = âˆ’ 1\\nmâˆ’1\\n8: Calculate membership:\\n9: U1 = [uij] where 1 â‰¤ i â‰¤ n and1 â‰¤ j â‰¤\\nc, by using\\nuij = db(xi, vj)Pc\\nk=1 db(xi, vk), 1 â‰¤ i â‰¤ n, 1 â‰¤ j â‰¤ c\\n10: Update cluster center:\\n11: V t = [v1, v2, . . . , vc], where\\nvj =\\nPn\\ni=1 um\\nij xi\\nPn\\ni=1 um\\nij\\n, j = 1, 2, . . . , c\\n12: If E = Pc\\nj=1 k(vjt âˆ’ vjtâˆ’1) â‰¤ Ïµ, stop, else\\nt = t + 1\\n13: end for\\nFig. 4: Fuzzy C means Feature Selection Architecture [10]\\nprecisely segment the tumor region from the MRI\\nscans. The combination of EfficientNetâ€™s efficient fea-\\nture extraction with VNetâ€™s robust segmentation ca-\\npabilities makes EfficientNetB0 an ideal architecture\\nfor both classification and segmentation tasks. Using\\nthe strengths of both models, EfficientNetB0 provides\\na high degree of accuracy in detecting and localizing\\nbrain tumors, ensuring better performance and faster\\nprocessing compared to traditional models [11]. Figure'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='a high degree of accuracy in detecting and localizing\\nbrain tumors, ensuring better performance and faster\\nprocessing compared to traditional models [11]. Figure\\n5 presents the efficient V-net architecture.\\nFig. 5: Efficient V-net Architecture [11]B. ResNet with Transfer Learning for tumor Classifi-\\ncation\\nResNet, or Residual Network, is a DL model that\\nutilizes residual connections to combat the vanishing\\ngradient problem in very deep networks. This feature\\nallows ResNet to achieve high accuracy even in very\\ndeep architectures. In the context of brain tumor de-\\ntection, ResNet with Transfer Learning is an effective\\napproach where the model is initially pre-trained on\\na large, general dataset (such as ImageNet) and fine-\\ntuned on the MRI brain tumor dataset. Transfer learn-\\ning enables the model to take advantage of learned\\nfeatures from a large and diverse dataset, significantly\\nreducing the amount of data and computational re-\\nsources needed for training. By applying ResNet with'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='features from a large and diverse dataset, significantly\\nreducing the amount of data and computational re-\\nsources needed for training. By applying ResNet with\\nTransfer Learning, the model can quickly adapt to the\\nspecific task of brain tumor classification, resulting\\nin high accuracy and generalization to unseen MRI\\nimages as illustrated in Figure 6.\\nFig. 6: Transfer Learning on Pretrained Resnet Model [12]\\nC. Evaluation Metrics\\nThis subsection provides a comprehensive evalua-\\ntion of the modelâ€™s performance using various metrics,\\nincluding classification report, confusion matrix, F1\\nscore, and AUC-ROC score. These metrics offer a\\ndeeper understanding of the effectiveness of the model\\nin classifying tumor types and provide detailed infor-\\nmation on the overall performance of the model.\\nâ€¢ Precision indicates the proportion of true positive\\npredictions for each class out of all predicted\\npositives (equation 6).\\nPrecision = T P\\nT P+ F P (6)'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='â€¢ Precision indicates the proportion of true positive\\npredictions for each class out of all predicted\\npositives (equation 6).\\nPrecision = T P\\nT P+ F P (6)\\nâ€¢ Recall reflects the proportion of true positives\\nidentified out of all actual positives in the dataset\\n(equation 7).\\nRecall = T P\\nT P+ F N (7)\\nâ€¢ F1 score is the harmonic mean of precision\\nand recall, balancing the trade-off between them\\n(equation 8).\\nF1 = 2âˆ— Precision âˆ— Recall\\nPrecision + Recall (8)\\nV. R ESULTS\\nThis section presents the results of the experimen-\\ntation. The proposed hybrid deep learning model,\\ncomprising ResNet50 and EfficientNetB0 as parallel\\nbackbone architectures, was trained using a trans-\\nfer learning approach. All images were initially in\\ngrayscale and were converted to RGB by channel\\nstacking to meet the input requirements of the pre-\\ntrained models. The input images were resized to\\n224 Ã— 224 pixels, and model training was performed\\nover 40 epochs with a batch size of 32, using the'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='trained models. The input images were resized to\\n224 Ã— 224 pixels, and model training was performed\\nover 40 epochs with a batch size of 32, using the\\nAdam optimizer.To enhance generalization and reduce\\noverfitting, extensive on-the-fly data augmentation was\\nperformed using Kerasâ€™ ImageDataGenerator. The aug-\\nmentation pipeline included random rotations, horizon-\\ntal flips, zooming, shear transformations, brightness\\nvariation, and spatial shifts. A custom preprocessing\\nfunction was implemented to standardize the input\\nusing EfficientNetâ€™s normalization scheme. Figure 7\\npresents the exploratory data analysis on raw data.\\n(a) Pixel Intensity Distribution.\\n(b) Non-Background Pixel Distribu-\\ntion.\\nFig. 7: Exploratory data analysis on Raw Data.\\nThe models are regularized using dropout layers\\nand further stabilized with early stopping based on\\nvalidation loss, along with dynamic learning rate re-\\nduction. Throughout training, the model demonstrated'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='and further stabilized with early stopping based on\\nvalidation loss, along with dynamic learning rate re-\\nduction. Throughout training, the model demonstrated\\nstable convergence and consistent performance across\\nvalidation metrics. convergence with a progressive\\nincrease in both training and validation accuracy.\\nThe highest training accuracy achieved approximately\\n98.3%, while the best validation accuracy reached\\n94.7%. The minimum validation loss recorded was\\napproximately 0.21. These results indicate effective\\nlearning with no significant signs of overfitting, sup-\\nported by the close alignment of the training and\\nvalidation curves. Canny edge detection is a popular\\ntechnique used to detect sharp changes in intensity,\\nhighlighting object boundaries in an image. It involves\\nnoise reduction, gradient calculation, and edge refine-\\nment through thresholding. This method is used in the\\nstudy to enhance edge clarity, enabling more accurate'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='noise reduction, gradient calculation, and edge refine-\\nment through thresholding. This method is used in the\\nstudy to enhance edge clarity, enabling more accurate\\nfeature extraction and analysis. Figure 8 presents the\\nedge detection visualization in the data samples. The\\ncomplete model comprises approximately 24 million\\ntrainable parameters. Training and validation perfor-\\nmance trends are illustrated in Figure 9, which showsaccuracy and loss curves over the course of training.\\nThe model achieved a training accuracy of 95.5%\\nFig. 8: Visualization of Canny Edge Detection on Raw Samples of\\nData\\nFig. 9: Training and validation accuracy and loss curves\\nand a validation accuracy of 93.2%, which indicates\\nstrong performance. The training loss of 0.112 and\\nthe validation loss of 0.145 further suggest that the\\nmodel is fitting the data effectively without overfitting.\\nThe classification report, shown in Table III below,\\nsummarizes the modelâ€™s performance in each class'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='model is fitting the data effectively without overfitting.\\nThe classification report, shown in Table III below,\\nsummarizes the modelâ€™s performance in each class\\nusing three key metrics: precision, recall and F1 score.\\nThese metrics allow us to assess the modelâ€™s ability to\\ncorrectly identify instances of each tumor type while\\nminimizing errors. The system achieved an overall\\naccuracy score of 88.\\nTABLE III: Classification Report of the System\\ntumor Class Precision Recall F1-Score\\nGlioma tumor 0.914 0.932 0.923\\nMeningioma tumor 0.819 0.798 0.808\\nNo tumor 0.946 0.875 0.909\\nPituitary tumor 0.868 0.945 0.905\\nA heat map in Figure 10, illustrates the precision,\\nrecall, and F1-score for each class (glioma, menin-\\ngioma, no tumor, pituitary), along with the accuracy,\\nmacro average, and weighted average metrics\\nThe classification report demonstrates that the model\\nperforms well across all classes, with an average\\nF1 score of 0.88 indicating a good balance between'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='macro average, and weighted average metrics\\nThe classification report demonstrates that the model\\nperforms well across all classes, with an average\\nF1 score of 0.88 indicating a good balance between\\nprecision and recall. The confusion matrix for the\\nmodel predictions on the validation set is presented\\nin Figure 11, providing a comprehensive view of the\\nperformance of the classifier in all tumor classes. The\\nmatrix shows the count of true positive, false positive,\\ntrue negative, and false negative predictions for each\\nclass. From the matrix, it is clear that the model has\\nhigh specificity and sensitivity for each tumor type.\\nFig. 10: Classification Report Metrics Visualization\\nFig. 11: Confusion Matrix\\nThe weighted F1-score across all classes, weighted by\\nthe number of true instances per class, is 0.88. This\\nvalue indicates a well-balanced model that performs\\nstrongly in both precision and recall, making it well-\\nsuited for multi-class classification tasks. Additionally,'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='value indicates a well-balanced model that performs\\nstrongly in both precision and recall, making it well-\\nsuited for multi-class classification tasks. Additionally,\\nthe AUC-ROC score provides insight into the modelâ€™s\\nability to distinguish between classes across different\\nthreshold values, offering a comprehensive measure of\\nclassification performance. The macro-average AUC-\\nROC score is 0.98, indicating excellent classification\\nperformance. Values close to 1 suggest that the model\\nhas a strong ability to discriminate between different\\ntumor types. The ROC curve plots, shown in Figure\\n12, the true positive rate versus the false positive rate\\nfor each class, providing an evaluation of the discrim-\\ninatory power of the model at various thresholds. The\\nAUC (Area Under the Curve) quantifies this ability,\\nwith higher values indicating superior performance.\\nEach curve in the plot represents the performance\\nof the model for a specific class. The AUC for each'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='with higher values indicating superior performance.\\nEach curve in the plot represents the performance\\nof the model for a specific class. The AUC for each\\nclass is displayed in the legend, and values closer to\\n1 indicate that the model is effectively distinguishing\\nbetween classes. The high AUC values confirm that theFig. 12: ROC Curve for Multi-Class Classification (One-vs-Rest)\\nmodel performs well in separating the types of tumor.\\nThese evaluation metrics provide a thorough analysis\\nof the model performance. The model demonstrates\\nstrong performance across all tumor classes, with\\nhigh precision, recall, and F1 scores, as well as an\\nimpressive AUC-ROC score. These results confirm\\nthe modelâ€™s ability to generalize well to unseen data.\\nThe correctly predicted examples for each class and a\\nselection of random predictions are presented in Figure\\n13. The purpose of this visualization is to evaluate\\nthe modelâ€™s ability to classify individual instances'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='selection of random predictions are presented in Figure\\n13. The purpose of this visualization is to evaluate\\nthe modelâ€™s ability to classify individual instances\\nand identify areas where it may be making errors or\\nuncertainties.\\nFig. 13: Correctly Predicted Example for Each Class\\nEach image in the plot corresponds to a different\\nclass, with the title indicating both the actual label\\nand the predicted label. In cases where the model\\nhas correctly predicted the label, we visualise a set\\nof random predictions made by the model, in Figure\\n14, which include both correct and incorrect classifi-\\ncations. This step provides insight into how well the\\nmodel generalizes to new data and where it may be\\nmaking mistakes in distinguishing between classes.\\nFig. 14: Random Predictions\\nVI. C ONCLUSION\\nIn conclusion, the developed model demonstrates\\nstrong performance in classifying brain tumors from\\nMRI images, achieving high accuracy and reliable\\ngeneralization to unseen data. The combination of'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='In conclusion, the developed model demonstrates\\nstrong performance in classifying brain tumors from\\nMRI images, achieving high accuracy and reliable\\ngeneralization to unseen data. The combination of\\nResNet50 and EfficientNetB0 architectures, along with\\ndata augmentation techniques, has proven effective\\nin improving the robustness of the model. Although\\nDeepBrainNet is promising, incorporating a larger and\\nmore diverse dataset, as well as integrating multimodal\\nimaging data, could further enhance its performance.\\nThis work highlights the potential of deep learning\\nin medical imaging, establishing and interpreting 88%\\naccurately and lays the foundation for future research\\nto improve AI-driven brain tumor diagnosis.\\nA. Discussion and Analysis\\nThis study demonstrates the effectiveness of deep\\nlearning models, specifically ResNet50 and Efficient-\\nNetB0, in accurately classifying brain tumor from MRI\\nimages. The model achieves high accuracy and demon-'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='learning models, specifically ResNet50 and Efficient-\\nNetB0, in accurately classifying brain tumor from MRI\\nimages. The model achieves high accuracy and demon-\\nstrates strong generalization to unseen data, showing\\nrobust performance across various tumor types. How-\\never, its performance could be further improved by\\nincorporating a larger and more diverse dataset, which\\nwould help mitigate potential biases and enhance\\ngeneralization capabilities. The models currently rely\\nsolely on magnetic resonance imaging (MRI) data.\\nIntegrating multi modal imaging data â€” such as CT\\nor PET scans â€” could provide complementary infor-\\nmation, potentially enhancing the modelâ€™s accuracy\\nand robustness, particularly in cases involving rare\\nor ambiguous tumor types. Table IV compares the\\nperformance of similar systems on the same dataset\\nused in this study, thereby highlighting the superior\\nperformance of DeepBrainNet.\\nTABLE IV: Comparative Analysis Study\\nPaper Models Score (Accuracy)\\n[14] EfficientNetB3,'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='used in this study, thereby highlighting the superior\\nperformance of DeepBrainNet.\\nTABLE IV: Comparative Analysis Study\\nPaper Models Score (Accuracy)\\n[14] EfficientNetB3,\\nResNet50, VGG-19\\n88.47%\\n[15] Resnet50, DenseNet201,\\nInception V3, MobileNet\\n85.30%\\nDeepBrainNet Hybrid System: ResNet +\\nEfficientNetB0\\n89%\\nB. Future work\\nSeveral important aspects exist for improving model\\nperformance, which might lead to superior results in\\ncurrent methodology. DeepBrainNet is expected to\\nachieve improved performance across diverse popu-\\nlations and tumor types when trained on an expanded\\ndataset that includes additional MRI samples from var-\\nied patient conditions. Higher accuracy and operational\\nefficiency could be achieved by implementing ad-\\nvanced hyper-parameter optimization strategies, such\\nas Bayesian optimization and genetic algorithms, to\\nfine-tune the model architecture and learning process.\\nIncorporating additional imaging techniques, such asCT or PET scans, would provide the model with'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='fine-tune the model architecture and learning process.\\nIncorporating additional imaging techniques, such asCT or PET scans, would provide the model with\\ncomplementary information, leading to improved per-\\nformance. The future success of DeepBrainNet relies\\non advancements in these areas, as they will enhance\\nits reliability and applicability in practical medical\\nsettings.\\nREFERENCES\\n[1] S. Anantharajan, S. Gunasekaran, and T. Sub-\\nramanian. â€œMRI brain tumour detection us-\\ning deep learning and machine learning ap-\\nproachesâ€. In: Measurement: Sensors 31.1\\n(2024), p. 101026. DOI : 10.1016/j.measen.2024.\\n101026.\\n[2] S. K. Mathivanan et al. â€œEmploying deep learn-\\ning and transfer learning for accurate brain\\ntumour detectionâ€. In: Scientific Reports 14.2\\n(2024), p. 7232. DOI : 10 . 1038 / s41598 - 024 -\\n57970-7.\\n[3] M. Z. Khaliki and M. S. Bas Â¸arslan. â€œBrain\\ntumour detection from images and compar-\\nison with transfer learning methods and 3-\\nlayer CNNâ€. In: Scientific Reports 14.3 (2024),'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='57970-7.\\n[3] M. Z. Khaliki and M. S. Bas Â¸arslan. â€œBrain\\ntumour detection from images and compar-\\nison with transfer learning methods and 3-\\nlayer CNNâ€. In: Scientific Reports 14.3 (2024),\\np. 2664. DOI : 10.1038/s41598-024-52823-9.\\n[4] B. Amin et al. â€œBrain tumour multi clas-\\nsification and segmentation in MRI images\\nusing deep learningâ€. In: arXiv preprint\\narXiv:2304.10039 4 (2023). DOI : 10 . 48550 /\\narXiv.2304.10039.\\n[5] E. Albalawi et al. â€œEnhancing brain tumour\\nclassification in MRI scans with a multi-layer\\ncustomized convolutional neural network ap-\\nproachâ€. In: Frontiers in Computational Neuro-\\nscience 18.5 (2024), p. 1418546. DOI : 10.3389/\\nfncom.2024.1418546.\\n[6] S. Natha et al. â€œAutomated brain tumour iden-\\ntification in biomedical radiology images: A\\nmulti-model ensemble deep learning approachâ€.\\nIn: Applied Sciences 14.6 (2024), p. 2210. DOI :\\n10.3390/app14052210.\\n[7] Ali M. Reza. â€œRealization of the contrast limited\\nadaptive histogram equalization (CLAHE) for'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='In: Applied Sciences 14.6 (2024), p. 2210. DOI :\\n10.3390/app14052210.\\n[7] Ali M. Reza. â€œRealization of the contrast limited\\nadaptive histogram equalization (CLAHE) for\\nreal-time image enhancementâ€. In: Journal of\\nVLSI signal processing systems for signal, im-\\nage and video technology 38.7 (2004), pp. 35â€“\\n44. DOI : 10.1023/B:VLSI.0000028532.53893.\\n82.\\n[8] W. A. Mustafa and M. M. M. Abdul Kader. â€œA\\nreview of histogram equalization techniques in\\nimage enhancement applicationâ€. In: Journal of\\nPhysics: Conference Series . V ol. 1019. 8. IOP\\nPublishing. 2018, p. 012026.\\n[9] Zuherman Rustam et al. â€œComparison of Fuzzy\\nC-Means, Fuzzy Kernel C-Means, and Fuzzy\\nKernel Robust C-Means to Classify Thalassemia\\nDataâ€. In: International Journal on Advanced\\nScience, Engineering and Information Technol-\\nogy 9.9 (2019), p. 1205. DOI : 10.18517/ijaseit.\\n9.4.9580.\\n[10] I. Hussain, K. P. Sinaga, and M.-S. Yang. â€œUn-\\nsupervised Multiview Fuzzy C-Means Cluster-\\ning Algorithmâ€. In: Electronics 12.10 (2023),'),\n",
       " Document(metadata={'arxiv_id': '2507.07011v1', 'title': 'Deep Brain Net: An Optimized Deep Learning Model for Brain tumor Detection in MRI Images Using EfficientNetB0 and ResNet50 with Transfer Learning', 'section': 'body', 'authors': 'Daniel Onah, Ravish Desai'}, page_content='9.4.9580.\\n[10] I. Hussain, K. P. Sinaga, and M.-S. Yang. â€œUn-\\nsupervised Multiview Fuzzy C-Means Cluster-\\ning Algorithmâ€. In: Electronics 12.10 (2023),\\np. 4467. DOI : 10.3390/electronics12214467.\\n[11] Mingxing Tan and Quoc V . Le. â€œEfficientnet:\\nRethinking model scaling for convolutional neu-\\nral networksâ€. In: International conference on\\nmachine learning. 11. PMLR. 2019, pp. 6105â€“\\n6114.\\n[12] V . L. Adluri et al. â€œPotato Leaf Disease De-\\ntection and Classification Using VGG16â€. In:\\nProceedings of International Joint Conference\\non Advances in Computational Intelligence . 12.\\nSpringer Nature Singapore. 2024, pp. 147â€“158.\\nDOI : 10.1007/978-981-97-0180-3 13.'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'title_abstract', 'authors': 'Razia Sultana Misu'}, page_content='Title: Brain Tumor Detection Using Deep Learning Approaches\\n\\nAbstract: Brain tumors are collections of abnormal cells that can develop into masses or clusters. Because they have the potential to infiltrate other tissues, they pose a risk to the patient. The main imaging technique used, MRI, may be able to identify a brain tumor with accuracy. The fast development of Deep Learning methods for use in computer vision applications has been facilitated by a vast amount of training data and improvements in model construction that offer better approximations in a supervised setting. The need for these approaches has been the main driver of this expansion. Deep learning methods have shown promise in improving the precision of brain tumor detection and classification using magnetic resonance imaging (MRI). The study on the use of deep learning techniques, especially ResNet50, for brain tumor identification is presented in this abstract. As a result, this study investigates the possibility of automating the detection procedure using deep learning techniques. In this study, I utilized five transfer learning models which are VGG16, VGG19, DenseNet121, ResNet50 and YOLO V4 where ResNet50 provide the best or highest accuracy 99.54%. The goal of the study is to guide researchers and medical professionals toward powerful brain tumor detecting systems by employing deep learning approaches by way of this evaluation and analysis.'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='Brain Tumor Detection Using Deep Learning Approaches \\n \\nBY \\nRAZIA SULTANA MISU \\nID: 193-15-2976 \\n \\n \\nThis Report Presented in Partial Fulfillment of the Requirements for the \\nDegree of Bachelor of Science in Computer Science and Engineering \\n \\nSupervised By \\n \\nNushrat Jahan Ria \\nLecturer \\nDepartment of CSE \\nDaffodil International University \\n \\nCo-Supervised By \\n \\nNaznin Sultana \\nAssociate Professor \\nDepartment of CSE \\nDaffodil International University \\n \\n \\n \\n \\n \\nDAFFODIL INTERNATIONAL UNIVERSITY  \\nDHAKA, BANGLADESH  \\n31 JULY 2023\\n \\nÂ©Daffodil International University  iv \\n \\nABSTRACT \\n  \\nBrain tumors are collections of abnormal cells that can develop into masses or clusters. \\nBecause they have the potential to infiltrate other tissues, they pose a risk to the patient.'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='ABSTRACT \\n  \\nBrain tumors are collections of abnormal cells that can develop into masses or clusters. \\nBecause they have the potential to infiltrate other tissues, they pose a risk to the patient. \\nThe main imaging technique used, MRI, may be able to identify a brain tumor with \\naccuracy. The fast development of Deep Learning methods for use in computer vision \\napplications has been facilitated by a vast amount of training data and improvement s in \\nmodel construction that offer better approximations in a supervised setting. The need for \\nthese approaches has been the main driver of this expansion. Deep learning methods have \\nshown promise in improving the precision of brain tumor detection and classification using \\nmagnetic resonance imaging (MRI). The study on the use of deep learning techniques, \\nespecially ResNet50, for brain tumor identification is presented in this abstract. As a result, \\nthis study investigates the possibility of automating the detection procedure using deep'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='especially ResNet50, for brain tumor identification is presented in this abstract. As a result, \\nthis study investigates the possibility of automating the detection procedure using deep \\nlearning techniques. In this study , I utilized five transfer learning models which are \\nVGG16, VGG19, DenseNet121, ResNet50 and YOLO V4 where ResNet50 provide the \\nbest or highest accuracy 99.54%. The goal of the study is to guide researchers and medical \\nprofessionals toward powerful brain tumor detecting systems by employing deep learning \\napproaches by way of this evaluation and analysis. \\n \\nKeyword: brain tumor; brain tumor detection; deep learning; image processing; ResNet50. \\n \\n \\n \\n \\n \\n \\n \\n                                                                                                                                   \\nÂ©Daffodil International University  v \\n \\nTABLE OF CONTENTS  \\nCONTENTS \\n \\nPAGE \\nBoard of examiners  i \\nDeclaration ii \\nAcknowledgments    iii \\nAbstract iv \\nCHAPTER'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='Â©Daffodil International University  v \\n \\nTABLE OF CONTENTS  \\nCONTENTS \\n \\nPAGE \\nBoard of examiners  i \\nDeclaration ii \\nAcknowledgments    iii \\nAbstract iv \\nCHAPTER  \\nCHAPTER 1:   INTRODUCTION  \\n \\n1-6 \\n1.1 Introduction  1-2 \\n \\n1.2 Motivation \\n \\n2 \\n \\n1.3 Research Question  2-3 \\n1.4 Objective \\n \\n3-4 \\n1.5 Expected Outcome 4-5 \\n1.6 Report layout 5-6 \\nCHAPTER 2 : BACKGROUND 7-14 \\n2.1 Introduction \\n \\n7 \\n2.2 Literature review \\n \\n7-10 \\n2.3 Tools and software \\n \\n11 \\n2.4 Scope of the problem \\n11  \\nÂ©Daffodil International University  vi \\n \\n2.5 Limitations and future scope  \\n12-15 \\nCHAPTER 3: RESEARCH METHODOLOGY \\n \\n16-23 \\n3.1 Introduction  \\n \\n15 \\n3.2 System design \\n \\n16 \\n3.3 Image pre-processing 17 \\n3.3.1 Remove Spackle noise \\n \\n17 \\n3.3.1.1 Median filter  17 \\n3.3.2 Artefact Removal 18 \\n3.3.2.1 Morphological Opening  18 \\n3.3.3 Image Enhancement 18 \\n3.3.3.1 CLAHE 18 \\n3.3.4 Verification 19 \\n3.3.5 Data Split 19 \\n3.4 Proposed Model 19 \\n3.4.1 VGG16 20 \\n3.4.2 VGG19 20 \\n3.4.3 DenseNet 121 21'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='3.3.3 Image Enhancement 18 \\n3.3.3.1 CLAHE 18 \\n3.3.4 Verification 19 \\n3.3.5 Data Split 19 \\n3.4 Proposed Model 19 \\n3.4.1 VGG16 20 \\n3.4.2 VGG19 20 \\n3.4.3 DenseNet 121 21 \\n3.4.4 ResNet50 21 \\n3.4.5 YOLO V4 22-23  \\nÂ©Daffodil International University  vii \\n \\n \\n \\n \\nCHAPTER 4:RESULT ANALYSIS \\n \\n24-28 \\n4.1 Introduction 24 \\n4.2 Evaluation Metrics 24 \\n4.3 Result of Transfer Learning model 25-26 \\n4.4 Performance analysis and statistical analysis 27-28 \\nCHAPTER 5:IMPACT ON SOCIETY,ENVIRONMENT AND \\nSUSTAINABILITY  \\n30-33 \\n5.1 Introduction 30 \\n5.2 Impact on Society 30 \\n5.3 Impact on the environment 31 \\n5.4 Ethical Aspects 32-33 \\n5.5 Sustainability Plan 33 \\nCHAPTER 6: CONCLUSION 34-35 \\n6.1 Introduction 34 \\n6.2 Conclusion 35 \\n6.3 Limitation and Future work 35 \\nAPPENDIX 36 \\nREFERENCES 37-39  \\nÂ©Daffodil International University  viii \\n \\nLIST OF FIGURES \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFIGURES \\n \\nPage \\nFigure 1. Overview of the entire study \\n \\n15 \\nFigure 2. Show the dataset description 16'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='Â©Daffodil International University  viii \\n \\nLIST OF FIGURES \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nFIGURES \\n \\nPage \\nFigure 1. Overview of the entire study \\n \\n15 \\nFigure 2. Show the dataset description 16 \\n \\n Figure 3: VGG16 Model Architecture 20 \\n Figure 4: VGG19 Model Architecture 20 \\nFigure 5: DenseNet 121 Model Architecture 21 \\nFigure 6: ResNet50 Model Architecture 22 \\nFigure 7: YOLO V4 Model Architecture 22 \\nFigure 8: Accuracy curve  26 \\nFigure 9:  Loss curve 26  \\nÂ©Daffodil International University  ix \\n \\nLIST OF TABLES  \\n \\n                                               \\n \\n                  \\n \\n \\n                                   \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\nTABLES \\n \\nPage \\nTable 1: Dataset Description 16 \\nTable 2: SSIM, PSNR, RMSE and MSE value 19 \\nTable 3: Result of Transfer learning model 25 \\nTable 4: Performance Analysis and Statistical Analysis 27 \\n \\nTable 5 : Performance Analysis about Precision, Recall and F1-Score \\n \\n27  \\nÂ©Daffodil International University  x \\n \\nLIST OF ABBRIVIATION'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='Table 5 : Performance Analysis about Precision, Recall and F1-Score \\n \\n27  \\nÂ©Daffodil International University  x \\n \\nLIST OF ABBRIVIATION \\n \\n \\n \\n \\nShort Form Full Form \\n \\nMRI Magnetic resonance imaging \\nCNN Convolutional Neural Network \\nResNet Residual Neural Network \\nVGG Visual Geometry Group \\nDenseNet \\n \\nDense Convolutional Network \\nYOLO You Only Look Once \\n Â©Daffodil International University                                                                                                                                                                                  \\n  \\n1 \\n \\nCHAPTER 1 \\n INTRODUCTION  \\n \\n \\n1.1 Introduction \\n \\nBrain tumor detection is a crucial component of medical imaging and is crucial to the \\ndiagnosis and management of illnesses associated with the brain. The manual examination \\nof medical pictures used in conventional techniques of tumor identification can be  time-\\nconsuming and prone to human error [1]. However, the quick development of deep learning'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='of medical pictures used in conventional techniques of tumor identification can be  time-\\nconsuming and prone to human error [1]. However, the quick development of deep learning \\nmethods, particularly in the area of computer vision, has created new opportunities for the \\nautomatic and precise diagnosis of brain tumors [2]. Convolutional neu ral networks \\n(CNNs), a type of deep learning algorithm, have displayed astounding performance in \\nimage analysis tasks including object detection and segmentation [3]. Researchers have \\nbeen investigating the possibility of these methods for identifying and categorizing brain \\ntumorâ€™s from magnetic resonance imaging (MRI) data by using the capabilities of deep \\nlearning [4]. Researchers are putting in a lot of effort to build CNNs that can properly \\nidentify and classify brain tumour, as well as other forms of m edical imaging, in order to \\nenhance the medical diagnostic and treatment results. With this potential in mind,'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='identify and classify brain tumour, as well as other forms of m edical imaging, in order to \\nenhance the medical diagnostic and treatment results. With this potential in mind, \\nresearchers are working hard to construct CNNs [5]. The benefit of deep learning is that it \\ncan learn intricate, hierarchical features directly f rom unprocessed data, eliminating the \\nneed for explicitly rule -based methods or hand -crafted features [6]. Particularly \\nconvolutional neural networks are made to capture spatial connections and local patterns \\ninside pictures, making them appropriate for jo bs involving medical image processing. \\nBecause of this, deep learning has become a very useful technique for the interpretation of \\nmedical images. It is possible to utilize it to diagnose illnesses with greater precision than \\nis possible with conventional approaches, as well as discover abnormalities in imaging \\ndata. In addition to this, it may be utilized to automate medical diagnoses, hence reducing'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='is possible with conventional approaches, as well as discover abnormalities in imaging \\ndata. In addition to this, it may be utilized to automate medical diagnoses, hence reducing \\nthe amount of labor that must be done by medical experts [7]. This work on brain tumor \\nidentification using deep learning techniques is presented in this publication. For this work,  \\nÂ©Daffodil International University  2 \\n \\nwe specifically concentrate on using a well -liked CNN architecture dubbed ResNet50. \\nWith its deep layers, ResNet50 is able to learn detailed characteristics that are essential for \\nreliable tumor diagnosis. It has demonstrated outstanding performance in a variety of image \\nanalysis areas. Our goal is to create a reliable and automated method for brain tumor \\nidentification by training ResNet50 on a sizable dataset of brain MRI images that includes \\nboth tumor and non-tumor instances. The suggested deep learning model has the potential'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='identification by training ResNet50 on a sizable dataset of brain MRI images that includes \\nboth tumor and non-tumor instances. The suggested deep learning model has the potential \\nto increase the effectiveness and precision of brain tumor diagnosis, resulting in early \\ntreatments, better patient outcomes, and a decreased dependence on manual analysis. \\n 1.2 Motivation  \\nThe urgent need to enhance the identification of brain tumors, a significant problem in the \\nfield of medicine that necessitates accurate and prompt diagnosis, is what prompted this \\nline of investigation in the first place. Traditional approaches to the diagnosis of brain \\ntumors frequently include manual analysis, which may be laborious, subjective, and \\nfraught with the possibility of making mistakes. This project attempts to automate and \\nincrease the accuracy of brain tumour identification by employing deep learning \\ntechniques, notably ResNet50. If successful, this would lead to early treatments, improved'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='increase the accuracy of brain tumour identification by employing deep learning \\ntechniques, notably ResNet50. If successful, this would lead to early treatments, improved \\npatient outcomes, and a reduced dependence on manual analysis. Individuals who are \\nafflicted by brain tumors will ultimately profit from this study, and the area of medical \\nimaging analysis will advance as a result. The potential effect of this research is \\nconsiderable, since it has the ability to contribute to the creation of tools for healthcare \\nprofessionals that are efficient and dependable. \\n1.3 Research Question   \\n1. Detecting brain tumors accurately requires the use of deep learning techniques, notably \\nResNet50. But how can they be used most effectively? \\n2. What are the drawbacks of the methods now used to identify brain tumors, and how may \\ndeep learning overcome these drawbacks?  \\nÂ©Daffodil International University  3'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='2. What are the drawbacks of the methods now used to identify brain tumors, and how may \\ndeep learning overcome these drawbacks?  \\nÂ©Daffodil International University  3 \\n \\n3. How does the performance of the ResNet50 deep learning architecture stack up to that \\nof other deep learning architectures when it comes to the accuracy of detecting brain \\ntumors? \\n4. Can the accuracy and reliability of deep learning -based brain tumor diagnosis be \\nimproved by the integration of multi -modal data, such as the combination of MRI with \\nother imaging modalities or clinical data? \\n5. How can the interpretability of deep learning models used in the detection of brain \\ntumors be enhanced to increase their level of confidence and acceptability in clinical \\nsettings? \\n6. How can explainable deep learning approaches for the identification of brain tumors be \\ndeveloped, and what problems and tactics must be overcome? \\n7. How can we collect huge datasets that are both diverse and unique, along with'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='developed, and what problems and tactics must be overcome? \\n7. How can we collect huge datasets that are both diverse and unique, along with \\nannotations that are thorough, in order to guarantee the generalization of deep learning \\nmodels for the detection of brain tumors across a variety of populations and types of \\ntumors? \\n 1.4 Objective \\nThe purpose of this study is to determine which of many transfer learning models, namely \\nVGG16, VGG19, DenseNet121, ResNet50, and YOLO V4, performs the best for a certain \\ntask in order to select that model as the one that achieves the maximum level of accuracy. \\nIt is our goal to identify, via the use of a  comparison study, which of these models is the \\nmost suitable for our purposes, taking into account aspects such as precision, computing \\nefficacy, and adaptability to situations that occur in the real world . The purpose of this \\nstudy is to provide  insights into the transfer learning model that is most effective for the'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='efficacy, and adaptability to situations that occur in the real world . The purpose of this \\nstudy is to provide  insights into the transfer learning model that is most effective for the \\njob at hand, so that these findings can direct future advancements and applications in \\ndisciplines that are relevant to this one. When evaluating the effectiveness of each model, \\nwe will make use of a wide array of measures. We will determine how effectively the  \\nÂ©Daffodil International University  4 \\n \\nmodels are able to learn from a limited amount of data, how soon they are able to converge \\nto their maximum performance, and how well the models are able to generalize to data that \\nthey have not seen before. When attempting to determine which model will be  most \\nsuccessful for the work at hand, each of these criteria should be taken into consideration. \\nIn order to determine whether or not the models are appropriate for the task at hand, we'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='successful for the work at hand, each of these criteria should be taken into consideration. \\nIn order to determine whether or not the models are appropriate for the task at hand, we \\nwill also investigate how quickly they can be educated and put into use. In the end, we will \\nconsider the expenses that are connected to each model in order to determine which option \\nis the most appropriate one for our undertaking. We will assess all of the metrics, training \\nand deployment speed, as well as related expenses , in order to determine which model is \\nthe most appropriate for our undertaking while keeping in mind the need of finding a \\nsolution that is both cost-effective and efficient. \\n1.5 Expected Outcome \\nThe anticipated result of this research will be the identification of the transfer learning \\nmodel that displays the best level of accuracy and performance for the particular task that \\nis currently being worked on. We are certain that via exhaustive testing and investigation,'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content=\"model that displays the best level of accuracy and performance for the particular task that \\nis currently being worked on. We are certain that via exhaustive testing and investigation, \\none of the models, such as ResNet50, will demonstrate higher accuracy in comparison to \\nthe others. The anticipated result will also contain insights into the advantages and \\ndisadvantages of each model, which will provide a full grasp of their applicability to the \\ntask at hand. In addition, we anticipate receiving useful information regarding the \\ncomputational efficiency of the models, which  will enable us to evaluate the models' \\npotential applicability in environments with limited resources or real-time requirements. It \\nis anticipated that the research will contribute to the current body of knowledge in transfer \\nlearning and assist future researchers and practitioners in picking the model that is the most \\nsuited for tasks that are comparable  to those being studied. The ultimate goal of this\"),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='learning and assist future researchers and practitioners in picking the model that is the most \\nsuited for tasks that are comparable  to those being studied. The ultimate goal of this \\nresearch is to give actionable insights that can improve the accuracy and efficacy \\nof applications that utilize transfer learning models for the particular activity that  is the \\nfocus of this inquiry. Since of this, practitioners should be able to cut down on the amount  \\nÂ©Daffodil International University  5 \\n \\nof time they spend experimenting and studying since they will have a better idea of which \\nmodel is the most effective when applied to a certain endeavor. This research will also give \\na method for evaluating the effectiveness of transfer learning models within the context of \\nthe job at hand. It is anticipated that the findings will be relevant to other activities that are \\nanalogous to this one. As a consequence of this, practitioners will be able to concentrate'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='the job at hand. It is anticipated that the findings will be relevant to other activities that are \\nanalogous to this one. As a consequence of this, practitioners will be able to concentrate \\non the implementation of the model that is most suitable for the job at hand, as opposed to \\nwasting time studying and experimenting with a variety of models. When practitioners \\nhave access to a standardized approach for assessing the performance of transfer learning \\nmodels, they are able to choose which model is the most appropriate for their endeavor in \\na more expedient and precise manner. Because of this, they will be able to bypass the stages \\nof research and experimentation and get right into the implementation phase, which will \\nsave them both time and effort. \\n1.6 Report Layout  \\nIn this report, Six individual chapters are discussed to make this research report more \\ncompact and efficient for any readers or researchers.'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='save them both time and effort. \\n1.6 Report Layout  \\nIn this report, Six individual chapters are discussed to make this research report more \\ncompact and efficient for any readers or researchers. \\nIn chapter 1, Overview the all project. Several section like 1.1 Introduction, 1.2 Motivation, \\n1.3 Research Question, 1.4 Objective, 1.5 Expected Outcome. \\nIn chapter 2,  Discuss about background history and workflow of this research . Sevaral \\nsection like 2.1 Introduction,2.2 literature review,2.3 Tools and software,2.4 scope of the \\nproblem,2.5 Limitations and future scope. \\nIn chapter 3,The research method, including its subsections, is covered in Chapter 3 they \\nare 3.1 introduction ,3.2 system design ,3.3 Image pre -processing,3.3.1 Remove Spackle \\nnoise,3.3.2 Artefact Removal , 3.3.3 Image Enhancement,3.3.4 Verification,3.3.5 Data \\nSplit,3.4 Proposed Model ,3.4.1 VGG16,3.4.2 VGG19,3.4.3 DenseNet 121 ,3.4.4 \\nResNet50,3.4.5 YOLO V4.  \\nÂ©Daffodil International University  6'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='Split,3.4 Proposed Model ,3.4.1 VGG16,3.4.2 VGG19,3.4.3 DenseNet 121 ,3.4.4 \\nResNet50,3.4.5 YOLO V4.  \\nÂ©Daffodil International University  6 \\n \\n In chapter 4, Find the main result and the analysis of all the outcomes of the algorithms . \\nCovered in this chapter,  4.1 Introduction, 4.2 Evaluation Metrics,4.3 Result of Transfer \\nLearning model,4.4 Performance analysis and statistical analysis. \\nIn chapter 5, Discuss about Impact on Society,  Environment and Sustainability  of this \\nresearch.Several section like 5.1 Introduction,5.2 Impact on Society ,5.3 Impact on the \\nenvironment,5.4 Ethical Aspects,5.5 Sustainability Plan. \\nIn chapter 6, Discuss about conclusion of this research. The fifth chapter  discusses the \\nsubsections 6.1 Introduction, 6.2 Conclusion, 6.3 Limitation and Future work.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\nÂ©Daffodil International University  7 \\n \\nCHAPTER 2 \\nBACKGROUND STUDY \\n2.1 Introduction'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='subsections 6.1 Introduction, 6.2 Conclusion, 6.3 Limitation and Future work.  \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\nÂ©Daffodil International University  7 \\n \\nCHAPTER 2 \\nBACKGROUND STUDY \\n2.1 Introduction  \\nDespite the fact that many academics study transfer learning approaches to identify several \\nobjects, a ResNet50  model has not been used to detect brain tumor detection or \\nclassification based on brain MRI images. B oth with and without the use of transfer \\nlearning techniques. The vast majority of the study in this area focuses on the behavior of \\ncomputers with huge assessment receptivity as well as resource computation in most \\ncircumstances. This research focuses on brain tumor detection or classification based on \\nbrain MRI images datasets.  \\n2.2 Literature review   \\nA method of deep learning that makes use of the ResNet50 architecture was suggested for \\nthe diagnosis of brain tumors by John Smith and colleagues  [8]. The authors utilized'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='A method of deep learning that makes use of the ResNet50 architecture was suggested for \\nthe diagnosis of brain tumors by John Smith and colleagues  [8]. The authors utilized \\ntransfer learning by making use of the pre-trained weights of ResNet50 in order to train the \\nmodel using a dataset consisting of brain MRI images as the input. An outstanding accuracy \\nof 92% was reached by the model in the detection of brain tumors thanks to a combination \\nof binary cross -entropy loss and gradient descent optimization. This demonstrated the \\npromise of deep learning -based technologies in clinical applications and outperformed \\nmore conventional methods. In order to increase the identification of brain tumors, Sarah \\nThompson et al. [9] used an ensemble technique that consisted of numerous convolutional \\nneural networks, or CNNs. Individual CNN models were trained by the authors utilizing \\ndifferent architectures such as ResNet50, VGG16, and InceptionV3. The ensemble model'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='neural networks, or CNNs. Individual CNN models were trained by the authors utilizing \\ndifferent architectures such as ResNet50, VGG16, and InceptionV3. The ensemble model \\nobtained an accuracy of 94% when it came to the detection of brain tumors. This was \\naccomplished by merging the predictions of various models through a voting system. When \\ncompared to the use of a single model, the ensemble technique displayed superior \\nperformance and resilience, presenting encouraging possibilities for diagnostic  \\nÂ©Daffodil International University  8 \\n \\nadvancements. Michael and his colleagues [10] concentrated their efforts on classifying \\nbrain tumors using deep learning methods combined with radiomic characteristics. The \\nauthors classified tumors using a mixture of a deep neural network (particularly ResNet50) \\nand more typical machine learning methods. The quantitative radiomic characteristics were \\nretrieved from brain MRI data and used. When it came to categorizing brain tumors into'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='and more typical machine learning methods. The quantitative radiomic characteristics were \\nretrieved from brain MRI data and used. When it came to categorizing brain tumors into \\ntheir respective subtypes, the suggested method was able to attain an accuracy rate of 88% \\noverall. The use of deep learning and radiomic characteristics resulted in an increase in \\nboth the accuracy and discriminatory power of tumor classification, which provided helpful \\ninsights for the development of personalized treatment plans. Jennifer and colleagues[11] \\ndeveloped a hybrid model for the segmentation of brain tumors by integrating the U -Net \\nand ResNet50 architectural frameworks. In the study, U-Net was used for the preliminary \\ncoarse segmentation, and then ResNet50 was use d for the subsequent fine segmentation. \\nThe model was trained using a large manually annotated dataset of brain MRI images. The \\nannotations on the dataset were contributed by experienced radiologists. The accuracy and'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content=\"The model was trained using a large manually annotated dataset of brain MRI images. The \\nannotations on the dataset were contributed by experienced radiologists. The accuracy and \\nprecision with which brain tumors were segmented was demonstrated by the hybrid model \\nthat was devised, which attained a Dice similarity coefficient (DSC) of 0.92. The \\ncombination of U-Net and ResNet50 made it easier to delineate the borders of the tumor in \\na precise and efficient manner, whic h aided in the process of treatment planning and \\nmonitoring. An attention -based kind of deep learning was the method that Ryan [12] \\npresented for the diagnosis of brain tumors. The authors included an attention mechanism \\nin the ResNet50 design, which allow ed the model to highlight significant regions inside \\nthe brain MRI images. This was made possible as a result of the authors' efforts. The \\ninterpret-ability of the model as well as its accuracy in tumor localization were both\"),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content=\"the brain MRI images. This was made possible as a result of the authors' efforts. The \\ninterpret-ability of the model as well as its accuracy in tumor localization were both \\nimproved by this attention mechanism. The suggested method was successful in detecting \\nbrain tumors with an accuracy of 90%, demonstrating the potential of attention-based deep \\nlearning to improve the performance of brain tumor detection systems while \\nsimultaneously increasing their ex plain ability. In this article, Jennifer et al. [13] give a \\ncomprehensive assessment of deep learning approaches that may be applied to medical \\npicture analysis. The authors cover the uses of many different architectures, including  \\nÂ©Daffodil International University  9 \\n \\nCNNs, RNNs, and GANs, in tasks such as picture classification, segmentation, and \\ndetection. The survey covers a wide range of medical imaging modalities, such as magnetic \\nresonance imaging (MRI), computed tomography (CT), and microscopy. The research\"),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='detection. The survey covers a wide range of medical imaging modalities, such as magnetic \\nresonance imaging (MRI), computed tomography (CT), and microscopy. The research \\ndemonstrates how useful deep learning can be in the field of medical image processing by \\ncomparing its accuracy and speed to those of more conventional approaches. Deep learning \\nhas the potential to significantly advance medical diagnosis and treatment, which t he \\nauthors highlight while also identifying a number of obstacles and future paths for study. \\nThe application of machine learning and deep learning algorithms for brain tumor \\nsegmentation in medical imaging is the primary subject of this review that was wr itten by \\nRobert et al[14]. The authors provide an overview of and provide comparisons between a \\nvariety of approaches, such as conventional machine learning algorithms, CNNs, and \\nsophisticated architectural frameworks such as U -Net and 3D CNNs. They addres s the'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='variety of approaches, such as conventional machine learning algorithms, CNNs, and \\nsophisticated architectural frameworks such as U -Net and 3D CNNs. They addres s the \\nbenefits of various strategies, as well as their limits and any current developments in the \\nfield. The paper highlights the tremendous progress made in brain tumor segmentation \\nusing machine learning and deep learning, with CNN -based architectures di splaying \\nsuperior performance in precisely defining tumor zones. These advancements were made \\npossible by the combination of machine learning and deep learning. The authors note the \\nneed for reliable and easily interpretable segmentation approaches that ca n assist doctors \\nwith treatment planning and monitoring. Through the use of a retrospective investigation, \\nDavid and colleagues[15] presented their findings on the potential of deep learning for \\nbrain tumor grading and survival prediction. The scientists u sed a CNN-based model that'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content=\"David and colleagues[15] presented their findings on the potential of deep learning for \\nbrain tumor grading and survival prediction. The scientists u sed a CNN-based model that \\nwas trained on a huge dataset consisting of clinical data and brain MRI images to get to \\ntheir conclusions. They contrasted the performance of the model with the performance of \\nestablished prognostic indicators in order to evalua te the performance of the model in \\npredicting tumor grades and patient survival. Deep learning was used in the study to grade \\nbrain tumors and forecast patients' chances of survival. The CNN -based model achieved \\nhigh accuracy in tumor grading and showed pr omising results in survival prediction. The \\nwork highlights the usefulness of deep learning in these areas. The findings imply that deep \\nlearning algorithms might give significant insights for treatment planning and prognosis  \\nÂ©Daffodil International University  10\"),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='learning algorithms might give significant insights for treatment planning and prognosis  \\nÂ©Daffodil International University  10 \\n \\nevaluations, as well as enhance established prognostic indicators. The article by Mary and \\nco-authors [16] explores the idea of transfer learning as well as its applications in medical \\nimaging. The authors investigate a variety of transfer learning methodologies, such as the \\nprocess of fine-tuning pre-trained models, the extraction of features, and the ada ption of \\ndomains. They look at research that use transfer learning to various medical imaging tasks, \\nincluding as illness categorization, lesion detection , and picture segmentation, and they \\ndiscuss the findings. This paper focuses on the advantages of transfer learning in the field \\nof medical imaging, including increased performance with fewer data points and decreased \\ntotal training time. In the research on transfer learning, the authors emphasize how'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='of medical imaging, including increased performance with fewer data points and decreased \\ntotal training time. In the research on transfer learning, the authors emphasize how \\nimportant it is to have benchmark datasets and standardized evaluation methodologies. \\nTransfer learning has shown to have a significant amount of promise in the field of medical \\nimaging. This is accomplished by drawing on the expertise contained within large -scale \\ndatasets and pre-trained models in order to overcome the obstacles posed by the scarcity of \\nannotated medical data . The article by Jessica et al. [17] offers a detailed review of the \\nmethods that are based on deep learning for the segmentation of brain tumors. The authors \\nprovide a synopsis of and comparison of a variety of methodologies, such as CNNs, U -\\nNets, and attention -based models. They examine the difficulties associated with \\nsegmenting brain tumors, such as a lack of data, class imbalance, and interpretability'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='Nets, and attention -based models. They examine the difficulties associated with \\nsegmenting brain tumors, such as a lack of data, class imbalance, and interpretability \\nconcerns, as well as the recent developments in this field. The paper underlines the \\nusefulness of deep learning in the segmentation of brain tumors, with CNN-based designs \\nand U -Net ver sions being commonly used. It has been demonstrated that attention \\nprocesses can improve tumor localization and the delineation of tumor boundaries. \\nHowever, there are still problems with class imbalance, minor lesion segmentation, and the \\ncapacity to gene ralize to many kinds of tumors. Deep learning -based brain tumor \\nsegmentation has a long way to go before it can be used successfully in clinical settings. \\nThe authors of this study offer future research areas to help overcome these problems and \\nincrease it s clinical usefulness .[32] shows that the issue of scarce medical data can be'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='The authors of this study offer future research areas to help overcome these problems and \\nincrease it s clinical usefulness .[32] shows that the issue of scarce medical data can be \\novercome by augmenting with realistic, artificial data generated from GANs. [32] \\nempirically shows that object detection performance can be considerably improved with  \\nÂ©Daffodil International University  11 \\n \\ndata augmentation from GAN -based techniques.To avoid the common problem of mode \\ncollapse in GAN training, [32] suggests using multiple features as input to the generator. \\nTo improve and expand the prediction capability of a detector in GANs, [32] uses two \\nclassification heads for the detector, one for binary classification (real/fake) data, and the \\nother for multi-class disease prediction from brain FC-matrix. For effective GAN training, \\n[32] suggests Biology/Medical application: [32] states that pairwise c o-activation of'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='other for multi-class disease prediction from brain FC-matrix. For effective GAN training, \\n[32] suggests Biology/Medical application: [32] states that pairwise c o-activation of \\ndifferent parts of the brain can be effectively represented with a functional connectivity \\n(FC) matrix. [32] is the first work to generate artificial FC-matrix from brain rs-fMRI scans \\nusing a GAN model. [33] notes that SSIM can be an effec tive metric to measure the \\nsimilarity of objects generated by GAN compared to the ground truth. \\n2.3 Tools and Software    \\nPython is the most widely used in my research. We use a high-level or pretrained transfer \\nlearning algorithm or model or network. We use Jupiter Notebook, Google Colab , and \\nSpyder for coding or implementation. We used traditional pretrained transfer learning \\nmodels or algorithms.  \\n2.4 Scope of the problem  \\nThe diagnosis of brain tumors through the use of deep learning methodologies is the focus'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content=\"models or algorithms.  \\n2.4 Scope of the problem  \\nThe diagnosis of brain tumors through the use of deep learning methodologies is the focus \\nof the problem's scope. It involves the construction and assessment of deep learning \\nmodels, in particular ResNet50, with the purpose of reliably detecting brain cancers using \\nmedical imaging data such as MRI scans. The scope also includes the investigation of a \\nvariety of facets connected to the problem, such as the preparation of data, the selection of \\nmodel architecture, training and optimization methodologies, assessment metrics, and \\npotential therapeutic application s. The scope of the challenge includes evaluating the \\nperformance and limits of the strategy that is based on deep learning,  as well as locating \\nareas that might benefit from more study and development in the future. The potential \\ninfluence of accurate brain tumor identification on patient outcomes, treatment planning,\"),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='areas that might benefit from more study and development in the future. The potential \\ninfluence of accurate brain tumor identification on patient outcomes, treatment planning, \\nand general healthcare practices is also included within the scope of this investigation. In  \\nÂ©Daffodil International University  12 \\n \\nthe end, the findings of this study will be utilized to influence decision-making in the field \\nof medical imaging and to create new ways for identifying brain tumors in a way that is \\nboth more accurate and more efficient. Through the provision of more prec ise diagnoses \\nand treatment strategies, this study may facilitate improvements in the final results for \\npatients. It will also give medical professionals with new methods for diagnosing brain \\ntumors in a quicker and more precise manner, which has the poten tial to lead to earlier \\ntreatments and improved patient outcomes. \\n2.5 Limitations and Future Scope \\nLimitations:'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='tumors in a quicker and more precise manner, which has the poten tial to lead to earlier \\ntreatments and improved patient outcomes. \\n2.5 Limitations and Future Scope \\nLimitations: \\nâš« Limited availability of annotated datasets: One of the obstacles is the lack of big, \\ndiversified, annotated datasets that have been created expressly for the purpose of \\nidentifying brain tumors through the use of deep learning techniques. The existence of \\nsuch datasets has the potential to have an effect on the generalizability as well as the \\nperformance of the models that are trained on them. \\nâš« Interpretability and explainability: Because deep learning models, such as ResNet50, \\nare sometimes referred to as \"black boxes,\" it can be difficult to analyze and make \\nsense of the judgments that these models produce. In therapeutic settings, where \\nopenness and explainability are essential, the lack of interpretability might be a barrier'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='sense of the judgments that these models produce. In therapeutic settings, where \\nopenness and explainability are essential, the lack of interpretability might be a barrier \\nto both the faith placed in these models and their adoption by patients. \\nâš« Computational resource requirements: Deep learning models, particularly those with \\nintricate architectures such as ResNet50, may place a large demand on the available \\ncomputational resources. These resources can include high -performance computing \\ninfrastructure as well as GPU accelerators. Because of this constraint, installing the \\nmodels in situations with limited resources may be difficult, which would reduce both \\ntheir accessibility and their practicability. \\nâš« Sensitivity to training data quality and bias: Models that use deep learning can be \\nsensitive to the quality of the training data as well as any bias that may be present.  \\nÂ©Daffodil International University  13'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='sensitive to the quality of the training data as well as any bias that may be present.  \\nÂ©Daffodil International University  13 \\n \\nBecause of the potential for biases in the data, such as imbalances in tumor subtypes \\nor changes in imaging methods, the performance and generalizability of the model \\nmay be negatively impacted. As a result, thorough dataset curation and augmentation \\nstrategies are required. \\nâš« Lack of real -time processing capabilities: The computational complexity of deep \\nlearning models such as ResNet50 may result in lengthier processing times, which \\nmight limit their applicability in real -time settings in which rapid decision -making is \\nnecessary. To circumvent this constraint and realize the full potential of real-time brain \\ntumor diagnosis, it is possible that effective optimization strategies and hardware \\nacceleration would be required. \\nFuture Scope: \\nâš« Improving interpretability and explain ability: The development of approaches that'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='acceleration would be required. \\nFuture Scope: \\nâš« Improving interpretability and explain ability: The development of approaches that \\nmight improve the interpretability and explain ability of deep learning models that are \\nemployed in the identification of brain tumors could be the focus of future study. In  \\nthis context, \"techniques\" might refer to things like \"attention mechanisms,\" \\n\"visualization methods,\" or \"model-agnostic interpretability approaches,\" all of which \\nare intended to provide physicians more insight into the decision -making process \\nunderlying the models. \\nâš« Integration of multimodal data: The identification of brain tumors may be made more \\naccurate and reliable by utilizing a combination of several imaging modalities. One \\nexample of this would be combining MRI with other sophisticated imaging methods \\nsuch as PET or functional MRI. In further study, the integration of multimodal data'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='example of this would be combining MRI with other sophisticated imaging methods \\nsuch as PET or functional MRI. In further study, the integration of multimodal data \\nmight be investigated to investigate the possibility of extracting complementary \\ninformation and improving the diagnostic skills of deep learning models. \\nâš« Transfer learning across institutions:  Important for the future is the investigation of \\ntransfer learning methodologies that might assist model adaption and generalization \\nacross a variety of universities and imaging centers. Increasing the practicality and \\nusability of deep learning models in a variety of clinical contexts may be accomplished  \\nÂ©Daffodil International University  14 \\n \\nby developing methods that are able to take into account differences in imaging \\nprocedures, technology, and patient demographics. \\nâš« Real-time processing and deployment: Real-time processing and deployment of deep'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='procedures, technology, and patient demographics. \\nâš« Real-time processing and deployment: Real-time processing and deployment of deep \\nlearning models for brain tumor detection may soon be possible because to recent \\ndevelopments in optimization methods, model compression approaches, and    \\nhardware acceleration. Facilitating real-time decision-making in clinical practice can \\nbe a primary focus of future research, which can be directed toward the development \\nof effective structures and algorithms that can offer correct findings within time \\nrestrictions. \\nâš« Clinical validation and integration: It is essential for the future to conduct large -scale \\nprospective clinical trials in order to evaluate the performance and clinical relevance \\nof deep learning-based brain tumor detection models. Integration of these models into \\nclinical workflows and evaluation of their influence on patient outcomes, treatment'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='of deep learning-based brain tumor detection models. Integration of these models into \\nclinical workflows and evaluation of their influence on patient outcomes, treatment \\nplanning, and healthcare practices are able to give useful insights into the practical \\nadvantages and obstacles connected with the implementation of these models i n real-\\nworld settings. \\nâš« Incorporating longitudinal data: It is possible to get useful insights into the growth of \\nbrain tumors and the treatment response through longitudinal data, which entails \\ngathering imaging images over time for specific patients. In the future, research might \\nconcentrate on building deep learning models that are able to efficiently interpret and \\nmake use of longitudinal data in order to improve the accuracy of tumor identification, \\nbetter track changes over time, and provide assistance with treatment monitoring and \\nprognosis. \\nâš« Integration of clinical data: The incorporation of clinical data into deep learning'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content=\"better track changes over time, and provide assistance with treatment monitoring and \\nprognosis. \\nâš« Integration of clinical data: The incorporation of clinical data into deep learning \\nmodels, including but not limited to patient demographics, medical history, and genetic \\ninformation, can be beneficial to the models. Future study might examine the \\npossibility of deep learning models to give individualized tumor identification and \\ntreatment recommendations by merging imaging data with important clinical factors.  \\nÂ©Daffodil International University  15 \\n \\nThis would take into account the unique features of each patient and improve the \\naccuracy of clinical decision-making. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\nÂ©Daffodil International University  16 \\n \\nCHAPTER 3 \\nRESEARCH METHODOLOGY  \\n3.1 Introduction :  \\nAccording to our methods, a data set containing images of brain MRI  dataset was collected \\nfrom Kaggle. The method of this work's embodiment is represented in this part. The\"),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content=\"3.1 Introduction :  \\nAccording to our methods, a data set containing images of brain MRI  dataset was collected \\nfrom Kaggle. The method of this work's embodiment is represented in this part. The \\nembodiment process is broken down into several steps, including the acquisition of  data, \\npreprocessing of the dataset, description of the proposed model, training, and finally \\nperformance assessment.  \\nIn this part, we will look into three important aspects: the preparation of the dataset, transfer \\nlearning models, and the description of the dataset. We investigate the processes and \\nprocedures that are utilized to curate and preprocess the data so that the model can perform \\nat its best. In addition, we describe the transfer learning models that were utilized, focusing \\non their effectiveness in utilizing pre -trained models for a variety of tasks and \\ndemonstrating how this was accomplished. In conclusion, w e present an exhaustive\"),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='on their effectiveness in utilizing pre -trained models for a variety of tasks and \\ndemonstrating how this was accomplished. In conclusion, w e present an exhaustive \\nexplanation of the dataset that was utilized, giving light on its construction as well as its \\nqualities. The overview of the whole work has given bellow in the figure 1. \\n \\nFigure 1. Overview of the entire study \\n \\nÂ©Daffodil International University  17 \\n \\n3.2 Dataset Description \\nThis investigation looked at 7022 MRIs of brain tumors. No tumor, meningioma, glioma, \\nand pituitary are all included in the dataset. There are 1621 images of glioma, 1645 of \\nmeningioma, 2000 of no tumors, and 1757 of pituitary. This dataset has 512 X 512 \\ngrayscale images. The open-source Kaggle dataset was utilized.  \\nThe dataset is described in Table 1: \\nTable 1 shown dataset description \\n \\nIn the figure 2 shows the dataset description. \\n \\nFigure 2. Show the dataset description \\nName Category \\nNumber of Imageâ€™s 7022 \\nDimensionâ€™s 512 x 512'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content=\"Table 1 shown dataset description \\n \\nIn the figure 2 shows the dataset description. \\n \\nFigure 2. Show the dataset description \\nName Category \\nNumber of Imageâ€™s 7022 \\nDimensionâ€™s 512 x 512 \\nColorâ€™s Grayscale \\nFormatâ€™s jpg \\nGliomaâ€™s 1621 \\nMeningiomaâ€™s 1645 \\nNo Tumorâ€™s 2000 \\nPituitaryâ€™s 1757  \\nÂ©Daffodil International University  18 \\n \\n3.3 Image pre-processing \\nSeveral approaches are used in brain MRI image preparation to improve the images' quality \\nand enable more thorough analysis. To ensure that image features are as clear as possible, \\nmedian filtering is used to lessen noise and smooth the pictures. Morphological opening \\naids in the removal of minute undesirable components and the improvement of images. In \\norder to increase contrast and make details i n the photos more visible, Contrast Limited \\nAdaptive Histogram Equalization (CLAHE) is used. This makes the images more suited \\nfor future analysis and identi fication tasks. The efficiency and precision of soil image\"),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='Adaptive Histogram Equalization (CLAHE) is used. This makes the images more suited \\nfor future analysis and identi fication tasks. The efficiency and precision of soil image \\nprocessing and interpretation systems are improved by these preprocessing methods. \\n 3.3.1 Remove Speckle noise \\nDigital photographs frequently contain speckle noise, which is most noticeable in \\nultrasound and synthetic aperture radar (SAR) images. It manifests as granular interference, \\nwhich reduces the quality of the picture and compromises the precision of future image \\nprocessing activities. Due to its multiplicative character, speckle noise is more difficult to \\neliminate than other forms of noise. To reduce speckle noise and improve the clarity of \\nimpacted photos, several filtering methods, including adaptive filt ers and wavelet -based \\napproaches, have been developed. As was mentioned before, the dataset has a lot of spackle'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content=\"impacted photos, several filtering methods, including adaptive filt ers and wavelet -based \\napproaches, have been developed. As was mentioned before, the dataset has a lot of spackle \\nnoise. A gaussian filter is used to eliminate spackle noise from soil recognition [18].  \\n3.3.1.1 Median filter \\nA popular non -linear filtering method for image processing is the median filter. It \\nsignificantly reduces impulsive noise while keeping picture details by replacing each pixel \\nin an image with the neighborhood's median value. With the median filter, salt-and-pepper \\nnoise may be effectively removed, leaving behind smoother pictures with maintained small \\ndetails. [19]. \\n \\n  \\nÂ©Daffodil International University  19 \\n \\n3.3.2 Artifact Removal \\nAn important step in image processing is artifact removal, which aims to remove any \\nundesirable distortions or abnormalities created during the capture, transmission, or storage\"),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='An important step in image processing is artifact removal, which aims to remove any \\nundesirable distortions or abnormalities created during the capture, transmission, or storage \\nof images. To find and eliminate artifacts, improve picture quality, and enhance visual \\ninterpretation, a variety of techniques are used, including spatial and frequency domain \\napproaches. The generated photos are more accurate, dependable, and suited for additional \\nanalysis or display by successfully eliminating artifacts [20]. \\n3.3.2.1 Morphological Opening  \\nMorphological opening [21] is a preprocessing method used in image analysis to get rid of \\nminor impurities and sharpen object shapes. It includes shrinking the things by erosion, \\nthen expanding them again through dilation, but the erosion stage eliminates minor \\nfeatures. When smoothing and improving the clarity of soil pictures, this procedure is very \\nhelpful since it makes feature extraction and subsequent analysis easier.'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='features. When smoothing and improving the clarity of soil pictures, this procedure is very \\nhelpful since it makes feature extraction and subsequent analysis easier.  \\n3.3.3 Image Enhancement \\nThe term \"image enhancement\" refers to a group of methods used to enhance the sharpness, \\ncontrast, brightness, and other pertinent aspects of digital photographs in order to enhance \\ntheir visual quality and interpretability. \\n3.3.3.1 CLAHE \\nThe contrast and visibility of features inside images are enhanced via the image \\nenhancement technique known as Contrast Limited Adaptive Histogram Equalization \\n(CLAHE) [22]. To create a more even and improved image, it divides the image into \\nsmaller sections, computes the histogram of each zone, and then redistributes the intensity \\nvalues. CLAHE can increase the visibility and recognizability of soil detection in soil \\nimage processing, resulting in more precise detection and interpretation of soil features. \\n  \\nÂ©Daffodil International University  20'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='image processing, resulting in more precise detection and interpretation of soil features. \\n  \\nÂ©Daffodil International University  20 \\n \\n3.3.4 Verification \\nSeveral statistical tests, including PSNR [23], SSIM [24], MSE [25], and RMSE [26], are \\nperformed in order to determine whether or not the image quality has been impacted. This \\nis because applying a variety of image preprocessing techniques might result in  a \\nconsiderable drop in picture quality. For a better understanding of the picture quality, MSE, \\nPSNR, SSIM, and RMSE values for five images are provided in Table 2. \\nTable 2. Show the SSIM, PSNR, RMSE and MSE value \\nImage MSE PSNR SSIM RMSE \\nImage-1 12.65 37.57 0.96 0.13 \\nImage-2 13.12 46.86 0.96 0.12 \\nImage-3 13.78 45.03 0.96 0.12 \\nImage-4 15.78 40.78 0.95 0.13 \\nImage-5 14.01 34.57 0.95 0.13 \\n \\n3.3.5 Data Split \\nAfter statistical analysis, the data is separated into three pieces (training set, validation set,'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content=\"Image-4 15.78 40.78 0.95 0.13 \\nImage-5 14.01 34.57 0.95 0.13 \\n \\n3.3.5 Data Split \\nAfter statistical analysis, the data is separated into three pieces (training set, validation set, \\nand testing set). Three training-testing data splitting ratiosâ€”90:10, 80:20, and 70:30â€”are \\nemployed to assess the model's accuracy. This research defines 70:30 as 70% train sets, \\n10% validation sets, and 20% test sets. \\n3.4 Proposed Model \\nIn this study we utilized traditional transfer learning models like VGG16, VGG19, \\nDenseNet121, ResNet50 and YOLO V4 to compare which model is the best for soil \\ndetection or classify the soil.  \\nÂ©Daffodil International University  21 \\n \\n3.4.1 VGG16 \\nSimonyan and Zisserman [27] introduced the VGG16 DCNN model. The model won the \\nOxford Visual Geometry Group's Large -Scale Visual Recognition Challenge (ILSVRC) \\nwith 92.7% top 5 test accuracy in ImageNet.  \\n \\n  Figure 3: VGG16 Model Architecture\"),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content=\"Oxford Visual Geometry Group's Large -Scale Visual Recognition Challenge (ILSVRC) \\nwith 92.7% top 5 test accuracy in ImageNet.  \\n \\n  Figure 3: VGG16 Model Architecture \\nTransfer learning efficiency tests showed that a pre-trained and fine -tuned VGG16 was \\nmore accurate than a fully trained network. The VGG model's depth helps the kernel learn \\nmore complex traits. \\n3.4.2 VGG19 \\nIn the VGG19 model, which is a variation of the VGG model, there are a total of 19 layers. \\nThe VGG19 model comes to a close with three more FC levels, bringing the total number \\nof layers to 19. Each of these layers has 4096, 4096, and 1000 neurons respectively.\\n \\n     Figure 4: VGG19 Model Architecture \\n \\nÂ©Daffodil International University  22 \\n \\n In addition to that, there are five Maxpool layers and a Softmax layer that are provided. \\nThe activation of ReLU is a feature that is present in layers that are convolutional in nature \\n[28]. \\n3.4.3 DenseNet 121\"),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='The activation of ReLU is a feature that is present in layers that are convolutional in nature \\n[28]. \\n3.4.3 DenseNet 121 \\nDense connections between layers are prioritized in the DenseNet121 convolutional neural \\nnetwork design. Researchers at Facebook AI Research created the 121 -layer neural \\nnetwork known as DenseNet121, which has convolutional, pooling, and fully connected \\nlayers. Unlike traditional CNN designs, DenseNet121 connects each layer to each other in \\na feed-forward manner, allowing for a direct flow of data and gradients across the network. \\n \\nFigure 5: DenseNet 121 Model Architecture \\n This thick connection improves gradient -based optimization and boosts model \\nperformance by encouraging feature reuse, reducing the number of parameters,  and \\nimproving gradient flow. Due to its exceptional performance in picture categorization and \\nobject identification tasks, DenseNet121 is a preferred choice for many computer vision \\napplications [29]. \\n3.4.4 ResNet50'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='object identification tasks, DenseNet121 is a preferred choice for many computer vision \\napplications [29]. \\n3.4.4 ResNet50 \\nA deep convolutional neural network architecture called ResNet50 has made major strides \\nin computer vision. It has 50 layers and residual blocks that help the network get around \\nthe difficulties associated with training very deep neural networks.  \\n \\nÂ©Daffodil International University  23 \\n \\n \\n     Figure 6: ResNet50 Model Architecture \\nResNet50 has demonstrated outstanding performance in a variety of tasks, including image \\nsegmentation, object identification, and classification. Its skip connections provide \\nimproved gradient flow and avoid network performance from declining as network de pth \\nincreases by allowing information to flow straight from early levels to later layers. \\nResNet50 has grown to be a popular option in several cutting -edge deep learning \\napplications because of its potent feature extraction capabilities [30].      \\n3.4.5 YOLO V4'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='ResNet50 has grown to be a popular option in several cutting -edge deep learning \\napplications because of its potent feature extraction capabilities [30].      \\n3.4.5 YOLO V4 \\nYOLOv4 is a cutting -edge object identification algorithm that is famous for the \\nextraordinary precision and speed with which it operates. It is an acronym for \"You Only \\nLook Once\" and is utilized frequently in many computer vision jobs. YOLOv4 is equipped \\nwith sophisticated features such as a CSPDarknet53 backbone and various scale \\npredictions, which give it the ability to identify objects of varying sizes and scales in an \\neffective manner. \\n \\nFigure 7: YOLO V4 Model Architecture \\n \\nÂ©Daffodil International University  24 \\n \\nIn addition to this, it implements the Mish activation function and presents a number of \\noptimizations, such as spatial pyramid pooling (SPP) and panoptic feature pyramid \\nnetworks (PFPN). In general, YOLOv4 has earned a stellar reputation for the reliable'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content=\"optimizations, such as spatial pyramid pooling (SPP) and panoptic feature pyramid \\nnetworks (PFPN). In general, YOLOv4 has earned a stellar reputation for the reliable \\nperformance it delivers in real-time object detection applications [31]. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\nÂ©Daffodil International University  25 \\n \\nCHAPTER 4 \\nRESULT ANALYSIS \\n4.1 Introduction \\nThe result analysis section presents a comprehensive evaluation of the proposed model's \\nperformance. It includes the evaluation metrics used to measure the performance of the \\nmodel, such as accuracy, precision, recall, and F1 score. The impact on the model's \\nperformance is also discussed, along with the confusion matrix to assess the model's ability \\nto correctly classify each class. Additionall y, the performance of the proposed model is \\ncompared with five other conventional transfer learning models to determine its \\nsuperiority. Overall, this section provides a detailed analysis of the proposed model's\"),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content=\"compared with five other conventional transfer learning models to determine its \\nsuperiority. Overall, this section provides a detailed analysis of the proposed model's \\nperformance and its strengths and weaknesses compared to other models. \\n4.2 Evaluation Metrics \\nThe accuracy, losses, and performance parameters of each of the five transfer learning \\nmodels are examined in the outcome analysis. Based on its performance analysis, which \\ntakes accuracy, recall, F1 score, and other pertinent parameters into account, the optimal \\nmodel is chosen. The best model for soil recognition may be found thanks to this thorough \\nevaluation, which also offers insights into how well it performs overall. Precision, recall, \\nF1-score, accuracy (ACC), sensitivity, and specificity determined the optimal model. \\nModels have confusion matrices. Thus, TPs, TNs, FPs, and FNs are identified. FPR, FNR, \\nFDR, MAE, and RMSE were computed for model statistical analysis.\"),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content=\"Models have confusion matrices. Thus, TPs, TNs, FPs, and FNs are identified. FPR, FNR, \\nFDR, MAE, and RMSE were computed for model statistical analysis. \\nAccuracy = (TP +TN)/(TP + TN + FP + FN )     (1) \\nRecall = TP/(TP + FN )     (2) \\nSpecificity = TN/(TN + FP )     (3) \\nPrecision = TP/(TP + FP)     (4) \\nF1 score = (2 Ã— Precision Ã— Recall)/(Precision + Recall)     (5) \\nFPR = FP/(FP+TN)     (6)  \\nÂ©Daffodil International University  26 \\n \\n4.3 Result of Transfer Learning Model \\nTable 3 illustrates the five transfer learning models' training, test, validation, and loss. The \\ntable shows that the ResNet50 model is the most accurate. \\nTable 3. Result of Transfer learning model \\nModel Train_Acc. Train_Loss Val_Acc. Val_loss Test_Acc. Test_loss \\nVGG19 96.63 0.21 95.93 0.21 95.22 0.25 \\nVGG16 96.21 0.20 96.95 0.12 96.12 0.20 \\nDenseNet \\n121 \\n97.41 0.19 97.23 0.28 97.21 0.31 \\nResNet50 99.98 0.23 99.54 0.32 99.54 0.37 \\nYOLO V4 91.23 0.39 91.21 0.392 91.21 0.94\"),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content=\"VGG16 96.21 0.20 96.95 0.12 96.12 0.20 \\nDenseNet \\n121 \\n97.41 0.19 97.23 0.28 97.21 0.31 \\nResNet50 99.98 0.23 99.54 0.32 99.54 0.37 \\nYOLO V4 91.23 0.39 91.21 0.392 91.21 0.94 \\nConfusion metrics, commonly referred to as confusion matrices, are an important technique \\nfor assessing how well categorization algorithms work. For a particular dataset, they offer \\na thorough overview of the projected and actual class labels. The matrix is set up as a square \\ngrid, with the genuine classes represented by rows and the anticipated classes by columns. \\nEach matrix column displays the number or percentage of cases that belong to a certain \\ncombination of true and anticipated classes. Confusion me trics include a number of \\nindicators, including accuracy, precision, recall, and F1 score, that help analyze the model's \\nstrengths and shortcomings, notably in terms of misclassifications and the trade -off \\nbetween various sorts of mistakes. These metrics a id in evaluating and improving\"),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content=\"strengths and shortcomings, notably in terms of misclassifications and the trade -off \\nbetween various sorts of mistakes. These metrics a id in evaluating and improving \\nclassification models, allowing data scientists and practitioners to decide on the \\neffectiveness of the model and any room for improvement. Important visualizations that \\nshed light on the effectiveness and development of mach ine learning models are loss and  \\nÂ©Daffodil International University  27 \\n \\naccuracy curves. The value of the loss function, such as cross -entropy or mean squared \\nerror, is plotted against the quantity of training epochs or iterations on the y-axis to create \\nthe loss curve. As it quantifies the difference between the expected and actual values, the \\nobjective is to minimize the loss. The accuracy curve, on the other hand, shows how the \\nmodel's accuracy changes with the number of epochs or iterations on the x -axis. Better\"),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content=\"objective is to minimize the loss. The accuracy curve, on the other hand, shows how the \\nmodel's accuracy changes with the number of epochs or iterations on the x -axis. Better \\nperformance is indicated by higher accuracy values. Data scientists may determine if the \\nmodel is converging, overfitting, or underfitting the data by examining these graphs. By \\nchoosing the right number of training epochs, seeing possible problems, and making wise \\nmodel improvement decisions, these visualizations aid in model optimization. \\n          \\n                                               Figure 8: Accuracy curve over 100 epochs \\n                                \\n                                                  Figure 9: Loss curve over 100 epochs                     \\n \\nÂ©Daffodil International University  28 \\n \\n4.4 Performance analysis and statistical analysis \\nShowing the FDR, FPR, FNR,  KC, MCC, MAE, and RMSE values of the best model \\n(ResNet50) in table 4.\"),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='Â©Daffodil International University  28 \\n \\n4.4 Performance analysis and statistical analysis \\nShowing the FDR, FPR, FNR,  KC, MCC, MAE, and RMSE values of the best model \\n(ResNet50) in table 4. \\nTable 4. Performance Analysis and Statistical Analysis \\nAccuracy FPR \\n(%) \\nFNR \\n(%) \\nFDR \\n(%) \\nKC \\n(%) \\nMCC \\n(%) \\nMAE RMSE \\n99.54 0.12 0.46 0.46 98.95 88.62 0.58 8.85 \\n \\nThe performance measures of the best model, ResNet50, are presented in Table 4. These \\nmetrics include the False Discovery Rate (FDR), the False Positive Rate (FPR), the False \\nNegative Rate (FNR), the Matthews Correlation Coefficient (MCC), the Kappa Coefficient \\n(KC), the Mean Absolute Error (MAE), and the Root Mean Square Error (RMS E). These \\nmetrics give insights into the accuracy, precision, and overall performance of ResNet50 in \\nthe identification of brain tumors, allowing for a full evaluation and compar ison of its \\nefficacy against the performance of other models or benchmarks.'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content=\"the identification of brain tumors, allowing for a full evaluation and compar ison of its \\nefficacy against the performance of other models or benchmarks. \\nIn tumor detection, we have used five transfer learning models. These are VGG16, VGG19, \\nDenseNet121, ResNet50, and YOLO V4. Among them, ResNet50 achieved the highest \\naccuracy of 99.54%. Table 5 shows the Performance Analysis of Precision, Recall, and F1-\\nScore for RestNet50. \\nTable 5.  Performance Analysis of Precision, Recall and F1-Score \\n \\nClass Precision \\n(%) \\nRecall \\n(%) \\nF1-Score \\n(%) \\nAccuracy \\n(%) \\nResNet50 99.54 99.54 99.54 99.54  \\nÂ©Daffodil International University  29 \\n \\n \\nThe accuracy of a model's positive predictions is measured by precision, also known as \\nPositive Predictive Value (PPV). A high accuracy number shows that the model does an \\nexcellent job of avoiding false positive predictions, which means that it seldom incorrectly\"),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='Positive Predictive Value (PPV). A high accuracy number shows that the model does an \\nexcellent job of avoiding false positive predictions, which means that it seldom incorrectly \\nlabels negative events as positive. Recall, often referred to as Sensitivity or True Positive \\nRate (TPR), gauges how well a model can pick out instances of positivity among the entire \\nnumber of real occurrences of positivity. When there is an imbalance between positive and \\nnegative cases in the dataset, the F1 -Score is helpful. The F1-value has a range of 0 to 1, \\nwith 1 representing the ideal value. It performs best when accuracy and recall are evenly \\ndistributed. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\nÂ©Daffodil International University  30 \\n \\nCHAPTER 5 \\nIMPACT ON SOCIETY, ENVIRONMENT AND SUSTAINABILITY \\n \\n5.1 Introduction \\nIn this part of the study, we look into the influence that the research has had on society as'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='CHAPTER 5 \\nIMPACT ON SOCIETY, ENVIRONMENT AND SUSTAINABILITY \\n \\n5.1 Introduction \\nIn this part of the study, we look into the influence that the research has had on society as \\na whole, as well as the environment and the capacity to support it. We investigate how the \\napplication of deep learning techniques in the diagnosis of brain tumors might provide \\nmajor advantages to society, including improvements in healthcare outcomes, greater \\ndiagnostic accuracy, and prompt treatments. In addition, we explore the possible \\nenvironmental consequences of our findings, such as a reduced dependence on invasive \\nprocedures and unneeded imaging tests, which will lead to more sustainable healthcare \\npractices and a lower environmental imprint in the area of medical imaging. \\n5.2 Impact on Society \\nThe use of deep learning methods to the diagnosis of brain tumors will have a significant \\nand far-reaching effect on society. It is possible to dramatically increase both the accuracy'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='The use of deep learning methods to the diagnosis of brain tumors will have a significant \\nand far-reaching effect on society. It is possible to dramatically increase both the accuracy \\nand the efficiency of diagnosing brain tumors by making use of more complex algorithms, \\nsuch as ResNet50. This will ultimately lead to improved medical outcomes for patients. \\nInterventions can be performed at the appropriate moment after early and correct discovery \\nof brain tumors, which increases the likelihood of successful treatment and may save lives. \\nAdditionally, the automation and simplification of the detection process that may be \\naccomplished with deep learning models can lighten the load on healthcare workers, \\nenabling them to devote more time and resources to the car e of their patients. The \\napplication of deep learning to the identification of brain tumors has a beneficial influence \\non society as a whole since it improves medical diagnostics, makes it possible to develop'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='application of deep learning to the identification of brain tumors has a beneficial influence \\non society as a whole since it improves medical diagnostics, makes it possible to develop \\nindividualized treatment plans, and eventually leads to an improvement in the wellbeing \\nand quality of life of people whose lives have been damaged by brain tumors. \\n  \\nÂ©Daffodil International University  31 \\n \\n5.3 Impact on the environment \\nThe use of methods including deep learning in the diagnosis of brain tumors has a further \\nbeneficial effect on the surrounding natural environment. It is possible to eliminate the \\nnecessity for invasive treatments such as biopsies by making use of more sop histicated \\nalgorithms such as ResNet50. This not only makes patients feel less discomfort and lessens \\nthe hazards connected with invasive operations, but it also results in less trash being \\nproduced by medical facilities. In addition, techniques that are based on deep learning make'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content=\"the hazards connected with invasive operations, but it also results in less trash being \\nproduced by medical facilities. In addition, techniques that are based on deep learning make \\nit possible to do imaging that is both more precise and more focused. This cuts down on \\nthe number of scans that aren't necessary and the related consumption of resources like \\nenergy, materials, and chemicals. Deep learning leads  to a more sustainable and \\nenvironmentally aware approach to the identification of brain tumors and medical imaging \\nas a whole by improving the diagnostic process and encouraging more efficient use of \\nresources. This is accomplished through the promotion of efficient resource use. \\n5.4 Ethical Aspects \\nWhen it comes to the diagnosis of brain tumors, the application of deep learning algorithms \\nraises a number of important ethical questions. Protecting the privacy of patients and their\"),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='When it comes to the diagnosis of brain tumors, the application of deep learning algorithms \\nraises a number of important ethical questions. Protecting the privacy of patients and their \\ndata should be considered one of the most important ethical considerations. It is necessary \\nto maintain tight confidentiality, acquire informed permission, and follow to data \\nprotection standards in order to preserve patient information while using deep learning \\nmodels because these models depend on vast volumes of patient data. Another ethical issue \\nto consider is the bias and fairness of algorithms. Deep learning models have the potential \\nto inherit biases from the data on which they are trained, which might result in diagnostic \\nand treatment discrepancies for some segments of the population. It is important to make \\nan effort to identify and reduce any bias that may exist in the models in order to guarantee'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='and treatment discrepancies for some segments of the population. It is important to make \\nan effort to identify and reduce any bias that may exist in the models in order to guarantee \\nthat they are fair, objective, and relevant to a wide range of patient groups. Additionally \\nessential to ethical deliberation are factors such as openness and explicability. It might be \\ndifficult to comprehend how deep learning models, such as ResNet50, come to the \\nconclusions that they do because these models are sometimes referred to as \"black boxes.\"  \\nÂ©Daffodil International University  32 \\n \\nIt is crucial to ensure that the models are interpretable and explainable in order to build \\ntrust, which will promote clinical adoption and enable clinicians to comprehend and dispute \\nthe model\\'s findings. Consenting after being fully informed and particip ating in decision-\\nmaking together are both ethical values that should be upheld. Patients should be given the'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content=\"the model's findings. Consenting after being fully informed and particip ating in decision-\\nmaking together are both ethical values that should be upheld. Patients should be given the \\nchance to participate in the decision -making process and should be sufficiently informed \\nabout the use of deep learning models in their diagnosis.  Patients should also be given \\nappropriate information on the use of deep learning models in their diagnosis. Clinicians \\nhave a responsibility to give patients with thorough explanations and assist them in making \\neducated decisions on their treatment optio ns based on the results produced by deep \\nlearning models. It is essential to keep monitoring, validating, and improving deep learning \\nmodels in order to make certain that they continue to be effective while also being safe. \\nAudits and evaluations should be  carried out on a regular basis in order to evaluate the\"),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content=\"models in order to make certain that they continue to be effective while also being safe. \\nAudits and evaluations should be  carried out on a regular basis in order to evaluate the \\nperformance of the models, as well as their correctness and any potential biases. In order \\nto preserve the health and safety of patients and the public's faith in the technology, any \\nflaws or restrictions should be pinpointed and immediately corrected. Ethical issues also \\nextend to the responsibilities of healthcare professionals and researchers to utilize deep \\nlearning models as tools to complement clinical decision -making rather than to replace \\nhuman knowledge. This is important because of the potential for unethical behavior if this \\nresponsibility is not met. It is important to remember that physicians bear the ultimate \\nresponsibility for patient care and should base their judgments not only on thei r own \\nknowledge but also on the results produced by the deep learning model. Another\"),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='responsibility for patient care and should base their judgments not only on thei r own \\nknowledge but also on the results produced by the deep learning model. Another \\ncomponent of ethics is making sure that everyone has the same opportunities. In order to \\nprevent further aggravating existing healthcare inequities, it is imperative that deep \\nlearning models be made available for use in a wide variety of healthcare settings, including \\nthose with limited access to resources. It is important that steps be taken to close the digital \\ngap and ensure that everyone has equal access to the advanta ges offered by deep learning \\ntechnology. Ethical issues also include the deployment of deep learning models in a \\nresponsible and transparent manner. In order to cultivate scientific rigor, reproducibility, \\nand accountability, adequate documentation, the sharing of methodology, and peer review  \\nÂ©Daffodil International University  33'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='and accountability, adequate documentation, the sharing of methodology, and peer review  \\nÂ©Daffodil International University  33 \\n \\nare all necessary components. Open communication and teamwork between physicians, \\nresearchers, and policymakers, as well as between patients and researchers, can assist \\nresolve ethical concerns and encourage responsible innovation. In conclusion, the fast \\ndeveloping area of deep learning calls for a continuous debate on ethics as well as ethical \\nadvice in order to successfully manage future ethical difficulties. In order to address the \\none-of-a-kind ethical issues that are raised by deep learning models in the context of brain \\ntumor detection, ethical frameworks, guidelines, and regulatory frameworks need to be \\nregularly updated. The ethical use of deep learning in brain tumor diagnosis can maximize \\nits advantages while simultaneously limiting potential haza rds and assuring patient -'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content=\"regularly updated. The ethical use of deep learning in brain tumor diagnosis can maximize \\nits advantages while simultaneously limiting potential haza rds and assuring patient -\\ncentered treatment if certain ethical norms are upheld and active obstacles connected with \\nthe usage are actively addressed. \\n5.5 Sustainability Plan \\nA sustainability strategy for the application of deep learning in the diagnosis of brain \\ntumors should incorporate a number of essential components. To begin, it should make the \\nresponsible management of resources a top priority by improving computing algo rithms \\nand infrastructure in order to reduce the amount of energy used and waste produced. In \\naddition, the strategy need to encourage the creation of and acceptance of open-source and \\ntransparent frameworks that make it possible for researchers to collabo rate, share their \\nexpertise, and reproduce their findings. It is essential to maintain deep learning models'\"),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content=\"transparent frameworks that make it possible for researchers to collabo rate, share their \\nexpertise, and reproduce their findings. It is essential to maintain deep learning models' \\nusefulness and impact over the long run by providing for their ongoing maintenance and \\nsupport. In addition, the plan need to advocate for the ethical acquisition and usage of data, \\nputting an emphasis on privacy, security, and consent as the highest priorities. Finally, the \\npromotion of multidisciplinary collaborations and partnerships between academia, \\nindustry, healthcare providers, and policymakers may promote the sustainable integration \\nof deep learning in brain tumor detection. This will ensure that the technology will continue \\nto bring long-term advantages to society as well as environmental considerations. \\n  \\nÂ©Daffodil International University  34 \\n \\nCHAPTER 6 \\nCONCLUSION \\n \\n6.1 Introduction \\nIn this part of the article, we will show the findings and conclusions of our research project\"),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content=\"Â©Daffodil International University  34 \\n \\nCHAPTER 6 \\nCONCLUSION \\n \\n6.1 Introduction \\nIn this part of the article, we will show the findings and conclusions of our research project \\non the application of transfer learning models to the identification of brain tumor on brain \\nMRI dataset. In addition, we emphasize prospective topics for furthe r investigation and \\nacknowledge the constraints that were encountered throughout the course of the research \\nprocess. This part gives a detailed summary of the study's outcomes by summarizing the \\nfindings, addressing the limits, and recommending future directions. It also paves the way \\nfor additional developments in the field of brain tumor detection based on MRI images \\nresearch by setting the stage for these developments. \\n6.2 Conclusion \\nIn conclusion, the use of deep learning strategies, in particular ResNet50, has shown \\ntremendous potential in the field of detecting brain tumors. When comparing tumor\"),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='6.2 Conclusion \\nIn conclusion, the use of deep learning strategies, in particular ResNet50, has shown \\ntremendous potential in the field of detecting brain tumors. When comparing tumor \\ninstances with non -tumor cases, the use of ResNet50, which has deep layers and robust \\nfeature extraction capabilities, has demonstrated exceptional accuracy and efficiency in \\nmaking the distinction between the two types of cases. Researchers have been abl e to \\nconstruct reliable and automated methods for accurate brain tumor identification by \\ntraining ResNet50 on big datasets of brain MRI images. This has allowed the researchers \\nto develop the systems more quickly. The use of deep learning strategies, such as \\nResNet50, in the analysis of medical images offers the potential to improve the speed, \\naccuracy, and objectivity of diagnosing brain tumors. This is one of the many areas in \\nwhich deep learning has shown promise. Deep learning -based approaches have the'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='accuracy, and objectivity of diagnosing brain tumors. This is one of the many areas in \\nwhich deep learning has shown promise. Deep learning -based approaches have the \\npotential to alter the area of brain tumor identification and contribute to improved patient \\noutcomes if additional improvements are made in the technology behind these methods \\nand research into them is maintained.  \\nÂ©Daffodil International University  35 \\n \\n6.3 Limitation and Future Work \\nIn spite of the substantial progress that has been made in detecting brain tumors through \\nthe use of deep learning algorithms, there are still certain limits and areas that need further \\nresearch. One of the limitations is the requirement for extensive annotated datasets that are \\nboth broad and varied in order to guarantee the generalizability of the model across a \\nvariety of patient groups and cancer types. Another obstacle to overcome is the difficulty'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='both broad and varied in order to guarantee the generalizability of the model across a \\nvariety of patient groups and cancer types. Another obstacle to overcome is the difficulty \\nof interpreting the results of deep learning models, w hich are sometimes referred to as \\n\"black boxes.\" If this problem were solved by the creation of explainable deep learning \\nalgorithms, the level of confidence and acceptance that these models would get in \\ntherapeutic contexts would increase. Integration of multi-modal data, such as combining \\nMRI with other imaging modalities or clinical data, might further increase the accuracy \\nand reliability of brain tumor identification. Other imaging modalities include PET,  CT, \\nand SPECT.  Continued research efforts shoul d focus on resolving these constraints, \\nenhancing deep learning algorithms, and undertaking prospective clinical trials to test the \\nperformance and effectiveness of deep learning -based brain tumor detection systems in'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='enhancing deep learning algorithms, and undertaking prospective clinical trials to test the \\nperformance and effectiveness of deep learning -based brain tumor detection systems in \\nreal-world settings. These are all important areas of attention. \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\nÂ©Daffodil International University  36 \\n \\nAPPENDIX: Research Reflections \\nBefore I began, I did not know anything about artificial intelligence or deep learning. My \\nsupervisor is a great person who is kind and honest. She was very helpful and gave \\nimportant advice right from the start. I gained a lot of knowledge while doing the research, \\nlike how to make a better set of data and how to get useful information from data that is \\nnot organized. How to use and apply algorithms and other related techniques.I applied five \\ntransfer learning models in this research project.  These are VGG16,  VGG19, DenseNet \\n121, ResNet50, and YOLO V4 where ResNet50 provide the best accuracy model which is'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='transfer learning models in this research project.  These are VGG16,  VGG19, DenseNet \\n121, ResNet50, and YOLO V4 where ResNet50 provide the best accuracy model which is \\n99.54%. Python is the most widely used in my research project. I use Jupiter Notebook, \\nGoogle Colab and Spyder for coding or implementation.  In the end, I studied AI, deep \\nlearning, and computer vision. This has motivated me to continue learning more about \\nthese subjects in the future. \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n  \\nÂ©Daffodil International University  37 \\n \\nReferences \\n[1] M. Arabahmadi, R. Farahbakhsh, and J. Rezazadeh, â€œDeep Learning for Smart Healthcare â€”A Survey \\non Brain Tumor Detection from Medical Imaging,â€ Sensors, no. 5, p. 1960, Mar. 2022, doi: \\n10.3390/s22051960. \\n \\n [2] N. A. Samee et al., â€œClassification Framework for Medical Diagnosis of Brain Tumor with an Effective \\nHybrid Transfer Learning Model,â€ Diagnostics, no. 10, p. 2541, Oct. 2022, doi: \\n10.3390/diagnostics12102541.'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='Hybrid Transfer Learning Model,â€ Diagnostics, no. 10, p. 2541, Oct. 2022, doi: \\n10.3390/diagnostics12102541. \\n \\n [3] P. Shanthakumar and P. Ganeshkumar, â€œPerformance analysis of classifier for brain tumor detection \\nand diagnosis,â€ Computers &amp; Electrical Engineering, pp. 302â€“311, Jul. 2015, doi: \\n10.1016/j.compeleceng.2015.05.011. \\n \\n[4] E. Irmak, â€œMulti-Classification of Brain Tumor MRI Images Using Deep Convolutional Neural \\nNetwork with Fully Optimized Framework,â€ Iranian Journal of Science and Technology, Transactions of \\nElectrical Engineering, no. 3, pp. 1015â€“1036, Apr. 2021, doi: 10.1007/s40998-021-00426-9. \\n \\n[5] J. Amin, M. A. Anjum, M. Sharif, S. Jabeen, S. Kadry, and P. Moreno Ger, â€œA New Model for Brain \\nTumor Detection Using Ensemble Transfer Learning and Quantum Variational Classifier,â€ Computational \\nIntelligence and Neuroscience, pp. 1â€“13, Apr. 2022, doi: 10.1155/2022/3236305.'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='Tumor Detection Using Ensemble Transfer Learning and Quantum Variational Classifier,â€ Computational \\nIntelligence and Neuroscience, pp. 1â€“13, Apr. 2022, doi: 10.1155/2022/3236305. \\n \\n[6] E. Lynch, â€œWhat Is Deep Learning? A Guide to Deep Learning Use Cases, Applications, and Benefits,â€ \\nClearML, Feb. 14, 2023. https://clear.ml/blog/what-is-deep-learning/ (accessed Jul. 25, 2023). \\n \\n[7] M. M. Taye, â€œUnderstanding of Machine Learning with Deep Learning: Architectures, Workflow, \\nApplications and Future Directions,â€ Computers, no. 5, p. 91, Apr. 2023, doi: \\n10.3390/computers12050091. \\n \\n[8] J. Amin, M. Sharif, A. Haldorai, M. Yasmin, and R. S. Nayak, â€œBrain tumor detection and classification \\nusing machine learning: a comprehensive survey,â€ Complex &amp; Intelligent Systems, no. 4, pp. 3161â€“\\n3183, Nov. 2021, doi: 10.1007/s40747-021-00563-y. \\n \\n[9] S. Thompson, J. Anderson, and E. Roberts, \"Brain tumor detection using deep learning with an'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='3183, Nov. 2021, doi: 10.1007/s40747-021-00563-y. \\n \\n[9] S. Thompson, J. Anderson, and E. Roberts, \"Brain tumor detection using deep learning with an \\nensemble of convolutional neural networks,\" Journal of Medical Imaging, vol. 12, no. 3, pp. 145-158, 2022. \\n \\n[10] M. Brown, J. Davis, and R. Wilson, \"Brain tumor classification using deep learning and radiomic \\nfeatures,\" International Journal of Computer Vision, vol. 8, no. 2, pp. 67-75, 2019. \\n \\n[11] J. White, A. Taylor, and J. Parker, \"Brain tumor segmentation using deep learning with U -Net and \\nResNet50,\" Medical Image Analysis, vol. 20, no. 5, pp. 230-245, 2020. \\n \\n[12] R. Johnson, S. Adams, and B. Hughes, \"Brain tumor detection using deep learning with attention \\nmechanism,\" IEEE Transactions on Biomedical Engineering, vol. 5, no. 1, pp. 12 -21, 2018. \\n \\n[13] J. Wilson, M. Thompson, and S. Davis, \"A comprehensive survey on deep learning techniques for'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='mechanism,\" IEEE Transactions on Biomedical Engineering, vol. 5, no. 1, pp. 12 -21, 2018. \\n \\n[13] J. Wilson, M. Thompson, and S. Davis, \"A comprehensive survey on deep learning techniques for \\nmedical image analysis,\" IEEE Journal of Biomedical and Health Informatics, vol. 6, no. 4, pp. 300 -315, \\n2023. \\n \\n[14] R. Johnson, E. Roberts, and B. Thompson, \"A review of brain tumor segmentation techniques using \\nmachine learning and deep learning algorithms,\" Pattern Recognition Letters, vol. 15, no. 7, pp. 500 -512, \\n2017.  \\nÂ©Daffodil International University  38 \\n \\n \\n[15] D. Roberts, J. Brown, and M. Wilson, \"Deep learning for brain tumor grading and survival prediction: \\nA retrospective study,\" NeuroImage, vol. 25, no. 9, pp. 100-115, 2021. \\n \\n[16] M. Davis, J. Wilson, and S. Thompson, \"Transfer learning in medical imaging: A review,\" IEEE \\nAccess, vol. 10, no. 12, pp. 450-465, 2016. \\n \\n[17] J. Anderson, R. Wilson, and B. Davis, \"Deep learning-based brain tumor segmentation: A'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='Access, vol. 10, no. 12, pp. 450-465, 2016. \\n \\n[17] J. Anderson, R. Wilson, and B. Davis, \"Deep learning-based brain tumor segmentation: A \\ncomprehensive review,\" Medical Physics, vol. 19, no. 6, pp. 350 -365, 2020. \\n \\n[18] N. Saxena and N. Rathore, \"A review on speckle noise filtering techniques for SAR images,\" \\nInternational Journal of Advanced Research in Computer Science and Electronics Engineering \\n(IJARCSEE), vol. 2, no. 2, pp. 243-247, 2013. \\n \\n[19] V. Y. Borole, S. S. Nimbhore, and D. S. S. Kawthekar, \"Image processing techniques for brain tumor \\ndetection: A review,\" International Journal of Emerging Trends & Technology in Computer Science \\n(IJETTCS), vol. 4, no. 5, pp. 10-18, 2015. \\n \\n[20] L. Griffanti et al., â€œICA-based artefact removal and accelerated fMRI acquisition for improved resting \\nstate network imaging,â€ NeuroImage, pp. 232â€“247, Jul. 2014, doi: 10.1016/j.neuroimage.2014.03.034.'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='state network imaging,â€ NeuroImage, pp. 232â€“247, Jul. 2014, doi: 10.1016/j.neuroimage.2014.03.034. \\n \\n[21] K. A. M. Said, A. B. Jambek, and N. Sulaiman, \"A study of image processing using morphological \\nopening and closing processes,\" International Journal of Control Theory and Applications, vol. 9, no. 31, \\npp. 15-21, 2017. \\n \\n[22] A. M. Reza, â€œRealization of the Contrast Limited Adaptive Histogram Equalization (CLAHE) for \\nReal-Time Image Enhancement,â€ The Journal of VLSI Signal Processing-Systems for Signal, Image, and \\nVideo Technology, no. 1, pp. 35â€“44, Aug. 2004, doi: 10.1023/b:vlsi.0000028532.53893.82. \\n \\n[23] S. Winkler and P. Mohandas, â€œThe Evolution of Video Quality Measurement: From PSNR to Hybrid \\nMetrics,â€ IEEE Transactions on Broadcasting, no. 3, pp. 660â€“668, Sep. 2008, doi: \\n10.1109/tbc.2008.2000733. \\n \\n[24] A. Hore and D. Ziou, â€œImage Quality Metrics: PSNR vs. SSIM,â€ 2010 20th International Conference \\non Pattern Recognition, Aug. 2010, doi: 10.1109/icpr.2010.579.'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='10.1109/tbc.2008.2000733. \\n \\n[24] A. Hore and D. Ziou, â€œImage Quality Metrics: PSNR vs. SSIM,â€ 2010 20th International Conference \\non Pattern Recognition, Aug. 2010, doi: 10.1109/icpr.2010.579. \\n \\n[25] H. Marmolin, â€œSubjective MSE Measures,â€ IEEE Transactions on Systems, Man, and Cybernetics, no. \\n3, pp. 486â€“489, 1986, doi: 10.1109/tsmc.1986.4308985. \\n \\n[26] T. Chai and R. R. Draxler, â€œRoot mean square error (RMSE) or mean absolute error (MAE)? â€“ \\nArguments against avoiding RMSE in the literature,â€ Geoscientific Model Development, no. 3, pp. 1247â€“\\n1250, Jun. 2014, doi: 10.5194/gmd-7-1247-2014. \\n \\n[27] S. Ghosh, A. Chaki, and K. Santosh, â€œImproved U-Net architecture with VGG-16 for brain tumor \\nsegmentation,â€ Physical and Engineering Sciences in Medicine, no. 3, pp. 703â€“712, May 2021, doi: \\n10.1007/s13246-021-01019-w. \\n \\n[28] V. Rajinikanth, A. N. Joseph Raj, K. P. Thanaraj, and G. R. Naik, â€œA Customized VGG19 Network'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='10.1007/s13246-021-01019-w. \\n \\n[28] V. Rajinikanth, A. N. Joseph Raj, K. P. Thanaraj, and G. R. Naik, â€œA Customized VGG19 Network \\nwith Concatenation of Deep and Handcrafted Features for Brain Tumor Detection,â€ Applied Sciences, no. \\n10, p. 3429, May 2020, doi: 10.3390/app10103429. \\n  \\nÂ©Daffodil International University  39 \\n \\n[29] N. Cinar, A. Ozcan, and M. Kaya, â€œA hybrid DenseNet121-UNet model for brain tumor segmentation \\nfrom MR Images,â€ Biomedical Signal Processing and Control, p. 103647, Jul. 2022, doi: \\n10.1016/j.bspc.2022.103647. \\n \\n[30] A. K. Sharma, A. Nandal, A. Dhaka, D. Koundal, D. C. Bogatinoska, and H. Alyami, â€œEnhanced \\nWatershed Segmentation Algorithm-Based Modified ResNet50 Model for Brain Tumor Detection,â€ \\nBioMed Research International, pp. 1â€“14, Feb. 2022, doi: 10.1155/2022/7348344. \\n \\n[31] N. F. Alhussainan, B. B. Youssef, and M. M. Ben Ismail, â€œA Deep Learning Approach for Brain'),\n",
       " Document(metadata={'arxiv_id': '2309.12193v1', 'title': 'Brain Tumor Detection Using Deep Learning Approaches', 'section': 'body', 'authors': 'Razia Sultana Misu'}, page_content='BioMed Research International, pp. 1â€“14, Feb. 2022, doi: 10.1155/2022/7348344. \\n \\n[31] N. F. Alhussainan, B. B. Youssef, and M. M. Ben Ismail, â€œA Deep Learning Approach for Brain \\nTumor Firmness Detection Using YOLOv4,â€ 2022 45th International Conference on Telecommunications \\nand Signal Processing (TSP), Jul. 2022, doi: 10.1109/tsp55681.2022.9851237. \\n \\n[32] D. Yan et al., â€œImproving Brain Dysfunction Prediction by GAN: A Functional-Connectivity \\nGenerator Approach,â€ 2021 IEEE International Conference on Big Data (Big Data), Dec. 2021, doi: \\n10.1109/bigdata52589.2021.9671402. \\n[33] M. Sami, I. Naiyer, E. Khan, and J. Uddin, â€œImproved Semantic Inpainting Architecture Augmented \\nwith a Facial Landmark Detector,â€ The International Arab Journal of Information Technology, no. 3, 2022, \\ndoi: 10.34028/iajit/19/3/9.'),\n",
       " Document(metadata={'arxiv_id': '1704.08134v1', 'title': 'Multimodal MRI brain tumor segmentation using random forests with features learned from fully convolutional neural network', 'section': 'title_abstract', 'authors': 'Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou'}, page_content='Title: Multimodal MRI brain tumor segmentation using random forests with features learned from fully convolutional neural network\\n\\nAbstract: In this paper, we propose a novel learning based method for automated segmenta-tion of brain tumor in multimodal MRI images. The machine learned features from fully convolutional neural network (FCN) and hand-designed texton fea-tures are used to classify the MRI image voxels. The score map with pixel-wise predictions is used as a feature map which is learned from multimodal MRI train-ing dataset using the FCN. The learned features are then applied to random for-ests to classify each MRI image voxel into normal brain tissues and different parts of tumor. The method was evaluated on BRATS 2013 challenge dataset. The results show that the application of the random forest classifier to multimodal MRI images using machine-learned features based on FCN and hand-designed features based on textons provides promising segmentations. The Dice overlap measure for automatic brain tumor segmentation against ground truth is 0.88, 080 and 0.73 for complete tumor, core and enhancing tumor, respectively.'),\n",
       " Document(metadata={'arxiv_id': '1704.08134v1', 'title': 'Multimodal MRI brain tumor segmentation using random forests with features learned from fully convolutional neural network', 'section': 'body', 'authors': 'Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou'}, page_content='Multimodal MRI brain tumor segmentation using \\nrandom forests with features learned from fully \\nconvolutional neural network  \\nMohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou, Nigel Allinson, \\nXujiong Ye  \\nLaboratory of Vision Engineering, School of Computer Science, University of Lincoln, UK \\nAbstract. In this paper, we propose a novel learning based method for automated \\nsegmentation of brain tumor in multimodal MRI images. The machine learned \\nfeatures from fully convolutional neural network (FCN) and hand-designed tex-\\nton features are used to classify the MRI image voxels. The score map with pixel-\\nwise predictions is used as a feature map which is learned from multimodal MRI \\ntraining dataset using the FCN. The learned features are then applied to random \\nforests to classify each MRI image voxel into normal  brain tissues and different \\nparts of tumor. The method was evaluated on BRATS 2013 challenge dataset.'),\n",
       " Document(metadata={'arxiv_id': '1704.08134v1', 'title': 'Multimodal MRI brain tumor segmentation using random forests with features learned from fully convolutional neural network', 'section': 'body', 'authors': 'Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou'}, page_content='forests to classify each MRI image voxel into normal  brain tissues and different \\nparts of tumor. The method was evaluated on BRATS 2013 challenge dataset. \\nThe results show that the application of the random forest classifier to multimodal \\nMRI images using machine-learned features based on FCN and hand -designed \\nfeatures based on textons provides promising segmentations. The Dice overlap \\nmeasure for automatic brain tumor segmentation against ground truth is 0.88, 080 \\nand 0.73 for complete tumor, core and enhancing tumor, respectively. \\n \\nKeywords: Deep learning, fully convolutional neural network, textons, random \\nforests, brain tumor segmentation, multimodal MRI \\n1 Introduction \\nSegmentation of brain tumors from multimodal magnetic resonance imaging (MRI) \\nis a challenging task due to different types and their complic ated structures in the im-\\nages [1] and also large variety and complexity within one type of tumor in terms of'),\n",
       " Document(metadata={'arxiv_id': '1704.08134v1', 'title': 'Multimodal MRI brain tumor segmentation using random forests with features learned from fully convolutional neural network', 'section': 'body', 'authors': 'Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou'}, page_content='is a challenging task due to different types and their complic ated structures in the im-\\nages [1] and also large variety and complexity within one type of tumor in terms of \\ncharacteristics such as intensity, texture, shape and location. The challenge is develop-\\ning a platform which creates accurate segmentation and works for  multiple tumor types \\nand different imaging equipment [2].  \\nIn recent decades, the research work for automatic brain tumor segmentation has \\nincreased which represents the demand for this area of research and it is still in progress \\n[3]. Several methods have been proposed in the literature for detection and segmenta-\\ntion of tumors in MRI images [4]. The brain tumor segmentation techniques can be \\ncategorized into generative and discriminative based models [3].  \\nDiscriminative approaches are based on extraction of the features from the images \\nand creating models based on the relationship between the image features and the voxel'),\n",
       " Document(metadata={'arxiv_id': '1704.08134v1', 'title': 'Multimodal MRI brain tumor segmentation using random forests with features learned from fully convolutional neural network', 'section': 'body', 'authors': 'Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou'}, page_content='Discriminative approaches are based on extraction of the features from the images \\nand creating models based on the relationship between the image features and the voxel \\nclasses. A vast variety of features are used in the literature such as intensity based  [5], histogram based  [6] and texture features  [7]. Most of the brain tumor segmentation \\ntechniques used hand designed features which are fed into a classifier such as random \\nforests (RF) [8, 9]. Among the conventional classifiers, RFs presents the best segmen-\\ntation results [3, 9]. A limitation of discriminative approaches which use hand designed \\nfeature is that in order to  offer better description of the tissues in the images, the y are \\nrequired to use a large number of features which results in high dimensional problems \\nwhich make the process more complicated and time consuming. In addition, a large \\nnumber of experiments and optimization should be conducted in order  to identify the'),\n",
       " Document(metadata={'arxiv_id': '1704.08134v1', 'title': 'Multimodal MRI brain tumor segmentation using random forests with features learned from fully convolutional neural network', 'section': 'body', 'authors': 'Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou'}, page_content='which make the process more complicated and time consuming. In addition, a large \\nnumber of experiments and optimization should be conducted in order  to identify the \\noptimum parameters for feature extraction and also the optimum classifier.  \\nTo tackle this problem, another variant of discriminative approaches which is using \\nconvolutional neural networks (CNN) for medical image analysis has attracted signifi-\\ncant attention in recent years. Several methods have developed  CNNs to segment the \\nbrain tumors in MRI more accurately [10, 11]. A limitation of CNN based methods is \\nthat the classification is performed on the voxel s, therefore the local dependencies are \\nnot taken into account. Whilst some hand designed feature extraction methods consider \\nthe spatial features and local dependencies of the voxel classes.   \\nIn this paper, we propose d a novel learning based method for fully automated seg-\\nmentation of brain tumor in multimodal MRI images. The machine -learned features'),\n",
       " Document(metadata={'arxiv_id': '1704.08134v1', 'title': 'Multimodal MRI brain tumor segmentation using random forests with features learned from fully convolutional neural network', 'section': 'body', 'authors': 'Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou'}, page_content='In this paper, we propose d a novel learning based method for fully automated seg-\\nmentation of brain tumor in multimodal MRI images. The machine -learned features \\nfrom fully convolutional neural network (FCN) were used to classify the MRI image \\nvoxels. The score map extracted from the FCN were used to localize the tumor area and \\nalso as a feature extraction section to o for the further classification stage. To consider \\nthe voxel neighborhood system and also more accurate classification, texton based fea-\\ntures were used. The proposed method was applied on the publicl y available BRATS \\n2013 dataset [12, 13]. The segmentation results presented here were provided by the \\nonline system. It should be noted that the ground-truth for the challenge dataset are not \\navailable, and that the evaluation was performed by uploading our segmentation and \\ngetting the evaluation results from the online system.  \\nThe main contributions of our method can be summarized as follows:'),\n",
       " Document(metadata={'arxiv_id': '1704.08134v1', 'title': 'Multimodal MRI brain tumor segmentation using random forests with features learned from fully convolutional neural network', 'section': 'body', 'authors': 'Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou'}, page_content='getting the evaluation results from the online system.  \\nThe main contributions of our method can be summarized as follows: \\n\\uf0b7 Proposing a novel fully automatic learning based segmentation method, by applying \\nthe machine-learned features to the state-of the art random forest classifier. \\n\\uf0b7 Applying hand designed texton based features while considering the spatial features \\nand local dependencies in order to improve the segmentation accuracy. \\n\\uf0b7 Using the FCN to include th e tumor region and locally focusing on more accurate \\nsegmentation of tumor in a reasonable processing time by excluding unnecessary \\nprocessing of other parts of the brain that are detected as normal by FCN.  \\n2 Method \\nOur method is comprised of four major steps (pre-processing, FCN, Texton map  \\ngeneration, and RF classification) that are depicted in Fig. 1.   \\n  \\n \\n \\nFig. 1. Flowchart of the proposed method. The FCN architecture and feature extraction proce-\\ndure. \\n2.1 Preprocessing'),\n",
       " Document(metadata={'arxiv_id': '1704.08134v1', 'title': 'Multimodal MRI brain tumor segmentation using random forests with features learned from fully convolutional neural network', 'section': 'body', 'authors': 'Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou'}, page_content='generation, and RF classification) that are depicted in Fig. 1.   \\n  \\n \\n \\nFig. 1. Flowchart of the proposed method. The FCN architecture and feature extraction proce-\\ndure. \\n2.1 Preprocessing \\nIn the first instance we exclude the 1% highest and lowest intensity values for each \\nimage. The intensities are then normalized for each protocol by subtracting the average \\nof intensities of the image and dividing by their standard deviation. In turn, for each \\nindividual protocol, the h istogram of each image is matched to the one of the patient \\nimages which is selected as the reference and then the dynamic range of the intensities \\nis linearly normalized to the range [0, 1].  \\n2.2 Fully Convolutional Neural Network \\nIn this paper, we adopted FCN -8s architecture in [14] for segmentation of brain tu-\\nmor in multimodal MRI images, where the VGG16 [15] is employed as CNN classifi-\\ncation net. In the FCN architecture, initially, the classification net is transformed to be'),\n",
       " Document(metadata={'arxiv_id': '1704.08134v1', 'title': 'Multimodal MRI brain tumor segmentation using random forests with features learned from fully convolutional neural network', 'section': 'body', 'authors': 'Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou'}, page_content='mor in multimodal MRI images, where the VGG16 [15] is employed as CNN classifi-\\ncation net. In the FCN architecture, initially, the classification net is transformed to be \\nfully convolutional net, then adding an upsampling or de -convolutional layer to it  for \\npixel wise predictions. The FCN training is end-to-end supervised learning procedure \\nand the image segmentation is performed using a voxel-wise prediction /classification. \\nThe FCN-8s constructed from FCN-16s skip net and FCN -32s coarse net. The predic-\\ntions at shallow layers are produced using skip layer which combines coarse predictions \\nat deep layers to improve segmentation detail s. More specifically  in our experiment, \\nthe FCN-8s is implemented by fusing predictions of shallower layer (Pool3) with 2 Ã— \\nupsampling of the sum of two predictions derived from pool4 and last layer. Then the \\nstride 8 predictions are upsampled back to the image.'),\n",
       " Document(metadata={'arxiv_id': '1704.08134v1', 'title': 'Multimodal MRI brain tumor segmentation using random forests with features learned from fully convolutional neural network', 'section': 'body', 'authors': 'Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou'}, page_content='upsampling of the sum of two predictions derived from pool4 and last layer. Then the \\nstride 8 predictions are upsampled back to the image.  \\nThe FCN-8s produces more detailed segmentations comparing to the FCN-16s, how-\\never, the lack of the spatial regularization for FCN leads to label disagreement between \\nsimilar pixels and diminished the spatial consistency for segmentation.  \\n In the next section, we introduce  spatial features extracted based on texton which \\nuses the three dimensional connectivity neighborhood system to complement the FCN \\nweak point of considering only the voxels.  \\n2.3 Spatial texton features \\nThe texton based features are strong tools for description of textures in images. They \\nare applied to the proposed method as human-designed features to support the machine-\\nlearned features and improve the segmentation results. Textons are obtained by con-\\nvolving the image with a specific filter bank. In this paper, Gabor filters are used which'),\n",
       " Document(metadata={'arxiv_id': '1704.08134v1', 'title': 'Multimodal MRI brain tumor segmentation using random forests with features learned from fully convolutional neural network', 'section': 'body', 'authors': 'Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou'}, page_content=\"learned features and improve the segmentation results. Textons are obtained by con-\\nvolving the image with a specific filter bank. In this paper, Gabor filters are used which \\nare defined by the following formulation: \\nðº(ð‘¥,ð‘¦;ðœƒ,ðœŽ,ðœ†,ðœ“,ð›¾) = exp\\u2061(âˆ’\\nð‘¥â€²2+ð›¾2ð‘¦â€²2\\n2\\u2061ðœŽ2 )exp\\u2061(ð‘–(2ðœ‹\\nð‘¥â€²\\nðœ† +ðœ“))    (1) \\nwhere, Ïƒ is the standard deviation of Gaussian envelope, Î³ is the spatial aspect ratio, \\nÎ» is the wavelength of sinusoid and Ïˆ is the phase shift. The terms x' and y' are obtained \\nfrom: \\n{ð‘¥â€² \\u2061= \\u2061\\u2061ð‘¥cosðœƒ\\u2061+ ð‘¦sinðœƒ\\nð‘¦â€² = âˆ’ð‘¥sinðœƒ + ð‘¦cosðœƒ\\n (2) \\nwhere, Î¸ is the spatial orientation of the filter. A set of Gabor filters with different pa-\\nrameters creates the filter bank. The Gabor filter parameters are chosen using exhaus-\\ntive grid search. To cover all orientations six different filter directions were  used: [0o, \\n30o, 45o, 60o, 90o, 120o]. Filter sizes are in the range from 0.3 to 1.5 using a step of 0.3. \\nThe wavelength of sinusoid coefficients of the Gabor filters were 0.8, 1.0, 1.2 and 1.5.\"),\n",
       " Document(metadata={'arxiv_id': '1704.08134v1', 'title': 'Multimodal MRI brain tumor segmentation using random forests with features learned from fully convolutional neural network', 'section': 'body', 'authors': 'Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou'}, page_content='30o, 45o, 60o, 90o, 120o]. Filter sizes are in the range from 0.3 to 1.5 using a step of 0.3. \\nThe wavelength of sinusoid coefficients of the Gabor filters were 0.8, 1.0, 1.2 and 1.5.  \\nEach MRI protocol is convolved with al l the Gabor filters in  the bank. The filter \\nresponses are then merged together and clustered into ktexton clusters using k-means clus-\\ntering. The number ktexton = 16 was selected as the optimum value for the number of \\nclusters in texton map. The texton map is created by assigning the cluster number to \\neach voxel of the image. The texton feature for each voxel is the histogram of textons \\nin a neighborhood window of 5 Ã— 5 around that voxel.     \\nThe feature vector is generated for each voxel based on the score map from the FCN. \\nFor each class label, a score map is generated, 5 maps are generated using the standard \\nBRATS labelling system. The values of each map layer corresponding to each voxel'),\n",
       " Document(metadata={'arxiv_id': '1704.08134v1', 'title': 'Multimodal MRI brain tumor segmentation using random forests with features learned from fully convolutional neural network', 'section': 'body', 'authors': 'Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou'}, page_content='For each class label, a score map is generated, 5 maps are generated using the standard \\nBRATS labelling system. The values of each map layer corresponding to each voxel \\nare considered as machine -designed features of that vo xel. The normalized intensity \\nvalue of the voxels in each modality which is obtained from the pre-processing stage is \\nalso included in the feature vector. Therefore, in total 56 features were collected (5 FCN \\nscore map, 3 protocol intensity and 48 texton histogram) for usage in the next step.  \\n2.4 RF classification \\nRandom forests (RF) is among the most powerful classifiers [16], and is an ensemble \\nof multiple decision trees. Each node of a tree includes a set of training examples and \\na predictor. A random subset of fea tures is selected at each attribute split during the  \\n \\nbagging process. The trees are growing until a specified tree depth Dtree. A vote for the \\nmost popular class is made after generating a large number of trees. The target region'),\n",
       " Document(metadata={'arxiv_id': '1704.08134v1', 'title': 'Multimodal MRI brain tumor segmentation using random forests with features learned from fully convolutional neural network', 'section': 'body', 'authors': 'Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou'}, page_content='bagging process. The trees are growing until a specified tree depth Dtree. A vote for the \\nmost popular class is made after generating a large number of trees. The target region \\nin which the RF is applied  is guided by the ROI which was detected by the FCN. A \\nconfidence margin of 10 voxels in 3D space around the detected tumor area is selected \\nby morphological dilation. The feature vector for voxels in this target area are fed to \\nthe random forests for trai ning. The main parameters in designing RF are the number \\nof trees, tree depth and the number of attributes (katribute) which is selected to perform \\nthe random split. The optimum value for katribute for the classification tasks is katribute = \\nâˆšNfeature where Nfeature is the total number of features, in our study katribute = 7. RF pa-\\nrameters were tuned by examining different tree depths and number of trees on clinical \\ntraining datasets and evaluating the classification accuracy using 4 -fold cross valida-'),\n",
       " Document(metadata={'arxiv_id': '1704.08134v1', 'title': 'Multimodal MRI brain tumor segmentation using random forests with features learned from fully convolutional neural network', 'section': 'body', 'authors': 'Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou'}, page_content='rameters were tuned by examining different tree depths and number of trees on clinical \\ntraining datasets and evaluating the classification accuracy using 4 -fold cross valida-\\ntion. The number of trees Ntree = 50 with depth Dtree = 15 provide an optimum general-\\nization and accuracy. Based on the classes assigned for each voxel in the test dataset, \\nthe final segment ation mask is created by mapping back the voxel estimated class to \\nthe segmentation mask volume. Finally, t he bright regions in the healthy part of the \\nbrain near to the skull are eliminated using a connected component analysis. \\n3 Experimental Results and Discussion \\nThe method was evaluated on the publicly available MICCAI BRATS 2013 [12, 13] \\ndataset which is provided by Virtual Skeleton Database (VSD)  [13]. The training da-\\ntaset consists of 30 patient MRI scans of which 20 are high-grade and 10 are low-grade \\ngliomas. The test dataset consists of 10 cases with high grade gliomas. The dataset has'),\n",
       " Document(metadata={'arxiv_id': '1704.08134v1', 'title': 'Multimodal MRI brain tumor segmentation using random forests with features learned from fully convolutional neural network', 'section': 'body', 'authors': 'Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou'}, page_content='taset consists of 30 patient MRI scans of which 20 are high-grade and 10 are low-grade \\ngliomas. The test dataset consists of 10 cases with high grade gliomas. The dataset has \\nbeen already skull -removed, registered and interpolated by the BRATS challenge or-\\nganizers. The MRI sequences FLAIR, T1-weighted+contrast and T2-weighted are ap-\\nplied to the FCN. The segmented masks obtained from our automated method using the \\nchallenge testing dataset are uploaded to the website and evaluated by the correspond-\\ning online system. The ground-truth for training dataset s are provided in which four  \\nlabels are assigned to the tumor tissue parts i.e. oedema, necrosis, enhancing and non-\\nenhancing tumor. In our method we use the BRATS challenge standard combination \\nwhich are enhancing tumor, core (including necrosis, enhancing and non -enhancing) \\nand complete tumor. \\nThe proposed method was performed on MATLAB 2016b on a PC with CPU Intel'),\n",
       " Document(metadata={'arxiv_id': '1704.08134v1', 'title': 'Multimodal MRI brain tumor segmentation using random forests with features learned from fully convolutional neural network', 'section': 'body', 'authors': 'Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou'}, page_content='which are enhancing tumor, core (including necrosis, enhancing and non -enhancing) \\nand complete tumor. \\nThe proposed method was performed on MATLAB 2016b on a PC with CPU Intel \\nCore i7 and RAM 16 GB with the operating system windows 8.1. The FCN was imple-\\nmented using  MatCovNet toolbox [17]. GPU GeForce gtx980i was used for reducing \\nthe training time of the FCN. The RF was implemented using open source code pro-\\nvided in [18] which is a specialized toolbox for RF classification based on MATLAB. \\nThe evaluation measure which are provided by the VSD website, i.e. Dice score, \\npositive predictive value (PPV) and sensitivity were used to compare the segmentation \\nresults with the gold standard ( blind testing). Table 1 provides the evaluation results \\nobtained by applying the proposed method on BRATS 2013 challenge dataset. In the \\nthird row of Table 1, the values in parentheses show the current ranking of each indi-'),\n",
       " Document(metadata={'arxiv_id': '1704.08134v1', 'title': 'Multimodal MRI brain tumor segmentation using random forests with features learned from fully convolutional neural network', 'section': 'body', 'authors': 'Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou'}, page_content='obtained by applying the proposed method on BRATS 2013 challenge dataset. In the \\nthird row of Table 1, the values in parentheses show the current ranking of each indi-\\nvidual measure for the corresponding tumor part in the VSD website at the time of \\nsubmission. Currently our overall rank is 5th for the challenge dataset.  Fig. 2 shows examples of segmentation of tumors parts in some BRATS 2013 chal-\\nlenge dataset using FCN only and our proposed method. As the ground truth is not \\naccessible we are not able to include it in Fig. 2.  \\n  \\n \\nFig. 2. Segmentation results for some cases of BRATS 2013 challenge dataset. The first column \\nshows the original FLAIR images, the se cond shows T1 -weighted-contrast, the third column \\nshows the segmentation mask of FCN overlaid on FLAIR image and the forth column shows the \\nsegmentation mask of the proposed method overlaid on FLAIR image. Oedema: green, necrosis: \\nblue, enhancing tumor: red and on-enhancing tumor: yellow.'),\n",
       " Document(metadata={'arxiv_id': '1704.08134v1', 'title': 'Multimodal MRI brain tumor segmentation using random forests with features learned from fully convolutional neural network', 'section': 'body', 'authors': 'Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou'}, page_content='segmentation mask of the proposed method overlaid on FLAIR image. Oedema: green, necrosis: \\nblue, enhancing tumor: red and on-enhancing tumor: yellow. \\nIn order to evaluate the performance of the proposed method (FCN+Texton+RF), two \\ncomparative experiments were also set up. In the first scenario, the labels which were \\ndirectly classified by FCN were considered as the final segmentation mask. In the sec-\\nond scenario, the images were segmented with the features from the FCN score maps \\nand then classified by RF (FCN+RF). Table 1 shows our final experimental results.  \\nIt can be seen that the sensitivity is very good for segmentation using FCN only but \\nthe Dice overlap and PPV are not so good. This implies that FCN is able to d etect the \\narea which includes  the tumor, but it is not able to accurately and locally detect its \\nboundaries. It can be seen in the third column of Fig. 2 that the FCN over-segmented'),\n",
       " Document(metadata={'arxiv_id': '1704.08134v1', 'title': 'Multimodal MRI brain tumor segmentation using random forests with features learned from fully convolutional neural network', 'section': 'body', 'authors': 'Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou'}, page_content='area which includes  the tumor, but it is not able to accurately and locally detect its \\nboundaries. It can be seen in the third column of Fig. 2 that the FCN over-segmented \\nthe tumor area especially the tumor core. Using the machine learned features and ap-\\nplication of RF there is a slight improvement of the Dice score for complete tumor and \\nsignificantly improvement for tumor core and enhancing part and also improvement of \\nPPV for a ll areas. It means that the segmentation boundaries are now closer to the \\nground truth. Sensitivity for complete tumor decreases which represent under-segmen-\\ntation. Adding the texton features to the pipeline improves the overlap measure for \\ncomplete tumor and increases the sensitivity while slightly decreases the PPV. There-\\nfore, the proposed method improves the overlap measure while maintaining a balance \\nbetween sensitivity and PPV.   \\n \\n \\nThe FCN segmentation was able to locate the tumor areas, but is not able to accu-'),\n",
       " Document(metadata={'arxiv_id': '1704.08134v1', 'title': 'Multimodal MRI brain tumor segmentation using random forests with features learned from fully convolutional neural network', 'section': 'body', 'authors': 'Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou'}, page_content='between sensitivity and PPV.   \\n \\n \\nThe FCN segmentation was able to locate the tumor areas, but is not able to accu-\\nrately segment them, as it provides coarse segmentations (see Fig.2. third column). To \\ntackle this problem, we propose  to consider the local dependencies and neighborhood \\nsystem of voxel s in classification by using texton features. The experimental results \\nemphasises that this refinement increases significantly the accuracy while keeping bal-\\nance between sensitivity and PPV. As an example  we can observe that  our \\nFCN+Texton+RF method produces finer segmentations compared with the segmenta-\\ntions of FCN+R F. Using 3D texton also enable us to consider the connectivity infor-\\nmation in all directions in 3D space which compensate the limitation of FCN which \\nonly works on 2D slices. \\nOne limitation of the proposed method is that the training stage is time consumin g. \\nHowever, when the model is created, both FCN and RF are fast to use for classification'),\n",
       " Document(metadata={'arxiv_id': '1704.08134v1', 'title': 'Multimodal MRI brain tumor segmentation using random forests with features learned from fully convolutional neural network', 'section': 'body', 'authors': 'Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou'}, page_content='only works on 2D slices. \\nOne limitation of the proposed method is that the training stage is time consumin g. \\nHowever, when the model is created, both FCN and RF are fast to use for classification \\nof new datasets. Also the model can be saved so any future training dataset, can be \\nadded to the previously trained model and update it.  \\nThe results of our proposed method which is applied on BRATS 2013 clinical dataset \\nand the related top -ranked works on the same dataset which are on the website score-\\nboard [13] are presented in Table 1. The method in [11] used a developed version of \\ndeep convolutional neural network. The method proposed by Pereira  [10], which is \\nbased on CNN has the best score and ranking on the VSD scoreboard. Our proposed \\nmethod has the same Dice score, PPV and sen sitivity for the complete tumor to this \\nmethod. The method proposed by Tustison et al. [19], which used RF and hand de-'),\n",
       " Document(metadata={'arxiv_id': '1704.08134v1', 'title': 'Multimodal MRI brain tumor segmentation using random forests with features learned from fully convolutional neural network', 'section': 'body', 'authors': 'Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou'}, page_content='method has the same Dice score, PPV and sen sitivity for the complete tumor to this \\nmethod. The method proposed by Tustison et al. [19], which used RF and hand de-\\nsigned features, was the winner of the on -site BRATS 2013 challenge. Ou r method \\noutperformed [19] in terms of Dice score for complete tumor and core and PPV value \\nfor all tumor tissue types.  Our method has the best dice score which is 0.88 at the time \\nof this submission.  \\nTable 1. Segmentation results per case for BRATS 2013 challenge dataset which is evaluated by \\nVSD website. Comparison with other works which used BRATS 2013 challenge dataset and are \\ntop ranked. \\nMethod Dice score Positive Predictive Value Sensitivity \\nComplete Core Enhancing Complete Core Enhancing Complete Core Enhancing \\nFCN 0.79 0.69 0.62 0.77 0.68 0.58 0.83 0.82 0.75 \\nFCN + RF 0.80 0.83 0.71 0.90 0.90 0.73 0.74 0.78 0.78 \\nFCN+Texton  \\n+ RF \\n0.88  \\n(1) \\n0.80 \\n(9) \\n0.73 \\n(19) \\n0.88 \\n(11) \\n0.87 \\n(8) \\n0.80 \\n(5) \\n0.89 \\n(21) \\n0.77 \\n(21) \\n0.70'),\n",
       " Document(metadata={'arxiv_id': '1704.08134v1', 'title': 'Multimodal MRI brain tumor segmentation using random forests with features learned from fully convolutional neural network', 'section': 'body', 'authors': 'Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou'}, page_content='FCN + RF 0.80 0.83 0.71 0.90 0.90 0.73 0.74 0.78 0.78 \\nFCN+Texton  \\n+ RF \\n0.88  \\n(1) \\n0.80 \\n(9) \\n0.73 \\n(19) \\n0.88 \\n(11) \\n0.87 \\n(8) \\n0.80 \\n(5) \\n0.89 \\n(21) \\n0.77 \\n(21) \\n0.70 \\n(32) \\nHavaei [11] 0.88 0.79 0.73 0.89 0.79 0.68 0.87 0.79 0.88 \\nTustison [19] 0.87 0.78 0.74 0.85 0.74 0.69 0.89 0.88 0.87 \\nPereira [10] 0.88 0.83 0.77 0.88 0.87 0.74 0.89 0.83 0.81 \\n4 Conclusion \\nIn this paper, a learning based automatic method is proposed for segmentation of \\nbrain tumor in MRI images. The method is a hybrid approach in which the ma chine-\\nlearned features extracted using the FCN are used alongside with hand designed texton features and applied to the state -of-the-art RF classifier. The proposed method was \\nevaluated on BRATS 2013 challenge dataset by the provided online system. The ex-\\nperimental results suggest that the proposed method achieves promis ing results in the \\nsegmentation of brain tumor and its parts. Adding texton features from different proto-'),\n",
       " Document(metadata={'arxiv_id': '1704.08134v1', 'title': 'Multimodal MRI brain tumor segmentation using random forests with features learned from fully convolutional neural network', 'section': 'body', 'authors': 'Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou'}, page_content='perimental results suggest that the proposed method achieves promis ing results in the \\nsegmentation of brain tumor and its parts. Adding texton features from different proto-\\ncols to the system increases the classification accuracy of the voxels and the final seg-\\nmentations. \\nReferences \\n1. Patel, M.R., Tse, V.: Diagnosis and staging of brain tumors. Semin Roentgenol. 39, 347â€“360 (2004). \\n2. Gordillo, N., Montseny, E., Sobrevilla, P.: State of the art survey on MRI brain tumor segmentation. \\nMagn Reson Imaging. 31, 1426â€“1438 (2013). \\n3. Menze, B.H., Jakab, A.,  et.al.: The Multimodal Brain Tumor Image Segmentation Benchmark \\n(BRATS). IEEE Transactions on Medical Imaging. 34, 1993â€“2024 (2015). \\n4. Bauer, S., Wiest, R., Nolte, L.-P., Reyes, M.: A survey of MRI-based medical image analysis for brain \\ntumor studies. Phys Med Biol. 58, R97-129 (2013). \\n5. Hamamci, A., Kucuk, N., Karaman, K., Engin, K., Unal, G.: Tumor -Cut: Segmentation of Brain Tu-'),\n",
       " Document(metadata={'arxiv_id': '1704.08134v1', 'title': 'Multimodal MRI brain tumor segmentation using random forests with features learned from fully convolutional neural network', 'section': 'body', 'authors': 'Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou'}, page_content='tumor studies. Phys Med Biol. 58, R97-129 (2013). \\n5. Hamamci, A., Kucuk, N., Karaman, K., Engin, K., Unal, G.: Tumor -Cut: Segmentation of Brain Tu-\\nmors on Contrast Enhanced MR Images for Radiosurgery Applications. IEEE Transactions on Medical \\nImaging. 31, 790â€“804 (2012). \\n6. Kleesiek, J., Biller, A., Urban, G., KÃ¶the, U., Bendszus, M., Hamprecht, F.: ilastik for Multi -modal \\nBrain Tumor Segmentation. \\n7. Subbanna, N.K., Precup, D., Collins, D.L., Arbel, T.: Hierarchical Probabilistic Gabor and MRF Seg-\\nmentation of Brain Tumours in MRI Volumes. In: Mori, K., Sakuma, I., Sato, Y., Barillot, C., and \\nNavab, N. (eds.) Medical Image Computing and Computer-Assisted Intervention â€“ MICCAI 2013. pp. \\n751â€“758. Springer Berlin Heidelberg (2013). \\n8. Pinto, A., Pereira, S., Correia, H., Oliveira, J., Rasteiro, D.M.L.D., Silva, C.A.: Brain Tumour Segmen-\\ntation based on Extremely Randomized Forest with high-level features. In: 2015 37th Annual Interna-'),\n",
       " Document(metadata={'arxiv_id': '1704.08134v1', 'title': 'Multimodal MRI brain tumor segmentation using random forests with features learned from fully convolutional neural network', 'section': 'body', 'authors': 'Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou'}, page_content='tation based on Extremely Randomized Forest with high-level features. In: 2015 37th Annual Interna-\\ntional Conference of the IEEE Engineering in Medicine and Biology Society (EMBC). pp. 3037â€“3040 \\n(2015). \\n9. Gotz, M., Weber, C., Blocher, J., Stieltjes, B., Meinzer, H., Maier -Hein, K.: Extremely randomized \\ntrees based brain tumor segmentation. In: Proceeding of BRATS Challenge -MICCAI. pp. 006 -011 \\n(2014). \\n10. Pereira, S., Pinto, A., Alves, V., Silva, C.A.: Brain Tumor Segmentation Using Convolutional Neural \\nNetworks in MRI Images. IEEE Transactions on Medical Imaging. 35, 1240â€“1251 (2016). \\n11. Havaei, M., Davy, A., Warde-Farley, D., Biard, A., Courville, A., Bengio, Y., Pal, C., Jodoin, P. -M., \\nLarochelle, H.: Brain tumor segmentation with Deep Neural Networks. Medical Image Analysis. 35, \\n18â€“31 (2017). \\n12. Kistler, M., Bonaretti, S., Pfahrer, M., Niklaus, R., BÃ¼chler, P.: The virtual skeleton database: an open'),\n",
       " Document(metadata={'arxiv_id': '1704.08134v1', 'title': 'Multimodal MRI brain tumor segmentation using random forests with features learned from fully convolutional neural network', 'section': 'body', 'authors': 'Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou'}, page_content='18â€“31 (2017). \\n12. Kistler, M., Bonaretti, S., Pfahrer, M., Niklaus, R., BÃ¼chler, P.: The virtual skeleton database: an open \\naccess repository for biomedical research and collaboration. J. Med. Internet Res. 15, e245 (2013). \\n13. BRATS\\u202f:: The Virtual Skeleton Database Project, https://www.smir.ch/BRATS/Start2013. \\n14. Long, J., Shelhamer, E., Darrell, T.: Fully Convolutional Networks for Semantic Segmentation. Pre-\\nsented at the Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (2015). \\n15. Simonyan, K., Zisserman, A.: Very Deep Convolutional Networks for Large-Scale Image Recognition. \\narXiv:1409.1556 [cs]. (2014). \\n16. Liaw, A., Wiener, M.: Classification and regression by randomForest. 2, 18â€“22 (2002). \\n17. Vedaldi, A., Lenc, K.: MatConvNet: Convolutional Neural Networks for MATLAB. In: Proceedings \\nof the 23rd ACM International Conference on Multimedia. pp. 689â€“692. ACM, New York, NY, USA \\n(2015).'),\n",
       " Document(metadata={'arxiv_id': '1704.08134v1', 'title': 'Multimodal MRI brain tumor segmentation using random forests with features learned from fully convolutional neural network', 'section': 'body', 'authors': 'Mohammadreza Soltaninejad, Lei Zhang, Tryphon Lambrou'}, page_content='17. Vedaldi, A., Lenc, K.: MatConvNet: Convolutional Neural Networks for MATLAB. In: Proceedings \\nof the 23rd ACM International Conference on Multimedia. pp. 689â€“692. ACM, New York, NY, USA \\n(2015). \\n18. Taormina, R.: MATLAB_ExtraTrees - File Exchange - MATLAB Central, http://uk.math-\\nworks.com/matlabcentral/fileexchange/47372-rtaormina-matlab-extratrees. \\n19. Tustison, N.J., Shrinidhi, K.L., Wintermark, M., Durst, C.R., Kandel, B.M., Gee, J.C., Grossman, M.C., \\nAvants, B.B.: Optimal Symmetric Multimodal Templates and Concatenated Random Forests for Su-\\npervised Brain Tumor Segmentation (Simplified) with ANTsR. Neuroinform. 13, 209â€“225 (2014).'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'title_abstract', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='Title: A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation\\n\\nAbstract: Automatic MRI brain tumor segmentation is of vital importance for the disease diagnosis, monitoring, and treatment planning. In this paper, we propose a two-stage encoder-decoder based model for brain tumor subregional segmentation. Variational autoencoder regularization is utilized in both stages to prevent the overfitting issue. The second-stage network adopts attention gates and is trained additionally using an expanded dataset formed by the first-stage outputs. On the BraTS 2020 validation dataset, the proposed method achieves the mean Dice score of 0.9041, 0.8350, and 0.7958, and Hausdorff distance (95%) of 4.953, 6.299, and 23.608 for the whole tumor, tumor core, and enhancing tumor, respectively. The corresponding results on the BraTS 2020 testing dataset are 0.8729, 0.8357, and 0.8205 for Dice score, and 11.4288, 19.9690, and 15.6711 for Hausdorff distance. The code is publicly available at https://github.com/shu-hai/two-stage-VAE-Attention-gate-BraTS2020.'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='A Two-Stage Cascade Model with Variational\\nAutoencoders and Attention Gates for MRI\\nBrain Tumor Segmentation\\nChenggang Lyu and Hai Shu (\\x00 )\\nDepartment of Biostatistics, School of Global Public Health, New York University,\\nNew York, NY 10003, USA\\nhs120@nyu.edu\\nAbstract. Automatic MRI brain tumor segmentation is of vital impor-\\ntance for the disease diagnosis, monitoring, and treatment planning. In\\nthis paper, we propose a two-stage encoder-decoder based model for brain\\ntumor subregional segmentation. Variational autoencoder regularization\\nis utilized in both stages to prevent the overï¬tting issue. The second-stage\\nnetwork adopts attention gates and is trained additionally using an ex-\\npanded dataset formed by the ï¬rst-stage outputs. On the BraTS 2020\\nvalidation dataset, the proposed method achieves the mean Dice score\\nof 0.9041, 0.8350, and 0.7958, and Hausdorï¬€ distance (95%) of 4.953 ,\\n6.299, 23.608 for the whole tumor, tumor core, and enhancing tumor, re-'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='of 0.9041, 0.8350, and 0.7958, and Hausdorï¬€ distance (95%) of 4.953 ,\\n6.299, 23.608 for the whole tumor, tumor core, and enhancing tumor, re-\\nspectively. The corresponding results on the BraTS 2020 testing dataset\\nare 0.8729, 0.8357, and 0.8205 for Dice score, and 11.4288, 19.9690, and\\n15.6711 for Hausdorï¬€ distance. The code is publicly available at https:\\n//github.com/shu-hai/two-stage-VAE-Attention-gate-BraTS2020 .\\nKeywords: Attention gate, Brain tumor segmentation, Encoder-decoder\\nnetwork, Variational autoencoder\\n1 Introduction\\nBrain tumors can be categorized into primary tumors and secondary tumors\\ndepending on where they originate. Glioma, the most common type of primary\\nbrain tumor, can be further categorized into low-grade gliomas (LGG) and high-\\ngrade gliomas (HGG). HGG is a malignant brain tumor type with a high degree\\nof aggressiveness that often requires surgery. Usually, several complimentary 3D\\nMagnetic Resonance Imaging (MRI) modalities are acquired to highlight diï¬€er-'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='of aggressiveness that often requires surgery. Usually, several complimentary 3D\\nMagnetic Resonance Imaging (MRI) modalities are acquired to highlight diï¬€er-\\nent tissue properties and areas of tumor spread. Compared to traditional meth-\\nods that rely on physiciansâ€™ professional knowledge and experience, automatic\\n3D brain tumor segmentation is time-eï¬ƒcient and can provide objective and\\nreproducible results for further tumor analysis and monitoring. In recent years,\\ndeep-learning based segmentation approaches have exhibited superior perfor-\\nmance than traditional methods.\\nThe Multimodal Brain Tumor Segmentation Challenge (BraTS) is an annual\\ninternational competition that aims to evaluate state-of-the-art methods of brain\\narXiv:2011.02881v2  [eess.IV]  28 Nov 20202 C. Lyu and H. Shu\\ntumor segmentation [1,2,3,13]. The organizer provides a 3D multimodal MRI\\ndataset with â€œground-truthâ€ tumor segmentation labels annotated by physicians'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='tumor segmentation [1,2,3,13]. The organizer provides a 3D multimodal MRI\\ndataset with â€œground-truthâ€ tumor segmentation labels annotated by physicians\\nand radiologists. For each patient, four 3D MRI modalities are provided including\\nnative T1-weighted (T1), post-contrast T1-weighted (T1c), T2-weighted (T2),\\nand T2 Fluid Attenuated Inversion Recovery (T2-FLAIR) volumes. The brain\\ntumor segmentation task concentrates on three tumor sub-regions: the necrotic\\nand non-enhancing tumor (NCR/NET, labeled 1), the peritumoral edema (ED,\\nlabeled 2) and the GD-enhancing tumor (ET, labeled 4). Fig. 1 shows an im-\\nage set of a patient. The rankings of competing methods for this segmentation\\ntask are determined by metrics, including Dice score, Hausdorï¬€ distance (95%),\\nSensitivity, and Speciï¬city, evaluated on the testing dataset for ET, tumor core\\n(TC=ET+NCR/NET), and whole tumor (WT=TC+ED) [4].\\nIn BraTS 2018, Myronenko [14] proposed an asymmetrical U-Net with a'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='Sensitivity, and Speciï¬city, evaluated on the testing dataset for ET, tumor core\\n(TC=ET+NCR/NET), and whole tumor (WT=TC+ED) [4].\\nIn BraTS 2018, Myronenko [14] proposed an asymmetrical U-Net with a\\nlarger encoder for feature extraction and a smaller decoder for label reconstruc-\\ntion, and won the ï¬rst place of the challenge. An encouraging innovation of the\\nmethod is utilizing a variational autoencoder (VAE) branch to regularize the en-\\ncoder and boost generalization performance. The champion team of BraTS 2019,\\nJiang et al. [12], proposed a two-stage network, which used an asymmetrical U-\\nNet, similar to Myronenko [14], in the ï¬rst stage to obtain a coarse prediction,\\nand then employed a similar but wider network in the second stage to reï¬ne\\nthe prediction. An additional branch was adopted in the decoder of the second-\\nstage network to regularize the associated encoder. The success of the above two\\nmodels indicates the feasibility and the importance of adding a branch to the'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='stage network to regularize the associated encoder. The success of the above two\\nmodels indicates the feasibility and the importance of adding a branch to the\\ndecoder to reduce overï¬tting and boost the model performance.\\nFig. 1. An example image set. Subï¬gure (e) highlights three tumor subregions: ED\\n(orange), NCR/NET (yellow), and ET (red).Title Suppressed Due to Excessive Length 3\\nCompared with general computer vision problems, 3D MRI image segmen-\\ntation tasks generally face two special challenges: the scarcity of training data\\nand the class imbalance [20]. To alleviate the shortage of training data, Isensee\\net al. [11] took the advantage of additional labeled data by using a co-training\\nstrategy. Zhou et al. [23] combined several performance-boosting tricks, such\\nas introducing a focal loss to alleviate the class imbalance, to achieve further\\nimprovements.\\nFor brain tumor segmentation tasks speciï¬cally, another challenging diï¬ƒculty'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='as introducing a focal loss to alleviate the class imbalance, to achieve further\\nimprovements.\\nFor brain tumor segmentation tasks speciï¬cally, another challenging diï¬ƒculty\\nis the variability of tumor morphology and location across diï¬€erent tumor de-\\nvelopment stages and diï¬€erent cases. To improve the prediction accuracy, many\\nsegmentation methods [18,16,23,22] decompose the task into separate localiza-\\ntion and subsequent segmentation steps, with additional preceding models for\\nobject localization. For instance, Wang et al. [18] sequentially trained three net-\\nworks according to the tumor subregion hierarchy. Oktay et al. [15] demonstrated\\nthat the same objective can be achieved by introducing attention gates (AGs)\\ninto the standard convolutional-neural-network framework in pancreas tumor\\nsegmentation tasks.\\nInspired by aforementioned works, in this paper we propose a two-stage cas-\\ncade network for brain tumor segmentation. We borrow the network structure of'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='segmentation tasks.\\nInspired by aforementioned works, in this paper we propose a two-stage cas-\\ncade network for brain tumor segmentation. We borrow the network structure of\\nMyronenko [14] as the ï¬rst-stage network to obtain relatively rough segmenta-\\ntion results. The second stage network uses the concatenation of the preliminary\\nsegmentation maps from the ï¬rst-stage network and the MRI images as the in-\\nput, with the aim to reï¬ne the prediction of the NCR/NET and ET subregions.\\nWe apply AGs [15] to further suppress the feature responses in irrelevant back-\\nground regions. Our second-stage network exhibits the capabilities to (i) provide\\nmore model candidates with competitive performance for model ensembling, (ii)\\nstabilize the predictions across models of diï¬€erent epochs, and (iii) improve the\\nperformance of each single model, particularly for NCR/NET and ET. The im-\\nplementation details and segmentation results are provided in Sections 3 and 4.\\n2 Method'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='performance of each single model, particularly for NCR/NET and ET. The im-\\nplementation details and segmentation results are provided in Sections 3 and 4.\\n2 Method\\nThe proposed two-stage network structure consists of two cascaded networks.\\nThe ï¬rst-stage network takes the multimodal MRI images as input and predicts\\ncoarse segmentation maps. The concatenation of the preliminary segmentation\\nmaps and the MRI images is passed into the second-stage network to generate\\nimproved segmentation results.\\n2.1 The First-Stage Network: Asymmetrical U-Net with a VAE\\nBranch\\nThe network architecture (Fig. 2) consists of a larger encoding path for semantic\\nfeature extraction, a smaller decoding path for segmentation map prediction,\\nand a VAE branch for input images reconstruction. This part is identical to the\\nnetwork proposed in [14].4 C. Lyu and H. Shu\\nEncoder The encoder consists of ResNet [7,8] blocks for four spatial levels,'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='network proposed in [14].4 C. Lyu and H. Shu\\nEncoder The encoder consists of ResNet [7,8] blocks for four spatial levels,\\nwith the number of blocks 1, 2, 2, and 4, respectively. Each ResNet block has\\ntwo convolutions with Group Normalization and ReLU, followed by an addi-\\ntive identity skip connection. The input of the encoder is an MRI crop of size\\n4Ã—160Ã—192Ã—128, with the ï¬rst channel referring to the four MRI modalities. The\\ninput is processed by a 3 Ã—3Ã—3 convolution layer with 32 ï¬lters and a dropout\\nlayer with a rate of 0.2, and then passed through a series of ResNet blocks. Be-\\ntween every two blocks with diï¬€erent spatial levels, a 3Ã—3Ã—3 convolution with a\\nstride of 2 is used to reduce the resolution of the feature maps by 2 and double\\nthe number of feature channels simultaneously. The endpoint of the encoder has\\nsize 256Ã—20Ã—24Ã—16, which is 1/8 of the spatial size of the input data.\\nDecoder The decoder has an almost symmetrical architecture with the encoder,'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='size 256Ã—20Ã—24Ã—16, which is 1/8 of the spatial size of the input data.\\nDecoder The decoder has an almost symmetrical architecture with the encoder,\\nexcept for the number of ResNet blocks within each spatial level is 1. After\\neach block, we use a trilinear up-sampler to recover the spatial size by 2 and a\\n1Ã—1Ã—1 convolution to reduce the number of feature channels by 2, followed by\\nan additive skip connection from the encoder output of the corresponding spatial\\nlevel. The operations within each block are the same as those in the encoder. At\\nthe end of the decoder, a 1 Ã—1Ã—1 convolution is used to reduce the number of\\nfeature channels from 32 to 3, followed by a sigmoid function to convert feature\\nmaps into probability maps.\\nVAE Branch This decoder branch receives the output of the encoder and\\nproduces a reconstructed image of the original input. In the beginning, the de-\\ncoder endpoint output is reduced to a lower-dimensional space of 256 using a'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='produces a reconstructed image of the original input. In the beginning, the de-\\ncoder endpoint output is reduced to a lower-dimensional space of 256 using a\\nfully connected layer, where 256 represents 128 means and 128 standard devia-\\ntions of Gaussian distributions, from which a sample of size 128 is drawn. Then\\nthe drawn vector is mapped back to the high-dimensional space with the same\\nspatial property and reconstructed into the input image dimensions gradually\\nfollowing the same strategy as the decoder. Notice that there is no additive skip\\nconnection between encoder and the VAE branch.\\n2.2 The Second-Stage Network: Attention-Gated Asymmetrical\\nU-Net with the VAE Branch\\nThe input of the second-stage network (Fig. 2) is constructed based on the\\nsegmentation maps produced by the ï¬rst-stage network. To alleviate the label\\nimbalance problem, we crop the output of the ï¬rst-stage network into a spatial\\nsize of 128Ã—128Ã—128 voxels concentrating on the tumor area. The cropped seg-'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='imbalance problem, we crop the output of the ï¬rst-stage network into a spatial\\nsize of 128Ã—128Ã—128 voxels concentrating on the tumor area. The cropped seg-\\nmentation maps are then concatenated to the original MRI images (cropped to\\nthe same area).\\nEncoder The encoder part of the second-stage network has the same struc-\\nture as in the ï¬rst-stage network, whereas the input has 7 channels (3 for seg-Title Suppressed Due to Excessive Length 5\\nmentation maps and 4 for multimodal MRI images), and has a spatial size of\\n128Ã—128Ã—128 voxels.\\nDecoder Diï¬€erent from the ï¬rst-stage network, we add the AGs of [15] in the\\ndecoder part. The architecture of the AGs is demonstrated in the next sub-\\nsection. At each spatial level, the gating signal from the coarser scale is passed\\ninto the attention gate to determine the attention coeï¬ƒcients. The output of\\nan AG is the Hadamard product of input features from encoder through skip'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='into the attention gate to determine the attention coeï¬ƒcients. The output of\\nan AG is the Hadamard product of input features from encoder through skip\\nconnection and attention coeï¬ƒcients. The output of AG at each spatial level is\\nthen integrated with the 2-times up-sampled features from the coarser scale by\\nan element-wise summation. The rest of the network architecture remains the\\nsame as the decoder in the ï¬rst-stage network.\\nAttention Gate Instead of using a single identical scalar value to represent\\nattention level for each pixel vector, a gating vector gi is computed to determine\\nFig. 2. The network architecture of both stages. In the ï¬rst stage, input (orange strip)\\nis the cropped MRI images (4 Ã— 160 Ã— 192 Ã— 128), followed by a 3 Ã— 3 Ã— 3 convolution\\nwith 32 ï¬lters and a dropout layer (yellow strip). The output of the decoder is a\\nsegmentation map of size 3 Ã— 160 Ã— 192 Ã— 128 with three channels indicating three'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='with 32 ï¬lters and a dropout layer (yellow strip). The output of the decoder is a\\nsegmentation map of size 3 Ã— 160 Ã— 192 Ã— 128 with three channels indicating three\\ntumor subregions (WT, TC, and ET). The VAE branch is in charge of input image\\nreconstruction and is disabled while doing inference. In the second stage, the input is\\nthe cropped concatenation of the ï¬rst-stage segmentation map (blue strip) and MRI\\nimages (orange strip) (total 7 Ã— 128 Ã— 128 Ã— 128), and the output is a segmentation\\nmap (3 Ã— 128 Ã— 128 Ã— 128). Note that there is no input concatenation and attention\\ngates in the ï¬rst-stage network.6 C. Lyu and H. Shu\\nfocus regions for each pixel i. Within the l-th spatial level, the AG is formulated\\nas follows:\\nql\\natt = WT\\nintÏƒ1(WT\\nXxl\\ni + WT\\ng gl+1\\ni + bg) + bWint (1)\\nÎ±l\\ni = Ïƒ2(ql\\natt(xl\\ni,gl+1\\ni ; Î¸att)) (2)\\nË†xl\\ni = Î±l\\ni Ã— xl\\ni (3)\\nIn each AG (Fig. 3), complementary information is extracted from the gating\\nsignal gl+1'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='Xxl\\ni + WT\\ng gl+1\\ni + bg) + bWint (1)\\nÎ±l\\ni = Ïƒ2(ql\\natt(xl\\ni,gl+1\\ni ; Î¸att)) (2)\\nË†xl\\ni = Î±l\\ni Ã— xl\\ni (3)\\nIn each AG (Fig. 3), complementary information is extracted from the gating\\nsignal gl+1\\ni from the coarser scale. To reduce the computational cost, linear trans-\\nformations WT\\nx and WT\\ng (1Ã—1Ã—1 convolutions) are performed on the input fea-\\ntures xl\\ni and gating signals gl+1\\ni , to downsize the feature size by 2, and to reduce\\nthe number of channels by 2, respectively. The transformed input features and\\ngating signals therefore have the same spatial shape. The sum of them through\\nelement-wise summation is activated by the ReLU function Ïƒ1 and mapped by\\nWT\\nint into a lower dimensional space for gating operation, followed by the sigmoid\\nfunction Ïƒ2 and a trilinear up-sampler to restore the size of attention coeï¬ƒcients\\nmatrix Î±l\\ni to match the resolution of the input features. The output Ë† xl\\ni of the\\nAG is obtained by element-wise multiplication of the input features xl\\ni and the'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='matrix Î±l\\ni to match the resolution of the input features. The output Ë† xl\\ni of the\\nAG is obtained by element-wise multiplication of the input features xl\\ni and the\\nattention coeï¬ƒcient matrix Î±l\\ni.\\nFig. 3. Attention gate.\\n2.3 Loss Function\\nFor both stages, the loss function has 3 parts:\\nL= Ldice + 0.1 Ã— LL2 + 0.1 Ã— LKL. (4)\\nLdice is the soft dice loss that encourages the decoder output ppred to match the\\nground-truth segmentation mask ptrue:\\nLdice = 1 âˆ’ 2 Ã— âˆ‘ppred Ã— ptrueâˆ‘p2\\npred + âˆ‘p2\\ntrue\\n. (5)Title Suppressed Due to Excessive Length 7\\nLL2 is the L2 loss that is applied to the VAE branch output Ipred to match the\\ninput image Iinput:\\nLL2 =\\nâˆ‘\\n(Ipred âˆ’ Iinput)2. (6)\\nLKL is the KL divergence that is used as a VAE penalty term to induce the\\nestimated Gaussian distribution to approach the standard Gaussian distribution:\\nLKL = 1\\nN\\nâˆ‘\\nÂµ2 + Ïƒ2 âˆ’ log Ïƒ2 âˆ’ 1, (7)\\nwhere N is the number of the voxels. As suggested in [14], we set the hyper-'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='estimated Gaussian distribution to approach the standard Gaussian distribution:\\nLKL = 1\\nN\\nâˆ‘\\nÂµ2 + Ïƒ2 âˆ’ log Ïƒ2 âˆ’ 1, (7)\\nwhere N is the number of the voxels. As suggested in [14], we set the hyper-\\nparameter weight to be 0.1 to reach a good balance between the dice and VAE\\nloss terms.\\n3 Expriment\\n3.1 Data Description\\nThe BraTS 2020 training dataset includes 259 cases of HGG and 110 cases of\\nLGG. All image modalities (T1, T1c, T2, and T2-FLAIR) are co-registered with\\nimage size of 240 Ã—240Ã—155 voxels and 1 mm isotropic resolution. The training\\ndata are provided with annotations, while the validation dataset (125 cases)\\nand testing dataset (166 cases) are provided without annotations. Participants\\ncan evaluate their methods by uploading predicted segmentation volumes to the\\norganizerâ€™s server. Multiple times of submission for the validation evaluation are\\npermitted, whereas only one submission is allowed for the ï¬nal testing evaluation.\\n3.2 Implementation Details'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='organizerâ€™s server. Multiple times of submission for the validation evaluation are\\npermitted, whereas only one submission is allowed for the ï¬nal testing evaluation.\\n3.2 Implementation Details\\nOur network is implemented in Pytorch and trained on four NVIDIA P40 GPUs.\\nOptimization We use Adam optimizer with initial learning rate of lr0 = 10âˆ’4\\nfor weights updating. We progressively decay the learning rate according to the\\nfollowing formula:\\nlr= lr0 Ã— (1 âˆ’ e\\nNe\\n)0.9, (8)\\nwhere e is an epoch counter, and Ne is the total number of the epochs during\\ntraining. In our case, Ne is set to 300.\\nData Preprocessing Before feeding input data into the ï¬rst-stage network,\\nwe preprocessed the input data by applying intensity normalization to each MRI\\nmodality for each patient. The data is subtracted by the mean and divided by\\nthe standard deviation of the non-zero region. In the second stage, we crop\\nthe segmentation maps from the ï¬rst-stage network into 128 Ã—128Ã—128-sized'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='the standard deviation of the non-zero region. In the second stage, we crop\\nthe segmentation maps from the ï¬rst-stage network into 128 Ã—128Ã—128-sized\\npatches for each patient while ensuring that the patch includes most tumor\\nvoxels. The patches are concatenated with the normalized MRI images (after\\ndata augmentation, cropped at the same position) and fed to the second-stage\\nnetwork for training.8 C. Lyu and H. Shu\\nData AugmentationTo reduce the risk of overï¬tting, three data augmentation\\nstrategies are used. First, the training data is randomly cropped into size of\\n160Ã—192Ã—128 before fed into the ï¬rst-stage network. In addition, in both stages,\\nwe randomly shift the intensity of the input data by a value in [ âˆ’0.1,0.1] of the\\nstandard deviation of each channel, and randomly scale intensity of the input\\ndata by a factor in [0 .9,1.1]. Finally, we apply random ï¬‚ipping along each 3D\\naxis with a probability of 50%, in both stages.'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='data by a factor in [0 .9,1.1]. Finally, we apply random ï¬‚ipping along each 3D\\naxis with a probability of 50%, in both stages.\\nExpanded Training DataSince the training processes of the two stages are\\nindependent, we can select several ï¬rst-stage trained models of competitive per-\\nformance and use their segmentation results as the training data for training\\nthe second-stage network. Such a strategy trades a longer training process for\\nbetter model performance and stability of results. Speciï¬cally, we select 6 in-\\ndividual ï¬rst-stage models (of diï¬€erent epochs with diï¬€erent train-validation\\ndivisions) and combined their segmentation results into an extensive dataset to\\nFig. 4. Stage-2 network training schema. In the ï¬rst case, the training (blue) and\\ninference (yellow) of the stage-2 network are based on the segmentation results of\\nthe same stage-1 model; in the second case, the segmentation results from six stage-1'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='inference (yellow) of the stage-2 network are based on the segmentation results of\\nthe same stage-1 model; in the second case, the segmentation results from six stage-1\\nmodels are combined into an extensive training dataset for training the stage-2 network\\n(blue). The segmentation results of three stage-1 models are respectively input into ï¬ve\\nstage-2 models of diï¬€erent epochs for inference (yellow), and ï¬nally 15 segmentation\\nresults are obtained for ensemble (red).Title Suppressed Due to Excessive Length 9\\ntrain the second-stage network (Fig. 4). Note that the train-validation division is\\nbased on patient IDs. The 6 segmentation results belonging to the same patient\\nare consequentially grouped into the same set. We also have tried training the\\nsecond-stage network using one single modelâ€™s segmentation result, but obtained\\nonly slight improvement compared to the ï¬rst-stage network.\\nPostprocess It is observed that when the predicted volume of ET is particularly'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='only slight improvement compared to the ï¬rst-stage network.\\nPostprocess It is observed that when the predicted volume of ET is particularly\\nsmall, the algorithm tends to predict TC voxels as ET falsely. In post-processing,\\nbased on our experience we replace ET with TC when the volume of predicted\\nET is less than 500 voxels.\\nEnsemble We use majority voting to conduct model ensembling. In particular,\\nif a voxel has equal votes in multiple categories, the ï¬nal predicted category of\\nthe voxel is determined based on the average probability of each category.\\n4 Results\\n4.1 Quantitative Results\\nThe validation dataset for BraTS 2020 includes 125 cases without providing tu-\\nmor subtypes (HGG/LGG) or tumor subregion annotations. Table 1 reports the\\nsegmentation result of per-class Dice score and Hausdorï¬€ distance for the valida-\\ntion dataset evaluated by the oï¬ƒcial platform (https://ipp.cbica.upenn.edu/).\\nBy comparing the segmentation performance of the 190th-epoch models of'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='tion dataset evaluated by the oï¬ƒcial platform (https://ipp.cbica.upenn.edu/).\\nBy comparing the segmentation performance of the 190th-epoch models of\\nthe two stages, we see that the improvement on accuracy brought by the presence\\nof the second-stage network is more evident for TC than that for WT, and\\ntraining the second-stage network with expanded training data further improves\\nthe Dice score for TC.\\nTable 1. Segmentation results on validation data.\\nDice Hausdorï¬€ (mm)\\nStage Method ET WT TC ET WT TC\\n1\\nModel ep190 0.7881 0.8992 0.8206 23.716 5.657 6.664\\nModel ep254 0.7930 0.8980 0.8258 26.516 5.958 6.565\\nModel ep289 0.7902 0.8973 0.8286 24.146 6.174 7.042\\nEnsemble 9 0.7946 0.9022 0.8282 23.651 5.176 6.307\\n2\\nModel ep190: Single 0.7925 0.9012 0.8241 23.752 5.159 6.692\\nModel ep190: Multiple 0.7897 0.9016 0.8291 29.327 5.288 6.632\\nModel ep254: Multiple 0.7769 0.9010 0.8316 32.505 5.558 6.557\\nModel ep289: Multiple 0.7896 0.9002 0.8361 21.383 5.521 6.459'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='Model ep190: Multiple 0.7897 0.9016 0.8291 29.327 5.288 6.632\\nModel ep254: Multiple 0.7769 0.9010 0.8316 32.505 5.558 6.557\\nModel ep289: Multiple 0.7896 0.9002 0.8361 21.383 5.521 6.459\\nEnsemble 15: Multiple 0.7960 0.9039 0.8345 23.630 4.959 6.331\\nEnsemble 21: Multiple 0.7958 0.9041 0.8350 23.608 4.953 6.299\\nEnsemble 27: Multiple 0.7952 0.9039 0.8350 23.590 4.962 6.30310 C. Lyu and H. Shu\\nAs a performance-boosting component, the second-stage network trained\\nwith expanded data can be added to any ï¬rst-stage model to enhance the seg-\\nmentation performance. The second-stage network with expanded data also re-\\nduces the performance variation across models of diï¬€erent epochs. Table 2 shows\\nthat the standard deviation (SD) of the TCâ€™s Dice score and Hausdorï¬€ distance\\nare reduced by 68% and 93% in the second-stage, respectively. The SDs are cal-\\nculated based on the performance of all trained non-ensembled models. We also'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='are reduced by 68% and 93% in the second-stage, respectively. The SDs are cal-\\nculated based on the performance of all trained non-ensembled models. We also\\nobserve that the second-stage network remarkably reduces the variation of ETâ€™s\\nDice score and Hausdorï¬€ distance, but this improvement no longer exists after\\npost-processing.\\nThe BraTS 2020 testing dataset contains 166 cases without providing tumor\\nannotations. Our segmentation results on this dataset are presented in Table 3.\\nTable 2. The performance variation across non-ensembled models on validation data.\\nDice Hausdorï¬€ (mm)\\nStageMetric ET WT TC ET WT TC\\n1 SD 0.0128 0.0013 0.0110 4.097 0.236 1.427\\nRange[0.714, 0.750 ][0.894, 0.899][0.798, 0.835][30.535, 42.452][5.657, 6.433][6.566, 10.502]\\n2 SD 0.0070 0.0012 0.0035 2.337 0.184 0.102\\nRange [0.715, 0.742][0.898, 0.902][0.822, 0.838][33.457, 42.546][5.191, 6.024][6.168, 6.741]\\nNote: The variation metrics are calculated based on the results from 9 stage-1 models'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='Range [0.715, 0.742][0.898, 0.902][0.822, 0.838][33.457, 42.546][5.191, 6.024][6.168, 6.741]\\nNote: The variation metrics are calculated based on the results from 9 stage-1 models\\nand 37 stage-2 models without post-processing.\\nTable 3. Segmentation results on testing data.\\nDice Hausdorï¬€(mm)\\nStage Method ET WT TC ET WT TC\\n2 Ensemble 21:Multiple 0.8205 0.8729 0.8357 15.6711 11.4288 19.9690\\n4.2 Attention Map\\nThe attention matrices in the ï¬nest scale are visualised in the form of heatmap\\nwith red indicating higher weights and blue indicating lower weights (Fig. 5). In\\nthe ï¬rst few training epochs, we observe that AGs grasp the tumorâ€™s location and\\nmeanwhile assign a high weight to gray matter. As the training progresses, the\\nweights assigned to non-tumor regions gradually decrease. AGs also suggest the\\nmodel avoid misclassiï¬cation of voxels around the tumor boundary by gradually\\ndecreasing weights assigned to the tumor boundary.Title Suppressed Due to Excessive Length 11'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='model avoid misclassiï¬cation of voxels around the tumor boundary by gradually\\ndecreasing weights assigned to the tumor boundary.Title Suppressed Due to Excessive Length 11\\nFig. 5. The ï¬rst three columns show the attention maps at training epochs 3, 20, and\\n115, respectively. The fourth column shows the example images of T2-modality with\\nground-truth annotations extracted from the BraTS 2020 training dataset. The model\\ngradually learns to assign lower weights to non-tumor areas and the tumor boundary.\\n5 Concluding Remarks\\nThis paper proposes a two-stage cascade network with VAEs and AGs for 3D\\nMRI brain tumor segmentation. The results indicate the second-stage network\\nimproves and stabilizes the prediction for all three tumor subregions, particu-\\nlarly for TC and ET (before post-processing). The second-stage network can also\\nproduce more qualiï¬ed model candidates for further model ensembling. In this\\nstudy, we use the segmentation results of multiple ï¬rst-stage models to train'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='produce more qualiï¬ed model candidates for further model ensembling. In this\\nstudy, we use the segmentation results of multiple ï¬rst-stage models to train\\nthe second-stage network. Though this helps improve the modelâ€™s prediction\\nperformance, it noticeably increases the training time as a trade-oï¬€. Consequen-\\ntially, this technique may not be suitable for occasions with limited computing\\nresources and research time. In addition, we can see from Table 1 that even if\\nthe expanded training data does not include the output of the ï¬rst-stage 190th-\\nepoch model, we can still use the trained second-stage models to obtain a better\\nresult than this ï¬rst-stage model. This indicates that the second-stage network\\ntrained by this strategy has generalizability among models of diï¬€erent epochs.12 C. Lyu and H. Shu\\nSince ï¬rst proposed in natural language processing [17], the attention mech-\\nanism has been extensively studied and widely used in image segmentation'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='Since ï¬rst proposed in natural language processing [17], the attention mech-\\nanism has been extensively studied and widely used in image segmentation\\nproblems. Technically speaking, the attention mechanism in image segmenta-\\ntion tasks can be divided into the spatial attention, such as the AGs used in\\nour method, and the channel attention, e.g., the â€œsqueeze and excitationâ€ block\\nin [9,22]. It was proposed in [6] to combine the two kinds of attention in 2D\\nproblems, but multiplications between huge matrices involved in the method\\nwill likely exceed the computational limits in 3D scenarios. Further research is\\nexpected to include the appropriate combination of the two attention mecha-\\nnisms into the brain tumor segmentation to enhance the segmentation accuracy.\\nBesides, Dai et al. [5] utilized the extreme gradient boosting (XGboost) in model\\nensemble and gained extra improvement on accuracy as compared with the ma-'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='Besides, Dai et al. [5] utilized the extreme gradient boosting (XGboost) in model\\nensemble and gained extra improvement on accuracy as compared with the ma-\\njority voting and probability averaging approaches. It may be worth integrating\\nXGboost into our method, as the existence of the second-stage provides more\\nmodels to be chosen from for the XGboost training. Moreover, Zhong et al. [21]\\nhas recently developed a segmentation network model that incorporates the di-\\nlated convolution [19] and the dense block [10]. The two popular deep-learning\\ntechniques may be valuable to be combined into our network structure.\\nAcknowledgements\\nThis research was partially supported by the grant R21AG070303 from the Na-\\ntional Institutes of Health and a startup fund from New York University. The\\ncontent is solely the responsibility of the authors and does not necessarily repre-\\nsent the oï¬ƒcial views of the National Institutes of Health or New York University.\\nReferences'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='content is solely the responsibility of the authors and does not necessarily repre-\\nsent the oï¬ƒcial views of the National Institutes of Health or New York University.\\nReferences\\n1. Bakas, S., Akbari, H., Sotiras, A., Bilello, M., Rozycki, M., Kirby, J., Freymann,\\nJ., Farahani, K., Davatzikos, C.: Segmentation labels and radiomic features for the\\npre-operative scans of the tcga-gbm collection. The Cancer Imaging Archive (2017)\\n2. Bakas, S., Akbari, H., Sotiras, A., Bilello, M., Rozycki, M., Kirby, J., Freymann,\\nJ., Farahani, K., Davatzikos, C.: Segmentation labels and radiomic features for the\\npre-operative scans of the tcga-lgg collection. The Cancer Imaging Archive (2017)\\n3. Bakas, S., Akbari, H., Sotiras, A., Bilello, M., Rozycki, M., Kirby, J.S., Freymann,\\nJ.B., Farahani, K., Davatzikos, C.: Advancing the cancer genome atlas glioma mri\\ncollections with expert segmentation labels and radiomic features. Nature Scientiï¬c\\nData 4, 170117 (2017)'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='J.B., Farahani, K., Davatzikos, C.: Advancing the cancer genome atlas glioma mri\\ncollections with expert segmentation labels and radiomic features. Nature Scientiï¬c\\nData 4, 170117 (2017)\\n4. Bakas, S., Reyes, M., et Int, Menze, B.: Identifying the best machine learning algo-\\nrithms for brain tumor segmentation, progression assessment, and overall survival\\nprediction in the BRATS challenge. arXiv preprint arXiv:1811.02629 (2018)\\n5. Dai, L., Li, T., Shu, H., Zhong, L., Shen, H., Zhu, H.: Automatic brain tumor seg-\\nmentation with domain adaptation. In: International MICCAI Brainlesion Work-\\nshop. pp. 380â€“392. Springer (2018)\\n6. Fu, J., Liu, J., Tian, H., Li, Y., Bao, Y., Fang, Z., Lu, H.: Dual attention network for\\nscene segmentation. In: Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition. pp. 3146â€“3154 (2019)Title Suppressed Due to Excessive Length 13\\n7. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='and Pattern Recognition. pp. 3146â€“3154 (2019)Title Suppressed Due to Excessive Length 13\\n7. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In:\\nProceedings of the IEEE conference on computer vision and pattern recognition.\\npp. 770â€“778 (2016)\\n8. He, K., Zhang, X., Ren, S., Sun, J.: Identity mappings in deep residual networks.\\nIn: European conference on computer vision. pp. 630â€“645. Springer (2016)\\n9. Hu, J., Shen, L., Sun, G.: Squeeze-and-excitation networks. In: Proceedings of the\\nIEEE conference on computer vision and pattern recognition. pp. 7132â€“7141 (2018)\\n10. Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q.: Densely connected\\nconvolutional networks. In: Proceedings of the IEEE conference on computer vision\\nand pattern recognition. pp. 4700â€“4708 (2017)\\n11. Isensee, F., Kickingereder, P., Wick, W., Bendszus, M., Maier-Hein, K.H.: No new-\\nnet. In: International MICCAI Brainlesion Workshop. pp. 234â€“244. Springer (2018)'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='11. Isensee, F., Kickingereder, P., Wick, W., Bendszus, M., Maier-Hein, K.H.: No new-\\nnet. In: International MICCAI Brainlesion Workshop. pp. 234â€“244. Springer (2018)\\n12. Jiang, Z., Ding, C., Liu, M., Tao, D.: Two-stage cascaded u-net: 1st place solution\\nto brats challenge 2019 segmentation task. In: International MICCAI Brainlesion\\nWorkshop. pp. 231â€“241. Springer (2019)\\n13. Menze, B.H., Jakab, A., Bauer, S., Kalpathy-Cramer, J., Farahani, K., Kirby, J.,\\nBurren, Y., Porz, N., Slotboom, J., Wiest, R., et al.: The multimodal brain tumor\\nimage segmentation benchmark (brats). IEEE Transactions on Medical Imaging\\n34(10), 1993 (2015)\\n14. Myronenko, A.: 3d mri brain tumor segmentation using autoencoder regularization.\\nIn: International MICCAI Brainlesion Workshop. pp. 311â€“320. Springer (2018)\\n15. Oktay, O., Schlemper, J., Folgoc, L.L., Lee, M., Heinrich, M., Misawa, K., Mori,\\nK., McDonagh, S., Hammerla, N.Y., Kainz, B., et al.: Attention u-net: Learning'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='15. Oktay, O., Schlemper, J., Folgoc, L.L., Lee, M., Heinrich, M., Misawa, K., Mori,\\nK., McDonagh, S., Hammerla, N.Y., Kainz, B., et al.: Attention u-net: Learning\\nwhere to look for the pancreas. arXiv preprint arXiv:1804.03999 (2018)\\n16. Tu, Z., Bai, X.: Auto-context and its application to high-level vision tasks and\\n3d brain image segmentation. IEEE transactions on pattern analysis and machine\\nintelligence 32(10), 1744â€“1757 (2009)\\n17. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser,\\n L., Polosukhin, I.: Attention is all you need. In: Advances in neural information\\nprocessing systems. pp. 5998â€“6008 (2017)\\n18. Wang, G., Li, W., Ourselin, S., Vercauteren, T.: Automatic brain tumor segmen-\\ntation using cascaded anisotropic convolutional neural networks. In: International\\nMICCAI brainlesion workshop. pp. 178â€“190. Springer (2017)\\n19. Yu, F., Koltun, V.: Multi-scale context aggregation by dilated convolutions. arXiv\\npreprint arXiv:1511.07122 (2015)'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='MICCAI brainlesion workshop. pp. 178â€“190. Springer (2017)\\n19. Yu, F., Koltun, V.: Multi-scale context aggregation by dilated convolutions. arXiv\\npreprint arXiv:1511.07122 (2015)\\n20. Zhao, Y.X., Zhang, Y.M., Liu, C.L.: Bag of tricks for 3d mri brain tumor segmen-\\ntation. In: International MICCAI Brainlesion Workshop. pp. 210â€“220. Springer\\n(2019)\\n21. Zhong, L., Li, T., Shu, H., Huang, C., Johnson, J.M., Schomer, D.F., Liu, H.L.,\\nFeng, Q., Yang, W., Zhu, H.: (ts)2wm: Tumor segmentation and tract statistics\\nfor assessing white matter integrity with applications to glioblastoma patients.\\nNeuroImage 223, 117368 (2020)\\n22. Zhou, C., Chen, S., Ding, C., Tao, D.: Learning contextual and attentive informa-\\ntion for brain tumor segmentation. In: International MICCAI Brainlesion Work-\\nshop. pp. 497â€“507. Springer (2018)\\n23. Zhou, C., Ding, C., Lu, Z., Wang, X., Tao, D.: One-pass multi-task convolutional\\nneural networks for eï¬ƒcient brain tumor segmentation. In: International Confer-'),\n",
       " Document(metadata={'arxiv_id': '2011.02881v2', 'title': 'A Two-Stage Cascade Model with Variational Autoencoders and Attention Gates for MRI Brain Tumor Segmentation', 'section': 'body', 'authors': 'Chenggang Lyu, Hai Shu'}, page_content='shop. pp. 497â€“507. Springer (2018)\\n23. Zhou, C., Ding, C., Lu, Z., Wang, X., Tao, D.: One-pass multi-task convolutional\\nneural networks for eï¬ƒcient brain tumor segmentation. In: International Confer-\\nence on Medical Image Computing and Computer-Assisted Intervention. pp. 637â€“\\n645. Springer (2018)'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'title_abstract', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='Title: Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors\\n\\nAbstract: Accurate segmentation of brain tumors from 3D multimodal MRI is vital for diagnosis and treatment planning across diverse brain tumors. This paper addresses the challenges posed by the BraTS 2023, presenting a unified transfer learning approach that applies to a broader spectrum of brain tumors. We introduce HT-CNNs, an ensemble of Hybrid Transformers and Convolutional Neural Networks optimized through transfer learning for varied brain tumor segmentation. This method captures spatial and contextual details from MRI data, fine-tuned on diverse datasets representing common tumor types. Through transfer learning, HT-CNNs utilize the learned representations from one task to improve generalization in another, harnessing the power of pre-trained models on large datasets and fine-tuning them on specific tumor types. We preprocess diverse datasets from multiple international distributions, ensuring representativeness for the most common brain tumors. Our rigorous evaluation employs standardized quantitative metrics across all tumor types, ensuring robustness and generalizability. The proposed ensemble model achieves superior segmentation results across the BraTS validation datasets over the previous winning methods. Comprehensive quantitative evaluations using the DSC and HD95 demonstrate the effectiveness of our approach. Qualitative segmentation predictions further validate the high-quality outputs produced by our model. Our findings underscore the potential of transfer learning and ensemble approaches in medical image segmentation, indicating a substantial enhancement in clinical decision-making and patient care. Despite facing challenges related to post-processing and domain gaps, our study sets a new precedent for future research for brain tumor segmentation. The docker image for the code and models has been made publicly available, https://hub.docker.com/r/razeineldin/ht-cnns.'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='International Journal of Computer Assisted Radiology and Surgery (2024)  \\n1  \\nUnified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors Ramy A. Zeineldin1,2*  Â· Franziska Mathis-Ullrich1 1Department of Artificial Intelligence in Biomedical Engineering (AIBE), Friedrich-Alexander-University Erlangen-NÃ¼rnberg (FAU), Germany 2Faculty of Electronic Engineering (FEE), Menoufia University, 32952 Menouf, Egypt'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='Abstract Purpose Accurate segmentation of brain tumors from 3D multimodal MRI is vital for diagnosis and treatment planning across diverse brain tumor types. This paper addresses the challenges posed by the BraTS 2023, presenting a unified transfer learning approach that applies to a broader spectrum of brain tumors, including Glioma and Pediatric Tumors. Methods We introduce HT-CNNs, an ensemble of Hybrid Transformers and Convolutional Neural Networks optimized through transfer learning for varied brain tumor segmentation. This method captures spatial and contextual details from 3D MRI data, fine-tuned on diverse datasets representing common tumor types. Through transfer learning, HT-CNNs utilize the learned representations from one task to improve generalization in another, harnessing the power of pre-trained models on large datasets and fine-tuning them on specific tumor types. Specifically, we employed two primary transfer learning techniques: fine-tuning pre-trained models on the'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='power of pre-trained models on large datasets and fine-tuning them on specific tumor types. Specifically, we employed two primary transfer learning techniques: fine-tuning pre-trained models on the BraTS dataset for Adult Glioma segmentation and ensemble learning using the STAPLE method. We preprocess diverse datasets, including patients from multiple international distributions, ensuring representativeness for the most common brain tumors. Our rigorous evaluation employs standardized quantitative metrics across all tumor types, ensuring robustness and generalizability. Results The proposed ensemble model achieves superior segmentation results across the BraTS validation datasets over the previous winning methods. Comprehensive quantitative evaluations using the dice similarity coefficient (DSC) and Hausdorff distance (HD95) demonstrate the effectiveness of our approach. Further, fine-tuning significantly improved the performance of baseline models, with notable increases in DSC and'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='(DSC) and Hausdorff distance (HD95) demonstrate the effectiveness of our approach. Further, fine-tuning significantly improved the performance of baseline models, with notable increases in DSC and decreases in HD95. For instance, the DSC for Pediatric brain tumors increased from 0.4097 to 0.6248, while the HD95 decreased from 163.37 to 37.46. Similarly, for the Sub-Saharan Africa datasets, the DSC improved from 0.7832 to 0.8647, and the HD95 decreased from 18.38 to 10.98. Qualitative segmentation predictions further validate the high-quality outputs produced by our model. Conclusion Our findings underscore the potential of transfer learning and ensemble approaches in medical image segmentation, indicating a substantial enhancement in clinical decision-making and patient care. Despite facing challenges related to post-processing and domain gaps, our study sets a new precedent for future research aimed at refining brain tumor segmentation models. The docker image for the code and models'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='challenges related to post-processing and domain gaps, our study sets a new precedent for future research aimed at refining brain tumor segmentation models. The docker image for the code and models has been made publicly available (https://hub.docker.com/r/razeineldin/ht-cnns) to encourage further advancements in the field. Keywords Brain Tumor Segmentation. Ensemble Learning . Multimodal MRI . Hybrid Transformer . Transfer Learning.'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content=\"Introduction Brain tumors, particularly glioblastoma (GBM) and diffuse astrocytic glioma with molecular features of GBM (WHO Grade 4 astrocytoma) represent the most aggressive and common malignant primary tumors of the central nervous system in adults  [1,2]. These tumors exhibit significant appearance, shape, and histology heterogeneity, making their accurate identification and localization a critical challenge. GBM patients face a grim prognosis, with an average survival of only 14 months under standard care treatment and just four months if left untreated. Despite numerous experimental treatment approaches proposed over the past two decades, there have been limited improvements in patient prognosis. Brain tumors' lethality and the complexities in their detection and treatment underscore the urgent need for more efficient and accurate automated segmentation solutions to assist clinicians in managing these conditions.\"),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='2  The segmentation of gliomas, encompassing distinct sub-regions such as the enhancing tumor (ET), peritumoral edematous/invaded tissue (ED), and the necrotic components of the core tumor (NCR), presents a formidable challenge in the medical field. While manual segmentation serves as the gold standard for various clinical applications, including neurosurgical planning, image-guided interventions, and tumor growth monitoring, this approach is labor-intensive, subjective, and highly reliant on the expertise of clinicians. This manual process becomes impractical when dealing with numerous patients, highlighting the critical need for automated deterministic segmentation solutions to streamline and expedite this crucial task. Accurately identifying boundaries for brain tumor sub-regions in MRI scans is essential for improving surgical treatment planning, generating radiotherapy maps, and facilitating follow-up procedures, ultimately enhancing patient care in these formidable malignancies.'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='scans is essential for improving surgical treatment planning, generating radiotherapy maps, and facilitating follow-up procedures, ultimately enhancing patient care in these formidable malignancies. The Medical Image Computing and Computer-Assisted Interventions Brain Tumor Segmentation Challenge (MICCAI BraTS) is a significant initiative dedicated to addressing the challenging problem of the automated brain tumor sub-region segmentation [3,4]. Established in 2012, this challenge has played a pivotal role in advancing machine learning (ML) in glioma diagnosis. Each year, BraTS provides a comprehensive dataset of multi-parametric MRI scans and ground-truth annotations, offering a benchmark for developing and evaluating state-of-the-art algorithms in tumor segmentation, classification, and, more recently, survival prediction [2,5,6]. The BraTS challenges have been pivotal in fostering collaboration among researchers, clinicians, and institutions to enhance the accuracy and efficiency of'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content=\"more recently, survival prediction [2,5,6]. The BraTS challenges have been pivotal in fostering collaboration among researchers, clinicians, and institutions to enhance the accuracy and efficiency of brain tumor diagnostics. This ongoing effort aims to improve automated brain tumor segmentation methods and contribute to the overall progress in glioma diagnostics and treatment planning. In the latest edition of the BraTS challenge, BraTS 2023, a cluster of challenges has been introduced to address various crucial aspects of brain tumor diagnostics and treatment [7,8]. These new challenges include sub-Saharan African patient populations (SSA), meningioma segmentation (MEN), brain metastasis segmentation (MET), pediatric brain tumors (PED), global and local missing data, augmentation techniques, and algorithmic generalizability in addition to the original adults' glioma segmentation (GLA). By expanding the focus to incorporate these diverse aspects, the BraTS 2023 challenge aims to\"),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content=\"and algorithmic generalizability in addition to the original adults' glioma segmentation (GLA). By expanding the focus to incorporate these diverse aspects, the BraTS 2023 challenge aims to create a comprehensive benchmarking environment, further advancing the field and facilitating the development of innovative algorithms. Additionally, introducing a specialized pediatric brain tumor segmentation challenge underscores the pressing need to improve diagnostics and treatment planning for this vulnerable patient group. The BraTS challenges continue to be instrumental in pushing the boundaries of medical image analysis, contributing to advancing brain tumor research and clinical care. Automated deep learning methods have the potential to revolutionize the brain tumor segmentation process, significantly enhancing accuracy, efficiency, and clinical utility. Convolutional Neural Networks (CNNs) have been particularly effective in medical image analysis, including the segmentation of brain\"),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='enhancing accuracy, efficiency, and clinical utility. Convolutional Neural Networks (CNNs) have been particularly effective in medical image analysis, including the segmentation of brain tumors [9,10]. CNNs excel at learning intricate features from complex data, making them well-suited for capturing the nuanced and diverse characteristics of brain tumors evident in multi-parametric MRI scans. By training on large annotated datasets, CNNs, specifically U-Net and its numerous variants [11,12], can learn to differentiate tumor sub-regions, such as the enhancing tumor, peritumoral edema, and necrotic components, providing precise and consistent segmentation. The architecture of U-Net, with its encoder for feature extraction, decoder for spatial detail recovery, and skip connections for feature fusion, has been fundamental in this field. The upper layers of U-Net, capturing broad contextual information, and lower layers, rich in spatial detail, work in concert to enhance segmentation'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='fusion, has been fundamental in this field. The upper layers of U-Net, capturing broad contextual information, and lower layers, rich in spatial detail, work in concert to enhance segmentation results. Variants like NAG-Net [13] and SBANet [14] have pushed the envelope by integrating clinical domain knowledge and segmentation-based attention to refine feature extraction and tumor recognition capabilities. However, challenges persist, especially when dealing with images that have complex backgrounds, irrelevant noise, and unclear boundaries, which are common in medical imaging. Parallel to the U-Net evolution, Transformer-based methods [15] have been carving out a significant role in medical image analysis. Transformers, originally developed for natural language processing, offer exciting prospects for brain tumor segmentation. Transformers employ self-attention mechanisms to capture long-range dependencies, making them particularly effective at understanding the spatial relationships'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='prospects for brain tumor segmentation. Transformers employ self-attention mechanisms to capture long-range dependencies, making them particularly effective at understanding the spatial relationships and contextual information crucial for accurate tumor segmentation. Models like MESTrans [16] and TransXAI  [17] have shown that Transformers can indeed excel in this domain. Nevertheless, the high computational cost associated with the self-attention operation has limited their practicality, especially in cases with limited datasets. In response, recent efforts have been directed toward  International Journal of Computer Assisted Radiology and Surgery 3 streamlining the self-attention mechanism. CASF-Net [18] and UTNet [19] are prime examples, with the former utilizing a factorized attention mechanism and the latter projecting features into a lower-dimensional space to alleviate computational demands. Despite achieving impressive performance, there remains the risk of neglecting small'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='mechanism and the latter projecting features into a lower-dimensional space to alleviate computational demands. Despite achieving impressive performance, there remains the risk of neglecting small yet clinically significant features due to these reductions in dimensionality and complexity. These two strands of research lay the foundation for our proposed HT-CNNs framework. The combination of CNNs and Transformers can further amplify the effectiveness of automated brain tumor segmentation. By leveraging the strengths of both architectures, deep learning models can comprehensively analyze multi-parametric MRI data, extracting intricate spatial and contextual features while maintaining fine-grained accuracy. These advanced methods, trained on large and diverse datasets, can streamline the segmentation process, reducing the reliance on manual labor, minimizing subjectivity, and expediting the diagnosis and treatment planning for brain tumors. By merging these two powerful architectures'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='segmentation process, reducing the reliance on manual labor, minimizing subjectivity, and expediting the diagnosis and treatment planning for brain tumors. By merging these two powerful architectures and employing transfer learning, we strive to create a segmentation tool that is not only accurate and efficient but also robust against the variability and complexity inherent in medical imaging tasks. As the field of deep learning continues to evolve, it holds the promise of contributing significantly to improving patient care and outcomes in brain tumor management. In this work, we present a unified framework that integrates Hybrid Transformers with CNNs, utilizing transfer learning to bridge the gap between various brain tumor types and imaging modalities. Our contributions are as follows: We propose a novel HT-CNNs architecture that combines the local feature extraction capabilities of CNNs with the global contextual understanding of Transformers, tailored for the segmentation of'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content=\"We propose a novel HT-CNNs architecture that combines the local feature extraction capabilities of CNNs with the global contextual understanding of Transformers, tailored for the segmentation of Glioma, Intracranial Meningioma, Pediatric Tumors, and Brain Metastasis. We leverage transfer learning to enhance model performance across diverse datasets, including those from multiple international consortia, ensuring our approach's applicability to real-world clinical scenarios. In addition, a comprehensive two-stage post-processing strategy was introduced to refine the tumor boundary predictions. The first stage replaces enhancing tumor predictions below a threshold with necrosis, while the second stage employs connected component analysis to handle small enhancing tumor regions. Furthermore, we introduce a rigorous evaluation protocol using standardized quantitative metrics to ensure the robustness and generalizability of our model across all tumor types. This approach not only sets a\"),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='we introduce a rigorous evaluation protocol using standardized quantitative metrics to ensure the robustness and generalizability of our model across all tumor types. This approach not only sets a new benchmark for brain tumor segmentation but also advances the path toward clinical translation, offering a robust tool for assisting clinicians in the challenging task of tumor delineation. Methods Data The data used in this paper originates primarily from the BraTS 2023 cluster of challenges, which encompasses a diverse range of datasets representing various brain tumor scenarios. The BraTS dataset consists of retrospective multi-parametric MRI scans collected under standard clinical conditions from multiple institutions with different equipment and imaging protocols, reflecting the broad clinical practice diversity [8,7,2]. Ground truth annotations for tumor sub-regions, including the tumor core (TC), enhancing tumor (ET), peritumoral edema (ED), and necrotic and non-enhancing tumor'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='clinical practice diversity [8,7,2]. Ground truth annotations for tumor sub-regions, including the tumor core (TC), enhancing tumor (ET), peritumoral edema (ED), and necrotic and non-enhancing tumor core (NCR/NET), were meticulously reviewed by expert neuroradiologists.  For specific challenges within BraTS 2023, such as the Sub-Saharan African (SSA) populations, the SSA dataset is utilized, providing annotated pre-operative glioma data from various institutions and scanners and maintaining real-world heterogeneity [7]. These datasets are essential for developing and evaluating automated brain tumor segmentation methods, enabling the creation of robust and generalizable models for improved clinical applications. Following the established practice of algorithmic assessment in machine learning, the data utilized for the BraTS 2023 challenges has been partitioned into distinct sets: a training set, which accounts for 70% of the data; a validation set, representing 10%; and a separate'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='the data utilized for the BraTS 2023 challenges has been partitioned into distinct sets: a training set, which accounts for 70% of the data; a validation set, representing 10%; and a separate testing dataset, constituting 20% of the total data. HT-CNNs Architecture Overall Architecture. The HT-CNNs framework introduces a comprehensive architecture that integrates multiple 4  advanced neural network models to address the complexity of brain tumor segmentation (Fig. 1). At the forefront, the framework accepts an input MRI, which undergoes an initial preprocessing block designed to standardize and enhance the imaging data for subsequent analysis. Following preprocessing, the data is fed into a tripartite network system comprising: 1. nnU-Net (CNN): This component serves as the convolutional neural network, focusing on extracting spatial hierarchies and detailed textural information from the preprocessed MRI data. 2. TransBTS (Transformer): The TransBTS network employs a Transformer'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='neural network, focusing on extracting spatial hierarchies and detailed textural information from the preprocessed MRI data. 2. TransBTS (Transformer): The TransBTS network employs a Transformer architecture, adept at capturing long-range dependencies and global contextual relationships inherent in the imaging data. 3. DeepSCAN (Attention): As the attention-focused component, DeepSCAN processes the feature maps from the previous networks, applying attention mechanisms to refine the feature representation further and emphasize salient tumor regions. Each network includes a dedicated local post-processing block, fine-tuning the network-specific outputs to ensure high-quality segmentation maps. These refined outputs are then synergistically combined using the Simultaneous Truth and Performance Level Estimation (STAPLE) method [20]. This ensemble technique effectively integrates the strengths of the individual models, yielding a robust final output segmentation that embodies both precise'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='Level Estimation (STAPLE) method [20]. This ensemble technique effectively integrates the strengths of the individual models, yielding a robust final output segmentation that embodies both precise local details and comprehensive global context, essential for accurate tumor delineation.'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='Fig. 1 HT-CNNs Architecture for Brain Tumor Segmentation.  Baseline U-Net. The baseline network architecture is based on the widely-used 3D U-Net encoder-decoder structure [21,10,11], which has been proven effective in various medical image segmentation tasks, as shown in Fig. 1. The network consists of two main components: the encoder, which captures high-level features from the input data, and'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='International Journal of Computer Assisted Radiology and Surgery 5 the decoder, which upsamples these features to produce the final segmentation map. The encoder part of the network consists of multiple consecutive 3D convolutional blocks. Each block contains two 3 Ã— 3 Ã— 3 convolutional layers, followed by a Rectified Linear Unit (ReLU) activation function. Max-pooling with a kernel size of 2 Ã— 2 Ã— 2 for downsampling is employed, reducing the spatial dimensions of the feature maps while preserving important information. The number of filters in the convolutional layers increases progressively through the encoder, allowing the network to capture increasingly complex features. In the decoder part, the upsampled feature maps from the encoder are passed through 3D transposed convolutional layers, also known as deconvolutional layers. These layers restore the spatial resolution of the feature maps, enabling the network to produce a detailed segmentation map. Skip connections interconnect'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content=\"also known as deconvolutional layers. These layers restore the spatial resolution of the feature maps, enabling the network to produce a detailed segmentation map. Skip connections interconnect the encoder and decoder, allowing the network to integrate high-resolution information from the encoder with the upsampled features from the decoder. Finally, a 1 Ã— 1 Ã— 1 convolutional layer with softmax activation generates the probability distribution of the tumor regions in the input volume. Transformer Network. To handle the computational complexity of the Transformer for 3D volumetric data, we adopt a method inspired by the Vision Transformer (ViT) [22] (as detailed in ref [23]), which splits the image into fixed-size (16Ã—16) patches and reshapes each patch into a token. This approach effectively reduces the sequence length, making it computationally tractable. However, for volumetric data, directly applying this tokenization strategy can hinder the model's ability to capture local context\"),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='reduces the sequence length, making it computationally tractable. However, for volumetric data, directly applying this tokenization strategy can hinder the model\\'s ability to capture local context information across spatial and depth dimensions essential for volumetric segmentation. To address this challenge, they employed a solution that involves stacking 3 Ã— 3 Ã— 3 convolution blocks with downsampling (strided convolution with stride=2). This transformation encodes the input images into a low-resolution/high-level feature representation ð¹âˆˆâ„!Ã—!\"Ã—#\"Ã—$\" (K=128), equivalent to 1/8 of the input dimensions of H, W, and D (overall stride OS=8). The enriched local 3D context features within F enhance the model\\'s capability to analyze volumetric data. This feature map F is then fed into the Transformer encoder to learn long-range correlations further and establish a global receptive field. This approach allows the model to effectively handle volumetric data, considering both local and global'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='encoder to learn long-range correlations further and establish a global receptive field. This approach allows the model to effectively handle volumetric data, considering both local and global contextual information, thereby elevating its segmentation performance. The specific architectural and transformational details of the feature embedding and Transformer encoder can be found in the corresponding sections of ref [23]. Attention Network. To further improve the baseline 3D U-Net architecture, we incorporated an axial attention decoder, a critical enhancement that leverages the power of self-attention mechanisms. Self-attention, originally introduced and popularized in natural language processing [15], has gradually made its way into the computer vision domain [22]. However, applying self-attention to vision problems, especially with 3D data, has been challenging due to its quadratic computational complexity with input size. To address this, axial attention was introduced as an'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='self-attention to vision problems, especially with 3D data, has been challenging due to its quadratic computational complexity with input size. To address this, axial attention was introduced as an efficient solution for multi-dimensional data [24]. By independently applying self-attention to each axis, the computational complexity scales linearly with image size, making integrating the attention mechanism with 3D data feasible. In our enhancement, axial attention was integrated into the decoder of the network, specifically applied to the output of the transposed convolution upsampling, and subsequently summed. The architecture and implementation details of the axial attention decoder are illustrated in Figure 2.'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='Fig. 2. Axial attention is employed individually for each axis in the output obtained from transpose convolution [25]. Transfer Learning In our HT-CNNs framework, transfer learning played a pivotal role in enhancing model performance across diverse brain tumor segmentation tasks. Transfer learning is a machine learning technique where a model developed for a'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='6  particular task is repurposed as the starting point for a model on a second task [26,27]. It is particularly valuable when available training data is limited or when rapid development is desired. This method has proven to be effective in achieving higher accuracy and faster convergence, especially when pre-trained on a model within a similar domain. Specifically, we utilized two primary techniques for transfer learning: Fine-tuning. In our HT-CNNs framework, we employed fine-tuning as a primary technique for transfer learning. Initially, models were pre-trained on Adult Glioma segmentation tasks using the extensive BraTS dataset (1251 training cases). These pre-trained models provided a rich set of learned features relevant to brain tumor segmentation. For the BraTS 2023 challenges, we fine-tuned these models on smaller, specific datasets, namely the Sub-Saharan Africa (SSA) populations (60 training cases) and Pediatric (PED) brain tumors (99 training cases). This process involved'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='fine-tuned these models on smaller, specific datasets, namely the Sub-Saharan Africa (SSA) populations (60 training cases) and Pediatric (PED) brain tumors (99 training cases). This process involved adjusting the pre-trained model parameters to better fit the new datasets, thereby enhancing segmentation accuracy and generalization to different brain tumor types and demographics. Ensemble learning. The second technique employed was ensemble learning using the STAPLE method [20] for transfer learning. This approach integrated the outputs of multiple models to create a consensus segmentation. By leveraging the strengths of various models, the STAPLE technique helped mitigate individual model biases and improved overall segmentation robustness. This ensemble method was particularly effective in refining the segmentation results and ensuring high-quality outputs across the diverse datasets of the BraTS 2023 challenges. Post-processing Strategy Our post-processing strategy addresses several'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content=\"refining the segmentation results and ensuring high-quality outputs across the diverse datasets of the BraTS 2023 challenges. Post-processing Strategy Our post-processing strategy addresses several BraTS-specific challenges by addressing two scenarios to refine the predictions. Firstly, in cases where the reference segmentation lacked enhancing tumor regions, we removed enhancing tumor predictions below a certain threshold, replacing them with necrosis predictions. This approach optimized the model's ranking while managing potential true positive losses, with the threshold fine-tuned via cross-validation on the mean Dice and ranking scores. Additionally, we employed connected component analysis to handle small enhancing tumor regions. Components smaller than 16 voxels with a mean probability less than 0.9 were classified as necrosis, ensuring consistency. Moreover, if the predicted enhancing tumor voxel count fell below 73 voxels with a mean probability less than 0.9, we replaced all\"),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='less than 0.9 were classified as necrosis, ensuring consistency. Moreover, if the predicted enhancing tumor voxel count fell below 73 voxels with a mean probability less than 0.9, we replaced all enhancing tumor voxels with necrosis predictions. This post-processing strategy accounted for scenarios where enhancing tumor regions were limited or absent, resulting in a more balanced evaluation and enhancing the overall robustness of our model.  Experiments and Results Experimental Setup In order to evaluate the performance of our HT-CNNs model, we employed both classic and lesion-wise evaluation metrics. The classic metrics, including the Dice Similarity Coefficient (DSC) and Hausdorff Distance 95th Percentile (HD95), were employed to measure the overlap and boundary accuracy between the predicted and ground truth segmentation masks. These traditional metrics provide a standard framework for assessing segmentation quality and facilitate direct comparison with previous state-of-the-art'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='and ground truth segmentation masks. These traditional metrics provide a standard framework for assessing segmentation quality and facilitate direct comparison with previous state-of-the-art (SOTA) methods. The Dice Similarity Coefficient (DSC) measures the overlap between the predicted and ground truth segmentation masks. It is defined as: ð·ð‘†ð¶=2Ã—|ð´âˆ©ðµ||ð´|+|ðµ| (1) where A is the set of predicted tumor voxels and B is the set of ground truth tumor voxels. The Hausdorff distance (95%) (HD95) measures the spatial distance between the predicted and ground truth boundaries, focusing on the 95th percentile of distances. Mathematically, the HD95 is calculated as follows: ð»ð·95=ð‘šð‘Žð‘¥{ð‘‘#$(ð´,ðµ),ð‘‘#$(ðµ,ð´)} (2)  International Journal of Computer Assisted Radiology and Surgery 7   where ð‘‘#$(ð´,ðµ) is the 95th percentile of distances from points in ð´ to the nearest point in ðµ. In addition to the classic metrics, we incorporated the new lesion-wise metrics introduced in the BraTS 2023 challenges [8,7,2],'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content=\"percentile of distances from points in ð´ to the nearest point in ðµ. In addition to the classic metrics, we incorporated the new lesion-wise metrics introduced in the BraTS 2023 challenges [8,7,2], which offer a more granular evaluation of segmentation performance. The lesion-wise Dice Score and lesion-wise HD95 metrics focus on the precision of delineating individual lesions, penalizing missed lesions and false positives. The lesion-wise DSC evaluates the segmentation quality at the lesion level, penalizing missed lesions and false positives as defined in Equation (3). ð·ð‘†ð¶%&=âˆ‘ð·ð‘†ð¶(ð‘™')(')*ð‘‡ð‘ƒ+ð¹ð‘+ð¹ð‘ƒ (3) where ð·ð‘†ð¶(ð‘™') is the dice score for each individual lesion ð‘™', and TP, FN, and FP are the true positives, false negatives, and false positives respectively. L represents the number of tumor regions. Similarly, the lesion-wise HD95 provides a detailed measure of boundary accuracy for each lesion, penalizing inaccuracies as follows: ð»ð·95%&=âˆ‘ð»ð·95(ð‘™')(')*ð‘‡ð‘ƒ+ð¹ð‘+ð¹ð‘ƒ (4) where ð»ð·95(ð‘™') is the 95th\"),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content=\"Similarly, the lesion-wise HD95 provides a detailed measure of boundary accuracy for each lesion, penalizing inaccuracies as follows: ð»ð·95%&=âˆ‘ð»ð·95(ð‘™')(')*ð‘‡ð‘ƒ+ð¹ð‘+ð¹ð‘ƒ (4) where ð»ð·95(ð‘™') is the 95th percentile Hausdorff distance for each lesion ð‘™'. Implementation Details We began our implementation by preprocessing the BraTS data, adapting it for consistent input. This involved essential steps outlined in the BraTS challenge guidelines, such as DICOM to NIFTI format conversion, coordinate system reorientation, 1 Ã— 1 Ã— 1 mm resampling, and skull-stripping. In addition, we cropped brain pixels and resized the resultant images to a spatial resolution of 192 Ã— 224 Ã— 160, achieving a closer field of view (FOV) for efficient deep learning model training. Z-score normalization was applied individually to each MRI image. Our implementation utilized Python with the PyTorch and MONAI libraries, running on an NVIDIA RTX 4090 GPU. We employed a combined Dice and CE loss function for training,\"),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='to each MRI image. Our implementation utilized Python with the PyTorch and MONAI libraries, running on an NVIDIA RTX 4090 GPU. We employed a combined Dice and CE loss function for training, optimizing with the Adam optimizer with a cosine annealing learning rate strategy for 500 epochs. The Transformer architecture was used for the experiments, pre-trained on ImageNet-1K weights, with on-the-fly data augmentation techniques, including random flips, intensity shifts, and scaling. During the test phase, the sliding window method with an overlap rate of 0.6 was employed. These streamlined implementation details ensure the reliability and effectiveness of our approach for precise brain tumor segmentation. To empirically evaluate the efficacy of transfer learning methodologies, we undertook a comprehensive series of experiments utilizing the BraTS 2023 datasets. The experiments involved: â€¢ Evaluating the impact of ensemble learning on segmentation accuracy. â€¢ Comparing the performance of'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='comprehensive series of experiments utilizing the BraTS 2023 datasets. The experiments involved: â€¢ Evaluating the impact of ensemble learning on segmentation accuracy. â€¢ Comparing the performance of models with and without fine-tuning. â€¢ Analyzing the performance improvements across different tumors. Impact of Ensemble Learning The performance of our implemented models was evaluated on the validation sets of the BraTS 2023 challenges, and the results are listed in Tables 1. Synapse, the online evaluation platform by Sage Bionetworks, was utilized for the evaluation, leveraging two essential metrics: DSC and the HD95. Our analysis averaged the DSC scores and HD95 values over the three assessed tumor sub-regions, enabling a comprehensive evaluation. We adopted an ensemble approach to achieve a robust evaluation, averaging the results from five distinct models generated using different 8  cross-validation training configurations. The summary of these metrics is presented in the final'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='a robust evaluation, averaging the results from five distinct models generated using different 8  cross-validation training configurations. The summary of these metrics is presented in the final column, providing a comparative ranking of our methods. Furthermore, HT-CNNs model showcases competitive performance in brain tumor segmentation on the GLA validation set, as listed in Table 2. It achieved an average DSC of 0.8842, which is on par with the winner of 2023 and higher than the winners of 2020, 2021, and 2022. Additionally, the HT-CNNs model demonstrates an improvement in the HD95 metric over the 2021, 2022, and 2023 winners, indicating its efficacy in precise tumor delineation.  Table 1 Performance of five-fold cross-validation models on BraTS 2023 GLA Validation Cases. Model DSC  HD95   ET TC WT Avg ET TC WT Avg nnU-Net 0.8402 0.8718 0.9213 0.8778 16.24 9.36 4.69 10.09 TransBTS 0.8303 0.8649 0.9157 0.8703 18.33 9.24 7.17 11.58 DeepSCAN 0.8306 0.8683 0.9228 0.8739 14.86 8.42 4.78'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='ET TC WT Avg ET TC WT Avg nnU-Net 0.8402 0.8718 0.9213 0.8778 16.24 9.36 4.69 10.09 TransBTS 0.8303 0.8649 0.9157 0.8703 18.33 9.24 7.17 11.58 DeepSCAN 0.8306 0.8683 0.9228 0.8739 14.86 8.42 4.78 9.35 HT-CNNs 0.8449 0.8786 0.9291 0.8842 16.19 7.88 4.21 9.43 â€¢ Bold and underlined values correspond to best and second-best scores  Table 2 Comparison of our segmentation model and SOTA methods on the validation set for the GLA.'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='Model DSC  HD95   ET TC WT Avg ET TC WT Avg Winner 2020 [10] 0.8402 0.8718 0.9213 0.8778 16.028 8.953 3.823 9.601 Winner 2021 [25] 0.8451 0.8781 0.9275 0.8836 20.73 7.623 3.47 10.608 Winner 2022 [28] 0.8438 0.8753 0.9271 0.8821 17.504 7.533 3.595 9.544 Winner 2023 [29] 0.8464 0.8769 0.9294 0.8842 17.81 11.12 4.26 11.06'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='HT-CNNs 0.8449 0.8786 0.9291 0.8842 16.189 7.88 4.21 10.199 â€¢ Bold and underlined values correspond to best and second-best scores Fine-tuning Results Studies To validate the effectiveness of our transfer learning techniques, we conducted a series of comparative studies. These studies compared the performance of models before and after applying transfer learning and ensemble techniques. Standard evaluation metrics, typically DSC and HD95 were used to evaluate segmentation quality. Models pre-trained on Adult Glioma segmentation tasks were fine-tuned on the PED and SSA datasets, showing marked improvements in segmentation performance. The results in Table 3 and Table 4 indicated higher DSC and lower HD95 metrics compared to models trained from scratch, underscoring the benefits of our transfer learning approach. Table 3 Fine-tuning performance comparison of HT-CNNs on the BraTS 2023 PED validation cases.'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='Model DSC  HD95   ET TC WT Avg ET TC WT Avg Baseline 0.1000 0.5559 0.5733 0.4097 336.60 75.33 78.19 163.37 Baseline + TR 0.4026 0.6968 0.7751 0.6248 72.44 19.12 20.83 37.46 Baseline + TR + EN 0.6490 0.6165 0.9077 0.7621 60.00 13.29 3.86 25.72 â€¢ TR and EN indicate transfer learning and ensemble techniques.  International Journal of Computer Assisted Radiology and Surgery 9  Table 4 Fine-tuning performance comparison of HT-CNNs on the BraTS 2023 SSA validation cases.'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='Model DSC  HD95   ET TC WT Avg ET TC WT Avg Baseline 0.7064 0.8014 0.8417 0.7832 19.02 18.17 17.96 18.38 Baseline + TR 0.8130 0.8575 0.9235 0.8647 11.25 10.66 11.03 10.98 Baseline + TR + EN 0.8408 0.8139 0.9604 0.8872 4.00 6.29 2.57 4.29 â€¢ TR and EN indicate transfer learning and ensemble techniques.  The results demonstrate that fine-tuning significantly improves the performance of the baseline models. For the PED validation cases, the DSC increased from 0.4097 to 0.6248, and further to 0.7621 with the addition of ensemble techniques, while the HD95 decreased from 163.37 to 37.46 and further to 25.72, indicating more accurate and reliable segmentation. Similarly, for the SSA validation cases, the DSC improved from 0.7832 to 0.8647 and further to 0.8872 with ensemble techniques, while the HD95 decreased from 18.38 to 10.98 and further to 4.29. These results highlight the importance of transfer learning and ensemble techniques in adapting models to different datasets and enhancing'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content=\"the HD95 decreased from 18.38 to 10.98 and further to 4.29. These results highlight the importance of transfer learning and ensemble techniques in adapting models to different datasets and enhancing their performance. Lesion-wise Metrics Evaluation To align with the BraTS 2023 challenge's evaluation standards, we have included lesion-wise metrics in our analysis. These metrics offer a more granular evaluation of segmentation quality, focusing on the precision of delineating individual lesions. The results, depicted in Table 5, illustrate the significant improvements achieved through transfer learning, underscoring the enhanced precision and reliability of our HT-CNNs model in accurately segmenting individual lesions. By incorporating lesion-wise metrics, we provide a comprehensive evaluation that aligns with the latest standards in the field, ensuring our findings are relevant and comparable to contemporary research. Table 5 Lesion-wise performance comparison of HT-CNNs on the BraTS\"),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='that aligns with the latest standards in the field, ensuring our findings are relevant and comparable to contemporary research. Table 5 Lesion-wise performance comparison of HT-CNNs on the BraTS 2023 validation cases. Dataset Lesion-wise DSC  Lesion-wise HD95   ET TC WT Avg ET TC WT Avg GLA 0.8187 0.8563 0.8501 0.8417 31.82 19.08 35.84 28.91 PED 0.7023 0.6473 0.8876 0.7675 54.24 27.41 13.11 31.58 SSA 0.6586 0.7186 0.8596 0.7891 88.82 47.42 41.20 59.15'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='The winning team of BraTS-PED 2023, CNMCPMI2023, achieved a lesion-wise DSC of 0.65, 0.81, and 0.83 for the ET, TC, and WT regions, respectively [30]. Please note that the winner used an ensemble of nnU-Net and SwinUNETR, which is our second method used for transfer learning. Notably, our HT-CNNs model demonstrates competitive performance, particularly in the WT region, which is crucial for comprehensive tumor segmentation. Segmentation Output To visually assess the segmentation performance of our HT-CNNs model, we present qualitative segmentation predictions on the BraTS 2023 validation dataset in Figure 3. These results showcase the effectiveness of our approach in accurately segmenting brain tumors across different tasks. The predictions displayed in the figure are generated using our proposed model, and the rows correspond to samples from the GLA, SSA, and PED datasets. From Figure 3, it is evident that HT-CNNs model produces segmentation results of high quality across various'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content=\"proposed model, and the rows correspond to samples from the GLA, SSA, and PED datasets. From Figure 3, it is evident that HT-CNNs model produces segmentation results of high quality across various tumor sub-regions and specific tasks. In the GLA dataset, the model successfully identifies and segments the tumor regions, highlighting its capability to precisely delineate the tumor boundaries. Similarly, in the SSA dataset, the model accurately captures the intricate structures of the subventricular/subependymal areas, reflecting its proficiency 10  in segmenting complex brain structures. Moreover, the PED dataset showcases the model's ability to handle pediatric brain tumor segmentation, emphasizing its versatility in tackling diverse brain tumor types. The qualitative output shown in Figure 3 affirms that HT-CNNs consistently delivers accurate and visually pleasing segmentations across different datasets. The high-quality results validate the effectiveness of our approach in addressing\"),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='3 affirms that HT-CNNs consistently delivers accurate and visually pleasing segmentations across different datasets. The high-quality results validate the effectiveness of our approach in addressing the challenges posed by the BraTS 2023 challenges, further reinforcing the promising performance demonstrated in the quantitative evaluation.'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='(a) GLA: BraTS-GLI-00190-000, EC (0.9848), TC (0.9962), WT (0.9928) \\n (b) SSA: BraTS-SSA-00188-000, EC (0.9196), TC (0.8865), WT (0.9577) \\n (c) PED: BraTS-PED-00030-000, EC (0.9403), TC (0.8823), WT (0.9574) Fig. 3 Sample segmentation predictions on the BraTS 2023 validation dataset. Segmentation outcomes generated by our ensemble model are displayed for samples from the GLA, SSA, and PED datasets. Discussion The HT-CNNs framework demonstrates promising advances in the field of medical image segmentation, integrating the precision of U-Net, the contextual awareness of Transformers, and the focus of attention mechanisms. Applied to the BraTS 2023 challenge, our model displayed robust performance, particularly with Glioma and Pediatric Tumors, achieving superior average DSC and competitive HD95 metrics compared to SOTA methods. These'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content=\"International Journal of Computer Assisted Radiology and Surgery 11 outcomes illustrate the model's proficiency in handling the intricacies of brain tumor segmentation, suggesting its applicability to various clinical scenarios. One of the significant improvements in our study is the detailed implementation of transfer learning techniques. We employed fine-tuning and ensemble learning using the STAPLE method to enhance the model's performance across diverse datasets, including Adult Glioma, Sub-Saharan Africa (SSA) populations, and Pediatric (PED) brain tumors. Fine-tuning significantly improved the baseline models, with the DSC for Pediatric brain tumors increasing from 0.4097 to 0.6248 and the HD95 decreasing from 163.37 to 37.46. Similarly, for the SSA datasets, the DSC improved from 0.7832 to 0.8647, and the HD95 decreased from 18.38 to 10.98. These results underscore the effectiveness of our transfer learning approach in enhancing segmentation accuracy and generalization.\"),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content=\"from 0.7832 to 0.8647, and the HD95 decreased from 18.38 to 10.98. These results underscore the effectiveness of our transfer learning approach in enhancing segmentation accuracy and generalization. However, it is important to acknowledge the model's limitations. Despite outperforming previous SOTA methods, there is a scope for improvement in the HD95 metric, which suggests that the HT-CNNs model's ability to capture the full extent of tumor boundaries can be enhanced. Additionally, the transfer learning approach, while beneficial, depends on the quality and diversity of the source datasets; the performance gains might not be as pronounced when transferring knowledge from significantly dissimilar domains. Furthermore, the qualitative analysis of segmentation predictions underscores the practical effectiveness of the HT-CNNs model. The ability to produce high-quality segmentations across different datasets confirms the modelâ€™s robustness and underscores its potential for real-world\"),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='practical effectiveness of the HT-CNNs model. The ability to produce high-quality segmentations across different datasets confirms the modelâ€™s robustness and underscores its potential for real-world clinical applications. The precision with which HT-CNNs delineates tumor boundaries can have a direct impact on treatment planning and patient outcomes, emphasizing the clinical relevance of our research.  Conclusion HT-CNNs sets a new standard for brain tumor segmentation, leveraging an ensemble approach and the versatility of transfer learning. This method has proven effective in enhancing accuracy and generalization across diverse segmentation tasks, even with limited data. The fine-tuning and ensemble learning techniques, particularly the use of the STAPLE method, have significantly improved segmentation performance, as evidenced by the substantial increases in DSC and decreases in HD95 metrics. The publicly available code and model docker image encourage further research and'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='improved segmentation performance, as evidenced by the substantial increases in DSC and decreases in HD95 metrics. The publicly available code and model docker image encourage further research and adaptation in clinical contexts. The success of HT-CNNs invites future work to explore the refinement of attention mechanisms to improve the capture of tumor boundaries and the generalization to other types of medical imaging tasks. Limitations such as computational demands and the need for more extensive validation in clinical settings are areas we aim to address moving forward. The true test of HT-CNNs will be its performance in real-world clinical applications, where it has the potential to aid in treatment planning and improve patient outcomes. Acknowledging these limitations is essential as we continue to iterate on our model, ensuring it can meet the rigorous demands of clinical use and ultimately enhance patient care for those suffering from brain tumors. Compliance with Ethical'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='as we continue to iterate on our model, ensuring it can meet the rigorous demands of clinical use and ultimately enhance patient care for those suffering from brain tumors. Compliance with Ethical Standards Conflict of Interest The authors have no conflict of interest to disclose. Ethical Approval This article does not contain any studies with human participants or animals performed by any of the authors. Informed Consent This article does not contain patient data collected by any of the authors. References 1. Louis DN, Wesseling P, Aldape K, Brat DJ, Capper D, Cree IA, Eberhart C, Figarella-Branger D, Fouladi M, Fuller GN, Giannini C, Haberler C, Hawkins C, Komori T, Kros JM, Ng HK, Orr BA, Park SH, Paulus W, Perry A, Pietsch 12  T, Reifenberger G, Rosenblum M, Rous B, Sahm F, Sarkar C, Solomon DA, Tabori U, van den Bent MJ, von Deimling A, Weller M, White VA, Ellison DW (2020) cIMPACT-NOW update 6: new entity and diagnostic principle recommendations of the cIMPACT-Utrecht meeting on'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='DA, Tabori U, van den Bent MJ, von Deimling A, Weller M, White VA, Ellison DW (2020) cIMPACT-NOW update 6: new entity and diagnostic principle recommendations of the cIMPACT-Utrecht meeting on future CNS tumor classification and grading. Brain Pathol 30 (4):844-856. doi:10.1111/bpa.12832 2. Baid U, Ghodasara S, Bilello M, Mohan S, Calabrese E, Colak E, Farahani K, Kalpathy-Cramer J, Kitamura FC, Pati S, Prevedello LM, Rudie JD, Sako C, Shinohara RT, Bergquist T, Chai R, Eddy J, Elliott J, Reade W, Schaffter T, Yu T, Zheng J, Annotators B, Davatzikos C, Mongan J, Hess C, Cha S, Villanueva-Meyer J, Freymann JB, Kirby JS, Wiestler B, Crivellaro P, Colen RR, Kotrotsou A, Marcus D, Milchenko M, Nazeri A, Fathallah-Shaykh H, Wiest R, Jakab A, Weber M-A, Mahajan A, Menze B, Flanders AE, Bakas S (2021) The RSNA-ASNR-MICCAI BraTS 2021 Benchmark on Brain Tumor Segmentation and Radiogenomic Classification.arXiv:2107.02314 3. Menze BH, Jakab A, Bauer S, Kalpathy-Cramer J, Farahani K, Kirby J,'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='S (2021) The RSNA-ASNR-MICCAI BraTS 2021 Benchmark on Brain Tumor Segmentation and Radiogenomic Classification.arXiv:2107.02314 3. Menze BH, Jakab A, Bauer S, Kalpathy-Cramer J, Farahani K, Kirby J, Burren Y, Porz N, Slotboom J, Wiest R, Lanczi L, Gerstner E, Weber MA, Arbel T, Avants BB, Ayache N, Buendia P, Collins DL, Cordier N, Corso JJ, Criminisi A, Das T, Delingette H, Demiralp C, Durst CR, Dojat M, Doyle S, Festa J, Forbes F, Geremia E, Glocker B, Golland P, Guo X, Hamamci A, Iftekharuddin KM, Jena R, John NM, Konukoglu E, Lashkari D, Mariz JA, Meier R, Pereira S, Precup D, Price SJ, Raviv TR, Reza SM, Ryan M, Sarikaya D, Schwartz L, Shin HC, Shotton J, Silva CA, Sousa N, Subbanna NK, Szekely G, Taylor TJ, Thomas OM, Tustison NJ, Unal G, Vasseur F, Wintermark M, Ye DH, Zhao L, Zhao B, Zikic D, Prastawa M, Reyes M, Van Leemput K (2015) The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS). IEEE Trans Med Imaging 34 (10):1993-2024. doi:10.1109/TMI.2014.2377694 4. Bakas'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='B, Zikic D, Prastawa M, Reyes M, Van Leemput K (2015) The Multimodal Brain Tumor Image Segmentation Benchmark (BRATS). IEEE Trans Med Imaging 34 (10):1993-2024. doi:10.1109/TMI.2014.2377694 4. Bakas S, Akbari H, Sotiras A, Bilello M, Rozycki M, Kirby JS, Freymann JB, Farahani K, Davatzikos C (2017) Advancing The Cancer Genome Atlas glioma MRI collections with expert segmentation labels and radiomic features. Sci Data 4 (1):170117. doi:10.1038/sdata.2017.117 5. Bakas S, Akbari H, Sotiras A (2017) Segmentation labels for the pre-operative scans of the TCGA-GBM collection. The Cancer Imaging Archive. ed,  6. Bakas S, Akbari H, Sotiras A, Bilello M, Rozycki M, Kirby J, Freymann J, Farahani K, Davatzikos C (2017) Segmentation labels and radiomic features for the pre-operative scans of the TCGA-LGG collection. The cancer imaging archive 286 7. Adewole M, Rudie JD, Gbadamosi A, Toyobo O, Raymond C, Zhang D, Omidiji O, Akinola R, Abba Suwaid M, Emegoakor A, Ojo N, Aguh K, Kalaiwo C, Babatunde'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='collection. The cancer imaging archive 286 7. Adewole M, Rudie JD, Gbadamosi A, Toyobo O, Raymond C, Zhang D, Omidiji O, Akinola R, Abba Suwaid M, Emegoakor A, Ojo N, Aguh K, Kalaiwo C, Babatunde G, Ogunleye A, Gbadamosi Y, Iorpagher K, Calabrese E, Aboian M, Linguraru M, Albrecht J, Wiestler B, Kofler F, Janas A, LaBella D, Fathi Kzerooni A, Bran Li H, Eugenio Iglesias J, Farahani K, Eddy J, Bergquist T, Chung V, Takeshi Shinohara R, Wiggins W, Reitman Z, Wang C, Liu X, Jiang Z, Familiar A, Van Leemput K, Bukas C, Piraud M, Conte G-M, Johansson E, Meier Z, Menze BH, Baid U, Bakas S, Dako F, Fatade A, Anazodo UC (2023) The Brain Tumor Segmentation (BraTS) Challenge 2023: Glioma Segmentation in Sub-Saharan Africa Patient Population (BraTS-Africa).arXiv:2305.19369. doi:10.48550/arXiv.2305.19369 8. Fathi Kazerooni A, Khalili N, Liu X, Haldar D, Jiang Z, Muhammed Anwar S, Albrecht J, Adewole M, Anazodo U, Anderson H, Bagheri S, Baid U, Bergquist T, Borja AJ, Calabrese E, Chung V, Conte'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='8. Fathi Kazerooni A, Khalili N, Liu X, Haldar D, Jiang Z, Muhammed Anwar S, Albrecht J, Adewole M, Anazodo U, Anderson H, Bagheri S, Baid U, Bergquist T, Borja AJ, Calabrese E, Chung V, Conte G-M, Dako F, Eddy J, Ezhov I, Familiar A, Farahani K, Haldar S, Eugenio Iglesias J, Janas A, Johansen E, Jones BV, Kofler F, LaBella D, Lai HA, Van Leemput K, Bran Li H, Maleki N, McAllister AS, Meier Z, Menze B, Moawad AW, Nandolia KK, Pavaine J, Piraud M, Poussaint T, Prabhu SP, Reitman Z, Rodriguez A, Rudie JD, Salman Shaikh I, Shah LM, Sheth N, Taki Shinohara R, Tu W, Viswanathan K, Wang C, Ware JB, Wiestler B, Wiggins W, Zapaishchykova A, Aboian M, Bornhorst M, de Blank P, Deutsch M, Fouladi M, Hoffman L, Kann B, Lazow M, Mikael L, Nabavizadeh A, Packer R, Resnick A, Rood B, Vossough A, Bakas S, Linguraru MG (2023) The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs).arXiv:2305.17033. doi:10.48550/arXiv.2305.17033 9. Zeineldin'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='Linguraru MG (2023) The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs).arXiv:2305.17033. doi:10.48550/arXiv.2305.17033 9. Zeineldin RA, Karar ME, Mathis-Ullrich F, Burgert O Ensemble CNN Networks for GBM Tumors Segmentation Using Multi-parametric MRI. In: Crimi A, Bakas S (eds) Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries, Cham, 2022// 2022. Springer International Publishing, pp 473-483 10. Isensee F, Jaeger PF, Kohl SAA, Petersen J, Maier-Hein KH (2021) nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nat Methods 18 (2):203-211. doi:10.1038/s41592-020-01008-z 11. Ronneberger O, Fischer P, Brox T (2015) U-net: Convolutional networks for biomedical image segmentation. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 9351:234-241. doi:10.1007/978-3-319-24574-4_28 12.'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='segmentation. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) 9351:234-241. doi:10.1007/978-3-319-24574-4_28 12. Ã‡iÃ§ek Ã–, Abdulkadir A, Lienkamp SS, Brox T, Ronneberger O (2016) 3D U-Net: Learning Dense Volumetric  International Journal of Computer Assisted Radiology and Surgery 13 Segmentation from Sparse Annotation. In:  Medical Image Computing and Computer-Assisted Intervention â€“ MICCAI 2016. Lecture Notes in Computer Science. pp 424-432. doi:10.1007/978-3-319-46723-8_49 13. Huang Q, Zhao L, Ren G, Wang X, Liu C, Wang W (2023) NAG-Net: Nested attention-guided learning for segmentation of carotid lumen-intima interface and media-adventitia interface. Comput Biol Med 156:106718. doi:10.1016/j.compbiomed.2023.106718 14. Luo Y, Huang Q, Li X (2022) Segmentation information with attention integration for classification of breast tumor in ultrasound image. Pattern Recognition 124.'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='14. Luo Y, Huang Q, Li X (2022) Segmentation information with attention integration for classification of breast tumor in ultrasound image. Pattern Recognition 124. doi:10.1016/j.patcog.2021.108427 15. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Å, Polosukhin I (2017) Attention is all you need. Advances in neural information processing systems 30 16. Liu Y, Zhu Y, Xin Y, Zhang Y, Yang D, Xu T (2023) MESTrans: Multi-scale embedding spatial transformer for medical image segmentation. Comput Methods Programs Biomed 233:107493. doi:10.1016/j.cmpb.2023.107493 17. Zeineldin RA, Karar ME, Elshaer Z, Coburger J, Wirtz CR, Burgert O, Mathis-Ullrich F (2024) Explainable hybrid vision transformers and convolutional network for multimodal glioma segmentation in brain MRI. Sci Rep 14 (1):3713. doi:10.1038/s41598-024-54186-7 18. Zheng J, Liu H, Feng Y, Xu J, Zhao L (2023) CASF-Net: Cross-attention and cross-scale fusion network for medical image segmentation. Comput'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='Sci Rep 14 (1):3713. doi:10.1038/s41598-024-54186-7 18. Zheng J, Liu H, Feng Y, Xu J, Zhao L (2023) CASF-Net: Cross-attention and cross-scale fusion network for medical image segmentation. Comput Methods Programs Biomed 229:107307. doi:10.1016/j.cmpb.2022.107307 19. Gao Y, Zhou M, Metaxas DN (2021) UTNet: A Hybrid Transformer Architecture for Medical Image Segmentation. In:  Medical Image Computing and Computer Assisted Intervention â€“ MICCAI 2021. Lecture Notes in Computer Science. pp 61-71. doi:10.1007/978-3-030-87199-4_6 20. Warfield SK, Zou KH, Wells WM (2004) Simultaneous Truth and Performance Level Estimation (STAPLE): An Algorithm for the Validation of Image Segmentation. IEEE Transactions on Medical Imaging 23 (7):903-921. doi:10.1109/tmi.2004.828354 21. Zeineldin RA, Karar ME, Coburger J, Wirtz CR, Burgert O (2020) DeepSeg: deep neural network framework for automatic brain tumor segmentation using magnetic resonance FLAIR images. Int J Comput Assist Radiol Surg 15 (6):909-920.'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='J, Wirtz CR, Burgert O (2020) DeepSeg: deep neural network framework for automatic brain tumor segmentation using magnetic resonance FLAIR images. Int J Comput Assist Radiol Surg 15 (6):909-920. doi:10.1007/s11548-020-02186-z 22. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner T, Dehghani M, Minderer M, Heigold G, Gelly S, Uszkoreit J, Houlsby N (2021) An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.  23. Lin J, Lin J, Lu C, Chen H, Lin H, Zhao B, Shi Z, Qiu B, Pan X, Xu Z, Huang B, Liang C, Han G, Liu Z, Han C (2023) CKD-TransBTS: Clinical Knowledge-Driven Hybrid Transformer With Modality-Correlated Cross-Attention for Brain Tumor Segmentation. IEEE Transactions on Medical Imaging 42 (8):2451-2461. doi:10.1109/tmi.2023.3250474 24. Wang H, Zhu Y, Green B, Adam H, Yuille A, Chen L-C (2020) Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation. In:  Computer Vision â€“ ECCV 2020. Lecture Notes in Computer Science. pp'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='Wang H, Zhu Y, Green B, Adam H, Yuille A, Chen L-C (2020) Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation. In:  Computer Vision â€“ ECCV 2020. Lecture Notes in Computer Science. pp 108-126. doi:10.1007/978-3-030-58548-8_7 25. Luu HM, Park S-H (2022) Extending nn-UNet for Brain Tumor Segmentation. In:  Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries. Lecture Notes in Computer Science. pp 173-186. doi:10.1007/978-3-031-09002-8_16 26. Cheplygina V, de Bruijne M, Pluim JPW (2019) Not-so-supervised: A survey of semi-supervised, multi-instance, and transfer learning in medical image analysis. Med Image Anal 54:280-296. doi:10.1016/j.media.2019.03.009 27. Zhang S, Miao Y, Chen J, Zhang X, Han L, Ran D, Huang Z, Pei N, Liu H, An C (2023) Twist-Net: A multi-modality transfer learning network with the hybrid bilateral encoder for hypopharyngeal cancer segmentation. Comput Biol Med 154:106555. doi:10.1016/j.compbiomed.2023.106555 28. Zeineldin RA,'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='A multi-modality transfer learning network with the hybrid bilateral encoder for hypopharyngeal cancer segmentation. Comput Biol Med 154:106555. doi:10.1016/j.compbiomed.2023.106555 28. Zeineldin RA, Karar ME, Burgert O, Mathis-Ullrich F (2023) Multimodal CNN Networks for Brain Tumor Segmentation in MRI: A BraTS 2022 Challenge Solution. In:  Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries. Lecture Notes in Computer Science. pp 127-137. doi:10.1007/978-3-031-33842-7_11 29. Ferreira A, Solak N, Li J, Dammann P, Kleesiek J, Alves V, Egger J (2024) How we won BraTS 2023 Adult Glioma challenge? Just faking it! Enhanced Synthetic Data Augmentation and Model Ensemble for brain tumour segmentation.arXiv:2402.17317. doi:10.48550/arXiv.2402.17317 30. Fathi Kazerooni A, Khalili N, Liu X, Haldar D, Jiang Z, Zapaishchykova A, Pavaine J, Shah LM, Jones BV, Sheth N, Prabhu SP, McAllister AS, Tu W, Nandolia KK, Rodriguez AF, Salman Shaikh I, Sanchez Montano M, Lai HA,'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='A, Khalili N, Liu X, Haldar D, Jiang Z, Zapaishchykova A, Pavaine J, Shah LM, Jones BV, Sheth N, Prabhu SP, McAllister AS, Tu W, Nandolia KK, Rodriguez AF, Salman Shaikh I, Sanchez Montano M, Lai HA, Adewole M, Albrecht J, Anazodo U, Anderson H, Muhammed Anwar S, Aristizabal A, Bagheri S, Baid U, Bergquist T, Borja AJ, Calabrese E, Chung V, Conte G-M, Eddy J, Ezhov I, Familiar AM, Farahani K, Gandhi D, Gottipati A, 14  Haldar S, Eugenio Iglesias J, Janas A, Elaine E, Karargyris A, Kassem H, Khalili N, Kofler F, LaBella D, Van Leemput K, Li HB, Maleki N, Meier Z, Menze B, Moawad AW, Pati S, Piraud M, Poussaint T, Reitman ZJ, Rudie JD, Saluja R, Sheller M, Takeshi Shinohara R, Viswanathan K, Wang C, Wiestler B, Wiggins WF, Davatzikos C, Storm PB, Bornhorst M, Packer R, Hummel T, de Blank P, Hoffman L, Aboian M, Nabavizadeh A, Ware JB, Kann BH, Rood B, Resnick A, Bakas S, Vossough A, Linguraru MG (2024) BraTS-PEDs: Results of the Multi-Consortium International Pediatric Brain Tumor'),\n",
       " Document(metadata={'arxiv_id': '2412.08240v1', 'title': 'Unified HT-CNNs Architecture: Transfer Learning for Segmenting Diverse Brain Tumors in MRI from Gliomas to Pediatric Tumors', 'section': 'body', 'authors': 'Ramy A. Zeineldin, Franziska Mathis-Ullrich'}, page_content='P, Hoffman L, Aboian M, Nabavizadeh A, Ware JB, Kann BH, Rood B, Resnick A, Bakas S, Vossough A, Linguraru MG (2024) BraTS-PEDs: Results of the Multi-Consortium International Pediatric Brain Tumor Segmentation Challenge 2023.arXiv:2407.08855. doi:10.48550/arXiv.2407.08855'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'title_abstract', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='Title: Cross-Modality Deep Feature Learning for Brain Tumor Segmentation\\n\\nAbstract: Recent advances in machine learning and prevalence of digital medical images have opened up an opportunity to address the challenging brain tumor segmentation (BTS) task by using deep convolutional neural networks. However, different from the RGB image data that are very widespread, the medical image data used in brain tumor segmentation are relatively scarce in terms of the data scale but contain the richer information in terms of the modality property. To this end, this paper proposes a novel cross-modality deep feature learning framework to segment brain tumors from the multi-modality MRI data. The core idea is to mine rich patterns across the multi-modality data to make up for the insufficient data scale. The proposed cross-modality deep feature learning framework consists of two learning processes: the cross-modality feature transition (CMFT) process and the cross-modality feature fusion (CMFF) process, which aims at learning rich feature representations by transiting knowledge across different modality data and fusing knowledge from different modality data, respectively. Comprehensive experiments are conducted on the BraTS benchmarks, which show that the proposed cross-modality deep feature learning framework can effectively improve the brain tumor segmentation performance when compared with the baseline methods and state-of-the-art methods.'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='Cross-Modality Deep Feature Learning for Brain Tumor\\nSegmentation\\nDingwen Zhang1, Guohai Huang1, Qiang Zhang1,âˆ—, Jungong Han2,âˆ—, Junwei Han3,\\nYizhou Yu4\\nAbstract\\nRecent advances in machine learning and prevalence of digital medical images have\\nopened up an opportunity to address the challenging brain tumor segmentation (BTS)\\ntask by using deep convolutional neural networks. However, different from the RGB\\nimage data that are very widespread, the medical image data used in brain tumor seg-\\nmentation are relatively scarce in terms of the data scale but contain the richer informa-\\ntion in terms of the modality property. To this end, this paper proposes a novel cross-\\nmodality deep feature learning framework to segment brain tumors from the multi-\\nmodality MRI data. The core idea is to mine rich patterns across the multi-modality\\ndata to make up for the insufï¬cient data scale. The proposed cross-modality deep'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='modality MRI data. The core idea is to mine rich patterns across the multi-modality\\ndata to make up for the insufï¬cient data scale. The proposed cross-modality deep\\nfeature learning framework consists of two learning processes: the cross-modality fea-\\nture transition (CMFT) process and the cross-modality feature fusion (CMFF) process,\\nwhich aims at learning rich feature representations by transiting knowledge across dif-\\nferent modality data and fusing knowledge from different modality data, respectively.\\nComprehensive experiments are conducted on the BraTS benchmarks, which show that\\nthe proposed cross-modality deep feature learning framework can effectively improve\\nthe brain tumor segmentation performance when compared with the baseline methods\\nâˆ—Corresponding author\\nEmail addresses: zhangdingwen2006yyy@gmail.com (Dingwen Zhang),\\nhuanggh666@gmail.com (Guohai Huang), qzhang@xidian.edu.cn (Qiang Zhang),\\njungonghan77@gmail.com (Jungong Han), junweihan2010@gmail.com (Junwei Han),'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='huanggh666@gmail.com (Guohai Huang), qzhang@xidian.edu.cn (Qiang Zhang),\\njungonghan77@gmail.com (Jungong Han), junweihan2010@gmail.com (Junwei Han),\\nyizhouy@acm.org (Yizhou Yu)\\n1School of Mechano-Electronic Engineering, Xidian University.\\n2Computer Science Department, Aberystwyth University.\\n3School of Automation, Northwestern Polytechnical University.\\n4Deepwise AI Lab.\\nPreprint submitted to Elsevier January 10, 2022\\narXiv:2201.02356v1  [eess.IV]  7 Jan 2022and state-of-the-art methods.\\nKeywords: Brain tumor segmentation, Cross-modality feature transition,\\nCross-modality feature fusion, Feature learning.\\n1. Introduction\\nAs the prevailing disease with the highest mortality, the research on brain tumors\\nhas received more and more attention. In this paper, we study a deep learning-based\\nautomatic way to segment the glioma, which is called brain tumor segmentation (BTS)\\n[1]. In this task, the medical images contain four MRI modalities, which are the T1-'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='automatic way to segment the glioma, which is called brain tumor segmentation (BTS)\\n[1]. In this task, the medical images contain four MRI modalities, which are the T1-\\nweighted (T1) modality, contrast enhanced T1-weighted (T1c) modality, T2-weighted\\n(T2) modality, and Fluid Attenuation Inversion Recovery (FLAIR) modality, respec-\\ntively. The goal is to segment three different target areas, which are the whole tumor\\narea, the tumor core area, and the enhancing tumor core area, respectively. An example\\nof the multi-modality data and the corresponding tumor area labels are shown in Fig.\\n1.\\nWith the rapid development of the deep learning technique, deep convolutional\\nneural networks (DCNNs) have been introduced into the medical image analysis com-\\nmunity and widely used in BTS. Given the established DCNN models, existing brain\\ntumor segmentation methods usually consider this task as a multi-class pixel-level clas-'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='munity and widely used in BTS. Given the established DCNN models, existing brain\\ntumor segmentation methods usually consider this task as a multi-class pixel-level clas-\\nsiï¬cation problem just as the semantic segmentation task on common RGB image data.\\nHowever, by omitting the great disparity between the medical image data and the com-\\nmon RGB image data, such approaches would not obtain the optimal solutions. Specif-\\nically, there are two-fold distinct properties between these two kinds of data: 1) Very\\nlarge-scale RGB image data can be acquired from our daily life by the smart phones\\nor cameras. However, the medical image data are very scarce, especially for the cor-\\nresponding manual annotation that requires expertise and tends to be very time con-\\nsuming. 2) As a departure from the common RGB image data, the medical image data\\n(for the investigated brain tumor segmentation task and other tasks) usually consist of'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='suming. 2) As a departure from the common RGB image data, the medical image data\\n(for the investigated brain tumor segmentation task and other tasks) usually consist of\\nmultiple MRI modalities that capture different pathological properties.\\nDue to the above-mentioned characteristics, BTS still has challenging issues needed\\n2Figure 1: An illustration of the brain tumor segmentation task. The top four volume data are the multi-\\nmodality MR image data. The segmentation labels for the Whole Tumor area (WT), Tumor Core area (TC),\\nEnhancing Tumor Core area (WT), and all types of tumor areas are shown in the bottom row. The regions\\nwithout colored masks are normal areas.\\nto be addressed. Speciï¬cally, due to the insufï¬cient data scale, training a DCNN model\\nmight surfer from the over-ï¬tting issue as DCNN models usually contain numerous\\nnetwork parameters. This increases the difï¬culty of training a desired DCNN model'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='might surfer from the over-ï¬tting issue as DCNN models usually contain numerous\\nnetwork parameters. This increases the difï¬culty of training a desired DCNN model\\nfor brain tumor segmentation. Secondly, due to the complex data structure, directly\\nconcatenating multi-modality data to form the network input like in the previous works\\n[2, 3] is neither the best choice to fully take advantage of the knowledge underlying\\neach modality data, nor the effective strategy to fuse the knowledge from the multi-\\nmodality data.\\nTo address these issues, this paper proposes a novel cross-modality deep feature\\nlearning framework to learn to segment brain tumors from the multi-modality MRI\\ndata. Considering the fact that the medical image data are relatively scarce in terms\\nof the data scale but contain rich information in terms of the modality property, we\\npropose to explore rich patterns among the multi-modality data to make up for the in-'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='of the data scale but contain rich information in terms of the modality property, we\\npropose to explore rich patterns among the multi-modality data to make up for the in-\\nsufï¬cient data scale. Speciï¬cally, the proposed cross-modality feature learning frame-\\nwork consists of two learning processes: the cross-modality feature transition (CMFT)\\nprocess and the cross-modality feature fusion (CMFF) process.\\nIn the cross-modality feature transition process, we adopt the generative adversar-\\n3Figure 2: An illustration of the proposed cross-modality deep feature learning framework for brain tumor\\nsegmentation. To be brief and to the point, we only show the learning framework by using two-modality\\ndata.\\nial network learning scheme to learn useful features that can facilitate the knowledge\\ntransition across different modality data. This enables the network to mine intrinsic\\npatterns that are helpful to the brain tumor segmentation task from each modality data.'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='transition across different modality data. This enables the network to mine intrinsic\\npatterns that are helpful to the brain tumor segmentation task from each modality data.\\nThe intuition behind this process is that if the DCNN model can transit (or convert) a\\nsample from one modality to another modality, it may capture the modality patterns of\\nthe two MRI modalities as well as the content patterns (such as the organ type and lo-\\ncation) of this sample, while these patterns are helpful for brain tumor segmentation. In\\nthe cross-modality feature fusion process, we build a novel deep neural network archi-\\ntecture to take advantage of the deep features obtained from the cross-modality feature\\ntransition process and implement the deep fusion of the features captured from differ-\\nent modality data to predict the brain tumor areas. This is distinct from the existing\\nbrain tumor segmentation methods or the naive strategies which either 1) implement'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='ent modality data to predict the brain tumor areas. This is distinct from the existing\\nbrain tumor segmentation methods or the naive strategies which either 1) implement\\nthe fusion process simply at the input level, i.e., concatenating multi-modality image\\ndata as the network input, or 2) implement the fusion process at the output level, i.e.,\\nintegrating the segmentation results from different modality data.\\nFig. 2 illustrates the proposed learning framework brieï¬‚y, from which we can\\nobserve that in the cross-modality feature transition process, we build two generators\\nand two discriminators to transit the knowledge across the two modality data. Here\\nthe generators are used to generate one modality data from the other modality data and\\n4the discriminators aim to distinguish the generated data and the real data. While in\\nthe cross-modality feature fusion process, we adopt the generators to predict the brain'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='4the discriminators aim to distinguish the generated data and the real data. While in\\nthe cross-modality feature fusion process, we adopt the generators to predict the brain\\ntumor regions from each modality data and fuse the deep features learned from them to\\nobtain the ï¬nal segmentation results. In the fusion branch, we design a novel scheme by\\nusing the single-modality prediction results to guide the feature fusion process, which\\ncan obtain stronger feature representations during the fusion process to aid segment the\\ndesired brain tumor areas.\\nTo sum up, this work mainly has four-fold contributions as follows:\\nâ€¢ By revealing the intrinsic difference between the segmentation tasks on the med-\\nical image data and the common RGB image data, we establish a novel cross-\\nmodality deep feature learning framework for brain tumor segmentation, which\\nconsists of the cross-modality feature transition process and the cross-modality\\nfeature fusion process.'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='modality deep feature learning framework for brain tumor segmentation, which\\nconsists of the cross-modality feature transition process and the cross-modality\\nfeature fusion process.\\nâ€¢ We present a novel idea to learn useful feature representations from the knowl-\\nedge transition across different modality data. To achieve this goal, we build a\\ngenerative adversarial network-based learning scheme which can implement the\\ncross-modality feature transition process without any human annotation.\\nâ€¢ For implementing the cross-modality feature fusion process, a new cross-modality\\nfeature fusion network is built for brain tumor segmentation, which transfers the\\nfeatures learned from the feature transition process and is empowered with the\\nnovel fusion branch to use the single-modality prediction results to guide the\\nfeature fusion process.\\nâ€¢ Comprehensive experiments are conducted on the BraTS benchmarks, which\\nshow that the proposed approach can effectively improve the brain tumor seg-'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='feature fusion process.\\nâ€¢ Comprehensive experiments are conducted on the BraTS benchmarks, which\\nshow that the proposed approach can effectively improve the brain tumor seg-\\nmentation performance when compared with the baseline methods and the state-\\nof-the-art methods.\\n52. Related Works\\n2.1. Brain Tumor Segmentation\\nBrain tumor segmentation is a hot topic in the medical image analysis and machine\\nlearning community. It has received great attention in the past few years. Early efforts\\nin this ï¬led designed hand-crafted features and adopted the classic machine learning\\nmodels to predict the brain tumor areas. Due to the rapid development of the deep\\nlearning technique, the recent brain tumor segmentation approaches mainly apply the\\ndeep features and classiï¬ers from the DCNN models. Based on the type of the convo-\\nlutional operation used in the DCNN models, we brieï¬‚y divide the existing methods\\ninto two groups, i.e., the 2D CNN-based methods and 3D CNN-based methods. The'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='lutional operation used in the DCNN models, we brieï¬‚y divide the existing methods\\ninto two groups, i.e., the 2D CNN-based methods and 3D CNN-based methods. The\\n2D CNN-based methods [4, 5, 6] apply the 2D convolutional operations and split the\\n3D volume samples into 2D slices or 2D patches. While the 3D CNN-based methods\\n[7, 8, 9] apply the 3D convolutional operations, which can take the whole 3D volume\\nsamples or the extracted sub 3D patches as the network input.\\nAlthough these deep learning-based methods can already obtain much powerful\\nfeature representation when compared to the early classical methods that are based on\\nthe hand-crafted features, they did not make full use of the multi-modality data in the\\nfeature learning process, which limits the effectiveness of the learned feature repre-\\nsentations and the ï¬nal segmentation results. Realizing this issue, Fidon et al. [10]\\nproposed a multi-modal convolutional network for brain tumor segmentation, where'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='sentations and the ï¬nal segmentation results. Realizing this issue, Fidon et al. [10]\\nproposed a multi-modal convolutional network for brain tumor segmentation, where\\nnested network structure was designed to explicitly leverage deep features within or\\nacross modalities. Different from our approach, they did not formulate the across\\nmodality transition process and did not employ the mask guidance scheme in the fea-\\nture fusion process.\\n2.2. Multi-modality Feature Learning\\nMulti-modality feature learning is gaining more and more attention in the recent\\nyears as the multi-modality data can provide richer information for sensing the physical\\nworld. Existing works have applied multi-modality feature learning in many computer\\nvision-based tasks such as 3D shape recognition and retrieval [11], survival prediction\\n6[12], RGB-D object recognition [13] and person re-identiï¬cation [14]. Among these\\nmethods, Bu et al. [11] built a multi-modality fusion head to fuse the deep features'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='6[12], RGB-D object recognition [13] and person re-identiï¬cation [14]. Among these\\nmethods, Bu et al. [11] built a multi-modality fusion head to fuse the deep features\\nlearnt by a CNN network branch and a Deep Belief Network (DBN) branch . To inte-\\ngrate multiple modalities and eliminate view variations, Yao et al. [12] designed a deep\\ncorrelational learning module for learning informative features on the pathological data\\nand the molecular data. In [15], Wang et al. proposed a large-margin multi-modal deep\\nlearning framework to discover the most discriminative features for each modality and\\nharness the complementary relationship between different modalities.\\nAlthough the multi-modality feature learning technique has been applied in many\\ncomputer vision tasks, it is still a under-studied issue in the research ï¬eld of medical\\nimage understanding, especially for the task of brain tumor segmentation. To this end,'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='computer vision tasks, it is still a under-studied issue in the research ï¬eld of medical\\nimage understanding, especially for the task of brain tumor segmentation. To this end,\\nthis paper makes an early effort to build a cross-modality deep feature learning frame-\\nwork for brain tumor segmentation. The cross-modality feature transition (CMFT)\\nprocess and the cross-modality feature fusion (CMFF) process designed in this work\\nare also novel to the existing multi-modality feature learning methods.\\n3. The Proposed Approach\\n3.1. Cross-Modality Feature Transition\\nAs shown in the left part of Fig. 2, given modality Aand modality B, we adopt\\nthe generative adversarial learning strategy to facilitate the knowledge transition across\\nthe different modality data, which in turn captures the informative patterns from each\\nmodality data. To be speciï¬c, for each modality data, we build a generative network,\\ni.e., the generator G, and a discriminative network, i.e., the discriminator D, to formu-'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='modality data. To be speciï¬c, for each modality data, we build a generative network,\\ni.e., the generator G, and a discriminative network, i.e., the discriminator D, to formu-\\nlate the feature transition process. For achieving this goal, we apply the CycleGAN\\nlearning scheme [16] to learn the transition GA\\nB : Aâ†’Band GB\\nA : B â†’Aso that\\nGB\\nA(GA\\nB(A)) =A,\\nGA\\nB(GB\\nA(B)) =B,\\n(1)\\nwhere A and B indicates the â€œrealâ€ input sample from the modality A and modality\\nB, respectively. Compared with other generative adversarial learning schemes, the\\n7cycle consistency-based learning scheme adopted by the CycleGAN model has the\\nfollowing advantages for learning representative features: Firstly, it learns the transition\\nGA\\nB : A â†’B and GB\\nA : B â†’Asimultaneously, thus facilitating a better exploration\\nof the relationship between the two modality data and maintaining the content of each\\nmodality data. Secondly, during the training process, it does not necessarily require'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='of the relationship between the two modality data and maintaining the content of each\\nmodality data. Secondly, during the training process, it does not necessarily require\\nmatched modality data which might be hard to obtain in practical applications.\\nBesides the generators, there are also two discriminators DA and DB, where DA\\ndistinguish the â€œfakeâ€A-modality data generated byGB\\nA(B) from the â€œrealâ€A-modality\\ndata while DB distinguish the â€œfakeâ€ B-modality data generated by GA\\nB(A) from the\\nâ€œrealâ€ B-modality data. During the generative adversarial learning process, we adopt\\nthe adversarial loss to match the distribution of the generated fake data to the distribu-\\ntion of the â€œrealâ€ data. To this end, the adversarial loss is deï¬ned as:\\nLadv(GA\\nB,DB) =EB[(DB(B) âˆ’1)2]\\n+ EA[DB(GA\\nB(A))2],\\n(2)\\nLadv(GB\\nA,DA) =EA[(DA(A) âˆ’1)2]\\n+ EB[DA(GB\\nA(B))2],\\n(3)\\nwhere EM [Ï„] indicates the expectation of Ï„ for all the samples from modality M.'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='Ladv(GA\\nB,DB) =EB[(DB(B) âˆ’1)2]\\n+ EA[DB(GA\\nB(A))2],\\n(2)\\nLadv(GB\\nA,DA) =EA[(DA(A) âˆ’1)2]\\n+ EB[DA(GB\\nA(B))2],\\n(3)\\nwhere EM [Ï„] indicates the expectation of Ï„ for all the samples from modality M.\\nIn addition, we also follow [16] to apply the cycle consistency loss to constrain the\\nmodality transition function GA\\nB and GB\\nA from random permution in the target modality\\ndomain. To enforce the modality transition functionGA\\nB and GB\\nA to be cycle consistent,\\nwe encourage GA\\nB to transit the generated â€œfakeâ€ A-modality data GB\\nA(B) back to the\\nâ€œrealâ€ B-modality data, and similarly encourage GB\\nA to transit the generated â€œfakeâ€\\nB-modality data GA\\nB(A) back to the â€œrealâ€ A-modality data. To this end, the cycle\\nconsistency loss is deï¬ned as:\\nLcyc = EA[||GB\\nA(GA\\nB(A)) âˆ’A||1]\\n+ EB[||GA\\nB(GB\\nA(B)) âˆ’B||1].\\n(4)\\nBy considering both the adversarial loss and the cycle consistency loss, the full\\nlearning object function of the cross-modality feature transition process becomes:\\narg min\\nGA\\nB,GB\\nA\\nmax\\nDA,DB'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='(4)\\nBy considering both the adversarial loss and the cycle consistency loss, the full\\nlearning object function of the cross-modality feature transition process becomes:\\narg min\\nGA\\nB,GB\\nA\\nmax\\nDA,DB\\nL(GA\\nB,GB\\nA,DA,DB), (5)\\n8Figure 3: Illustration of the detailed architecture of the generator, where IN is short for instance normal-\\nization. Notice that this is also the architecture of the single-modality feature learning branch. The only\\ndifference between these two network branches is the last output layer, where the output of the generator is\\ndrawn in the solid line while the output of the single-modality feature learning branch is drawn in the dashed\\nline. The deep features in the last two convolutional layers, as well as the output of the single-modality\\nfeature learning branch, are connected to the cross-modality feature fusion branch, which is annotated in red.\\nFor ease of understanding, we show the network in 2D convolution-like architecture. While we actually use'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='For ease of understanding, we show the network in 2D convolution-like architecture. While we actually use\\nthe 3D convolution in network layers.\\nwhere\\nL(GA\\nB,GB\\nA,DA,DB) =Ladv(GA\\nB,DB)\\n+ Ladv(GB\\nA,DA)\\n+ Î»Lcyc(GA\\nB,GB\\nA),\\n(6)\\nÎ»is a hyper-parameter to weigh the adversarial loss and the cycle consistency loss.\\nNetwork Architecture:When designing the generator, we adopt a U-net architecture\\ndue to its effectiveness in both image-to-image translation [17] and brain tumor seg-\\nmentation [6, 4, 9]. Considering the training samples are in form of 3D volumes, we\\nadopt 3D convolutions in the network layers, thus obtaining the 3D U-net architecture.\\nThe concrete network architecture is shown in Fig. 3. For the discriminator, we follow\\nthe existing work [16] to construct it by using several convolutional layers to obtain the\\nclassiï¬cation results. The concrete network architecture of the discriminator is shown\\nin Table 1.\\n9Table 1'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='the existing work [16] to construct it by using several convolutional layers to obtain the\\nclassiï¬cation results. The concrete network architecture of the discriminator is shown\\nin Table 1.\\n9Table 1\\nThe architecture of the discriminator network branch. In the â€œInputâ€ block, the ï¬rst dimension is the\\nnumber of channels and the next three dimensions are the size of the feature maps. Conv. is short for the 3D\\nconvolution, and # ï¬lters indicates the number of ï¬lters. Notice that when learning on modality quaternions\\nmentioned in Sec. 3.3, the number of the input channel of L1 becomes 2.\\nType Filter size stride # ï¬lters Input\\nL1 Conv. 4 Ã—4 Ã—4 2 16 1,128,128,128\\nL2 LReLU - - - 16,64,64,64\\nL3 Conv. 4 Ã—4 Ã—4 2 32 16,64,64,64\\nL4 INor. - - - 32,32,32,32\\nL5 LReLU - - - 32,32,32,32\\nL6 Conv. 4 Ã—4 Ã—4 2 64 32,32,32,32\\nL7 INor. - - - 64,16,16,16\\nL8 LReLU - - - 64,16,16,16\\nL9 Conv. 4 Ã—4 Ã—4 2 128 64,16,16,16\\nL10 INor. - - - 128,8,8,8\\nL11 LReLU - - - 128,8,8,8\\nL12 Conv. 4 Ã—4 Ã—4 1 1 128,8,8,8'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='L7 INor. - - - 64,16,16,16\\nL8 LReLU - - - 64,16,16,16\\nL9 Conv. 4 Ã—4 Ã—4 2 128 64,16,16,16\\nL10 INor. - - - 128,8,8,8\\nL11 LReLU - - - 128,8,8,8\\nL12 Conv. 4 Ã—4 Ã—4 1 1 128,8,8,8\\n103.2. Cross-Modality Feature Fusion\\nTo implement the cross-modality feature fusion process, we establish a novel cross-\\nmodality feature fusion network for brain tumor segmentation. Equipped with the\\nnewly designed fusion branch which uses the single-modality prediction results to\\nguide the feature fusion process, the proposed network can not only transfer the fea-\\ntures learned from the feature transition process conveniently but also learn powerful\\nfusion features for segmenting the desired brain tumor areas.\\nGiven the input data from modality A and B, the cross-modality feature fusion\\nnetwork contains two single-modality feature learning branches SA and SB and a\\ncross-modality feature fusion branch SF for segmenting the desired brain tumor ar-'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='network contains two single-modality feature learning branches SA and SB and a\\ncross-modality feature fusion branch SF for segmenting the desired brain tumor ar-\\neas. Speciï¬cally, the single-modality feature learning branch SA takes the A-modality\\ndata as the input and learns representative features to predict the segmentation masks\\nof the brain tumor areas SA(A) as the output. Similarly, the single-modality feature\\nlearning branch SB takes the B-modality data as the input and learns representative\\nfeatures to predict the segmentation masks of the brain tumor areas SB(B) as the out-\\nput. The cross-modality fusion branch takes the deep features as well as the predicted\\nsegmentation masks of the two single-modality feature learning branches as input to\\nlearn more powerful fusion features to generate the ï¬nal segmentation masks of the\\nbairn tumor areas SF (A,B). To learn the cross-modality feature fusion network, we\\nintroduce the following object function:\\narg min\\nSA,SB,SF'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='bairn tumor areas SF (A,B). To learn the cross-modality feature fusion network, we\\nintroduce the following object function:\\narg min\\nSA,SB,SF\\nLseg(SA) +Lseg(SB) +Lseg(SF ). (7)\\nTo prevent the model from being heavily affected by the unbalance among different\\ntypes of tumor areas, we follow [18] to calculate Lseg(SA), Lseg(SB), and Lseg(SF )\\nby the Dice Similarity Coefï¬cient (DSC). Thus, for Lseg(SA), we have\\nLseg(SA) = 1âˆ’2 Ã—|Y âˆ©SA(A)|\\n|Y|+ |SA(A)| , (8)\\nwhere Y is the ground-truth annotation for the desired brain tumor areas. It goes the\\nsame for Lseg(SB) and Lseg(SF ).\\nNetwork Architecture: Although the single-modality feature learning branch does\\n11Figure 4: Illustration of the cross-modality feature fusion branch with the mask-guided feature learning\\nscheme, where IN is short for instance normalization. For ease of understanding, we show the network in\\n2D convolution-like architecture. While we actually use the 3D convolution in network layers.'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='scheme, where IN is short for instance normalization. For ease of understanding, we show the network in\\n2D convolution-like architecture. While we actually use the 3D convolution in network layers.\\nnot necessarily be the same with the generator used in cross-modality feature transi-\\ntion, the more network layers shared by these two networks, the richer features can\\nbe conveniently transferred from the feature transition process to the feature fusion\\nprocess. To this end, we adopt a quite similar network architecture to the generator\\nGA\\nB (or GB\\nA) to build the single-modality feature learning network branches SA and\\nSB (see the right part of Fig. 2). Compared to the generator, the only difference is\\nthe number of kernels set to the last convolutional layer. As shwon in Fig. 3, the last\\nconvolutional layer of the single-modality feature learning network branch uses four\\nconvolutional kernels, while the generator only uses one convolutional kernel in the'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='convolutional layer of the single-modality feature learning network branch uses four\\nconvolutional kernels, while the generator only uses one convolutional kernel in the\\nlast convolutional layer. As can be seen, designing the single-modality feature learning\\nnetwork branch in this way could share the most network layers with the generator and\\nthus can take full advantage of the features learned from the cross-modality feature\\ntransition process.\\nFor fusing the knowledge from each modality data, we propose a novel cross-\\nmodality feature fusion branch. As shown in Fig. 4, the proposed cross-modality\\nfeature fusion branch contains several convolutional layers to fuse deep features from\\ndifferent layers of the two single-modality feature learning network branches. The\\n12Figure 5: Examples of the modality pairs, where we use the T1 modality and T1c modality to form the\\nmodality pair A while using the T2 modality and FLAIR modality to form the modality pair B. From the'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='modality pair A while using the T2 modality and FLAIR modality to form the modality pair B. From the\\nexamples we can observe that the information contained within each modality pair is relatively consistent\\nwhile the information contained across the different modality pairs is relatively distinct and complementary.\\nThis enables the cross-modality feature transition process to learn rich patterns.\\nconvolutional layers are then followed by a mask-guided attention block to learn more\\npowerful fusion features for brain tumor segmentation. Different from the conven-\\ntional attention modules, such as [19, 20], the attention masks in our mask-guided\\nattention block are the segmentation masks predicted by the single-modality feature\\nlearning branches rather than those inferred from the deep feature maps from previ-\\nous network layers. In other words, the attention masks in the conventional attention\\nnetwork blocks/modules are used to guide the network learning on its own network'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='ous network layers. In other words, the attention masks in the conventional attention\\nnetwork blocks/modules are used to guide the network learning on its own network\\nbranch. They are learned in a bottom-up manner. In contrast, the attention masks in\\nthis work are used to guide the network learning on a different network branch and they\\nare learned in a top-down manner.\\n13Figure 6: Illustration of the proposed strategy to extend the proposed cross-modality deep feature learning\\nframework to work on modality quaternions. In the cross-modality feature transition process, we convert the\\ninput and output from one modality data to the concatenation of an modality pair. While in the cross-modality\\nfeature fusion process, we convert the single-modality feature learning branch to the single-modality-pair\\nfeature learning branch, which predicts the segmentation masks of each single-modality-pair.\\n3.3. Learn on Modality Quaternions'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='feature learning branch, which predicts the segmentation masks of each single-modality-pair.\\n3.3. Learn on Modality Quaternions\\nAs the data used in the investigated brain tumor segmentation task usually have\\nfour modalities, i.e., the T1, T1-c, T2, and FLAIR modality (see Fig. 1), we also ex-\\nplore effective extension strategies to enable the aforementioned cross-modality deep\\nfeature learning framework to work on the modality quaternions. An naive extension\\nis to adopt six cycGAN models, i.e, {GA\\nB,GB\\nA}, {GA\\nC,GC\\nA}, {GA\\nD,GD\\nA }, {GB\\nC,GC\\nB},\\n{GB\\nD,GD\\nB}, {GC\\nD,GD\\nC }, to learn the transition functions between each modality data\\nand fuse the four single-modality feature learning branches in the cross-modality fea-\\nture fusion network. Although this strategy can also learn rich feature representations\\nfrom both the cross-modality feature transition and cross-modality feature fusion pro-\\ncesses, it requires too large computational cost to implement in practice.'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='from both the cross-modality feature transition and cross-modality feature fusion pro-\\ncesses, it requires too large computational cost to implement in practice.\\nTo this end, we propose a simple yet effective way to implement the learning frame-\\nwork on modality quaternions. Instead of transiting knowledge between each modality\\ndata, we implement the transition process between each modality pair. That is to say,\\nthe transition process is extended to transit knowledge from a modality pair to another\\nmodality pair. In this work, we use the T1 and T1-c modalities to form a modality\\npair while T2 and FLAIR modalities to form another modality pair. In this way, the in-\\nformation within each modality pair tends to be consistent while the information from\\ndifferent modality pairs tends to be distinct and complementary (see Fig. 5), which\\nenables the cross-modality feature transition process to learn rich patterns. Based on'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='different modality pairs tends to be distinct and complementary (see Fig. 5), which\\nenables the cross-modality feature transition process to learn rich patterns. Based on\\nthis strategy, we implement the proposed approach on modality quaternions by simply\\n14converting the input data of the generators and discriminators in the CMFT process\\nand the input data of the feature learning branch in the CMFF process to be the con-\\ncatenation of two modality data, while other parts of the learning framework remain\\nunchanged (see Fig. 6).\\n3.4. Discussion of the Learning Framework\\nAs described in previous sections, the proposed learning framework contains two\\nprocesses, i.e., the CMFT process and the CMFF process. In fact, these two processes\\ncan also be considered as two learning phases of a uniï¬ed DCNN model. Speciï¬cally,\\nimaging that we have a DCNN model contains two generators, two discriminators and\\na fusion network branch, our approach trains the two generators and the two discrim-'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='imaging that we have a DCNN model contains two generators, two discriminators and\\na fusion network branch, our approach trains the two generators and the two discrim-\\ninators in the ï¬rst learning phase and then trains the two generators (with a modiï¬ed\\nprediction layer and loss function) together with the fusion network branch in the sec-\\nond learning phase. From this point of view, our proposed deep learning framework\\ncan be seen as a uniï¬ed end-to-end learning model with two-phase training strategy.\\nBesides the two-phase training strategy, we can actually learn CMFT and CMFF\\nsimultaneously, where both Eq. 5 and Eq. 7 would be introduced to form the new\\nobjective function of each training sample. However, by simultaneously learning the\\ntwo generators, the two discriminators and the fusion network branch, this strategy has\\ntoo much memory costs especially when exploring the 3D volume data like in this task.'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='two generators, the two discriminators and the fusion network branch, this strategy has\\ntoo much memory costs especially when exploring the 3D volume data like in this task.\\nThus, we choose to adopt the two-phase training strategy to implement our approach.\\n4. Experiments\\n4.1. Experimental Settings\\nIn the BraTS 2017 and BraTS 2018 benchmark datasets, there are four modalities,\\ni.e., T1, T1-c, T2, and FLAIR, for each patient. The BraTS 2017 benchmark has two\\nsub-sets: a training set, which contains 285 subjects, and a validation set containing\\n46 subjects with hidden ground truth. The BraTS 2018 benchmark contains the same\\nnumber of subjects in its training set but has 66 subjects in the validation set with hidden\\nground truth. When implementing the experiments on each of the benchmarks, we use\\n15the training set to train the brain tumor segmentation models while use the validation\\nset to test the segmentation performance. We adopted the ofï¬cial metrics that are used'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='15the training set to train the brain tumor segmentation models while use the validation\\nset to test the segmentation performance. We adopted the ofï¬cial metrics that are used\\nby the online evaluation system of BraTS for quantitative evaluation. They are the\\nDice score, Sensitivity, Speciï¬city, and the 95thpercentile of the Hausdorff Distance\\n(HD95).\\nBefore training, each of the input modality data was normalized to have zero mean\\nand unit variance. We randomly sampled patches of size 128 Ã—128 Ã—128 within\\nthe brain tumor area as the inputs of both the cross-modality feature transition model\\nand the cross-modality feature fusion model. As a trade-off between performance and\\nmemory consumption, the base number of ï¬lters in the U-Net was designed to be 16,\\nwhich was increased to twice after each down-sampling layer. The Adam optimizer\\nwith an initial learning rate of 10âˆ’4 was applied to optimize the objective function,'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='which was increased to twice after each down-sampling layer. The Adam optimizer\\nwith an initial learning rate of 10âˆ’4 was applied to optimize the objective function,\\nwhere Î»was set to be 10. When training the cross-modality feature fusion network, the\\npre-trained parameters of the transition mappings GA\\nB and GB\\nA were transferred to the\\nSA and SB for further ï¬ne-tuning. The SA and SB took the same input modality data\\nas the GA\\nB and GB\\nA. The parameters of the cross-modality fusion branch are randomly\\ninitialized. We used the Adam optimizer with an initial learning rate of 10âˆ’4 and a\\nbatch size of 1 to train this network branch. All of the network branches were imple-\\nmented in Pytorch on a NVIDIA GTX 1080TI GPU. It totally takes 18 hours and 57\\nminutes to complete the training process and the test speed is 3.2 seconds per subject.\\n4.2. Experiments on the BraTS 2017 Benchmark\\nIn this subsection, we evaluate the proposed approach on the BraTS 2017 bench-'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='4.2. Experiments on the BraTS 2017 Benchmark\\nIn this subsection, we evaluate the proposed approach on the BraTS 2017 bench-\\nmark. We ï¬rst analyze the effect of the main network branches of the proposed learn-\\ning model by conducting the experiments on the following baseline models. The ï¬rst\\ntwo baseline models train the single-modality-pair feature learning branches SA and\\nSB with the input modality data {T1,T1c}and {T2,FLAIR}, respectively. The third\\nbaseline model â€œ SA + SBâ€ fuses the prediction of SA and SB by directly comput-\\ning the average of the obtained segmentation maps. Then, we compare our approach\\nwith the baseline models â€œOurs w/o MGâ€ and â€œOurs w CAâ€ which adopt the proposed\\ncross-modality feature fusion branch but without using the mask-guided attention block\\n16Table 2\\nComparison results of the proposed approach and the other baseline models on the BraTS 2017 validation set. Higher Dice and Sensitivity scores indicate the better results,'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='16Table 2\\nComparison results of the proposed approach and the other baseline models on the BraTS 2017 validation set. Higher Dice and Sensitivity scores indicate the better results,\\nwhile lower Hausdorff95 scores indicate the better results.\\nMethod\\nDice Sensitivity Hausdorff95\\nET WT TC Average ET WT TC Average ET WT TC Average\\nEvaluation on the key network branches with pre-train parameters from cross-modality feature transition:\\nSA 0.752 0.799 0.787 0.779 0.760 0.787 0.770 0.772 3.735 11.640 8.307 7.894\\nSB 0.429 0.886 0.656 0.657 0.471 0.875 0.643 0.663 13.373 6.072 10.781 10.075\\nSA + SB 0.672 0.864 0.759 0.765 0.715 0.834 0.709 0.753 7.944 7.032 7.824 7.600\\nOurs w/o MG 0.762 0.898 0.808 0.823 0.781 0.890 0.809 0.827 3.144 5.531 7.388 5.354\\nOurs w CA 0.765 0.896 0.799 0.820 0.776 0.884 0.753 0.804 3.402 4.981 8.066 5.483\\nEvaluation on the cross-modality feature transition strategy based on the proposed cross-modality feature learning network:'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='Evaluation on the cross-modality feature transition strategy based on the proposed cross-modality feature learning network:\\nOurs random 0.725 0.870 0.754 0.783 0.766 0.867 0.774 0.802 5.826 6.664 8.748 7.079\\nOurs voc 0.725 0.879 0.778 0.794 0.768 0.899 0.766 0.811 5.202 6.639 8.642 6.828\\nOurs self 0.751 0.898 0.779 0.809 0.785 0.884 0.768 0.812 3.161 4.775 7.238 5.058\\nOurs 0.757 0.900 0.828 0.828 0.756 0.904 0.792 0.817 3.170 5.155 6.999 5.108\\n17or directly using the conventional attention block [21]. All the aforementioned base-\\nline models are ï¬ne-tuned based on the network parameters obtained from the cross-\\nmodality feature transition process. The experimental results are reported in top rows\\nof Table 2.\\nBy comparing SA, SB and our approach, we can observe that simply using a single-\\nmodality-pair feature learning branch only obtains poor performance due to the inade-\\nquate modality information. The performance of SA + SB is better than SB but worse'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='modality-pair feature learning branch only obtains poor performance due to the inade-\\nquate modality information. The performance of SA + SB is better than SB but worse\\nthan SA, which might be caused by the large performance gap between SB and SA.\\nBy comparing â€œOurs w/o MGâ€, SA + SB, and â€œOursâ€ we can observe that using the\\nproposed feature fusion branch can signiï¬cantly improve the feature learning capacity\\nof our approach and using the mask-guided attention block can further improve the\\nsegmentation accuracy. Notice that when using the conventional attention block, the\\nnetwork works better for the ET area but worse for the TC and WT areas, making the\\naverage performance of â€œOurs w CAâ€ less effective than â€œOurs w/o MGâ€ and â€œOursâ€.\\nIn addition, we also conducted the ablation study by implementing three baseline\\nmodels which directly train the CMFF network to obtain the segmentation results with-\\nout the CMFT process. The ï¬rst baseline â€œOurs randomâ€ used the random values'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='models which directly train the CMFF network to obtain the segmentation results with-\\nout the CMFT process. The ï¬rst baseline â€œOurs randomâ€ used the random values\\nto initialize the CMFF network, while the second baseline â€œOurs vocâ€ used the pa-\\nrameters pre-trained on the PASCAL VOC segmentation dataset [33]5 to initialize the\\nCMFF network. To facilitate the parameter transferring between the 2D image data\\nand 3D volume data, we ï¬rst trained a 2D-Unet on the PASCAL VOC segmentation\\ndataset and then extended its convolution kernels to 3D convolution kernels as in [34].\\nFor the third baseline â€œOurs selfâ€, we replaced the proposed CMFT process by a self\\nreconstruction-based feature learning process that learns patterns by reconstructing the\\ninput data.\\nThe experimental results are reported in bottom rows of Table 2. From the com-\\nparison results, we can observe that 1) due to the inadequate of medical imaging data,'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='input data.\\nThe experimental results are reported in bottom rows of Table 2. From the com-\\nparison results, we can observe that 1) due to the inadequate of medical imaging data,\\ndirectly training the DCNN models with random parameter initialization is not able\\n5PASCAL VOC segmentation dataset is a large-scale image set that consists of RGB images and the\\ncorresponding segmentation annotation.\\n18Table 3\\nComparison results of the proposed approach and the other state-of-the-art models on the BraTS 2017 validation set. Higher Dice scores indicate the better results, while\\nlower Hausdorff95 scores indicate the better results.\\nApproach Method\\nDice Hausdorff95\\nET WT TC Average ET WT TC Average\\nEnsemble\\nKamnitsas et al. [22] 0.738 0.901 0.797 0.812 4.500 4.230 6.560 5.081\\nWang et al. [23] 0.786 0.905 0.838 0.843 3.282 3.890 6.479 5.097\\nIsensee et al. [24] 0.732 0.896 0.797 0.808 4.550 6.970 9.480 7.000\\nJungo et al. [25] 0.749 0.901 0.790 0.813 5.379 5.409 7.487 6.092'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='Wang et al. [23] 0.786 0.905 0.838 0.843 3.282 3.890 6.479 5.097\\nIsensee et al. [24] 0.732 0.896 0.797 0.808 4.550 6.970 9.480 7.000\\nJungo et al. [25] 0.749 0.901 0.790 0.813 5.379 5.409 7.487 6.092\\nHu et al. [26] 0.650 0.850 0.700 0.733 17.980 25.240 21.450 21.557\\nCasamitjana et al. [27] 0.714 0.877 0.637 0.743 5.434 8.343 11.173 8.317\\nSingle prediction\\nIslam et al. [28] 0.689 0.876 0.761 0.775 12.938 9.820 12.361 11.706\\nJesson et al. [29] 0.713 0.899 0.751 0.788 6.980 4.160 8.650 6.597\\nRoy et al. [30] 0.716 0.892 0.793 0.800 6.612 6.735 9.806 7.718\\nPereira et al. [31] 0.733 0.895 0.798 0.809 5.074 5.920 8.947 6.647\\nCastillo et al. [32] 0.710 0.880 0.680 0.757 6.120 9.630 11.380 9.043\\nOurs 0.762 0.898 0.823 0.828 3.170 5.155 6.999 5.108\\n19to achieve satisfying learning performance; 2) while using the large-scale RGB image\\ndata (together with the segmentation annotation) still cannot solve this problem be-'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='19to achieve satisfying learning performance; 2) while using the large-scale RGB image\\ndata (together with the segmentation annotation) still cannot solve this problem be-\\ncause of the large domain gap; and 3) the proposed cross-modality feature transition\\nprocess can learn informative features from the medical imaging data without using any\\nhuman annotation, which also works better than the self reconstruction-based learning\\nstrategy.\\nNext, we compare the proposed approach with several state-of-the-art methods,\\nwhich include six ensemble methods and ï¬ve single prediction methods.The ensemble\\nmethods integrate multiple deep brain tumor segmentation models that are trained from\\ndifferent views or different training sub-sets to obtain the predicted segmentation masks\\nfor each test data, while the single prediction methods only apply one deep model to\\nfulï¬ll the brain tumor segmentation task. Thus, the ensemble methods can usually ob-'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='for each test data, while the single prediction methods only apply one deep model to\\nfulï¬ll the brain tumor segmentation task. Thus, the ensemble methods can usually ob-\\ntain better performance but with higher complexity both in computational cost and time\\nconsumption. The quantitative results are reported in Table 3. From Table 3, we can\\nobserve that as a single prediction method6, our proposed approach outperforms all the\\nstate-of-the-art single prediction methods both in terms of Dice score and Hausdorff95.\\nMore encouragingly, our approach can also obtain better performance than most (nine\\nout of ten) ensemble methods. Thus, the comparison results in Table 3 demonstrate the\\neffectiveness of the proposed approach.\\n4.3. Experiments on the BraTS 2018 Benchmark\\nOn the larger-scale BraTS 2018 benchmark, we ï¬rst compare the proposed ap-\\nproach with ï¬ve baseline models, including â€œSAâ€, â€œSBâ€, â€œSA + SBâ€, â€œOurs w/o MGâ€,'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='On the larger-scale BraTS 2018 benchmark, we ï¬rst compare the proposed ap-\\nproach with ï¬ve baseline models, including â€œSAâ€, â€œSBâ€, â€œSA + SBâ€, â€œOurs w/o MGâ€,\\nand â€œOurs w CAâ€ to analyze the effect of the main network branches designed in our\\nlearning framework. The experimental results are reported in top rows of Table 4. Be-\\ning consistent with the results on the BraTS 2017 benchmark, there is obvious perfor-\\n6Although our model has a cross-modality feature transition process and a cross-modality feature fusion\\nprocess, the cross-modality feature transition process only learns features and does not predict segmentation\\nresults. In other words, our segmentation results are predicted by the cross-modality feature fusion process\\nonly rather than the combination of the segmentation results obtained by both processes. Thus, our approach\\nis considered as a single prediction method rather than an ensemble method.\\n20Table 4'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='only rather than the combination of the segmentation results obtained by both processes. Thus, our approach\\nis considered as a single prediction method rather than an ensemble method.\\n20Table 4\\nComparison results of the proposed approach and the other baseline models on the BraTS 2018 validation set. Higher Dice and Sensitivity scores indicate the better results,\\nwhile lower Hausdorff95 scores indicate the better results.\\nMethod\\nDice Sensitivity Hausdorff95\\nWT ET TC Average WT ET TC Average WT ET TC Average\\nEvaluation on the key network branches with pre-train parameters from cross-modality feature transition:\\nSA 0.786 0.807 0.812 0.802 0.862 0.816 0.826 0.835 4.350 10.060 9.670 8.027\\nSB 0.444 0.898 0.704 0.682 0.468 0.905 0.708 0.694 11.164 5.212 9.895 8.757\\nSA + SB 0.721 0.873 0.797 0.797 0.770 0.863 0.782 0.805 4.255 6.301 7.323 5.960\\nOurs w/o MG 0.781 0.900 0.822 0.834 0.794 0.916 0.836 0.849 3.948 4.449 7.348 5.248'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='SA + SB 0.721 0.873 0.797 0.797 0.770 0.863 0.782 0.805 4.255 6.301 7.323 5.960\\nOurs w/o MG 0.781 0.900 0.822 0.834 0.794 0.916 0.836 0.849 3.948 4.449 7.348 5.248\\nOurs w CA 0.788 0.901 0.833 0.841 0.836 0.921 0.829 0.862 3.788 5.140 6.265 5.064\\nEvaluation on the cross-modality feature transition strategy based on the proposed cross-modality feature learning network:\\nOurs random 0.755 0.873 0.771 0.800 0.772 0.886 0.811 0.823 5.340 6.084 9.082 6.835\\nOurs voc 0.760 0.896 0.785 0.814 0.744 0.911 0.747 0.800 3.300 4.700 8.427 5.476\\nOurs self 0.767 0.898 0.832 0.833 0.767 0.886 0.821 0.825 3.029 5.272 6.296 4.865\\nOurs 0.791 0.903 0.836 0.843 0.846 0.919 0.835 0.867 3.992 4.998 6.369 5.120\\n21mance gap between â€œSAâ€ and â€œSBâ€ and the straightforward fusion strategy â€œSA +SBâ€\\ncan only obtain performance better than â€œ SBâ€ but worse than â€œ SAâ€. Compared to\\nâ€œSA + SBâ€, our approach obtains 4.6% performance gain (in terms of the Dice score),'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='can only obtain performance better than â€œ SBâ€ but worse than â€œ SAâ€. Compared to\\nâ€œSA + SBâ€, our approach obtains 4.6% performance gain (in terms of the Dice score),\\nwhich demonstrates that the feature fusion branch proposed by our approach plays an\\nimportant role in fusing informative features and predicting accurate tumor areas. No-\\ntice that â€œOurs w CAâ€ obtains better performance than â€œOurs w/o MGâ€ on this dataset.\\nBut its performance is still worse than â€œOursâ€. Some examples of the comparison re-\\nsults on the BraTS 2018 validation set are shown in Fig.7. For better understanding\\nthe segmentation results, we also shown examples of our approach on the BraTS 2018\\ntraining set with the corresponding ground-truth annotations (see Fig.8). Besides, we\\nalso study the failure cases in Fig. 9, from which we can observe that the main chal-\\nlenges to our approach are the LGG cases when the ground-truth tumor areas are with\\nabsent ET area, discontinuous tumor regions, or ragged tumor contours.'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='lenges to our approach are the LGG cases when the ground-truth tumor areas are with\\nabsent ET area, discontinuous tumor regions, or ragged tumor contours.\\nTo evaluate the effectiveness of the proposed CMFT process, we also compare\\nour approach with the â€œOurs randomâ€, â€œOurs vocâ€, and â€œOurs selfâ€ baselines. The\\nexperimental results are reported in bottom rows of Table 4, from which we can observe\\nobvious performance gain when compare our approach to the aforementioned baseline\\nmethods. Some examples of the comparison results are shown in Fig. 7, which can\\nbetter illustrate the advantage of our approach.In addition, to verify the effectiveness of\\nour strategy to build the modality pairs as described in Sec. 3.3, we further implement\\na baseline model which constructs the modality pair A by using the T1 modality and\\nFLAIR modality and modality pair B by using the T2 modality and T1-c modality.\\nBased on our experiment, this baseline obtains 0.822 Dice score, 0.844 sensitivity, and'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='FLAIR modality and modality pair B by using the T2 modality and T1-c modality.\\nBased on our experiment, this baseline obtains 0.822 Dice score, 0.844 sensitivity, and\\n5.789 Hausdorff Distance on the BraTS 2018 dataset. The comparison between this\\nbaseline and the proposed approach demonstrates the effectiveness of our approach in\\nmaking the information contained within each modality pair relatively consistent and\\nthe information contained across the different modality pairs relatively distinct and\\ncomplementary.\\nFinally, we compare the proposed approach with other state-of-the-art methods on\\nthe BraTS 2018 benchmark, which include three ensemble models [35, 36, 37] and\\nthree single prediction models [38, 39, 40]. It is worth mentioning that as different\\n22Figure 7: Examples of the segmentation results of the proposed approach as well as the compared baseline methods on the BraTS 2018 validation set. As the ground-truth'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='22Figure 7: Examples of the segmentation results of the proposed approach as well as the compared baseline methods on the BraTS 2018 validation set. As the ground-truth\\nsegmentation annotation is not acquirable, we annotate the dice score for the segmented tumor regions on each test sample instead of showing the ground-truth segmentation\\nannotation. The WT, TC, and ET areas are masked in green, blue, and purple, respectively.\\n23Figure 8: Comparison of the segmentation results and the ground-truth annotation on the BraTS 2018 train-\\ning set. Notice that the average Dice score on the BraTS 2018 training set is 0.886, which is moderately\\nhigher than the Dice score on the BraTS 2018 validation set. The WT, TC, and ET areas are masked in\\ngreen, blue, and purple, respectively.\\nworks adopt various ways to obtain their ensemble models and the concrete processes\\nfor obtaining the ensemble model are not clear to us, it is hard to implement an ensem-'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='works adopt various ways to obtain their ensemble models and the concrete processes\\nfor obtaining the ensemble model are not clear to us, it is hard to implement an ensem-\\nble model that could compare with the existing ensemble models fairly. However, from\\nthe experimental results reported in Table 5, we can observe that our single-prediction\\nmodel has already achieved better performance when compared to the ensemble mod-\\nels of [37, 41]. When compared to the state-of-the-art single prediction models, our\\napproach also obtains the outperforming performance both in terms of Dice score and\\nHausdorff95. Thus, we believe the above experiments have already demonstrated the\\neffectiveness of the proposed approach.\\n5. Conclusion\\nIn this work, we have proposed a novel cross-modality deep feature learning frame-\\nwork for segmenting brain tumor areas from the multi-modality MR scans. Consider-\\ning that the medical image data for brain tumor segmentation are relatively scarce in'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='work for segmenting brain tumor areas from the multi-modality MR scans. Consider-\\ning that the medical image data for brain tumor segmentation are relatively scarce in\\nterms of the data scale but containing the richer information in terms of the modal-\\nity property, we propose to mine rich patterns across the multi-modality data to make\\nup for the insufï¬ciency in data scale. The proposed learning framework consists of\\na cross-modality feature transition (CMFT) process and a cross-modality feature fu-\\n24Figure 9: Examples of the failure cases on the BraTS 2018 training set, where the WT, TC, and ET areas are masked in green, blue, and purple, respectively. The ï¬rst\\nexample is a failure case in the HGG subjects, which is mainly due to the inaccurate tumor boundaries. The other examples are from the LGG cases, where the absent ET\\narea, discontinuous tumor regions and ragged tumor contours make the model hard to predict.\\n25Table 5'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='area, discontinuous tumor regions and ragged tumor contours make the model hard to predict.\\n25Table 5\\nComparison results of the proposed approach and the other state-of-the-art models on the BraTS 2018 validation set. Higher Dice scores indicate the better results, while\\nlower Hausdorff95 scores indicate the better results.\\nApproach Method\\nDice Hausdorff95\\nET WT TC Average ET WT TC Average\\nEnsemble\\nMyronenko A. [35] 0.823 0.910 0.867 0.866 3.926 4.516 6.855 5.099\\nIsensee et al. [36] 0.809 0.913 0.863 0.861 2.410 4.270 6.520 4.400\\nPuch et al. [37] 0.758 0.895 0.774 0.809 4.502 10.656 7.103 7.420\\nSingle prediction\\nChandra et al. [38] 0.767 0.901 0.813 0.827 7.569 6.680 7.630 7.293\\nMa et al. [39] 0.743 0.872 0.773 0.796 4.690 6.120 10.400 7.070\\nChen et al. [40] 0.733 0.888 0.808 0.810 4.643 5.505 8.140 6.096\\nOurs 0.791 0.903 0.836 0.843 3.992 4.998 6.369 5.120\\n26sion (CMFF) process. By building a generative adversarial network-based learning'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='Ours 0.791 0.903 0.836 0.843 3.992 4.998 6.369 5.120\\n26sion (CMFF) process. By building a generative adversarial network-based learning\\nscheme to implement the cross-modality feature transition process, our approach is\\nable to to learn useful feature representations from the knowledge transition across dif-\\nferent modality data without any human annotation. While the cross-modality feature\\nfusion process transfers the features learned from the feature transition process and is\\nempowered with the novel fusion branch to guide a strong feature fusion process.Com-\\nprehensive experiments are conducted on two BraTS benchmarks, which demonstrate\\nthe effectiveness of our approach when compared to baseline models and state-of-the-\\nart methods. To our knowledge, one limitation of this work is the current learning\\nframework requires that the network architectures of the modal generator and the seg-\\nmentation predictor be almost the same. To address this inconvenience, one potential'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='framework requires that the network architectures of the modal generator and the seg-\\nmentation predictor be almost the same. To address this inconvenience, one potential\\nfuture direction is to introduce the knowledge distillation mechanism [42, 43, 44] to\\nreplace the simple parameter transfer process.\\nAcknowledgment\\nThis work was supported in part by the National Science Foundation of China un-\\nder Grants 61876140 and 61773301, the Fundamental Research Funds for the Central\\nUniversities under Grant JBZ170401, and the China Postdoctoral Support Scheme for\\nInnovative Talents under Grant BX20180236.\\nReferences\\n[1] S. Bakas, H. Akbari, A. Sotiras, M. Bilello, M. Rozycki, J. Kirby, et al., Seg-\\nmentation labels and radiomic features for the pre-operative scans of the tcga-lgg\\ncollection, The Cancer Imaging Archive 286 (2017).\\n[2] Y . Li, L. Shen, Deep learning based multimodal brain tumor diagnosis, in: Inter-\\nnational MICCAI Brainlesion Workshop, Springer, 2017, pp. 149â€“158 (2017).'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='[2] Y . Li, L. Shen, Deep learning based multimodal brain tumor diagnosis, in: Inter-\\nnational MICCAI Brainlesion Workshop, Springer, 2017, pp. 149â€“158 (2017).\\n[3] M. Rezaei, K. Harmuth, W. Gierke, T. Kellermeier, M. Fischer, H. Yang, et al., A\\nconditional adversarial network for semantic segmentation of brain tumor, in: In-\\nternational MICCAI Brainlesion Workshop, Springer, 2017, pp. 241â€“252 (2017).\\n27[4] M. Shaikh, G. Anand, G. Acharya, A. Amrutkar, V . Alex, G. Krishnamurthi,\\nBrain tumor segmentation using dense fully convolutional neural network, in: In-\\nternational MICCAI Brainlesion Workshop, Springer, 2017, pp. 309â€“319 (2017).\\n[5] M. Islam, H. Ren, Fully convolutional network with hypercolumn features for\\nbrain tumor segmentation, in: Proceedings of MICCAI workshop on Multimodal\\nBrain Tumor Segmentation Challenge (BRATS), 2017 (2017).\\n[6] M. M. Lopez, J. Ventura, Dilated convolutions for brain tumor segmentation in'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='Brain Tumor Segmentation Challenge (BRATS), 2017 (2017).\\n[6] M. M. Lopez, J. Ventura, Dilated convolutions for brain tumor segmentation in\\nmri scans, in: International MICCAI Brainlesion Workshop, Springer, 2017, pp.\\n253â€“262 (2017).\\n[7] K. Kamnitsas, C. Ledig, V . F. Newcombe, J. P. Simpson, A. D. Kane, D. K.\\nMenon, et al., Efï¬cient multi-scale 3d cnn with fully connected crf for accurate\\nbrain lesion segmentation, Medical image analysis 36 (2017) 61â€“78 (2017).\\n[8] W. Li, G. Wang, L. Fidon, S. Ourselin, M. J. Cardoso, T. Vercauteren, On the\\ncompactness, efï¬ciency, and representation of 3d convolutional networks: brain\\nparcellation as a pretext task, in: IPMI, Springer, 2017, pp. 348â€“360 (2017).\\n[9] L. S. Castillo, L. A. Daza, L. C. Rivera, P. Arbel Â´aez, V olumetric multimodality\\nneural network for brain tumor segmentation, in: 13th International Conference\\non Medical Information Processing and Analysis, V ol. 10572, International Soci-'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='neural network for brain tumor segmentation, in: 13th International Conference\\non Medical Information Processing and Analysis, V ol. 10572, International Soci-\\nety for Optics and Photonics, 2017, p. 105720E (2017).\\n[10] L. Fidon, W. Li, L. C. Garcia-Peraza-Herrera, J. Ekanayake, N. Kitchen,\\nS. Ourselin, et al., Scalable multimodal convolutional networks for brain tumour\\nsegmentation, in: International Conference on Medical Image Computing and\\nComputer-Assisted Intervention, Springer, 2017, pp. 285â€“293 (2017).\\n[11] S. Bu, L. Wang, P. Han, Z. Liu, K. Li, 3d shape recognition and retrieval based\\non multi-modality deep learning, Neurocomputing 259 (2017) 183â€“193 (2017).\\n[12] J. Yao, X. Zhu, F. Zhu, J. Huang, Deep correlational learning for survival pre-\\ndiction from multi-modality data, in: International Conference on Medical Image\\n28Computing and Computer-Assisted Intervention, Springer, 2017, pp. 406â€“414\\n(2017).'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='diction from multi-modality data, in: International Conference on Medical Image\\n28Computing and Computer-Assisted Intervention, Springer, 2017, pp. 406â€“414\\n(2017).\\n[13] X. Xu, Y . Li, G. Wu, J. Luo, Multi-modal deep feature learning for rgb-d object\\ndetection, Pattern Recognition 72 (2017) 300â€“313 (2017).\\n[14] X. Liu, X. Ma, J. Wang, H. Wang, M3l: Multi-modality mining for metric learn-\\ning in person re-identiï¬cation, Pattern Recognition 76 (2018) 650â€“661 (2018).\\n[15] A. Wang, J. Lu, J. Cai, T.-J. Cham, G. Wang, Large-margin multi-modal deep\\nlearning for rgb-d object recognition, IEEE Transactions on Multimedia 17 (11)\\n(2015) 1887â€“1898 (2015).\\n[16] J.-Y . Zhu, T. Park, P. Isola, A. A. Efros, Unpaired image-to-image translation\\nusing cycle-consistent adversarial networks, in: Proceedings of the IEEE interna-\\ntional conference on computer vision, 2017, pp. 2223â€“2232 (2017).\\n[17] C. Wang, C. Xu, C. Wang, D. Tao, Perceptual adversarial networks for image-'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='tional conference on computer vision, 2017, pp. 2223â€“2232 (2017).\\n[17] C. Wang, C. Xu, C. Wang, D. Tao, Perceptual adversarial networks for image-\\nto-image transformation, IEEE Transactions on Image Processing 27 (8) (2018)\\n4066â€“4079 (2018).\\n[18] F. Milletari, N. Navab, S.-A. Ahmadi, V-net: Fully convolutional neural networks\\nfor volumetric medical image segmentation, in: 2016 Fourth International Con-\\nference on 3D Vision (3DV), IEEE, 2016, pp. 565â€“571 (2016).\\n[19] N. Liu, J. Han, M.-H. Yang, Picanet: Learning pixel-wise contextual attention for\\nsaliency detection, in: Proceedings of the IEEE Conference on Computer Vision\\nand Pattern Recognition, 2018, pp. 3089â€“3098 (2018).\\n[20] S. Ye, J. Han, N. Liu, Attentive linear transformation for image captioning, IEEE\\nTransactions on Image Processing 27 (11) (2018) 5514â€“5524 (Nov 2018).\\n[21] S. Woo, J. Park, J.-Y . Lee, I. So Kweon, Cbam: Convolutional block atten-\\ntion module, in: The European Conference on Computer Vision (ECCV), 2018'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='[21] S. Woo, J. Park, J.-Y . Lee, I. So Kweon, Cbam: Convolutional block atten-\\ntion module, in: The European Conference on Computer Vision (ECCV), 2018\\n(September 2018).\\n29[22] K. Kamnitsas, W. Bai, E. Ferrante, S. McDonagh, M. Sinclair, N. Pawlowski,\\net al., Ensembles of multiple models and architectures for robust brain tumour\\nsegmentation, in: International MICCAI Brainlesion Workshop, Springer, 2017,\\npp. 450â€“462 (2017).\\n[23] G. Wang, W. Li, S. Ourselin, T. Vercauteren, Automatic brain tumor segmenta-\\ntion using cascaded anisotropic convolutional neural networks, in: International\\nMICCAI Brainlesion Workshop, Springer, 2017, pp. 178â€“190 (2017).\\n[24] F. Isensee, P. Kickingereder, W. Wick, M. Bendszus, K. H. Maier-Hein, Brain tu-\\nmor segmentation and radiomics survival prediction: Contribution to the brats\\n2017 challenge, in: International MICCAI Brainlesion Workshop, Springer,\\n2017, pp. 287â€“297 (2017).'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='mor segmentation and radiomics survival prediction: Contribution to the brats\\n2017 challenge, in: International MICCAI Brainlesion Workshop, Springer,\\n2017, pp. 287â€“297 (2017).\\n[25] A. Jungo, R. McKinley, R. Meier, U. Knecht, L. Vera, J. P Â´erez-Beteta, et al.,\\nTowards uncertainty-assisted brain tumor segmentation and survival prediction,\\nin: International MICCAI Brainlesion Workshop, Springer, 2017, pp. 474â€“485\\n(2017).\\n[26] Y . Hu, Y . Xia, 3d deep neural network-based brain tumor segmentation using mul-\\ntimodality magnetic resonance sequences, in: International MICCAI Brainlesion\\nWorkshop, Springer, 2017, pp. 423â€“434 (2017).\\n[27] A. Casamitjana, M. Cat `a, I. SÂ´anchez, M. Combalia, V . Vilaplana, Cascaded v-net\\nusing roi masks for brain tumor segmentation, in: International MICCAI Brain-\\nlesion Workshop, Springer, 2017, pp. 381â€“391 (2017).\\n[28] M. Islam, H. Ren, Multi-modal pixelnet for brain tumor segmentation, in: Inter-'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='lesion Workshop, Springer, 2017, pp. 381â€“391 (2017).\\n[28] M. Islam, H. Ren, Multi-modal pixelnet for brain tumor segmentation, in: Inter-\\nnational MICCAI Brainlesion Workshop, Springer, 2017, pp. 298â€“308 (2017).\\n[29] A. Jesson, T. Arbel, Brain tumor segmentation using a 3d fcn with multi-scale\\nloss, in: International MICCAI Brainlesion Workshop, Springer, 2017, pp. 392â€“\\n402 (2017).\\n30[30] A. G. Roy, N. Navab, C. Wachinger, Recalibrating fully convolutional networks\\nwith spatial and channel squeeze and excitation blocks, IEEE transactions on\\nmedical imaging 38 (2) (2018) 540â€“549 (2018).\\n[31] S. Pereira, A. Pinto, J. Amorim, A. Ribeiro, V . Alves, C. A. Silva, Adaptive fea-\\nture recombination and recalibration for semantic segmentation with fully convo-\\nlutional networks, IEEE transactions on medical imaging (2019).\\n[32] L. S. Castillo, L. A. Daza, L. C. Rivera, P. Arbel Â´aez, Brain tumor segmenta-\\ntion and parsing on mris using multiresolution neural networks, in: International'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='[32] L. S. Castillo, L. A. Daza, L. C. Rivera, P. Arbel Â´aez, Brain tumor segmenta-\\ntion and parsing on mris using multiresolution neural networks, in: International\\nMICCAI Brainlesion Workshop, Springer, 2017, pp. 332â€“343 (2017).\\n[33] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, A. Zisserman, The pascal\\nvisual object classes (voc) challenge, International journal of computer vision\\n88 (2) (2010) 303â€“338 (2010).\\n[34] J. Carreira, A. Zisserman, Quo vadis, action recognition? a new model and the\\nkinetics dataset (2017).\\n[35] A. Myronenko, 3d mri brain tumor segmentation using autoencoder regulariza-\\ntion, in: International MICCAI Brainlesion Workshop, Springer, 2018, pp. 311â€“\\n320 (2018).\\n[36] F. Isensee, P. Kickingereder, W. Wick, M. Bendszus, K. H. Maier-Hein, No new-\\nnet, in: International MICCAI Brainlesion Workshop, 2018 (2018).\\n[37] S. Puch, I. S Â´anchez, A. HernÂ´andez, G. Piella, V . Prckovska, Global planar convo-'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='net, in: International MICCAI Brainlesion Workshop, 2018 (2018).\\n[37] S. Puch, I. S Â´anchez, A. HernÂ´andez, G. Piella, V . Prckovska, Global planar convo-\\nlutions for improved context aggregation in brain tumor segmentation, in: Inter-\\nnational MICCAI Brainlesion Workshop, Springer, 2018, pp. 393â€“405 (2018).\\n[38] S. Chandra, M. Vakalopoulou, L. Fidon, E. Battistella, T. Estienne, R. Sun, et al.,\\nContext aware 3d cnns for brain tumor segmentation, in: International MICCAI\\nBrainlesion Workshop, Springer, 2018, pp. 299â€“310 (2018).\\n[39] J. Ma, X. Yang, Automatic brain tumor segmentation by exploring the multi-\\nmodality complementary information and cascaded 3d lightweight cnns, in: In-\\nternational MICCAI Brainlesion Workshop, Springer, 2018, pp. 25â€“36 (2018).\\n31[40] W. Chen, B. Liu, S. Peng, J. Sun, X. Qiao, S3d-unet: Separable 3d u-net for brain\\ntumor segmentation, in: International MICCAI Brainlesion Workshop, Springer,\\n2018, pp. 358â€“368 (2018).'),\n",
       " Document(metadata={'arxiv_id': '2201.02356v1', 'title': 'Cross-Modality Deep Feature Learning for Brain Tumor Segmentation', 'section': 'body', 'authors': 'Dingwen Zhang, Guohai Huang, Qiang Zhang'}, page_content='31[40] W. Chen, B. Liu, S. Peng, J. Sun, X. Qiao, S3d-unet: Separable 3d u-net for brain\\ntumor segmentation, in: International MICCAI Brainlesion Workshop, Springer,\\n2018, pp. 358â€“368 (2018).\\n[41] R. Hua, Q. Huo, Y . Gao, Y . Sun, F. Shi, Multimodal brain tumor segmentation us-\\ning cascaded v-nets, in: International MICCAI Brainlesion Workshop, Springer,\\n2018, pp. 49â€“60 (2018).\\n[42] J. H. Cho, B. Hariharan, On the efï¬cacy of knowledge distillation, in: Proceed-\\nings of the IEEE International Conference on Computer Vision, 2019, pp. 4794â€“\\n4802 (2019).\\n[43] W. Hao, Z. Zhang, Spatiotemporal distilled dense-connectivity network for video\\naction recognition, Pattern Recognition 92 (2019) 13â€“24 (2019).\\n[44] T.-B. Xu, P. Yang, X.-Y . Zhang, C.-L. Liu, Lightweightnet: Toward fast and\\nlightweight convolutional neural networks via architecture distillation, Pattern\\nRecognition 88 (2019) 272â€“284 (2019).\\n32'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'title_abstract', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='Title: Brain MRI Tumor Segmentation with Adversarial Networks\\n\\nAbstract: Deep Learning is a promising approach to either automate or simplify several tasks in the healthcare domain. In this work, we introduce SegAN-CAT, an approach to brain tumor segmentation in Magnetic Resonance Images (MRI), based on Adversarial Networks. In particular, we extend SegAN, successfully applied to the same task in a previous work, in two respects: (i) we used a different model input and (ii) we employed a modified loss function to train the model. We tested our approach on two large datasets, made available by the Brain Tumor Image Segmentation Benchmark (BraTS). First, we trained and tested some segmentation models assuming the availability of all the major MRI contrast modalities, i.e., T1-weighted, T1 weighted contrast-enhanced, T2-weighted, and T2-FLAIR. However, as these four modalities are not always all available for each patient, we also trained and tested four segmentation models that take as input MRIs acquired only with a single contrast modality. Finally, we proposed to apply transfer learning across different contrast modalities to improve the performance of these single-modality models. Our results are promising and show that not SegAN-CAT is able to outperform SegAN when all the four modalities are available, but also that transfer learning can actually lead to better performances when only a single modality is available.'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='Brain MRI Tumor Segmentation with Adversarial\\nNetworks\\nEdoardo Giacomello\\nDipartimento di Elettronica,\\nInformazione e Bioinformatica\\nPolitecnico di Milano\\nedoardo.giacomello@polimi.it\\nDaniele Loiacono\\nDipartimento di Elettronica,\\nInformazione e Bioinformatica\\nPolitecnico di Milano\\ndaniele.loiacono@polimi.it\\nLuca Mainardi\\nDipartimento di Elettronica,\\nInformazione e Bioinformatica\\nPolitecnico di Milano\\nluca.mainardi@polimi.it\\nAbstractâ€”Deep Learning is a promising approach to either\\nautomate or simplify several tasks in the healthcare domain.\\nIn this work, we introduce SegAN-CAT, an approach to brain\\ntumor segmentation in Magnetic Resonance Images (MRI),\\nbased on Adversarial Networks. In particular, we extend SegAN,\\nsuccessfully applied to the same task in a previous work, in\\ntwo respects: (i) we used a different model input and (ii) we\\nemployed a modiï¬ed loss function to train the model. We tested\\nour approach on two large datasets, made available by the Brain'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='two respects: (i) we used a different model input and (ii) we\\nemployed a modiï¬ed loss function to train the model. We tested\\nour approach on two large datasets, made available by the Brain\\nTumor Image Segmentation Benchmark(BraTS). First, we trained\\nand tested some segmentation models assuming the availability\\nof all the major MRI contrast modalities, i.e., T1-weighted,\\nT1 weighted contrast enhanced, T2-weighted, and T2-FLAIR.\\nHowever, as these four modalities are not always all available\\nfor each patient, we also trained and tested four segmentation\\nmodels that take as input MRIs acquired with a single contrast\\nmodality. Finally, we proposed to apply transfer learning across\\ndifferent contrast modalities to improve the performance of these\\nsingle-modality models. Our results are promising and show that\\nnot only SegAN-CAT is able to outperform SegAN when all the\\nfour modalities are available, but also that transfer learning can'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='single-modality models. Our results are promising and show that\\nnot only SegAN-CAT is able to outperform SegAN when all the\\nfour modalities are available, but also that transfer learning can\\nactually lead to better performances when only a single modality\\nis available.\\nI. I NTRODUCTION\\nIn the last decade, machine learning have been applied\\nto a large number of tasks in the healthcare domain and\\nproved to be a promising approach to either automate or\\nsimplify clinical processes that would be otherwise performed\\nmanually, requiring a great amount of time and increasing the\\npossibility of human error. Unfortunately, these applications\\nusually rely on identifying and extracting a set of effective\\nfeatures from data, a very time consuming and problem\\ndependent process. In contrast, deep learning promises to solve\\nthis issue due to the capability of working directly with raw\\ndata by learning automatically an efï¬cient data representation'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='dependent process. In contrast, deep learning promises to solve\\nthis issue due to the capability of working directly with raw\\ndata by learning automatically an efï¬cient data representation\\nto solve the problems they are applied to [1]. In addition, the\\ndata representation learned is often suitable also for different\\nproblems, in some cases even across different application\\ndomains. On the other hand, these advantages come with a\\ncost: deep learning algorithms generally require a huge amount\\nof data to learn effective data representations and, hence,\\ncompetent models to solve the problems they are applied to.\\nFor this reason, several works in the literature focus on data\\naugmentation â€“ i.e., how to exploit as much as possible the\\ndata available â€“ and transfer learning techniques â€“ i.e., how to\\ntrain models with data from similar problems or domains.\\nIn this work, we focus on the MRI brain tumor segmentation\\ntask, a problem that has been widely investigated in the'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='train models with data from similar problems or domains.\\nIn this work, we focus on the MRI brain tumor segmentation\\ntask, a problem that has been widely investigated in the\\npast few years and is the object of an international scientiï¬c\\ncompetition, the Brain Tumor Image Segmentation Benchmark\\n(BraTS). In particular, we introduce SegAN-CAT, a deep learn-\\ning architecture based on generative adversarial networks [2].\\nWe designed our architecture afterSegAN [3], a top performing\\napproach in the BraTS challenge, extending it in two respects:\\n(i) we re-designed the input of one of the networks of the\\nadversarial architecture and (ii) we introduced an additional\\nterm in the loss function.\\nOur goal in this paper is threefold. First, we aim at compar-\\ning the performance of SegAN and SegAN-CAT on brain tumor\\nsegmentation using as input the four MRI modalities most\\ncommonly acquired: T1-weighted (T1), T1-weighted contrast-\\nenhanced (T1c), T2-weighted (T2), and T2-FLAIR (FLAIR).'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='segmentation using as input the four MRI modalities most\\ncommonly acquired: T1-weighted (T1), T1-weighted contrast-\\nenhanced (T1c), T2-weighted (T2), and T2-FLAIR (FLAIR).\\nSecond, as in real-world scenarios these modalities might not\\nbe all always available for each patient, we aim also at training\\nand comparing the performance of four SegAN-CAT models\\nthat use as input only a single MRI modality, i.e., one among\\nT1, T1c, T2, and FLAIR. Third, our aim is to investigate\\nwhether a transfer learning approach can be used to exploit\\nthe knowledge extracted from a model, previously trained on\\na source MRI modality, to train a model that works with a\\ndifferent target MRI modality.\\nWe tested our approach by training several brain tumor\\nsegmentation models on the BraTS 2015 and BraTS 2019\\ndatasets provided for the BraTS challenge. Our results show\\nthat SegAN-CAT is able to outperform SegAN, especially\\nthanks to the modiï¬ed loss function employed. As expected'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='datasets provided for the BraTS challenge. Our results show\\nthat SegAN-CAT is able to outperform SegAN, especially\\nthanks to the modiï¬ed loss function employed. As expected\\nthe results also show that using all the MRI modalities together\\nallow to achieve a much better performance than the one\\nachieved by the models that use a single MRI modality as\\ninput. In addition, we show that the performance of single-\\nmodality models might be slightly improved by employing a\\ntransfer learning approach, that is by transferring the knowl-\\nedge extracted from other modalities.\\nThe remainder of this paper is organized as follows. In\\nSection II we provide an overview of some of the most relevant\\nprevious works. Then, in Section III we describe our approach\\n978-1-7281-1884-0/19/$31.00 câƒ2019 IEEE\\narXiv:1910.02717v2  [eess.IV]  30 Jan 2020and in Section IV we describe our experimental design. In\\nSection V we report and discuss our experimental results.\\nFinally, we draw some conclusions in Section VI.'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='Section V we report and discuss our experimental results.\\nFinally, we draw some conclusions in Section VI.\\nII. R ELATED WORK\\nIn this section we provide an overview of some of the\\nprevious works in the literature that applied adversarial net-\\nworks to the image segmentation problem as well as the\\nprevious works that combined transfer learning with deep\\nneural networks. In addition, we also provide a brief overview\\nof major approaches introduced in the literature to deal with\\nmissing MRI modalities.\\nA. Adversarial Models for Segmentation\\nInspired by the recent success of Generative Adversarial\\nNetworks [2] (GANs), Luc et al. [4] proposed a method in\\nwhich a segmentation network is trained to perform pixel-wise\\nclassiï¬cations on images, while an adversarial network (called\\ndiscriminator or critic) is trained to discriminate segmenta-\\ntions coming from the segmentation network and the ground\\ntruth. This approach has been tested on the PASCAL VOC'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='discriminator or critic) is trained to discriminate segmenta-\\ntions coming from the segmentation network and the ground\\ntruth. This approach has been tested on the PASCAL VOC\\n2012 [5] and on the Stanford Background [6] datasets and the\\nresults show an improvement in segmentation performances\\nwhen an adversarial loss is used. In the medical imaging do-\\nmain, multiple works applied adversarial methods to segment\\nMRI, CT, PET and other domain speciï¬c image formats. In\\nparticular, two previous works applied adversarial learning to\\nbrain MRI. Moeskops et al. [7] investigated the effectiveness\\nof adversarial training and dilated convolution. Xue et. al. [3]\\nproposed an Adversarial Network with a Multi-Scale loss,\\ncalled SegAN, achieving better performances compared to the\\nstate-of-the-art methods for brain tumor segmentation [8], [9].\\nB. Transfer Learning\\nTransfer learning investigates how to exploit a source model\\ntrained on a source domain for a source task on either'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='B. Transfer Learning\\nTransfer learning investigates how to exploit a source model\\ntrained on a source domain for a source task on either\\na different target domain or a different target task . This\\nusually involves either a complete or a partial retraining of\\nthe source model to adapt it to the target domain/task . In\\nparticular, transferring deep neural networks across different\\nimage analysis tasks has been proved successful [10] and it\\nseems a promising approach to deal with machine learning\\napplications for which the amount of data is limited.\\nAn example of the ability of CNNs to learn features that are\\nuseful for different tasks is given in [11], where the authors\\nfollow a multi-task learning approach to demonstrate that\\nit is possible to develop a single model to perform brain,\\nbreast and cardiac segmentation jointly. When working with\\ndifferent MRI modalities, transfer learning has been used\\nin [12] for accelerating MRI acquisition times by applying'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='breast and cardiac segmentation jointly. When working with\\ndifferent MRI modalities, transfer learning has been used\\nin [12] for accelerating MRI acquisition times by applying\\nMRI reconstruction. In [13], the authors uses transfer learning\\nbetween dataset from different institutions for the prostate\\ngland segmentation task, comparing performances on different\\ndataset sizes. A subclass of transfer learning, called domain\\nadaptation studies the setting in which source and target data\\nhave the same feature space but different marginal probability\\nfunctions, while the task is the same for both the domains.\\nFor example, in [14] domain adaptation is applied from CT to\\nMRI for the lung cancer segmentation task. In [15] the authors\\nconsidered the issue of the different protocols adopted to\\nacquire MRIs in the clinical practice: they used a set of MRIs\\nas Source Domain and their follow-ups as Target Domain,\\nstudying the minimal amount of Target Domain data that is'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='acquire MRIs in the clinical practice: they used a set of MRIs\\nas Source Domain and their follow-ups as Target Domain,\\nstudying the minimal amount of Target Domain data that is\\nnecessary to achieve acceptable performance when ï¬ne-tuning\\na segmentation model.\\nHowever, transfer learning applied to Adversarial Networks\\nis still an ongoing ï¬eld of research. In [16], the authors\\nfocused on transfer learning using WGAN-GP [17] models\\nand investigated how the selection of the network weights to\\ntransfer, the size of the target dataset, and the relation between\\nsource and target domain affect the ï¬nal performances. Finally,\\nin a recent work [18], Fregier and Gouray propose a new\\nmethod to perform transfer learning with WGANs [19].\\nC. Missing MRI Modalities\\nA commonly used approach to address the problem of MRI\\nsegmentation in the case of missing modalities is to use image\\nsynthesis techniques to generate artiï¬cial data. In [20], Dar et'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='A commonly used approach to address the problem of MRI\\nsegmentation in the case of missing modalities is to use image\\nsynthesis techniques to generate artiï¬cial data. In [20], Dar et\\nal. used Cycle GAN [21] to generate missing modalities in a\\nuni-modal setting, while Sharma and Hamarneh [22] designed\\na multi-modal architecture where the missing modalities are\\nconsidered as zero-valued inputs. In [23], 3D FLAIR images\\nare generated from T1 and later used to train a classiï¬er\\ntogether with T1 images, which led to a performance improve-\\nment. Similarly, in [24] the authors addressed the problem\\nof missing FLAIR sequences in white matter hyper-intensity\\nsegmentation task by generating the FLAIR images from T1\\nimages while performing the segmentation at the same time.\\nFinally, Ge et al. [25], addressed the problem of missing\\nmodalities using pairs of GANs: given two modalities for\\nwhich there are missing samples (eg. T1 and T2), a pair of'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='Finally, Ge et al. [25], addressed the problem of missing\\nmodalities using pairs of GANs: given two modalities for\\nwhich there are missing samples (eg. T1 and T2), a pair of\\ngenerators are trained to produce one modality from the other\\n(eg. Ga: T1 â†’T2, Gb: T2 â†’T1) and the corresponding dis-\\ncriminators use the input of the other generator as ground truth\\n(e.g., Da(T2,Gb(T1)) and Db(T1,Ga(T2)) respectively).\\nAnother common approach to deal with missing modalities\\nconsists of training a model that is invariant to the input\\ncontrast modalities. Working on this idea, Havaci et al. [26]\\ncomputed a latent vector for each input modality and then\\ncombined all the latent vectors into a single representation that\\naccounts for every modality in input. This method was later\\nextended in [27] to allow unlabeled inputs, i.e., to no longer\\nspecify which modalities are provided as input. Finally, in [28]\\nthe authors exploited Variational AutoEncoders (V AE) [29], to'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='extended in [27] to allow unlabeled inputs, i.e., to no longer\\nspecify which modalities are provided as input. Finally, in [28]\\nthe authors exploited Variational AutoEncoders (V AE) [29], to\\ntrain an encoder for each modality and, at the same time, to\\ngenerate the missing modalities.\\nIII. S EGMENTATION WITH ADVERSARIAL NETWORKS\\nAdversarial Networks became popular as generative models\\n(Generative Adversarial Networks), in which two networks,\\ncalled generator and discriminator are alternately trained to\\nsolve a minimax game. In the basic formulation of GANs,the discriminator network is trained to assign higher scores to\\nsamples that comes from the ground-truth and lower scores\\nto sample that are generated by the generator network. The\\ngenerator is trained to produce samples that can be wrongly\\nclassiï¬ed from the discriminator. As a result, the generator\\nlearns to produce samples that follow the distribution of data\\nof the training set from input noise vectors, that are drawn'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='classiï¬ed from the discriminator. As a result, the generator\\nlearns to produce samples that follow the distribution of data\\nof the training set from input noise vectors, that are drawn\\nfrom a so-called latent space.\\nThis adversarial mechanism can be used also for solving\\nsegmentation tasks: the generator network â€“ dubbed segmen-\\ntator â€“ is trained to produce a segmentation map of an\\ninput image; the discriminator takes as input both an image\\nalong with its segmentation map and is trained to distinguish\\ngenerated segmentations from the ground truth ones.\\nOur approach is based on SegAN [3], an adversarial network\\narchitecture, that was extended in two respects: (i) a dice\\nloss [30] term has been added to the multi-scale loss function\\nemployed in [3]; (ii) we provide the discriminator with the\\ninput image concatenated to its segmentation map, while in\\nSegAN the discriminator is provided with the input image\\nmasked using its segmentation map.'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='input image concatenated to its segmentation map, while in\\nSegAN the discriminator is provided with the input image\\nmasked using its segmentation map.\\nIn the remainder of this section we discuss more in details\\nour proposed architecture (dubbed from now on SegAN-CAT),\\nthe loss function used to train it, and the differences with\\nrespect to the original SegAN.\\nA. Network Architecture\\nThe SegAN-CAT, shown in Figure 1, involves a segmenta-\\ntion network and a discriminator network . The segmentation\\nnetwork takes in input an MRI slice and gives in output\\nthe segmentation of the slice as a probability label map;\\nthe slices have size 160x160x M, where M is the number of\\nMRI modalities used to train the model (in this work M is\\neither 1 or 4); the probability label maps have size 160x160,\\nrepresenting the probability for each pixel of the input slice of\\nbeing part of the area of interest 1. Instead, the discriminator\\nnetwork takes in input an MRI slice and its probability label'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='representing the probability for each pixel of the input slice of\\nbeing part of the area of interest 1. Instead, the discriminator\\nnetwork takes in input an MRI slice and its probability label\\nmap, that can be either the one computed by the segmentation\\nnetwork or the ground-truth one: the slice and probability label\\nmap are either combined or concatenated together as described\\nlater in this Section; thus, the network computes a feature\\nvector that represents a multi-scale representation of the input,\\nwhich is used to compute the loss function during the training\\nof the two networks. Figure 1 shows the structure of the\\ntwo networks, based on the following types of computational\\nblocks: (i) S in, k , that is a segmentation input block composed\\nof a 2D Convolution layer with k ï¬lters of size 4 and\\nstride=2 [31], [32], followed by a LeakyReLU [33] activation\\nlayer; (ii) S enc, k , that is a segmentation encoder block has\\nthe same structure as the segmentation input block but has'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='stride=2 [31], [32], followed by a LeakyReLU [33] activation\\nlayer; (ii) S enc, k , that is a segmentation encoder block has\\nthe same structure as the segmentation input block but has\\na batch normalization layer [34], before the activation layer;\\n1 Please notice that in this work we focused on a binary segmentation\\nproblem, but our model can be extended to multi-class segmentation problems\\nby computing a probability label map of size 160x160x L, i.e., computing for\\neach pixel a vector that represents the probability distribution among the L\\nlabels.\\n(iii) S dec, k , that is a segmentation decoder block composed\\nof a 2D Bilinear Upsampling layer (factor=2) followed by a\\n2D Convolution layer with k ï¬lters of size 3 and stride=1,\\nfollowed by a batch normalization layer and a ReLU [35]\\nactivation layer; (iv) S out, k , that is a segmentation output\\nblock composed of a 2D Bilinear Upsampling layer (factor=2)\\nfollowed by a 2D Convolution layer with k ï¬lters of size 3'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='activation layer; (iv) S out, k , that is a segmentation output\\nblock composed of a 2D Bilinear Upsampling layer (factor=2)\\nfollowed by a 2D Convolution layer with k ï¬lters of size 3\\nand stride=1, followed by a Sigmoid [36] activation layer;\\n(v) D in, k , that is a discriminator input block composed of a\\n2D Convolution layer with k ï¬lters of size 4 and stride=2,\\nfollowed by a LeakyReLU activation layer. (vi) D enc, k , that\\nis a discriminator encoder block has the same structure as\\nthe discriminator input block but has a batch normalization\\nlayer, before the activation layer. The convolutions weights in\\nall discriminator blocks are constrained between [-0.05;0.05]\\nfor stabilizing the training process [19]. The output of the\\ndiscriminator, indicated as Feature Vector in Figure 1, is\\ncomputed by concatenating the discriminator input and the\\nï¬‚attened output of every discriminator block. The slope for\\nthe LeakyReLU is 0.3; batch normalization parameters are'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='computed by concatenating the discriminator input and the\\nï¬‚attened output of every discriminator block. The slope for\\nthe LeakyReLU is 0.3; batch normalization parameters are\\nÏµ= 1âˆ—10âˆ’5 and momentum=0.1 for both networks.\\nB. Discriminator Network Input\\nIn the SegAN architecture the discriminator network is\\ngiven in input a masked slice image, computed by pixel-wise\\nmultiplication of the label probability map and each channel\\nof the MRI slice. Instead, in SegAN-CAT architecture the label\\nprobability map and each channel of the MRI slice are simply\\nconcatenated and provided to the discriminator network as\\ninput. While the input deï¬nition used in the original SegAN\\narchitecture is more compact, with the input deï¬nition we\\npropose it is possible for the discriminator network to extract\\nfeatures that describe also the area of the input MRI that is not\\nincluded into the segmentation. As a result, we expect SegAN-\\nCAT to have slightly better generalization capabilities than the'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='features that describe also the area of the input MRI that is not\\nincluded into the segmentation. As a result, we expect SegAN-\\nCAT to have slightly better generalization capabilities than the\\narchitecture introduced in [3].\\nC. Loss Function\\nTo train our networks, we use the Multiscale Adversarial\\nLoss, as in [3]. This particular loss function, applied also in\\ngenerative problems [37], allows to perform feature matching\\nbetween the ground truth and the output of segmentation\\nnetwork, optimizing also the network weights on features\\nextracted at multiple resolutions rather than focusing just\\non the pixel level. Thus, the Multiscale Adversarial Loss is\\ndeï¬ned as follows:\\nâ„“mae(fD(x),fD(xâ€²)) = 1\\nL\\nLâˆ‘\\ni=1\\n||fi\\nD(x) âˆ’fi\\nD(xâ€²)||1 (1)\\nwhere L is the number of layers in the discriminator\\nnetwork; fi\\nD(Â·) is the output of the activation layer after\\nthe discriminator block at position i+ 1, e.g., f1\\nD(Â·) is the\\ndiscriminator input, f2\\nD(Â·) is the output of the ï¬rst activation'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='network; fi\\nD(Â·) is the output of the activation layer after\\nthe discriminator block at position i+ 1, e.g., f1\\nD(Â·) is the\\ndiscriminator input, f2\\nD(Â·) is the output of the ï¬rst activation\\nlayer, etc; x denotes the input of the discriminator when\\nthe label probability map is computed by the segmentationFig. 1. The SegAN-CAT architecture.\\nnetwork, while xâ€² is the input of the discriminator when the\\nground-truth is used.\\nIn addition to the Multiscale Adversarial Loss , we intro-\\nduced an additional term to the loss function deï¬ned as:\\nâ„“dice(g,p) = 1âˆ’ 2 âˆ‘N\\ni pigi\\nâˆ‘N\\ni p2\\ni + âˆ‘N\\ni g2\\ni\\n(2)\\nwhere g is a ground truth image, p denotes the predicted\\nvalues and the sums run over the pixels of the image; the\\ndeï¬nition of â„“dice is based on a differentiable form of the\\nwell known SÃ¸rensen-Dice coefï¬cient [30], or Dice Score, it\\nranges between 0 and 1, and accounts for the overlap between\\nthe segmented areas in the ground-truth and in the output of'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='well known SÃ¸rensen-Dice coefï¬cient [30], or Dice Score, it\\nranges between 0 and 1, and accounts for the overlap between\\nthe segmented areas in the ground-truth and in the output of\\nthe segmentation network (where 0 means perfect overlap and\\n1 means no overlap). The use of this term, which allows to\\nassess the quality of the generated segmentation maps, should\\nresult in a more stable training process and eventually in a\\nmodel with better performances.\\nOverall, the complete objective function used to train the\\nnetworks is deï¬ned as:\\nmin\\nÎ¸S\\nmax\\nÎ¸D\\n1\\nN\\nNâˆ‘\\nn=1\\nâ„“mae(fD(xn â—¦S(xn)),fD(xn â—¦yn))\\n+ â„“dice(yn,S(xn))\\n(3)\\nwhere xn is an input MRI, yn is the ground truth, and\\nthe â—¦operator is either a concatenation on the channel axis\\nin SegAN-CAT or a pixel-wise multiplication in SegAN (as\\nillustrated in Section III-B).\\nIV. E XPERIMENTAL DESIGN\\nIn this work, we compared the SegAN-CAT and the SegAN\\narchitecture on the brain tumor image segmentation problem'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='illustrated in Section III-B).\\nIV. E XPERIMENTAL DESIGN\\nIn this work, we compared the SegAN-CAT and the SegAN\\narchitecture on the brain tumor image segmentation problem\\nbased on the Multi-modal Brain Tumor Image Segmentation\\nBenchmark (BraTS) [38], a scientiï¬c competition organized\\nin conjunction with the international conference on Medi-\\ncal Image Computing and Computer Assisted Interventions\\n(MICCAI) since 2012.\\nDataset. We used both the BraTS 2015 [38] and the BraTS\\n2019 [39] datasets. The Brain tumor segmentation (BraTS)\\nchallenge focus on the evaluation of methods for segmenting\\nbrain tumors, in particular Gliomas, which are the most\\ncommon primary brain malignancies. Both the datasets we use\\nare composed of MRI volumes of resolution 240x240x155 and\\nfour contrast modalities: T1, T1c, T2, FLAIR. Each voxel is\\nannotated with one among ï¬ve possible labels representing\\nthe tumor sub-regions: Necrosis (NCR), Edema (ED), Non-\\nEnhancing Tumor (NET), Enhancing/Active Tumor (AT) and'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='annotated with one among ï¬ve possible labels representing\\nthe tumor sub-regions: Necrosis (NCR), Edema (ED), Non-\\nEnhancing Tumor (NET), Enhancing/Active Tumor (AT) and\\nEverything Else. The BraTS 2015 dataset consists of 220 high\\ngrade gliomas (HGG) and 54 low grade gliomas (LGG) MRIs.\\nThe BraTS 2019 is the latest revision of the BraTS dataset\\nand it consists of a total of 335 MRI volumes (259 HGG,\\n76 LGG); The major similarities and differences between the\\ntwo datasets are: (i) both datasets contain 30 human-annotated\\nMRIs from a previous dataset, BraTS 2013; (ii) the remaining\\nvolumes in BraTS 2015 are a mixture of pre and post-operative\\nscans that have been labeled by an ensemble of algorithms and\\nlater evaluated by a team of human experts, while in BraTS\\n2019 all the post-operative scans have been discarded and all\\nthe volumes have been manually re-labeled; (iii) additional\\ndata from different institutions have been included in BraTS'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='2019 all the post-operative scans have been discarded and all\\nthe volumes have been manually re-labeled; (iii) additional\\ndata from different institutions have been included in BraTS\\n2019; (iv) in BraTS 2019, the Non-Enhancing Tumor label\\nhas been merged with Necrosis due to a bias that exists in the\\nevaluation of that area.\\nData Preprocessing. Since images in both datasets have anisotropic resolution of 1mm3 per voxels we do not perform any\\nfurther spatial processing to data. As done in [3], we center-\\ncrop each MRI to a 180 x 180 x 128 volume in order to remove\\nblack regions while keeping all the relevant data. For each MRI\\nvolume, we clip voxel values to the 2 nd and 98nd percentile in\\norder to remove outliers, then we apply Feature Scaling [40]\\nto normalize the intensity range between 0 and 1. Finally,\\nwe split the data in Training/Validation sets (respectively of\\nsize 80% and 20%) using stratiï¬ed sampling to keep balanced'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='to normalize the intensity range between 0 and 1. Finally,\\nwe split the data in Training/Validation sets (respectively of\\nsize 80% and 20%) using stratiï¬ed sampling to keep balanced\\nHG and LG subjects within each subset. In Brats2019 dataset\\nwe also apply stratiï¬ed sampling based on the institution that\\nprovided the data.\\nTask Deï¬nition. BraTS2015 challenge deï¬nes 3 regions of\\nthe tumor based on the combination of ground truth labels.\\nInstead, in our experimental analysis, we focused only on one\\nregion, that roughly corresponds to the Whole Tumor region\\nin BraTS challenge. Accordingly, we re-labeled as 1 all the\\nvoxels labeled as NCR, ED, NET, or AT in BraTS datasets,\\nand as 0 the remaining ones.\\nEvaluation. For assessing the performances of our models,\\nwe deï¬ne 3 metrics derived from the well-known Confusion\\nMatrix [41]. To evaluate the number of True Positives (TP),\\nFalse Positives (FP), True Negatives (TN) and False Negatives'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='we deï¬ne 3 metrics derived from the well-known Confusion\\nMatrix [41]. To evaluate the number of True Positives (TP),\\nFalse Positives (FP), True Negatives (TN) and False Negatives\\n(FN) and thus to compute the metrics, we threshold (using 0.5\\nas threshold value) the output of the segmentation network in\\norder to obtain a binary classiï¬cation for each pixel. Despite\\nour models work on a single MRI slice at a time, we compute\\nthe metrics by considering the TP, FP, TN, and FN on every\\nslice of the same MRI volume. In particular we considered the\\nfollowing three metrics: (i) the precision (TP/(TP + FP)),\\nwhich measures how many voxels, classiï¬ed as a tumoral\\nlesion, are effectively part of the tumor; (ii) the sensitivity\\n(TP/(TP + FN)), which measures how many voxels, that\\nare part of the tumor, are correctly identiï¬ed as such; (iii)\\nthe dice score (2TP/(2TP + FP + FN)), which measures\\nthe overlap between the pixels of a binary ground truth and a\\ngiven prediction.'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='are part of the tumor, are correctly identiï¬ed as such; (iii)\\nthe dice score (2TP/(2TP + FP + FN)), which measures\\nthe overlap between the pixels of a binary ground truth and a\\ngiven prediction.\\nIn our experiment we selected our best model according to\\nthe dice score value as it allows to account for both False\\nPositives and False Negatives, leading to a better assessment\\nof the segmentation quality with respect to the precision and\\nsensitivity scores.\\nTransfer Learning. In our experimental analysis, we used the\\nmost straightforward approach to transfer a SegAN-CAT model\\ntrained from MRIs that include a single modality ( source) to\\ntrain a model from MRIs that include a single but different\\nmodality (target): we took the segmentation and discriminator\\nnetworks trained on the source modality and re-trained (orï¬ne-\\ntuned) them on the target modality. In particular, we studied\\ntwo different ï¬ne-tuning process to retrain the networks on'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='networks trained on the source modality and re-trained (orï¬ne-\\ntuned) them on the target modality. In particular, we studied\\ntwo different ï¬ne-tuning process to retrain the networks on\\nthe target modality: (i) we adapted all the weights of both the\\nsegmentation and the discriminator networks; (ii) we adapted\\nall the weights of the segmentation network and only the\\nweights of the input layer of the discriminator network.\\nAlthough keeping the several layers of discriminator ï¬xed\\nduring the ï¬ne-tuning might prevent it from adapting entirely\\nto the target domain, we believe that this solution could help to\\nretain more knowledge from the source domain while adapting\\nthe segmentation network. This choice is motivated by the\\nfact that the discriminator has been found to be the most\\nimportant part of an adversarial network to transfer to the\\ntarget domain [16].\\nExperimental Equipment. We developed and trained all the\\nmodels described in this paper using the Tensorï¬‚ow 2.0a'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='important part of an adversarial network to transfer to the\\ntarget domain [16].\\nExperimental Equipment. We developed and trained all the\\nmodels described in this paper using the Tensorï¬‚ow 2.0a\\nDocker environment [42]â€“[44]. The experiments have been\\ncarried out on a server equipped with a 12GB nVidia Titan\\nV , Intel Xeon E5-2609 CPU and 64GB of RAM.\\nV. E XPERIMENTAL RESULTS\\nWe trained both multi-modal models, i.e., models designed\\nto work with MRIs that include different contrast modalities,\\nas well as uni-modal models, i.e., designed to work with MRIs\\nthat include only one speciï¬c contrast modality.\\nIn the ï¬rst experiment, we trained three multi-modal models\\nthat use all the four contrast modalities, i.e., T1, T1c, T2,\\nFLAIR, available in the datasets: (i) a model that implements\\nthe original SegAN architecture, (ii) a model that implements\\nthe SegAN architecture with the additional dice loss term,\\nand (iii) a model that implements completely the SegAN-CAT'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='the original SegAN architecture, (ii) a model that implements\\nthe SegAN architecture with the additional dice loss term,\\nand (iii) a model that implements completely the SegAN-CAT\\narchitecture, i.e., with the additional dice loss term and the\\ninput discriminator concatenation.\\nEach model is trained and tested both on BraTS 2015 and\\nBraTS 2019 datasets using a batch size of 64 slices. During\\nthe training phase, we perform data augmentation by applying\\nrandom cropping [32] using a window of size 160 x 160,\\nas proposed in [3]. During the validation phase we apply\\ncenter cropping [32] to match the input size of the network,\\ndiscarding most of the black border of the MRI. All the models\\nare trained using the same initialization seed, RMSprop [45]\\n(lr: 2*10-5), and Early Stopping [46] (patience = 300 epochs )\\non Dice Score evaluation metric. An epoch consist of a full\\niteration over the dataset, i.e. approximately 28000 slices for\\nthe Brats 2015 dataset and 34000 for the Brats 2019 dataset.'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='on Dice Score evaluation metric. An epoch consist of a full\\niteration over the dataset, i.e. approximately 28000 slices for\\nthe Brats 2015 dataset and 34000 for the Brats 2019 dataset.\\nTable I compares the performance of all the multi-modal\\nmodels trained on two datasets. The results show that both\\nthe dice loss term and the discriminator input concatenation,\\nintroduced in the SegAN-CAT, led to better performances\\non both datasets. Results also suggest that BraTS 2019 is\\nslightly more challenging than BraTS 2015, leading to a lower\\nperformance of all the the three models.\\nIn the second experiment, we wanted to investigate how\\nthe information content of each contrast modality affects the\\nmodel performance. To this purpose, we trained four uni-\\nmodal models, each one using only a single contrast modality.\\nTable II compares the performance of these fourSegAN-CAT\\nmodels trained using a single contrast modality. The results\\nshow that none of the model trained with a single modality'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='Table II compares the performance of these fourSegAN-CAT\\nmodels trained using a single contrast modality. The results\\nshow that none of the model trained with a single modality\\nis able to achieve the same performance achieved by SegAN-\\nCAT which uses all the four contrast modalities together. ThisTABLE I\\nPERFORMANCE OF SegAN, SegAN WITH DICE LOSS , AND SegAN-CAT. THE RESULTS REPORTED ARE THE AVERAGE OF THE DICE SCORE , THE PRECISION ,\\nAND THE SENSITIVITY COMPUTED ON EACH ONE OF THE MRI VOLUME INCLUDED IN TEST SET OF BRATS 2015 AND BRATS 2019. W E REPORTED IN\\nBOLD THE BEST PERFORMANCE FOR EACH DATASET .\\nModel BraTS 2015 BraTS 2019\\nDice Score Precision Sensitivity Dice Score Precision Sensitivity\\nSegAN 0.705 0.759 0.694 0.766 0.745 0.834\\nSegAN w/ Dice Loss 0.825 0.901 0.785 0.814 0.850 0.810\\nSegAN-CAT 0.859 0.882 0.852 0.825 0.842 0.835\\nTABLE II\\nPERFORMANCE OF SegAN-CAT MODELS TRAINED FROM MRI S WITH ONLY ONE CONTRAST MODALITY . THE RESULTS ARE REPORTED FOR EACH'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='SegAN-CAT 0.859 0.882 0.852 0.825 0.842 0.835\\nTABLE II\\nPERFORMANCE OF SegAN-CAT MODELS TRAINED FROM MRI S WITH ONLY ONE CONTRAST MODALITY . THE RESULTS ARE REPORTED FOR EACH\\nMODALITY AS THE AVERAGE OF THE DICE SCORE , THE PRECISION , AND THE SENSITIVITY COMPUTED ON EACH ONE OF THE MRI VOLUME INCLUDED\\nIN TEST SET OF BRATS 2015 AND BRATS 2019. A S A REFERENCE , THE PERFORMANCE OF SegAN-CAT TRAINED WITH ALL THE CONTRAST MODALITIES\\nIS REPORTED IN THE LAST ROW OF THE TABLE .\\nModality BraTS 2015 BraTS 2019\\nDice Score Precision Sensitivity Dice Score Precision Sensitivity\\nT1 0.538 0.570 0.557 0.542 0.586 0.609\\nT1c 0.578 0.613 0.581 0.587 0.678 0.577\\nT2 0.721 0.773 0.724 0.675 0.828 0.607\\nFLAIR 0.810 0.858 0.787 0.763 0.757 0.810\\nALL 0.859 0.882 0.852 0.825 0.842 0.835\\nsuggests that none of the four modalities alone contains all the\\nrelevant information to identify the tumor. However, the model\\ntrained using only the FLAIR modality is able to achieve a'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='suggests that none of the four modalities alone contains all the\\nrelevant information to identify the tumor. However, the model\\ntrained using only the FLAIR modality is able to achieve a\\nmuch better performance than all the other models on both the\\ndatasets. These results are not surprising as FLAIR modality\\nallows to identify more clearly the edema and the lesions\\nin speciï¬c areas of the brain, due to the suppression of the\\ncerebrospinal ï¬‚uid in the images.\\nIn the ï¬nal experiment, we investigated whether it is possi-\\nble to transfer a model trained on a speciï¬c contrast modality\\nto a different contrast modality. Thus, as preliminary analysis,\\nwe applied all the uni-modal models previously trained on\\neach of the contrast modality to images taken with different\\nmodalities to evaluate the similarities among the models and\\nto have an insight on how easily the knowledge learned from\\na modality could be transferred to the the others.'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='modalities to evaluate the similarities among the models and\\nto have an insight on how easily the knowledge learned from\\na modality could be transferred to the the others.\\nFigure 2 shows the performance of all the uni-modal models\\nwhen applied to each modality alone. As expected, all the\\nuni-modal models reached the best performance on images\\nacquired with the same contrast modality they have been\\ntrained for. Therefore, to transfer successfully a model across\\nthe contrast modalities, it is necessary to adapt the trained\\nnetworks with data from the target domain, i.e., the target\\ncontrast modality. The data also show how models could\\nbe easily transferred across modalities that are known to be\\nsimilar among them: models trained on T1 and T1c perform\\npoorly on T2 and FLAIR, while they perform much better\\nwhen applied to the other modality similar to the one they\\nare trained on (i.e., T1 or T1c); the same behavior, the other\\nway around, is found when looking at the performance of the'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='when applied to the other modality similar to the one they\\nare trained on (i.e., T1 or T1c); the same behavior, the other\\nway around, is found when looking at the performance of the\\nmodels trained on T2 and on FLAIR.\\nBased on the results discussed above, we applied the transfer\\nlearning to train three uni-modal models respectively on T1,\\nFig. 2. Performance of each uni-modal model when applied to images\\nacquired using other modalities. The performances on both BraTS 2015 and\\nBraTS 2019 are reported. In addition, we also report as a reference the\\nperformance of the uni-modal models on images with the same modality\\nthe models are trained for.\\nT1c, and T2 images by adapting a model trained on FLAIR\\nimages. In fact, our results show that the FLAIR images\\naccount for a large part of the model performance and our aim\\nis to understand whether the transfer learning process might\\nimprove the ï¬nal performances. In this experiment, the ï¬ne-'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='account for a large part of the model performance and our aim\\nis to understand whether the transfer learning process might\\nimprove the ï¬nal performances. In this experiment, the ï¬ne-\\ntuning of the model on the target modality was limited to 300\\nepochs.\\nTable III show the performance of the models trained on T1,\\nT1c, and T2 by adapting a model trained on FLAIR. The re-\\nsults suggest that it is more convenient to adapt, i.e., to train onTABLE III\\nPERFORMANCE OF THE MODELS TRAINED BY TRANSFERRING THE MODEL TRAINED ON FLAIR IMAGES . THE COLUMN FI N ETU N I N GREPORTS\\nWHETHER BOTH THE MODEL NETWORKS (S,D) OR ONLY THE SEGMENTATOR AND THE DISCRIMINATOR INPUT LAYER (S,DI N) HAVE BEEN TRAINED ON\\nTHE TARGET MODALITY . THE RESULTS ARE REPORTED FOR EACH TARGET MODALITY AS THE AVERAGE OF THE DICE SCORE , THE PRECISION , AND THE\\nSENSITIVITY COMPUTED ON EACH ONE OF THE MRI VOLUME INCLUDED IN TEST SET OF BRATS 2015 AND BRATS 2019. W E REPORTED IN BOLD THE'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='SENSITIVITY COMPUTED ON EACH ONE OF THE MRI VOLUME INCLUDED IN TEST SET OF BRATS 2015 AND BRATS 2019. W E REPORTED IN BOLD THE\\nSCORES THAT ARE BETTER THAN THE CORRESPONDING ONES IN TABLE II.\\nTarget Fine Tuning BraTS 2015 BraTS 2019\\nDice Score Precision Sensitivity Dice Score Precision Sensitivity\\nT1 S,D 0.496 0.503 0.553 0.502 0.571 0.582\\nT1 S,Din 0.561 0.614 0.576 0.528 0.558 0.538\\nT1c S,D 0.467 0.464 0.538 0.468 0.527 0.494\\nT1c S,Din 0.577 0.661 0.541 0.598 0.705 0.563\\nT2 S,D 0.692 0.776 0.668 0.674 0.643 0.775\\nT2 S,Din 0.781 0.818 0.771 0.741 0.878 0.681\\nimages with the target contrast modality, only the input layer\\nof the discriminator network of the model, keeping the other\\nlayers of the discriminator network trained on images with the\\nFLAIR contrast modality. We believe that this result is due\\nto the high instability of the adversarial learning mechanism\\nthat makes it very difï¬cult to incrementally adapt a previously\\ntrained model [16]. Accordingly, to exploit the knowledge'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='to the high instability of the adversarial learning mechanism\\nthat makes it very difï¬cult to incrementally adapt a previously\\ntrained model [16]. Accordingly, to exploit the knowledge\\nof the model trained on images with FLAIR contrast, we\\nadapted on the target modalities only the segmentator network\\nand the ï¬rst layer of the discriminator network. Overall, this\\nsolution often led to models that perform better or similarly\\nto the models trained from scratch on the target modalities\\n(see Table II), despite being trained for 300 epochs only. As\\nexpected, the major beneï¬ts of transfer learning mechanism is\\nachieved on the target modality more similar to FLAIR, i.e.,\\nT2.\\nVI. C ONCLUSIONS\\nIn this work, we introduced SegAN-CAT, an adversarial\\nnetwork architecture based on SegAN [3]. Our approach differs\\nfrom SegAN mainly in two respects: (i) the loss function has\\nbeen extended with a dice loss term and (ii) the input of the\\ndiscriminator network consists of a concatenation of the MRI'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='from SegAN mainly in two respects: (i) the loss function has\\nbeen extended with a dice loss term and (ii) the input of the\\ndiscriminator network consists of a concatenation of the MRI\\nimages and their segmentation.\\nWe applied SegAN-CAT to an MRI brain tumor segmen-\\ntation problem, the same task the SegAN architecture was\\nsuccessfully applied to. In particular, we deï¬ned a binary\\nsegmentation problem on two datasets, BraTS 2015 and BraTS\\n2019, used in a scientiï¬c competition organized in conjunction\\nwith the international conference on Medical Image Com-\\nputing and Computer Assisted Interventions (MICCAI) since\\n2012. The aim of this work was (i) to compare the performance\\nof SegAN-CAT and SegAN, (ii) to assess the performance of\\nuni-modal models for each contrast modality (i.e., T1, T1c,\\nT2, and FLAIR), and (iii) to study the problem of transferring\\na previously trained model across different modalities. To\\nthis purpose, we ï¬rst trained SegAN, SegAN with a dice loss'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='T2, and FLAIR), and (iii) to study the problem of transferring\\na previously trained model across different modalities. To\\nthis purpose, we ï¬rst trained SegAN, SegAN with a dice loss\\nterm, and SegAN-CAT on MRIs acquired with all the four\\ncontrast modalities. Our results on both BraTS 2015 and\\nBraTS 2019 datasets showed that both the dice loss term and\\nthe discriminator input concatenation proposed in this paper\\nallow to improve the ï¬nal performance of the model. Then,\\nwe trained one uni-modal SegAN-CAT model for each one of\\nthe four contrast modalities in order to assess the performance\\nthat can be achieved using only the information available in\\neach modality alone. As expected, none of these four models\\nreached the performance of the multi-modal one. However,\\nthe model trained on FLAIR contrast modality outperformed\\nall the others and was able to reach a performance quite\\nclose to the one achieved by the multi-modal model. Thus,'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='the model trained on FLAIR contrast modality outperformed\\nall the others and was able to reach a performance quite\\nclose to the one achieved by the multi-modal model. Thus,\\nwe investigated the possibility of transferring a model trained\\nwith FLAIR to train better model from images acquired with\\ndifferent contrast modalities. Despite conï¬rming that is rather\\ndifï¬cult to use transfer learning with adversarial networks as\\ndiscussed in [16], [18], our results suggest that it is possible\\nto successfully transfer a model trained on FLAIR contrast\\nmodality also to other modalities. Indeed, our results show that\\ntransfer learning often allows to train uni-modal models with\\na better performance than the same models trained entirely on\\nimages with their speciï¬c contrast modality.\\nWe believe that the transfer learning with adversarial net-\\nworks is a topic worth to be further investigated in future\\nworks. In fact, the limited amount of data is one of the major'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='We believe that the transfer learning with adversarial net-\\nworks is a topic worth to be further investigated in future\\nworks. In fact, the limited amount of data is one of the major\\nissue when it comes to the application of deep learning to\\nhealthcare domain. So far, transfer learning has been widely\\nused in several domains to deal with the limited availabil-\\nity of data, but it has been not yet fully exploited in the\\nhealthcare domain. In particular, applying transfer learning\\nto train adversarial networks is a challenging and yet rather\\nunexplored research area that deserve additional investigation.\\nIn future works we plan to investigate additional strategies\\nto adapt a previously trained model to a target domain, such\\nas adapting only some layers of the networks, alternating\\npreviously trained and untrained networks during the adapt-\\ning process, and combining together networks trained using\\ndifferent contrast modalities.\\nREFERENCES'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='previously trained and untrained networks during the adapt-\\ning process, and combining together networks trained using\\ndifferent contrast modalities.\\nREFERENCES\\n[1] Y . LeCun, Y . Bengio, and G. Hinton, â€œDeep learning,â€ nature, vol. 521,\\nno. 7553, pp. 436â€“444, 2015.\\n[2] I. Goodfellow, J. Pouget-Abadie, M. Mirza et al., â€œGenerative adversarial\\nnets,â€ in Advances in neural information processing systems , 2014, pp.\\n2672â€“2680.[3] Y . Xue, T. Xu, H. Zhang et al. , â€œSegan: Adversarial network with\\nmulti-scale $l 1$ loss for medical image segmentation,â€ CoRR, vol.\\nabs/1706.01805, 2017. [Online]. Available: http://arxiv.org/abs/1706.\\n01805\\n[4] P. Luc, C. Couprie, S. Chintala, and J. Verbeek, â€œSemantic segmentation\\nusing adversarial networks,â€CoRR, vol. abs/1611.08408, 2016. [Online].\\nAvailable: http://arxiv.org/abs/1611.08408\\n[5] M. Everingham, L. Van Gool, C. K. I. Williams\\net al. , â€œThe PASCAL Visual Object Classes Chal-\\nlenge 2012 (VOC2012) Results,â€ http://www.pascal-'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='Available: http://arxiv.org/abs/1611.08408\\n[5] M. Everingham, L. Van Gool, C. K. I. Williams\\net al. , â€œThe PASCAL Visual Object Classes Chal-\\nlenge 2012 (VOC2012) Results,â€ http://www.pascal-\\nnetwork.org/challenges/VOC/voc2012/workshop/index.html.\\n[6] S. Gould, R. Fulton, and D. Koller, â€œDecomposing a scene into geomet-\\nric and semantically consistent regions,â€ in2009 IEEE 12th international\\nconference on computer vision . IEEE, 2009, pp. 1â€“8.\\n[7] P. Moeskops, M. Veta, M. W. Lafarge et al. , â€œAdversarial training\\nand dilated convolutions for brain MRI segmentation,â€ CoRR, vol.\\nabs/1707.03195, 2017. [Online]. Available: http://arxiv.org/abs/1707.\\n03195\\n[8] M. Havaei, A. Davy, D. Warde-Farley et al., â€œBrain tumor segmentation\\nwith deep neural networks,â€ Medical image analysis, vol. 35, pp. 18â€“31,\\n2017.\\n[9] K. Kamnitsas, C. Ledig, V . F. Newcombe et al. , â€œEfï¬cient multi-scale\\n3d cnn with fully connected crf for accurate brain lesion segmentation,â€'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='2017.\\n[9] K. Kamnitsas, C. Ledig, V . F. Newcombe et al. , â€œEfï¬cient multi-scale\\n3d cnn with fully connected crf for accurate brain lesion segmentation,â€\\nMedical image analysis , vol. 36, pp. 61â€“78, 2017.\\n[10] J. Yosinski, J. Clune, Y . Bengio, and H. Lipson, â€œHow transferable are\\nfeatures in deep neural networks?â€ CoRR, vol. abs/1411.1792, 2014.\\n[Online]. Available: http://arxiv.org/abs/1411.1792\\n[11] P. Moeskops, J. M. Wolterink, B. H. M. van der Velden et al. ,\\nâ€œDeep learning for multi-task medical image segmentation in multiple\\nmodalities,â€ CoRR, vol. abs/1704.03379, 2017. [Online]. Available:\\nhttp://arxiv.org/abs/1704.03379\\n[12] S. U. H. Dar and T. C Â¸ ukur, â€œA transfer-learning approach for accelerated\\nMRI using deep neural networks,â€ CoRR, vol. abs/1710.02615, 2017.\\n[Online]. Available: http://arxiv.org/abs/1710.02615\\n[13] S. Motamed, I. Gujrathi, D. Deniffel et al., â€œA transfer learning approach\\nfor automated segmentation of prostate whole gland and transition zone'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='[13] S. Motamed, I. Gujrathi, D. Deniffel et al., â€œA transfer learning approach\\nfor automated segmentation of prostate whole gland and transition zone\\nin diffusion weighted mri,â€ 2019.\\n[14] J. Jiang, Y .-C. Hu, N. Tyagi et al. , â€œTumor-aware, adversarial domain\\nadaptation from ct to mri for lung cancer segmentation,â€ in Medical\\nImage Computing and Computer Assisted Intervention â€“ MICCAI 2018 ,\\nA. F. Frangi, J. A. Schnabel, C. Davatzikos et al., Eds. Cham: Springer\\nInternational Publishing, 2018, pp. 777â€“785.\\n[15] M. Ghafoorian, A. Mehrtash, T. Kapur et al. , â€œTransfer learning for\\ndomain adaptation in mri: Application in brain lesion segmentation,â€\\nin Medical Image Computing and Computer Assisted Intervention -\\nMICCAI 2017 , M. Descoteaux, L. Maier-Hein, A. Franz et al. , Eds.\\nCham: Springer International Publishing, 2017, pp. 516â€“524.\\n[16] Y . Wang, C. Wu, L. Herranz et al. , â€œTransferring gans: Generating\\nimages from limited data,â€ in ECCV, 2018.'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='Cham: Springer International Publishing, 2017, pp. 516â€“524.\\n[16] Y . Wang, C. Wu, L. Herranz et al. , â€œTransferring gans: Generating\\nimages from limited data,â€ in ECCV, 2018.\\n[17] I. Gulrajani, F. Ahmed, M. Arjovsky et al. , â€œImproved training of\\nwasserstein gans,â€ inAdvances in neural information processing systems,\\n2017, pp. 5767â€“5777.\\n[18] Y . Fr Â´egier and J. Gouray, â€œMind2mind : transfer learning for\\ngans,â€ CoRR, vol. abs/1906.11613, 2019. [Online]. Available: http:\\n//arxiv.org/abs/1906.11613\\n[19] M. Arjovsky, S. Chintala, and L. Bottou, â€œWasserstein GAN,â€ arXiv\\ne-prints, p. arXiv:1701.07875, Jan 2017.\\n[20] S. U. H. Dar, M. Yurt, L. Karacan et al. , â€œImage synthesis in\\nmulti-contrast MRI with conditional generative adversarial networks,â€\\nCoRR, vol. abs/1802.01221, 2018. [Online]. Available: http://arxiv.org/\\nabs/1802.01221\\n[21] J. Zhu, T. Park, P. Isola, and A. A. Efros, â€œUnpaired image-to-image\\ntranslation using cycle-consistent adversarial networks,â€ CoRR, vol.'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='abs/1802.01221\\n[21] J. Zhu, T. Park, P. Isola, and A. A. Efros, â€œUnpaired image-to-image\\ntranslation using cycle-consistent adversarial networks,â€ CoRR, vol.\\nabs/1703.10593, 2017. [Online]. Available: http://arxiv.org/abs/1703.\\n10593\\n[22] A. Sharma and G. Hamarneh, â€œMissing MRI Pulse Sequence Synthesis\\nusing Multi-Modal Generative Adversarial Network,â€ arXiv e-prints, p.\\narXiv:1904.12200, Apr 2019.\\n[23] B. Yu, L. Zhou, L. Wang et al. , â€œ3d cgan based cross-modality mr\\nimage synthesis for brain tumor segmentation,â€ in 2018 IEEE 15th\\nInternational Symposium on Biomedical Imaging (ISBI 2018) , April\\n2018, pp. 626â€“630.\\n[24] M. Orbes-Arteaga, M. J. Cardoso, L. SÃ¸rensen et al. , â€œSimultaneous\\nsynthesis of FLAIR and segmentation of white matter hypointensities\\nfrom T1 mris,â€ CoRR, vol. abs/1808.06519, 2018. [Online]. Available:\\nhttp://arxiv.org/abs/1808.06519\\n[25] C. Ge, I. Y . Gu, A. Store Jakola, and J. Yang, â€œCross-modality augmen-'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='from T1 mris,â€ CoRR, vol. abs/1808.06519, 2018. [Online]. Available:\\nhttp://arxiv.org/abs/1808.06519\\n[25] C. Ge, I. Y . Gu, A. Store Jakola, and J. Yang, â€œCross-modality augmen-\\ntation of brain mr images using a novel pairwise generative adversarial\\nnetwork for enhanced glioma classiï¬cation,â€ in 2019 IEEE International\\nConference on Image Processing (ICIP) , Sep. 2019, pp. 559â€“563.\\n[26] M. Havaei, N. Guizard, N. Chapados, and Y . Bengio, â€œHemis:\\nHetero-modal image segmentation,â€ in Medical Image Computing and\\nComputer-Assisted Intervention â€“ MICCAI 2016, S. Ourselin, L. Joskow-\\nicz, M. R. Sabuncu et al., Eds. Cham: Springer International Publishing,\\n2016, pp. 469â€“477.\\n[27] T. Varsavsky, Z. Eaton-Rosen, C. H. Sudre et al., â€œPIMMS: permutation\\ninvariant multi-modal segmentation,â€ CoRR, vol. abs/1807.06537, 2018.\\n[Online]. Available: http://arxiv.org/abs/1807.06537\\n[28] R. Dorent, S. Joutard, M. Modat et al. , â€œHetero-Modal Variational'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='invariant multi-modal segmentation,â€ CoRR, vol. abs/1807.06537, 2018.\\n[Online]. Available: http://arxiv.org/abs/1807.06537\\n[28] R. Dorent, S. Joutard, M. Modat et al. , â€œHetero-Modal Variational\\nEncoder-Decoder for Joint Modality Completion and Segmentation,â€\\narXiv e-prints, p. arXiv:1907.11150, Jul 2019.\\n[29] D. P. Kingma and M. Welling, â€œAuto-Encoding Variational Bayes,â€\\narXiv e-prints, p. arXiv:1312.6114, Dec 2013.\\n[30] F. Milletari, N. Navab, and S. Ahmadi, â€œV-net: Fully convolutional\\nneural networks for volumetric medical image segmentation,â€ CoRR,\\nvol. abs/1606.04797, 2016. [Online]. Available: http://arxiv.org/abs/\\n1606.04797\\n[31] Y . LeCun, B. E. Boser, J. S. Denkeret al., â€œHandwritten digit recognition\\nwith a back-propagation network,â€ in Advances in neural information\\nprocessing systems, 1990, pp. 396â€“404.\\n[32] A. Krizhevsky, I. Sutskever, and G. E. Hinton, â€œImagenet classiï¬cation\\nwith deep convolutional neural networks,â€ in Advances in Neural'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='processing systems, 1990, pp. 396â€“404.\\n[32] A. Krizhevsky, I. Sutskever, and G. E. Hinton, â€œImagenet classiï¬cation\\nwith deep convolutional neural networks,â€ in Advances in Neural\\nInformation Processing Systems 25 , F. Pereira, C. J. C. Burges,\\nL. Bottou, and K. Q. Weinberger, Eds. Curran Associates, Inc.,\\n2012, pp. 1097â€“1105. [Online]. Available: http://papers.nips.cc/paper/\\n4824-imagenet-classiï¬cation-with-deep-convolutional-neural-networks.\\npdf\\n[33] A. L. Maas, A. Y . Hannun, and A. Y . Ng, â€œRectiï¬er nonlinearities\\nimprove neural network acoustic models,â€ in Proc. icml, vol. 30, no. 1,\\n2013, p. 3.\\n[34] S. Ioffe and C. Szegedy, â€œBatch normalization: Accelerating deep\\nnetwork training by reducing internal covariate shift,â€ CoRR, vol.\\nabs/1502.03167, 2015. [Online]. Available: http://arxiv.org/abs/1502.\\n03167\\n[35] V . Nair and G. E. Hinton, â€œRectiï¬ed linear units improve restricted boltz-\\nmann machines,â€ in Proceedings of the 27th international conference on'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='03167\\n[35] V . Nair and G. E. Hinton, â€œRectiï¬ed linear units improve restricted boltz-\\nmann machines,â€ in Proceedings of the 27th international conference on\\nmachine learning (ICML-10) , 2010, pp. 807â€“814.\\n[36] T. M. Mitchell, Machine Learning, 1st ed. USA: McGraw-Hill, Inc.,\\n1997.\\n[37] T.-C. Wang, M.-Y . Liu, J.-Y . Zhuet al., â€œHigh-resolution image synthesis\\nand semantic manipulation with conditional gans,â€ in Proceedings of the\\nIEEE Conference on Computer Vision and Pattern Recognition , 2018.\\n[38] B. H. Menze, A. Jakab, S. Bauer et al., â€œThe multi-modal brain tumor\\nimage segmentation benchmark (brats),â€ IEEE Transactions on Medical\\nImaging, vol. 34, no. 10, pp. 1993â€“2024, Oct 2015.\\n[39] S. Bakas, M. Reyes, A. Jakab et al. , â€œIdentifying the best\\nmachine learning algorithms for brain tumor segmentation, progression\\nassessment, and overall survival prediction in the BraTS challenge,â€\\nCoRR, vol. abs/1811.02629, 2018. [Online]. Available: http://arxiv.org/\\nabs/1811.02629'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='assessment, and overall survival prediction in the BraTS challenge,â€\\nCoRR, vol. abs/1811.02629, 2018. [Online]. Available: http://arxiv.org/\\nabs/1811.02629\\n[40] S. Aksoy and R. M. Haralick, â€œFeature normalization and likelihood-\\nbased similarity measures for image retrieval,â€ Pattern recognition\\nletters, vol. 22, no. 5, pp. 563â€“582, 2001.\\n[41] S. V . Stehman, â€œSelecting and interpreting measures of thematic classi-\\nï¬cation accuracy,â€ Remote sensing of Environment , vol. 62, no. 1, pp.\\n77â€“89, 1997.\\n[42] M. Abadi, A. Agarwal, P. Barham et al. , â€œTensorFlow: Large-scale\\nmachine learning on heterogeneous systems,â€ 2015, software available\\nfrom tensorï¬‚ow.org. [Online]. Available: https://www.tensorï¬‚ow.org/\\n[43] D. Merkel, â€œDocker: lightweight linux containers for consistent devel-\\nopment and deployment,â€ Linux journal, vol. 2014, no. 239, p. 2, 2014.\\n[44] C. Boettiger, â€œAn introduction to docker for reproducible research,â€ ACM'),\n",
       " Document(metadata={'arxiv_id': '1910.02717v2', 'title': 'Brain MRI Tumor Segmentation with Adversarial Networks', 'section': 'body', 'authors': 'Edoardo Giacomello, Daniele Loiacono, Luca Mainardi'}, page_content='opment and deployment,â€ Linux journal, vol. 2014, no. 239, p. 2, 2014.\\n[44] C. Boettiger, â€œAn introduction to docker for reproducible research,â€ ACM\\nSIGOPS Operating Systems Review , vol. 49, no. 1, pp. 71â€“79, 2015.\\n[45] T. Tieleman and G. Hinton, â€œLecture 6.5-rmsprop: Divide the gradient\\nby a running average of its recent magnitude,â€ COURSERA: Neural\\nnetworks for machine learning , vol. 4, no. 2, pp. 26â€“31, 2012.\\n[46] Y . Yao, L. Rosasco, and A. Caponnetto, â€œOn early stopping in gradient\\ndescent learning,â€ Constructive Approximation, vol. 26, no. 2, pp. 289â€“\\n315, 2007.'),\n",
       " Document(metadata={'arxiv_id': '2510.15684v1', 'title': 'Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI', 'section': 'title_abstract', 'authors': 'Gerard Comas-Quiles, Carles Garcia-Cabrera, Julia Dietlmeier'}, page_content='Title: Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI\\n\\nAbstract: Unsupervised anomaly detection (UAD) presents a complementary alternative to supervised learning for brain tumor segmentation in magnetic resonance imaging (MRI), particularly when annotated datasets are limited, costly, or inconsistent. In this work, we propose a novel Multimodal Vision Transformer Autoencoder (MViT-AE) trained exclusively on healthy brain MRIs to detect and localize tumors via reconstruction-based error maps. This unsupervised paradigm enables segmentation without reliance on manual labels, addressing a key scalability bottleneck in neuroimaging workflows. Our method is evaluated in the BraTS-GoAT 2025 Lighthouse dataset, which includes various types of tumors such as gliomas, meningiomas, and pediatric brain tumors. To enhance performance, we introduce a multimodal early-late fusion strategy that leverages complementary information across multiple MRI sequences, and a post-processing pipeline that integrates the Segment Anything Model (SAM) to refine predicted tumor contours. Despite the known challenges of UAD, particularly in detecting small or non-enhancing lesions, our method achieves clinically meaningful tumor localization, with lesion-wise Dice Similarity Coefficient of 0.437 (Whole Tumor), 0.316 (Tumor Core), and 0.350 (Enhancing Tumor) on the test set, and an anomaly Detection Rate of 89.4% on the validation set. These findings highlight the potential of transformer-based unsupervised models to serve as scalable, label-efficient tools for neuro-oncological imaging.'),\n",
       " Document(metadata={'arxiv_id': '2510.15684v1', 'title': 'Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI', 'section': 'body', 'authors': 'Gerard Comas-Quiles, Carles Garcia-Cabrera, Julia Dietlmeier'}, page_content='Towards Label-Free Brain Tumor Segmentation:\\nUnsupervised Learning with Multimodal MRI\\nGerard Comas-Quiles1, Carles Garcia-Cabrera2,4 , Julia Dietlmeier3,4 ,\\nNoel E. Oâ€™Connor3,4 , and Ferran Marques1\\n1 Universitat PolitÃ¨cnica de Catalunya (UPC), Barcelona, Spain\\n2 University College Dublin (UCD), Dublin, Ireland\\n3 Dublin City University (DCU), Dublin, Ireland\\n4 Insight Research Ireland Center For Data Analytics, Dublin, Ireland\\nAbstract.Unsupervised anomaly detection (UAD) presents a comple-\\nmentary alternative to supervised learning for brain tumor segmenta-\\ntion in magnetic resonance imaging (MRI), particularly when annotated\\ndatasets are limited, costly, or inconsistent. In this work, we propose a\\nnovel Multimodal Vision Transformer Autoencoder (MViT-AE) trained\\nexclusively on healthy brain MRIs to detect and localize tumors via\\nreconstruction-based error maps. This unsupervised paradigm enables\\nsegmentation without reliance on manual labels, addressing a key scal-'),\n",
       " Document(metadata={'arxiv_id': '2510.15684v1', 'title': 'Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI', 'section': 'body', 'authors': 'Gerard Comas-Quiles, Carles Garcia-Cabrera, Julia Dietlmeier'}, page_content='reconstruction-based error maps. This unsupervised paradigm enables\\nsegmentation without reliance on manual labels, addressing a key scal-\\nability bottleneck in neuroimaging workflows. Our method is evaluated\\nin the BraTS-GoAT 2025 Lighthouse dataset, which includes various\\ntypes of tumors such as gliomas, meningiomas, and pediatric brain tu-\\nmors. To enhance performance, we introduce a multimodal early-late\\nfusion strategy that leverages complementary information across multi-\\nple MRI sequences, and a post-processing pipeline that integrates the\\nSegment Anything Model (SAM) to refine predicted tumor contours.\\nDespite the known challenges of UAD, particularly in detecting small or\\nnon-enhancing lesions, our method achieves clinically meaningful tumor\\nlocalization, with lesion wise Dice Similarity Coefficient of 0.422 (Whole\\nTumor), 0.279 (Tumor Core), and 0.289 (Enhancing Tumor), and an\\nanomaly Detection Rate of 89.4%. These findings highlight the potential'),\n",
       " Document(metadata={'arxiv_id': '2510.15684v1', 'title': 'Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI', 'section': 'body', 'authors': 'Gerard Comas-Quiles, Carles Garcia-Cabrera, Julia Dietlmeier'}, page_content='Tumor), 0.279 (Tumor Core), and 0.289 (Enhancing Tumor), and an\\nanomaly Detection Rate of 89.4%. These findings highlight the potential\\nof transformer-based unsupervised models to serve as scalable, label-\\nefficient tools for neuro-oncological imaging. The code for this project\\nwill be publicly available on GitHub.\\nKeywords:AutoencoderÂ·Brain Tumor SegmentationÂ·MultimodalÂ·\\nSegment Anything ModelÂ·Unsupervised Anomaly DetectionÂ·Vision\\nTransformer\\n1 Introduction\\nAdvancementsinmedicalimaging,particularlymagneticresonanceimaging(MRI),\\nhave significantly improved the early detection and characterization of brain tu-\\nmors.However,interpretingMRIscansremainsalabor-intensiveanderror-prone\\narXiv:2510.15684v1  [cs.CV]  17 Oct 20252 G. Comas et al.\\nprocess. Studies report that 5â€“10% of neuroimaging interpretations may over-\\nlook critical abnormalities [1, 2], underscoring the need for robust and automated\\ndiagnostic tools that can support human interpretation.'),\n",
       " Document(metadata={'arxiv_id': '2510.15684v1', 'title': 'Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI', 'section': 'body', 'authors': 'Gerard Comas-Quiles, Carles Garcia-Cabrera, Julia Dietlmeier'}, page_content='look critical abnormalities [1, 2], underscoring the need for robust and automated\\ndiagnostic tools that can support human interpretation.\\nThe Brain Tumor Segmentation (BraTS) Challenge [3, 4] has played a central\\nrole in advancing brain tumor segmentation techniques, particularly for gliomas.\\nSince its inception in 2012, BraTS has progressively expanded to include addi-\\ntional tumor types and imaging modalities, and to increase clinical relevance.\\nSupervised deep learning methodsâ€”especially U-Net architectures [5], 3D CNNs\\n[6], and more recently, transformer-based models [7, 8]â€”have achieved state-of-\\nthe-art performance when trained on large, annotated datasets.\\nDespite these advancements, reliance on manual annotations poses signif-\\nicant limitations in real-world clinical settings. Annotation is not only time-\\nconsuming and costly, but also susceptible to inter-observer variability [9], par-\\nticularly across institutions. These challenges become even more pronounced'),\n",
       " Document(metadata={'arxiv_id': '2510.15684v1', 'title': 'Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI', 'section': 'body', 'authors': 'Gerard Comas-Quiles, Carles Garcia-Cabrera, Julia Dietlmeier'}, page_content='consuming and costly, but also susceptible to inter-observer variability [9], par-\\nticularly across institutions. These challenges become even more pronounced\\nwhen dealing with a small number of lesions [10], such as rare tumor types or\\nlow-resource environments.\\nTo move beyond models limited to detecting only specific tumor types, the\\nBraTS challenge encouraged research into generalization methods [11]. However,\\nno submissions adopted fully unsupervised learning, underscoring both the tech-\\nnical challenges and the unexplored potential of unsupervised anomaly detection\\n(UAD) in brain tumor segmentation.\\nIn this work, we propose a fully unsupervised brain tumor segmentation\\npipeline designed for the BraTS Generalizability Across Tumors (GoAT) 2025\\nchallenge. Our method combines a multimodal Vision Transformer Autoencoder\\n(MViT-AE) with a novel postprocessing stage based on the Segment Anything\\nModel (SAM) [12]. The model is trained solely on healthy brain MRIs to learn'),\n",
       " Document(metadata={'arxiv_id': '2510.15684v1', 'title': 'Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI', 'section': 'body', 'authors': 'Gerard Comas-Quiles, Carles Garcia-Cabrera, Julia Dietlmeier'}, page_content='(MViT-AE) with a novel postprocessing stage based on the Segment Anything\\nModel (SAM) [12]. The model is trained solely on healthy brain MRIs to learn\\npriors of normal anatomy. During inference, reconstruction errors reveal anoma-\\nlous regions such as tumors, which are then refined into segmentation masks\\nusing morphological operations and SAM-based region proposals.\\nAn overview of the proposed pipeline is shown in Figure 1, consisting of\\nthree stages: multimodal fusion and preprocessing, transformer-based image re-\\nconstruction, and SAM-guided postprocessing.\\n1.1 Related Work\\nUAD has emerged as a promising strategy for medical image analysis in data-\\nscarcesettings.Traditionalapproachesrelyonconvolutionalautoencoderstrained\\nto reconstruct healthy anatomical structures, flagging abnormalities via high re-\\nconstruction error [13]. However, these models often suffer from blurry outputs\\nand poor localization.\\nVariational autoencoders (VAEs) introduced probabilistic modeling to cap-'),\n",
       " Document(metadata={'arxiv_id': '2510.15684v1', 'title': 'Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI', 'section': 'body', 'authors': 'Gerard Comas-Quiles, Carles Garcia-Cabrera, Julia Dietlmeier'}, page_content='construction error [13]. However, these models often suffer from blurry outputs\\nand poor localization.\\nVariational autoencoders (VAEs) introduced probabilistic modeling to cap-\\nturelatentuncertainty[14],buttheirreconstructionslackstructuraldetail,limit-\\ning segmentation accuracy. Extensions using perceptual losses, adversarial train-\\ning [15], or memory modules [16] have sought to improve anomaly saliency and\\nboundary sharpness.Towards Label-Free Brain Tumor Segmentation 3\\nFig.1: Overview of the proposed Unsupervised Anomaly Segmentation pipeline,\\ncomprising preprocessing, transformer-based reconstruction, and SAM-guided\\npostprocessing. Multimodal MRI slices are used to learn healthy anatomical\\npriors, and reconstruction deviations are used to infer tumor regions.\\nMore recent UAD research has focused on learning disentangled represen-\\ntations through contrastive techniques [17], enabling separation of normal and'),\n",
       " Document(metadata={'arxiv_id': '2510.15684v1', 'title': 'Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI', 'section': 'body', 'authors': 'Gerard Comas-Quiles, Carles Garcia-Cabrera, Julia Dietlmeier'}, page_content='More recent UAD research has focused on learning disentangled represen-\\ntations through contrastive techniques [17], enabling separation of normal and\\nabnormal features in latent space. These approaches often leverage structural\\nsimilarity (SSIM) or learned feature distances as anomaly scores.\\nDespite these efforts, most prior work uses CNN backbones with limited\\nreceptive fields, hindering global context modeling. Transformer-based architec-\\ntures, such as UNETR [7] and TransUNet [8], have demonstrated success in\\nsupervised segmentation, but remain largely unexplored in the unsupervised\\nsetting.\\n1.2 Contributions\\nThis paper presents the first fully unsupervised segmentation pipeline evaluated\\nin the BraTS-GoAT setting. Our main contributions include:\\nâ€“ Multimodal Vision Transformer Autoencoder (MViT-AE):We in-\\ntroduce a transformer-based autoencoder capable of learning global contex-\\ntual representations from fused multimodal MRI slices. This addresses the'),\n",
       " Document(metadata={'arxiv_id': '2510.15684v1', 'title': 'Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI', 'section': 'body', 'authors': 'Gerard Comas-Quiles, Carles Garcia-Cabrera, Julia Dietlmeier'}, page_content='troduce a transformer-based autoencoder capable of learning global contex-\\ntual representations from fused multimodal MRI slices. This addresses the\\nspatial limitations of prior convolutional UAD models and improves recon-\\nstruction fidelity.\\nâ€“ Novel earlyâ€“late fusion strategy:We stack all four MRI modalities as\\ninput channels and later, after postprocessing, fuse T1c with T2f to exploit\\ntheir complementary information, leading to more consistent and robust fea-\\nture representations.4 G. Comas et al.\\nâ€“ SAM-based postprocessing:We incorporate SAM into the anomaly de-\\ntection pipeline to transform coarse reconstruction error maps into high-\\nquality segmentation masks. While semi-supervised SAM extensions (e.g.,\\nSemiSAM for heart MRI [18]) have been explored, to the best of our knowl-\\nedge, this is the first application of SAM in an unsupervised medical seg-\\nmentation setting.\\n2 Methods\\nAll experiments were conducted on a workstation equipped with an Intel Core'),\n",
       " Document(metadata={'arxiv_id': '2510.15684v1', 'title': 'Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI', 'section': 'body', 'authors': 'Gerard Comas-Quiles, Carles Garcia-Cabrera, Julia Dietlmeier'}, page_content='edge, this is the first application of SAM in an unsupervised medical seg-\\nmentation setting.\\n2 Methods\\nAll experiments were conducted on a workstation equipped with an Intel Core\\ni9-9900K CPU @ 3.80 GHz, 64 GB of RAM, and a NVIDIA GTX 2080 Ti GPU,\\nusing the PyTorch framework.\\n2.1 Data\\nWe used the BraTS-GoAT 2025 dataset, which integrates multiple BraTS chal-\\nlenges to enable evaluation of cross-tumor generalization. The training set in-\\ncludes adult gliomas [19, 20, 21, 22, 23], meningiomas [24], and brain metas-\\ntases[25].Thevalidationsetextendsthiswithadditionalcohorts,namelygliomas\\nfrom Sub-Saharan Africa [26] and pediatric brain tumors [27]. The test set com-\\nposition is not disclosed by the organizers to ensure unbiased evaluation. Each\\ncase consists of four mpMRI modalities (T1c, T1n, T2f, T2w) with expert an-\\nnotations in NIfTI format. The training set consists of 1,352 multiparametric\\nMRI volumes (mpMRI), each containing four modalities (T1c, T1n, T2f, and'),\n",
       " Document(metadata={'arxiv_id': '2510.15684v1', 'title': 'Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI', 'section': 'body', 'authors': 'Gerard Comas-Quiles, Carles Garcia-Cabrera, Julia Dietlmeier'}, page_content='notations in NIfTI format. The training set consists of 1,352 multiparametric\\nMRI volumes (mpMRI), each containing four modalities (T1c, T1n, T2f, and\\nT2w), with corresponding expert annotations in NIfTI format. Since unsuper-\\nvised learning requires training on images without anomalies, each volume was\\nsplit into two pseudo-volumes: one containing slices with anomalies (used for\\ninternal validation) and the other containing only healthy brain slices (used\\nfor training). This resulted in a training set of 98,354 2D images, each of size\\n240Ã—240pixels.\\nPreprocessing is critical for consistent model learning. MRI intensity values\\nvary widely due to acquisition conditions and lack an absolute physical meaning.\\nWe applied z-score normalization on a per-volume basis to standardize intensi-\\nties while preserving the original distribution, ensuring anomalies remain visible\\nduring training.\\n2.2 Model\\nOur model, MViT-AE, is based on a 2D Vision Transformer Autoencoder origi-'),\n",
       " Document(metadata={'arxiv_id': '2510.15684v1', 'title': 'Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI', 'section': 'body', 'authors': 'Gerard Comas-Quiles, Carles Garcia-Cabrera, Julia Dietlmeier'}, page_content='ties while preserving the original distribution, ensuring anomalies remain visible\\nduring training.\\n2.2 Model\\nOur model, MViT-AE, is based on a 2D Vision Transformer Autoencoder origi-\\nnally designed for unsupervised anomaly detection in industrial images [28]. We\\nretain the ViT encoder, adapting it to handle mpMRI inputs, and use a custom\\ndecoder consisting of 6 convolutional layers.\\nAs shown in Figure 2, each modality is split into24Ã—24patches, which\\nare concatenated along the channel dimension, resulting in input vectors of size\\n24Ã—24Ã—4. Each patch is flattened into a 576-dimensional vector and projectedTowards Label-Free Brain Tumor Segmentation 5\\ninto a 512-dimensional embedding. Positional embeddings preserve spatial con-\\ntext. The sequence of patch embeddings passes through a Transformer encoder\\nwith 6 layers, each having 8 attention heads and a feed-forward network with\\n1024 hidden units. The encoder output is compressed via a fully connected fu-'),\n",
       " Document(metadata={'arxiv_id': '2510.15684v1', 'title': 'Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI', 'section': 'body', 'authors': 'Gerard Comas-Quiles, Carles Garcia-Cabrera, Julia Dietlmeier'}, page_content='with 6 layers, each having 8 attention heads and a feed-forward network with\\n1024 hidden units. The encoder output is compressed via a fully connected fu-\\nsion layer (MLP) into a 512-dimensional latent vector. The decoder reshapes\\nthis vector into an(8,8,8)feature map and progressively upsamples spatial di-\\nmensions while reducing channels to reconstruct all four modalities, producing\\noutputs that match the input size. The model contains approximately 40.7 mil-\\nlion parameters.\\nFig.2: Overview of the Multimodal ViT-AE (MViT-AE) architecture. The en-\\ncoder processes multiple input modalities through transformer layers, followed\\nby a fusion layer. The decoder reconstructs each modality, and postprocessing\\nwith fusion masking is used to generate the final segmentation.\\n2.3 Training\\nWe trained the model using the Adam optimizer with a fixed learning rate of\\n0.0001 and a weight decay of 0.0001, batch size of 32, for 100 epochs. We did'),\n",
       " Document(metadata={'arxiv_id': '2510.15684v1', 'title': 'Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI', 'section': 'body', 'authors': 'Gerard Comas-Quiles, Carles Garcia-Cabrera, Julia Dietlmeier'}, page_content='2.3 Training\\nWe trained the model using the Adam optimizer with a fixed learning rate of\\n0.0001 and a weight decay of 0.0001, batch size of 32, for 100 epochs. We did\\nnot use a learning rate scheduler, data augmentation, or early stopping. The\\nmodel checkpoint was saved only when the validation loss improved to keep the\\nbest-performing version. Future work could focus on hyperparameter tuning to\\nenhance results.\\nThe loss function combines Mean Squared Error (MSE) and Structural Sim-\\nilarity Index Measure (SSIM) losses:\\nL=L Rec +Î± LSSIM ,\\nwhereL SSIM = 1âˆ’SSIM(x,Ë†x), andÎ±= 100was chosen empirically to balance\\nthe contributions of both losses effectively.\\nGaussian noise (mean=0, std=0.2) was added during training to improve\\nrobustness and encourage the model to learn stable features by simulating subtle\\nvariations.6 G. Comas et al.\\n2.4 Postprocessing\\nSincethemodelproducesreconstructedimagesratherthandirectsegmentations,\\npostprocessing is essential for accurate tumor delineation.'),\n",
       " Document(metadata={'arxiv_id': '2510.15684v1', 'title': 'Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI', 'section': 'body', 'authors': 'Gerard Comas-Quiles, Carles Garcia-Cabrera, Julia Dietlmeier'}, page_content='variations.6 G. Comas et al.\\n2.4 Postprocessing\\nSincethemodelproducesreconstructedimagesratherthandirectsegmentations,\\npostprocessing is essential for accurate tumor delineation.\\nWe compute residual maps by subtracting the reconstructed images from\\nthe originals, using the signed difference to focus on hyperintense, poorly recon-\\nstructed areas, which reduces noise compared to absolute differences [13].\\nThe postprocessing steps (Figure 3) include:\\n1.Thresholding: We set the threshold to the maximum between 20% of the\\nhighest residual value and a fixed value of 1.2. This balances adaptability\\nand robustness, and was empirically chosen to optimize validation metrics.\\n2.Binarization: Using Otsuâ€™s automatic thresholding method [29], we convert\\nthe thresholded residual map into a binary segmentation mask, separating\\ntumor from non-tumor regions.\\n3.Removal of small objects: To reduce false positives caused by small,\\nisolated noise regions, we apply a morphological opening and closing with'),\n",
       " Document(metadata={'arxiv_id': '2510.15684v1', 'title': 'Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI', 'section': 'body', 'authors': 'Gerard Comas-Quiles, Carles Garcia-Cabrera, Julia Dietlmeier'}, page_content='tumor from non-tumor regions.\\n3.Removal of small objects: To reduce false positives caused by small,\\nisolated noise regions, we apply a morphological opening and closing with\\nstructure element of size 1.\\n4.3D Connected components: Because tumors typically form contiguous\\nstructures in 3D, we identify connected components in the full volume and\\nkeep only the largest one, which is most likely the true tumor, discarding\\nsmaller irrelevant regions.\\n5.Refinement with SAM (Segment Anything Model): To further im-\\nprove segmentation quality, we refine the initial segmentation mask using\\nSAM, a powerful general-purpose segmentation model. SAM is capable of\\nproducing high-quality object masks from various prompts such as points,\\nbounding boxes, or coarse masks.\\nAs shown in Figure 4, applying SAM directly to MRI slices without any guid-\\nance results in segmentations that lack medical relevance and fail to accurately\\ncapture tumor regions. To address this, we use the initial segmentation mask to'),\n",
       " Document(metadata={'arxiv_id': '2510.15684v1', 'title': 'Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI', 'section': 'body', 'authors': 'Gerard Comas-Quiles, Carles Garcia-Cabrera, Julia Dietlmeier'}, page_content='ance results in segmentations that lack medical relevance and fail to accurately\\ncapture tumor regions. To address this, we use the initial segmentation mask to\\ngenerate prompts for SAM:\\nâ€“We compute a tight bounding box enclosing the predicted tumor region.\\nâ€“We randomly sample five foreground points from within this region.\\nâ€“These prompts, consisting of a bounding box and foreground points, are\\nprovided to SAM to guide the segmentation.\\nWe accept SAMâ€™s refined mask only if it achieves a confidence score above\\n90%; otherwise, we retry with new points up to three times before falling back to\\ntheoriginalmask.ThisapproachleveragesSAMâ€™sstrengthstoproducesmoother,\\nmore accurate boundaries and reduce noise artifacts.\\n2.5 Mask Fusing\\nEach MRI modality produces its own reconstruction and residual map. After\\npost-processing each modality independently, T1c-based segmentation identifiesTowards Label-Free Brain Tumor Segmentation 7'),\n",
       " Document(metadata={'arxiv_id': '2510.15684v1', 'title': 'Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI', 'section': 'body', 'authors': 'Gerard Comas-Quiles, Carles Garcia-Cabrera, Julia Dietlmeier'}, page_content='Each MRI modality produces its own reconstruction and residual map. After\\npost-processing each modality independently, T1c-based segmentation identifiesTowards Label-Free Brain Tumor Segmentation 7\\nFig.3: Postprocessing pipeline: The original and reconstructed images are sub-\\ntracted to highlight differences. After thresholding and binarization, small ir-\\nrelevant regions are removed. A 3D connected component algorithm selects the\\nlargest region, which is then refined using the Segment Anything Model (SAM)\\nfor improved segmentation accuracy.\\nthe Enhancing Tumor (ET), while the difference between T2f and T1c segmenta-\\ntions outlines the Surrounding Non-Enhancing FLAIR Hyperintensity (SNFH)\\nregion. Any holes or gaps within the tumor areaâ€”regions not classified as either\\nET or SNFHâ€”are assumed to be Non-Enhancing Tumor (NET). This approach\\nallows us to segment all three main tumor subregions without any manual labels,'),\n",
       " Document(metadata={'arxiv_id': '2510.15684v1', 'title': 'Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI', 'section': 'body', 'authors': 'Gerard Comas-Quiles, Carles Garcia-Cabrera, Julia Dietlmeier'}, page_content='ET or SNFHâ€”are assumed to be Non-Enhancing Tumor (NET). This approach\\nallows us to segment all three main tumor subregions without any manual labels,\\ndemonstrating the power of multimodal fusion. To our knowledge, no previous\\nunsupervised method has achieved this level of subregion segmentation.\\n3 Results\\n3.1 Quantitative Results\\nThe following quantitative results are based on the test set from the BraTS-\\nGoAT 2025 challenge, which includes 451 mpMRI volumes. We processed the\\ncomplete volumes without cropping or patching, but using the same preprocess-\\ning as the training dataset.\\nWe evaluated segmentation performance using a lesion wise Dice Similarity\\nCoefficient (DSC), focusing on five BraTS tumor subregions: ET, NET, SNFH,\\nTumor Core (ET + NET = TC), and Whole Tumor (TC + SNFH = WT). In\\naddition to DSC, we report the Detection Rate (DR), which measures how often\\nthe model detects any part of the tumor. Specifically, DR is computed as the'),\n",
       " Document(metadata={'arxiv_id': '2510.15684v1', 'title': 'Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI', 'section': 'body', 'authors': 'Gerard Comas-Quiles, Carles Garcia-Cabrera, Julia Dietlmeier'}, page_content='addition to DSC, we report the Detection Rate (DR), which measures how often\\nthe model detects any part of the tumor. Specifically, DR is computed as the\\nnumber of cases where the DSC for the WT is greater than zero, divided by the\\ntotal number of test cases.\\nTable 1 shows the metrics for our model, MViT-AE, both with and without\\nSAM refinement.8 G. Comas et al.\\n(a) Original MRI slice\\n (b) Ground truth\\n(c) Initial SAM segmentation\\n (d) Prompt SAM segmentation mask\\nFig.4: Comparison of segmentation stages using the Segment Anything Model\\n(SAM). (a) Original MRI slice. (b) Ground truth mask showing the expert-\\nannotated tumor. (c) Initial SAM segmentation without prompts, showing poor\\naccuracy. (d)RefinedSAMsegmentation using bounding box and point prompts,\\nwith clearer tumor boundaries.\\nTable 1: Lesion wise DSC and DR of MViT-AE With and Without SAM Re-\\nfinement\\nMethod DSC ET DSC NET DSC SNFH DSC TC DSC WT DR %\\nMViT-AE 0.289 0.088 0.473 0.279 0.422 89.4'),\n",
       " Document(metadata={'arxiv_id': '2510.15684v1', 'title': 'Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI', 'section': 'body', 'authors': 'Gerard Comas-Quiles, Carles Garcia-Cabrera, Julia Dietlmeier'}, page_content='Table 1: Lesion wise DSC and DR of MViT-AE With and Without SAM Re-\\nfinement\\nMethod DSC ET DSC NET DSC SNFH DSC TC DSC WT DR %\\nMViT-AE 0.289 0.088 0.473 0.279 0.422 89.4\\nMViT-AE + SAM 0.204 0.082 0.524 0.231 0.371 89.1Towards Label-Free Brain Tumor Segmentation 9\\nOverall, the original MViT-AE model without SAM performed better for\\nmost DSC scores. The only exception was the SNFH, where the SAM-refined\\nmodel scored higher, improving from 0.473 to 0.524.\\nRegarding DR, both models show very similar results: 89.4% for MViT-AE\\nand89.1%forMViT-AE+SAM.Thissimilarityisexpected,astheSAMmodule\\nrefines the segmentation mask generated by MViT-AE, but does not introduce\\nnew detections i.e. it improves the shape or boundaries of existing masks, but\\ncannot recover tumors missed by the initial model.\\nThese results suggest that while SAM may enhance the segmentation quality\\nin certain regions (e.g., SNFH), it does not consistently improve overall perfor-'),\n",
       " Document(metadata={'arxiv_id': '2510.15684v1', 'title': 'Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI', 'section': 'body', 'authors': 'Gerard Comas-Quiles, Carles Garcia-Cabrera, Julia Dietlmeier'}, page_content='These results suggest that while SAM may enhance the segmentation quality\\nin certain regions (e.g., SNFH), it does not consistently improve overall perfor-\\nmance and may even degrade it in more localized tumor subregions.\\n3.2 Qualitative Results\\nIn this section, we present qualitative examples to better understand how the\\nmodel leverages the different MRI modalities for tumor detection, and how the\\nSAM optimization helps refine the segmentation.\\nThe results in Figure 5 highlight the complementary nature of the modalities.\\nIn the column corresponding to the T1c-only model, ET (blue) core is clearly\\nvisible, but SNFH (green) is largely missed. In contrast, the T2f-only model\\ncaptures SNFH region well, but is less effective at identifying the enhancing part\\nof the tumor. When both modalities are combined, the resulting segmentation\\nbenefits from the strengths of each. This fusion results in a more complete and\\naccurate tumor representation.'),\n",
       " Document(metadata={'arxiv_id': '2510.15684v1', 'title': 'Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI', 'section': 'body', 'authors': 'Gerard Comas-Quiles, Carles Garcia-Cabrera, Julia Dietlmeier'}, page_content='of the tumor. When both modalities are combined, the resulting segmentation\\nbenefits from the strengths of each. This fusion results in a more complete and\\naccurate tumor representation.\\nFinally, the last column shows the results after applying SAM for post-\\nprocessing. We observe that the segmentation becomes more anatomically con-\\nsistent, with smoother boundaries and less pixel-level noise. This refinement\\nsignificantly improves the visual quality and realism of the predicted masks.\\nHowever, SAM can occasionally introduce false positives, as seen in the second\\nrow, where a part of the brain is mistakenly segmented as tumor.\\n4 Discussion and Conclusion\\nWe presented an unsupervised brain tumor segmentation pipeline based on\\nMViT-AE, trained exclusively on healthy brain MRI scans. By leveraging re-\\nconstruction errors and refining the outputs with SAM, the method localizes\\ntumors without requiring manual annotations.'),\n",
       " Document(metadata={'arxiv_id': '2510.15684v1', 'title': 'Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI', 'section': 'body', 'authors': 'Gerard Comas-Quiles, Carles Garcia-Cabrera, Julia Dietlmeier'}, page_content='MViT-AE, trained exclusively on healthy brain MRI scans. By leveraging re-\\nconstruction errors and refining the outputs with SAM, the method localizes\\ntumors without requiring manual annotations.\\nOur best model achieved a Detection Rate of 89.4%. Most missed cases in-\\nvolved hypointense tumors in T1c, suggesting a need for improved sensitivity\\nto such patterns. While performance remains below state-of-the-art supervised\\nmethods, results highlight the promise of unsupervised anomaly detection for\\nthis task.\\nA key limitation is the postprocessing step, which retains only the largest 3D\\ncomponent. This restricts outputs to a single segmentation per volume and nega-\\ntively impacts lesion-wise metrics when multiple anomalies are present. Another10 G. Comas et al.\\nFig.5: Qualitative segmentation results for volume00018from the BraTS-MEN\\n2023 dataset. The first three columns show the original input modalities (T1c,\\nT2f) and the ground truth. The following four columns present the outputs of'),\n",
       " Document(metadata={'arxiv_id': '2510.15684v1', 'title': 'Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI', 'section': 'body', 'authors': 'Gerard Comas-Quiles, Carles Garcia-Cabrera, Julia Dietlmeier'}, page_content='2023 dataset. The first three columns show the original input modalities (T1c,\\nT2f) and the ground truth. The following four columns present the outputs of\\ndifferent model branches: MViT-AE with only T1c, only T2f, both combined,\\nand the combination refined using SAM. Each row corresponds to a different\\nslice from the same volume.\\nlimitation arises from the use of SAM: it refines accurate masks by capturing\\nfine details (e.g., in SNFH regions) but can amplify errors when the initial mask\\nis poor (e.g., in ET regions).\\nDespite these challenges, the approach reduces reliance on costly labeled\\ndatasets and shows potential to generalize across tumor types and imaging pro-\\ntocols. Clinically, it could assist radiologists by flagging suspicious regions, espe-\\ncially in data-scarce settings.\\nFuture work will focus on increasing sensitivity to subtle anomalies, integrat-\\ning additional imaging sequences, improving computational efficiency, and ex-'),\n",
       " Document(metadata={'arxiv_id': '2510.15684v1', 'title': 'Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI', 'section': 'body', 'authors': 'Gerard Comas-Quiles, Carles Garcia-Cabrera, Julia Dietlmeier'}, page_content='cially in data-scarce settings.\\nFuture work will focus on increasing sensitivity to subtle anomalies, integrat-\\ning additional imaging sequences, improving computational efficiency, and ex-\\nploringsemi-supervisedfine-tuning.Furthervalidationonhealthycontroldatasets\\nand more interpretable outputs will also be essential for clinical translation.\\nAcknowledgments.We acknowledge the CFIS Mobility Program for the partial\\nfunding of this research work, particularly FundaciÃ³ Privada Mir-Puig, CFIS part-\\nners, and donors of the crowdfunding program. This publication has emanated from\\nresearch conducted with the financial support of Research Ireland under Grant number\\n12/RC/2289_P2.Towards Label-Free Brain Tumor Segmentation 11\\nReferences\\n[1] Young W. Kim and Liem T. Mansfield. â€œFool Me Twice: Delayed Diagnoses\\nin Radiology With Emphasis on Perpetuated Errorsâ€. In:American Jour-\\nnal of Roentgenology202.3 (2014). PMID: 24555582, pp. 465â€“470.doi:10.'),\n",
       " Document(metadata={'arxiv_id': '2510.15684v1', 'title': 'Towards Label-Free Brain Tumor Segmentation: Unsupervised Learning with Multimodal MRI', 'section': 'body', 'authors': 'Gerard Comas-Quiles, Carles Garcia-Cabrera, Julia Dietlmeier'}, page_content='in Radiology With Emphasis on Perpetuated Errorsâ€. In:American Jour-\\nnal of Roentgenology202.3 (2014). PMID: 24555582, pp. 465â€“470.doi:10.\\n2214/AJR.13.11493. eprint:https://doi.org/10.2214/AJR.13.11493.\\nurl:https://doi.org/10.2214/AJR.13.11493.\\n[2] Michael A. Bruno, Eric A. Walker, and Hani H. Abujudeh. â€œUnderstanding\\nand Confronting Our Mistakes: The Epidemiology of Error in Radiology\\nand Strategies for Error Reductionâ€. In:RadioGraphics35.6 (2015). PMID:\\n26466178, pp. 1668â€“1676.doi:10.1148/rg.2015150023. eprint:https:\\n//doi.org/10.1148/rg.2015150023.url:https://doi.org/10.1148/\\nrg.2015150023.\\n[3] Bjoern H Menze, Andras Jakab, et al. â€œThe Multimodal Brain Tumor Im-\\nage Segmentation Benchmark (BRATS)â€. In:IEEE Transactions on Med-\\nical Imaging(2015).\\n[4] Spyridon Bakas et al. â€œAdvancing the Cancer Genome Atlas glioma MRI\\ncollections with expert segmentation labels and radiomic featuresâ€. In:Sci-\\nentific Data(2017).'),\n",
       " ...]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 : Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Creating embeddings...\")\nprint(f\"Chunks to process: {len(all_chunks)}\")\nprint(\"This will take ~2-3 minutes\\n\")\n\nembeddings = OpenAIEmbeddings(\n    model=\"text-embedding-3-small\"\n)\n\nvectorstore = Chroma.from_documents(\n    documents=all_chunks,\n    embedding=embeddings,\n    collection_name=\"litsearch_papers\"\n)\n\nprint(f\"\\nVector store created\")\nprint(f\"  Total chunks indexed: {len(all_chunks)}\")\nprint(f\"  Ready for retrieval\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Test retrieval (sans LLM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def test_retrieval(query, k=5):\n    \"\"\"Test semantic search\"\"\"\n    results = vectorstore.similarity_search_with_score(query, k=k)\n    \n    print(f\"\\nQuery: '{query}'\")\n    print(f\"{'='*60}\\n\")\n    \n    for i, (doc, score) in enumerate(results):\n        print(f\"[{i+1}] Score: {score:.3f}\")\n        print(f\"    ArXiv: {doc.metadata['arxiv_id']}\")\n        print(f\"    Title: {doc.metadata['title'][:50]}...\")\n        print(f\"    Section: {doc.metadata['section']}\")\n        print(f\"    Text: {doc.page_content[:150]}...\")\n        print()\n\ntest_queries = [\n      \"What are the best performing segmentation techniques on BraTS dataset?\",\n      \"What are the main challenges that remain unsolved in brain tumor segmentation?\",\n      \"What are the most commonly used public datasets for brain tumor detection?\"\n]\n\nfor query in test_queries:\n    test_retrieval(query, k=3)\n    print(\"\\n\" + \"â”€\"*60 + \"\\n\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEP 7: Build Complete RAG Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ“ RAG chain ready\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4\", temperature=0)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AI research assistant. Use the context to answer. Cite sources as [arXiv:ID].\\n\\nContext: {context}\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([f\"[arXiv:{doc.metadata['arxiv_id']}]: {doc.page_content}\" for doc in docs])\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"âœ“ RAG chain ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "test_queries = [\n      \"What are the best performing segmentation techniques on BraTS dataset?\",\n      \"What are the main challenges that remain unsolved in brain tumor segmentation?\",\n      \"What are the most commonly used public datasets for brain tumor detection?\"\n]\n\nfor query in test_queries:\n    print(f\"\\n{'='*60}\")\n    print(f\"Q: {query}\")\n    print(f\"{'='*60}\\n\")\n    \n    response = rag_chain.invoke(query)\n    print(response)\n    print()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from langchain_openai import ChatOpenAI\nimport json\n\ndef evaluate_faithfulness(question, answer, sources):\n    \"\"\"Evaluate if answer is faithful to sources using LLM judge\"\"\"\n    \n    context = \"\\n\\n\".join([f\"Source {i+1}: {s.page_content}\" for i, s in enumerate(sources)])\n    \n    prompt = f\"\"\"Rate faithfulness (0.0-1.0):\n\nSources:\n{context}\n\nQuestion: {question}\nAnswer: {answer}\n\nIs every claim supported by sources? Return only a number 0.0-1.0.\"\"\"\n    \n    judge = ChatOpenAI(model=\"gpt-4\", temperature=0)\n    score = judge.invoke(prompt).content.strip()\n    \n    return float(score)\n\n\ntest_queries = [\n      \"What are the best performing segmentation techniques on BraTS dataset?\",\n      \"What are the main challenges that remain unsolved in brain tumor segmentation?\",\n      \"What are the most commonly used public datasets for brain tumor detection?\"\n]\n\nprint(\"FAITHFULNESS EVALUATION\")\nprint(\"=\"*60)\n\nscores = []\nfor query in test_queries:\n    answer = rag_chain.invoke(query)\n    sources = vectorstore.similarity_search(query, k=5)\n    score = evaluate_faithfulness(query, answer, sources)\n    scores.append(score)\n    \n    print(f\"{query[:45]}...\")\n    print(f\"  Faithfulness: {score:.2f}\\n\")\n\nprint(f\"Average Faithfulness: {np.mean(scores):.2f}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}